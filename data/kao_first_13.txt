All right, everyone. Can people hear me use the system? Alright, give me get started for today. Before we begin. The first announcement is to look at where are we gave instructions on how to sign up for my scope, as well as Piazza and we add a colon, the substance gets centralized scheduling. The TAs and I are still figuring out the discussion sections and office hours. ******* balls. We're almost done, but we don't have them right now. So we're going to send a ruined learn announcements when the discussions and office hours are all configured. Said, we anticipate doing that likely this evening, tomorrow. Alright. We uploaded a few supporting files to Bruner. This is a PDF on setting up Python and two books. And then if you wanted to use Google Colab, we also put up a PDF on that. Welcome to use whatever setup in desire. But if you want to know what I personally would prefer, if I was thinking this class, I would personally install Anaconda and effort each assignment I would create a conda virtual environment to do that assignment. All right. Any questions there? I still here but to shattering cell, so if you can hear me, can you just go? Also uploaded the formal notes of his classic examples dating back to 2018. Alright. Any questions on any logistics or anything from the syllabus? I think that needs to be clarified or anything that I prefer. All right. No questions. So we're gonna get back to material. So we were again doing this review of the basics of machine learning just to make sure that we're all on the same page. And last time we talked about this very simple regression problem, where our inputs are going to be the square footage of booms and busts for x and y is gonna be the rents. And we want to train an algorithm that is going to predict the rent y from the square footage. And we said you're going to model this. This, again, with a linear model. The choice of the model is entirely ours. And so we chose to use a model Y equals AX plus B to model this data. Where a and b are the parameters, are the things that I get to set to make this linear model and read the data as well as possible. All right, Any questions there? Okay, so we were then, I've created a second. So actually in our machine learning problem, which is the bind, the loss or the cost function for the problem, right? And be motivated it from if I have the model Y equals b, the y-intercept plus a x, where a is the slope of the line. I could set a to whatever value I want a certain setting up being a gift to be this red line. Another setting of a gives me this purple line. And the red wine is better than the purple line. But they needed to come up with a rigorous mathematical way to say red is better than purple. And after we come up with a rigorous mathematical way or the loss function to quantify how good our model is. Then we can optimize that loss function with respect to our parameters to choose the best or this model. So at the end of last lecture, we said that each of these data points, of which I have, let's say 20 of them will label each of these data points is x and x superscript I. So that's the five data points. I am putting the output. And for the five data points, what I can do is I can measure the error of my model in predicting this particular data point. So if my model is the red line, the error epsilon I, which we'll define as the distance from our prediction to our actual data point will be epsilon equals the actual y value, the true y value from the data, this data point minus the value predicted by my model and red. And that's going to be this here. That's gotten by computing v1 plus A1. Okay? Any questions there? Alright, so I have, let's call this 20 data points. And so I would need to compute Epsilon for all 20th, 13 of points. And then what I could do is I can get a loss function by summing them all up together. So my boss, which is also called the cost function. So we'll say loss, which is equal to a cost, will be the sum of these errors. Alright, so I'm gonna write that as. The sum from I equals one to big N, the number of data points I have, in this case 20. And then it's going to be my actual data point value. That is the y superscript i minus what's predicted by my model, which is this y hat i, which is equal to f of x. So it's gonna be x. That's how I can quantify how good the model is. Alright, there's one thing that's still wrong with this. Can someone tell me what's wrong with this? Great, yeah, so students mentioned, right now, this error is sorry. Alright, so for this boot data points, the error will be positive. But if I were to take the error for this data point down here, this would be negative. And if I have both positive and negative values being summed together, they will cancel out each other, right? But this error in the negative direction is just as meaningful as an error in the positive direction. So I need to make these errors positive. One way I can do that is by taking the absolute value of these. Another way I could do that is just by squaring this, which is what we're going to choose to do, at least to start. So if we were to quantify this loss function, can usually, we will write this as a script L, right? We will see that this L to be a lot lower for the red line and the purple line. Because if we look at these data points, they have a really large epsilon i in magnitude to the purple line, but relatively small. Right? So that's gonna be the mathematical quantity that we will optimize to choose the best values of a and B. By defining this, you can now get to questions. So the student asked, we will oftentimes put a one-half in front. Really, we could put any constant in front, oldest continuous scale the losses so it won't change, but the optimal. Usually people put a one-half here because we're going to see that we have to differentiate this quantity. And when I bring down the two, that'll cancel out this one. Another thing that people will often do, and we probably ask you to do this in the homework, is to normalize this by big N. So it's like a lost e.g. but you can optimize this quantity without this scaling factor that was still get the same optimal and it just scales the value lost. Other questions. Alright, so this loss function is the next important component of the machine learning problem that we will derive later on for neural networks. But it tells us how good our model is. We have to choose this very judiciously. Otherwise, they may be optimizing for something that doesn't actually tell us how good a model is. So in our simple example, we'll have our model y-hat equals a x plus b. We call again that these y's and the x's. These are the output and the input data. These are data that are less. So there are values are known. Then what is unknown parameters? In this case a and B. These are the parameters, which means I get to choose the values of a and B to make the model as good as possible. When I say, I get to choose, I'm not saying like I actually like I am going to set the value of AB that's going to be chosen, or that's going to be set by an optimization process. But these are the free, these are the things that we can to make this model. And so this is going to be determined by making all of our parameters, in this case AMD, as small as possible. Okay? Any questions on that setup? So the question is, could also define a loss where the error is the projection onto this red line. And you could do that, you would just have to thank quantify what these projection distances are and then sum them. Or some distributors. Oh, okay, the question is, how do you know which loss function is a better one to use? So this is a bit more nuanced when we get to neural networks, we both talked about the different types of loss functions that you could choose and how some of them are better choices for searching the hyper parameters of our neural network. So I'm going to table that for when we get to neural network loss functions. But if it's still there, please pick it up again. Yeah, let us follow up on that. Okay. So I'm gonna do one more thing, which is I'm going to rewrite this AX plus B as a simple dot product. The way then once you do this is the machine learning using the parameters of your model are going to be denoted, denoted by vectors theta. So theta is going to contain my parameters, which are a and B. And to make this equation equal AX plus B, what I'm going to do is I'm going to define an x hat. And x hat is going to equal our original x and one. Alright? So when I do Theta transpose x hat, I'm going to do a x plus b, right? Question, sir. Alright, so then with our loss function that we question. Okay, yeah, so tomboy and remain are saying that if we were to take a loss function that's the perpendicular distance, it would end up being the same. Alright? So for this loss function, I'm going to now take my wife hat for the i'th example. I replace that by theta transpose hat. Sorry. Alright. So this again tells me how good my model is. And you'll notice that now the loss function is going to be a function of my parameters theta. And I again get to choose or I get to optimize the values of beta AND to make this loss as small as possible. And what's possible. That means that all of these errors are minimized. Then, now comes the question, how do I choose the Theta to minimize this elevator? And the way that we do this as w constructed as an optimization problem. So our goal is to choose the parameters, the values of beta, to make a boss, I love Beta as small as possible. Okay, so the diagnostic have in mind when we get to this, when we think of this optimization problem, is that on the x-axis, we have our choice of theta or parameters. On the y-axis is our loss function. I love data. And as I change the values of a and B, I'm going to move where my line is, and that's going to lead to a different boss. And so L of theta is going to look something like this. For this particular problem that we're doing. L of theta is going to be convex with respect to beta, which means that we're going to have one global optimum. And if that is the case, how do we find what the minimum value or how do we find the value of Theta that minimizes this convex loss function L of Theta. Perfect, Yeah, so in the case where there's just one global minimum, we can use our result from calculus that at the global minimum, the derivative is equal to zero. And so what we can do is we can go ahead and calculate the derivative of the loss with respect to Theta and set that equal to zero. When we solve for that data, that'll tell me the optimal setting up data that minimizes loss. Alright? Any questions there? Yeah. The question is, how do we know that the loss function is convex or not with respect to the parameters. So there's going to be an entire class called convex optimization that we'll go over the definition of this. For this class. I'm not going to go into the details of that would be beyond the scope because actually this is just a very simple contrived example. This actually motivates me to say something else, which is that in general, for neural networks and optimizing them, the loss function will never look. Convex, is going to have many local minima, like crazy loss function electron here. And even for these examples though, we're going to learn how to optimize these with an algorithm called gradient descent. And for those you still need to know DLD theta, which tells you the direction that you stepped in to make this possible small as possible. Alright? So we are going to go ahead and differentiate this with respect to theta. And then for this simple problem, we'll take the derivative and set it equal to zero. For later on, problems are going to know what the gradient is and use that to, to optimize the plastic region. Right? So this gets us then to vector and matrix derivatives. So in machine learning, we are often going to have to take derivatives of quantities with respect to vectors and matrices. And these are typically called gradients. All right, we're going to discuss three of these today. The derivative of a scalar with respect to a vector, derivative of a vector with respect to a vector and just derivative of a scalar with respect to a matrix. In general, if a variable is lowercase italicized is going to be a scalar. This lowercase bold, there's gonna be a vector. Then if it's uppercase fold is going to be a matrix in this class. So if I want to know the gradient or the derivative of a scalar or little y with respect to a vector x. I can denote it in two ways. I might write DY DX. That's pretty common notation from capitalists. Or we might also use this nabla operator. This means I'm taking the gradient or the derivative of a scalar y with respect to the vector x, right? So we're going to make a few definitions. The first will be the gradient of a scalar y with respect to a vector x. So first off, if y is a scalar and x is a vector, the vector x is an n-dimensional vector. But wants to do is we're going to define the gradient DY, DX or this other nabla notation that we used to also be an n-dimensional vector. And the elements of that vector are as follows. The first element is d y with respect to dx one where X1 is the first element of the vector x and d y, d x2, or x2 is a second element of the vector x, etc. Any questions on that definition? Alright, so x is a vector, derivative of a scalar with respect to a vector is a vector of the same size as x. And these are the elements of that texture. So if I go ahead and I tell you e.g. so let me first write out that x is going to be this vector elements X1, X2, all the way down to Zen. I tell you DY, DX is equal to 10.5 000, et cetera. This DY DX, this gradient tells me that d, dy dx1 equals one and d y dx2, it was 0.5. Then we know that in calculus, basically the delta y change in y, change in x is approximated by the derivative. So if I go ahead and I wiggle x one, I know that that's going to lead to a wiggling in Y as well. And that we're going in y delta y due to changing at x1 is approximately equal to d-y, d-x One times an x y. So if I wiggle X1 by 0.01, I would expect Y to wiggle by 0.01 as well. Given that d-y, d-x is equal to one, I wiggled by 0.01, I would expect y to be equal by 0.00. Any questions here? The question is, will there be cases where we cannot take the derivative of the loss function with respect to the parameters in this class. So when we build neural networks, will be careful to use operations that we can always differentiate. And then there will be operations that are strictly not differentiable. What did they have like a discontinuity. But what we'll do is formally, we're going to take the gradient of those informally in this class, but they do have e.g. a. Function. The absolute value. When x equals zero, it is derivative is undefined or equal to zero. If you don't follow that, no worries, we can get some more detail when we get to ingredient to second, what does it mean for the operation to? What does it mean for something to be differentiable? Here I'm saying the derivative exists. And so like for the absolute value, the limits become defined when I have the two lines intersecting. Sorry, can you repeat that? Operations are analogous to pump in this case. Yeah. Good rocks with the setting for it to be differentiable, the left limit and the right equal to each other. So this is recapping what we just said. A vector, the gradient of a scalar with respect to a vector. It's gonna be a vector that has the same size as what we're differentiating with respect to each dimension of the gradient tells us how a small change in x and that dimension is going to be to a small change in y. Okay? So now let's say that I were to give you a vector. Let me give you, let me say that I give you a vector x. Maybe this vector x is equal to 0.050, 0.010, 0.02, et cetera. And I want to know if I wiggled x by delta x is small vector. How much does, why change? And in this case, I know what the gradient of y with respect to taxes. Okay? Raise their hand and tell me how much this delta y change, or how much does y change given that I know the gradient and I know what Delta x is. Great. The students says he just talked out with delta x. That is correct. So the change in y, right, is going to be approximately equal to the sum from I equals one to big, to little n, where n is the dimension of X. And then it's going to be DY dxi times delta x. So basically the change in y is going to be how the changes in x and every single dimension modify, live all of those together. And we can see is that this is a dot product of the gradient where our change in x. Any questions there? The question is when I write approximately here, is it because this mathematical relationship is approximate? The answer is yes. So this approximation becomes more and more true as delta x goes closer and closer to zero. But when Delta x I is large, it's going to be a linear approximation of how much the change. Other questions. Alright, so we're gonna go ahead and take our first gradient. So let's say that f of x equals theta transpose x wasn't know the gradients of f of x with respect to x. And here I'm just going to write that y equals f of x. So I'll use my Excellent Boy notation. Alright, so first, we should always check dimensionality, so make sure things make sense. So let's say that x is a vector that is little n dimensional, right? Since I'm dotting it with Theta, Theta also has to be an n-dimensional vector. And I know that if they've got an extra n-dimensional vectors and the dot product is just going to be a scalar. So y is going to be a scalar in R. And therefore, by gradient of y with respect to x should also be a vector in R n by definition. Any questions? Alright, so let's go ahead and take this gradient. So y is equal to Theta transpose x. So I'm just gonna write out this dot product as Beta one x1 plus beta two x2, all the way to Theta and x, right? In this case, then the gradient of y with respect to x is going to be. Dy, dx1, dx2, all the way down to d y, d x. And this is going to equal. So for the first one, right? I take the derivative of this quantity with respect to X1. X1 only appears in one place. That's this expression and it will apply theta one. So if I differentiate with respect to x one, I just get beta one. Similarly for theta-2 all the way down to theta N. So I can say that the gradients of my function f of x with respect to x is just theta. Data is a vector in RN. So dimensionally, things still work out because my gradient, I expected to also be a vector. Any questions there? All right, that was hopefully the ones. We're going to just do, a bit more of a complex one. So what we're going to do is we're going to take the derivative of a vector. Sorry. Never mind. This is still a derivative of a scalar with respect to a vector, but it'll be a bit more mortality. So here we're going to have f of x equals x transpose a times x. We want to know what does the gradient of f of x with respect to x. Alright? So here again, x is going to be a vector in R n. X is a vector or n, then x transpose is n by one, sorry, is one by n. And x is n by one. Which means that a has to be an n by n matrix for everything to work on. So a is going to be an n by n matrix. Then we know that a transpose a x is just going to be a scalar. Therefore, we also expect that the gradient of effects with respect to x, It's gonna be a vector with the same side as x, will be a gradient, which will be a vector. And any questions there on the dimensions also track something I said earlier, I said today we're talking about the derivative with respect to a Dr. and that isn't sure if we'll get to that later on. That's called the Jacobian. But today we're just going to do derivatives of scalars with respect to a vector or matrix. So this matrix a is going to be m by n, and its elements are going to be A11. All the way up to A1, just gonna be a, N1, going to be a. So with this, let's go ahead and try to compute the gradient. To compute the gradient, I will need to compute how y changes, or I'll need to compute the derivative of y with respect to all the elements of x, x one to x. Then I have my X equals X transpose a X. Someone to tell me what the first step trying to get this gradient would be. You're confronted with this problem. Perfect. Yeah, We should do something. That's correct. We should do something similar to what we did. The last problem, which is to expand things out in terms of our X1, X2 so that we can differentiate this expression with respect to the x ones, the size in general. So we change the color back to black. We know that x transpose a x can be written as double sum over I and J AIJ. That's just the definition of matrix vector multiplication applied to this quantity here. Given by not have this, some are constructed, try to compute the elements of the gradients. So why don't we start off by trying to compute the derivative with respect to x one. Alright, so I'm gonna try to compute d f of x dx one. When I do that, I'm going to go ahead and I'm going to expand this expression. Simple quite a bit. Well, note two things in this expression, right? All right. Is Iterating from one to n and j is iterating from one to n. But what I wanted to take the derivative with respect to x one, all I care about are terms that have x in them, right? So one term where X1 is when I have I equals one and j equals one, right? So if I equals one and j equals one, then this summation will have a term that is equal to a 11 times x one squared. Because again, i and j are both equal to one. Any questions there? So when I differentiate this with respect to x one, this is going to give me a term to A11 times X4. And so that's going to be one element that comes out of the derivative. So I'm going to have a two A11 times x for now. And let's just consider the next set of terms that has an a one. So I can consider an excellent, I mean, when I equals one and j does not equal one. So this summation I equals one, this term will be an X1. This will be an a one j times x j. Here j is not equal to one. Alright? So what this is going to give me is this is going to give me a summation from j equals two to n, where I have a one j times x, one times x j. All that's doing is taking this term, let me rewrite this term really quickly so that we have it as ai j times x times x j. These are all the terms where i equals one. Mj does not have any questions there. Alright? And then if I differentiate this with respect to x one, so if I go ahead and do derivative of this whole thing with respect to x one, that is going to just give me the X1 will be differentiated away and I'm just gonna be left with the summation from j equals two to n of A1 j times x j. So I'm going to write that term over here in purple would be the sum from j equals two to n a j times x j. Similarly, they're going to be terms where I does not equal one, but j equals one. And that's going to give me a summation over I equals two to n a i one times x times x one. To get this expression, I'm accepting at the terms with x one in them when j equals one. But I just started. Now if I were to differentiate this term with respect to x one, I would get a plus sum from I equals two to n a i one times x. Any questions there? Yes, thank you. I from two to n. Any questions? Just asking you, this is just a scalar. The answer is yes. So f of x is a scalar, x one is a scalar, so the derivative will be a scalar. And all these terms are also just scalars. So now I'm going to simplify these expressions. You can see that these sums are from j equals two to n and from I equals two to n. And then I have, if this works to b from j equals one to n, right? I would have a term, A11 times X1. That term A11 times X1 is here in pink, and I have two of them. So I can put one of these A11 X1 into this sum, making this song. Sum from j equals one to n of A1 j times x j. I can put the other A11 X1 into this summation, which would make this plus the sum from I equals one to n of I one times x by simplification, by consolidating terms. Any questions there? Alright, so we're gonna continue on. We have that. I'm going to copy and paste this onto the next page. We have that D f of x with respect to X1 is equal to this quantity. The quantity is in terms of matrix silence in factors. So if I have my matrix a and its A11, A12, all the way up to A11, A21, A22 to n a n, a n and my matrix a. I were to take my vector x, which is X1, X2, all the way down to x. Then what I can see is for the purple some. What we're doing is we're taking a one j for j equals one to n. So we're taking this first row of the matrix a, and we're taking this dot product with the vector, okay? And so one way that I can write this term in purple is to just say that what the term purple is is if I do a matrix vector, multiply a times x, I take the very first element of that resultant vector. So this is equal to the matrix multiply a times x, taking the first element. Similarly, for the green term, we have that is a I1 from I equals one to n times x psi. Which means that we're going to be taking this entire column here in green and dawdling it with the vector. And this can be written as the first element of the matrix vector multiply a transpose Ax. Okay? So here what I'm doing is I'm taking our summation notation and converting it back into just vector and matrix notation. Questions here. Alright, so then I'm not going to do this, but you're going to find is that if you were to do this, we did this for X1. But D f of x dx and dy f of x dx, three, et cetera, all follow the same pattern. So if I were to take d f of x dx, the result would be this. But now, so the first element, the second element is, alright, so in total, if I wanted to write my gradient, which would be d f of x dx one, D f of x dx two, all the way down to D f of x dx. And this is going to equal. So the first term, VFX dx1, we got it already would be a x one transpose x one brace there. So I have a bit more space. And similarly for the second element, it would be a two plus a transpose x two, all the way down to a x n plus h transpose x M. Alright? And then what that tells me then is the overall gradient f of x with respect to x. Remember this function was x transpose a x. This is equal to a X plus a transpose x. Right? Any questions there? You raise your hand if you're following. Okay, that's everyone or almost everyone. Great. So I'm gonna do a few laptops. I'm gonna write this as equal to a plus a transpose x. And there's one more simplification case. And you can make if a is symmetric, then this simplifies to two a x. Alright? Whenever we can answer like this, we should do a few sanity checks. Especially if you're doing it for the first time. The first is again dimensionally, does it make sense? So we said that the gradient scalar with respect to a vector that is of size n should be a vector of size, right? This is a vector of size n. You betcha, because a is n by n and x is n by one, right? There's another sanity check that you can often run when you're doing matrices and vectors. Which is to consider what happens when the dimensionality little n reduces toward the scalar case. Alright? So can someone tell me if this is true in the scalar case and if so, why? Perfect, Good. Yeah. So Blake says, when n equals one, our original function was x transpose a x. But when n equals one X matrix big aid, they're both scalars, right? So when n equals one, then f of x equals x times a times x, which is just a times x squared. And if I differentiate a times x squared and D f of x, dx is equal to two a x, right? And then that matches what this gradient is. Alright? Any questions? Alright, so if you look at the formal notice that we posted on ruler, they will have the LaTex version written up of how we calculated this gradient. And so this is just a recapitulation of that will define one more thing, which is a matrix derivative. So let's say we're going to find here the derivative of a scalar with respect to a matrix. We're going to say that some function y is equal to z transpose a x, e.g. and in this example, let's say that Z is a vector that is n dimensional. X is a vector that is n-dimensional. I said the major dimensions work must be a matrix that is n by n dimensional. Then we define the gradient of y with respect to a in the following way. D y, d big a, which can also be written with our nabla notation, is equal to a matrix that is the same size as a. And its elements are d y, d A11, A12, Dui, dA1. And this will be d y, d a d y with respect to d, Then the element. So basically it's analogous to the vector case, right? The first element, the one-one element here tells me how wiggling a 11, the first row, first column entry of a, how are we going? This changes? Why? Then in general, be wiggling? If I wiggle the AIGA term, a, d y, d a I, j will tell me how much I expect. Okay? So if a is a matrix that is m by n, then the gradient of y with respect to a will also be a matrix that is n by n, where every single element of the matrix again tells me how much I wiggled up elements of a. How much will widen. Any questions here. Okay, so the next thing I'm going to say is really important, because I already know that even though I'm saying it here, students are going to be asking this on Piazza and what they do, whatever they could respond if you're paying attention right now. Which is that this thing here is called denominator layer. In denominator, the x is a vector that is n by one. Then the gradient of a scalar with respect to x is also a vector that's n by one. And if a is m by n, and the gradient of y with respect to a is also a matrix that is m by n. When we do questions, written questions in this class, we are always going to follow these conventions of denominator layer. There's another layer that is commonly used called numerator layout. And numerator layout is simply the transpose is of the denominator value definitions. So enumerator layout, the gradient of y with respect to x. Instead of being a Dr. the same size as x column vector is going to be a row vector. It's gonna be this transfer. So enumerator layout. The gradient is going to be a row vector which is R1 by n. And the gradient of y with respect to a is gonna be this matrix transpose. So it's going to be a matrix that is R. And by both conventions can be used. But basically in a class where we give out questions, we have to choose one and stick with it. So in this class we're going to choose denominator Leah, and stick with it. A lot of other classes may use numerator layout and there are good reasons to use numerator layout. So enumerator layout, the Jacobian, which is the derivative of a vector with respect to a vector, I will not have an extra transpose symbol. And when we derived the chain rule, the numerator that is going to read from left to right versus in denominator Leah, and reads from right to left. Those are easy to choose. The Amrita that the reason I liked denominator layout is because I know that the gradients are going to be the same size as what we're differentiating with respect to. So basically, moving forward in this class, we're going to use the denominator lab. And when you do, right or when you look up things on Google gradients, you may see everything transpose. And that's because whatever we are looking at would have been using numerator. They are. Right. Any questions here? I'm sorry, can you repeat the question? Yes. I got the student is asking is the numerator layout just the transpose of the gradients and the denominator. The answer is yes. Any other questions? All right. Let's take a five-minute break and when we come back, we're going to use this to solve our pharmacy money problem. This isn't like a silly question, but I suppose we find that it follows rotate and became, Yeah, so can you make a machine on the send button and mathematically then yeah, can you just exactly like yeah, that's not a silly question at all. So that is a critical thing for how these neural networks for the container. So in week seven or eight of class, we're going to show you my torch and TensorFlow, exactly working that fourth named Steve works by composing gradients. So basically for every operation by a process of multiplications, et cetera, tensorflow or Pi-fourths, will know what the derivative of that is along the computer to give it a programs might be, could be. Does that make decent every woman, I didn't just make healthy thinking. And you don't have to calculate radius or is this term AI can be trained to do that. But I don't know, I don't know if something back to that, but I don't think it would be I think would be possible to train. Would be classified as yeah. Yeah. You mentioned that you showed the derivation. So I was wondering for you to post your right out of the class. Yes, those will all be posted here. Great, Awesome. Either way, I'm asking you to do fast at it. From my advice there. I just thought that this week. Watch afterwards, I'll walk into your office. Yeah. Yeah. Yeah. Let me check. Oh, yeah. Exactly. Of course. If it's turned on because ultimately I will match the names to what's on the right. The last thing for the project. All right, everyone will get back to you. There are a few logistics questions during the break. All right. The first question was if these annotated notes, the notes that I'm making right now on despite what I'm ready done, will be posted to Boomer and the answer is yes. Everything that I write down here will be posted to grow and learn. And then there was a question about the final exam for this class. If there's no final exam for this class, there'll be a final project that will be due Monday of finals week. All right? Okay. Any questions on any of the vector or matrix derivatives we talked about? The question is, can I explain the difference between the denominator and numerator of my app? Yes. So the only difference is that they're transpose. And so in the numerator layout, let me just add a slide here. In numerator layout, I would define d y, d k. So in denominator layout, this was d y, d A11, A12. Numerator layout. Everything would be transposed. So numerator layout would find this to be d y, d A11, A12 down to d y, d a V1 n, right? So this row will now become a column. Then this element would be d y, d a two-one, cetera. So it's just the definition of the denominator layout matrix. The question is, why would you use one over the other? Just as a matter of whether these two are what's convenient, mathematically correct. You just have to remember that they will have the chain rule written either left to right or right to left. There will be less transpose is it gives the numerator lab, which is why people prefer, especially for which we will discuss later on in this class. So we're going to be then solve our simple supervisor an example. Remember the entire goal of these gradients so that I can differentiate a loss function with respect to my parameters Theta, right? So let's go ahead and do that. We have our loss function. Our loss function is going to be a function of our parameters theta. And recall that in our example data where the values of a and B that we get to optimize for to make the model is fit as possible to reduce the loss as much as possible. And I remember that this x hat and x hat was our input square footage and the value one. So what we're gonna do is we're going to extend this expression and then we're going to differentiate with respect to Theta. All right? So then after that we're going to set the derivative equal to zero. So the first thing I'm going to do this, I'm going to write out the squared as a quantity transpose itself. So this is equal to one-half sum from I equals one to big N y i minus Theta transpose x hat by this whole thing, transpose itself. Alright, and that's just running out. The squared. Remember these superscript eyes refer to data points, right? Examples. So each of these would be a different example. So this is summing across all of my data points. That's what this summation. Alright, I'm gonna make this question. Great. Yeah, the question is, why is there this is 1/2. It doesn't have to be there. Because when we differentiate, this two is going to come down and cancel it out. So it comes out to be nicer. You don't have to write the two everywhere. The question is, why do I need to write a transpose here? If this thing is a scalar, you are right, that we don't have to write the chance because, because the transpose of a scalar is just itself. However, we're going to do something called vectorization, but we're going to write this in terms of vectors and matrices without a summation so that we can easily differentiate it. So it's on the way towards that factorization, which we'll see in two steps. All right, so the next thing I'm going to do is I'm going to write this as one-half sum from I equals one to big N. And I'm going to do something. But I'm going to do is I'm gonna write this as y minus. I'm going to make this instead of Theta transpose x hat, I'm going to make it x hat transpose theta. So what I've done is I've just taken this expression here and I flipped the order of the accident that data. Why am I allowed to do that? Yeah, I agree. They're the same thing. And so this is actually related to the question that was just asked. Theta transpose x i is a scalar, right? It's a number. Let's say that the number was fine, okay, if I transpose phi is still equal to five. So you can always take the transpose of a scalar and it will equal itself, right? To go from here to here, I use this rule from linear algebra where if we have a, B, C transpose, this is equal to c transpose, B transpose a transpose. So when I take the transpose of theta transpose x, i, ai transpose each individual element and I took their orders, that's going to give me x transpose times theta. Any questions? Alright, Now this is a key step called vectorization and you're going to do this on homework number two, when you implement an algorithm called K-nearest neighbors. So this is called vectorization. Basically in vectorization, if we were to write this in Python, right? Would be that we could write it is with a for loop. But we know that for loops are relatively slow. And so there's actually a way to speed up computation by running. This is just a matrix vector operation. We know that a song can be written as a dot product of two vectors. So what I want to do is I'm going to write this sum as a dot-product by in the following way. So what I'm going to do is I'm gonna write this as one-half. And then I'm going to take my white eyes from I equals one to big N for my big and examples and stack them in a vector. So this is going. 
Okay, I'll record the second half of this. Makes sense that once you cross connection, okay, I'll record the second part of this. Thanks Jeremy. Alright. So to turn this into a summation, I will transpose this quantity with itself. I'm going to share a bit in writing and just copy and paste. This thing I wrote, copy and paste. So if I take this thing transpose, but the cell is going to implement this summation. Alright? We're going to check the dimensionality. So I'm gonna call this vector of the water wants to buy anyways. I'm going to call this quantity a big matrix, Y. And Y we can see is a big m-dimensional vector. We'll call this thing here big X. Someone told me what the dimensionality of the axis. Once students have any other takers, heard, someone say n by two. Yes. So remember that because I'm one step here, that each x i is a two by one vector. When I transpose it, it becomes a one-by-two. And I start to begin again. So this is in our big N by two. And then I theta isn't art here. So this big X times beta will indeed gives you just an m-dimensional vector back. Each element of an n-dimensional vector is one of these terms. And so if i.it with itself that implements this entire nation, any questions on that vectorization? Alright, so this will be like playing around with something in a vector or a matrix for that removes the formative and it'll automatically scale computation. You'll do this in homework number two. Alright, so this is going to be equal to one-half. And I'm going to write this as big y minus x Theta transpose y minus x Theta. Lastly, I could just do oil to write out all these terms. So this is equal to one-half, will have a Y, Y, Y transpose Y minus Y transpose X Theta minus Theta transpose X transpose y minus Theta transpose X transpose X Theta. Doing like foil for these two expressions. Sorry. Thank you. Questions here. And just say, thank you for the questions ahead of time. I definitely will make mistakes while I'm lecturing up here. So if you ever see some mistakes that I make, feel free to just call it an ***. Alright, so what I'm going to do is I'm going to take this expression. Remember that this is 0. And you can see here I combined these two inner terms into just one to y transpose X Theta, right? The reason that I can do that is again, because these expressions are scalars and do the transpose of each other so I can combine them. The same exact logic that took us from here. Alright, so this thing is equal to ls data. Tomboy is referring to this expression here. Yeah, tomboy is pointing out one way to write this notationally is as follows. The two norm of y minus x Theta squared. And Tom, I also raised something which is, this is called the squares. And probably many of you have seen this example so far. And I know that this will give us the least square solution. Thank you, a tomboy. Alright, so we're going to differentiate this guy with respect to Theta. So I'm going to compute DL. D Theta. And to do this, you're going to use the matrix and vector derivatives that we just derived, actually just a vector of derivatives. So if I have d theta transpose theta d theta, we know from the prior slide that this is equal to a plus a transpose theta. Alright? And then I'm going to write this in terms. If we have a w equals z transpose Beta, then dw d theta is going to equal z. I'm going to differentiate with respect to Theta. I'm going to have my one-half to turn come out. Y transpose Y has no data in it, so gradient with respect to theta is equal to zero. All right, For this next term, I have a two, y transpose X times Theta. And y transpose we know is going to be a one by big N vector. X is going to be an n by two matrix. And so overall, y transpose x, which I'm going to call z transpose, is going to be a one by two. I'm going to call this thing z transpose. So this is gonna be a two times z transpose Theta. Gradient of z transpose Theta with respect to theta is just z. So the gradient of this term with respect to Theta, it's going to just be z and z is x transpose y. Me questions there. Okay, and then the last question I had is this Beta transpose X transpose X Theta X transpose X. I'm going to call a matrix a. And I know the derivative, the gradient of theta transpose theta with respect to Theta is gonna be a plus a transpose Theta. So this is going to equal X transpose X plus X transpose X times questions, question, what is the gradient of w with respect to z? Oh, so the question is, why is this gradient to z and z transpose? Because we're using denominator over. Yeah, so here theta would be a vector that is in R2. So this should also be a vector in R2. It could be Z transpose if you're using numerical that. But in the past people used denominator, they will also still BC. So for that, just play around with and then use the definition of the gradient. Yeah, it's, it's something that we actually derived an order. It doesn't conference but because we derived theta transpose x instead of x transpose beta, but using the same exact logic you'll, you'll get because in denominator. Alright, so I'm going to continue simplifying. This is equal to minus X transpose y because the 2's cancel out. Then here I have Q X transpose X is, but then that two will cancel out with this one-half b plus X transpose X times beta. Then to get the optimal Theta, I, set this bin to be equal to zero. So now if I set this thing equal to zero, I'm going to give me the equation, X transpose Y equals X transpose X Theta. As long as X transpose X is invertible. This gives me my solution, theta equals X transpose X inverse times X transpose y. Any questions? Alright, so if you've had linear algebra and machine learning before, right? This is something that you'd like to you already know. We went through this example. So again highlight portions of machine learning problem, which is that we need to first define a model. In this case, linear model means to find an extra or loss function that tells us how good or bad. That's my Theta. And then I'm going to need to know how to differentiate L of theta with respect to Theta. So that's something that we've done here. And in this case, we can solve directly for theta. The more general case where there are many minima and maxima, you will never be able to solve for theta because you won't be able to write down a closed form analytical solution. Hover, will use gradient descent when we aren't those cases. Questions. Next question is, is it because of denominator layout that X transpose X is a matrix instead of a scaler. Note in both numerator and denominator, we have x transpose x will be a matrix. They'll just be the transpose of the chapter. Did that answer your question? Yeah. The question is, what is X transpose X a dot product with itself? It is, but actually call is an n by two matrix. So it's a two by n times n by two. I think it's a two-by-two. Oh great. Roxanne is asking here. We stepped it up in terms of rows, can be stacked them in terms of the columns. For x. Change data for the other side. Yeah. As long as everything works dimensionally and you haven't orders partner, you just stack them however you want. Question is, can I remind you what x is an n by two matrix? Yes, because remember this X hat is r square footage x and a value one. So it's a two by one vector. And I stacking up and have them. So each of these is a two dimensional row vector and I'm Stephanie. The question is, can I stay wide? X transpose X is invertible. X transpose X may not always be invertible, but you will see that x, right? In this example is X transpose X could be a two-by-two matrix, right? So it will only be non-invertible if the rank is foreigners thereof. But when we do x transpose x, we have big N examples. And as long as big N is larger than two, usually we only multiply them together. It would be high probability. There'll be likely that it'll be full. If you have many more dimensions of this too became ten and you only had two good examples like big N equals two, then it wouldn't be invertible. Other questions, actually, a transpose X, transpose X. And the reason that is is because if a, I'm just gonna write this as b, c, right? If I do a transpose Ax equal to c transpose B transpose. So if I transpose this quantity, I take this thing and transpose it and put it in front. That gives me the first X transpose. And I think take this x transpose, a transpose if I just put it to the bathroom, That's just one Other questions. Alright? So this solution is called the squares. It appears in a variety of linear application, but we're also going to see in just a few slides have non-linear polynomial models. Okay? So this is code and kept up the code from what I prefer the password was in, I believe 20 2017. So some of the syntax is a bit outdated, like Python, you can use an app symbol for doing matrix vector. Matrix multiplies as opposed to dark tones. But in this code, which you'll do something similar in your homework, why is gonna be a big n-dimensional array of rents? And x is going to be a big N-dimensional array of square footage is. And then this code is going to implement the least squares solution that we saw on that we just derived in part with the model isn't that model will be this green line. Alright? Alright. See, you might look at this thing. Can't we do better, right? We talked about before how they could make e.g. polynomial model that maybe does a better job of going through these data points. Alright, so here's a question for you all to get on, and I'll ask them to answer the question. The question is, how does our current least squares formula allow for learning non-linear polynomial models? Like, let's say I wanted to be an nth order polynomial instead of just the law. Alright? So I'm telling you that what we derived already can do polynomial. And I want something to think about how that's possible for their current machinery. I see some hands going up already on here, w1, like 20 s to think about this and then I'll call someone to answer it. Received one. All right. Someone raise your habits. How many? Polynomial regression? Yeah. Perfect. Yeah, so this student says, what I can do is I can make a new attack feature right? Before, it was just x and one. But now I can make this be, let's say I wanted a third order polynomial fit, right? I can make it one, x squared and x cubed. If I do that, then let me write it more generally here. What I can do is I can define X hat to be one, x squared all the way to XN. If I define theta to be B, then what multiplies x one is a one multiplies x squared is A2. So it'd be, be A1, A2, all the way down to AN. Then I've written this wide as a theta transpose x, where I get to choose the values of B, A1A2 am, that are the coefficients of my polynomial powers of x. Any questions here? So this is one way in which you can make a nonlinear model, which is that you choose what the nonlinear features are there ever choosing that are nominator features are squared all the way up to x n. And these are their coefficients. Alright? And using the exact same these squares formulation we derived, we derived how to minimize that loss function, the squared loss respect to theta of this form. You can now fit polynomials. So all of these data points. Oh, thank you. Yeah, the student raises that I missed my factor. Other questions. The question is just num pi used denominator or numerator. So NumPy is a collection of like a Foxconn's operations. But you can keep track of whether like how you define the gradient with num pi citation. It's up to you. And so you define the derivative of a scalar with respect the conductivity, conductivity denominator reaction. And the rest would even later. So don't fight doesn't have a preference if it's just a way for you to do these compensations. The question is, what is linear regression? So this solution here that we derived for the data is the least squares solution. But then a linear regression or linear model, or an affine model would be y equals b plus a one, x one. So the linear part refers to the model. This gear is not a linear model because it has non-linear functions of x. The question is Theta transpose x that we derived? We can generalize the higher parts of that. Yes, and that's what we're doing here. Oh, the question is, what if x were apertures? Does this still work out? The answer is yes. Oh, yeah. Error on despite here, there should not be a one here. Alright, so use this to polynomial functions of x to your data. And I have now a question on the slide. A higher degree polynomial will always get the provided data as well as the lower order polynomial. What I'm saying is that if you were to measure the error from these blue points to your model, prediction, error will always be higher. If I make my polynomial higher order or not better, it will always be worse than it will always perform as well as someone told me why this is true. Perfect gas or the student says, in the worst case, you could always set the coefficients of the higher-order polynomial term Cicero and model lower one. And that's exactly right. So if I had the model, let's say I had a third order polynomial is the plus A1, B1, A1 plus A2 x squared plus a3 x cubed. This is a third order polynomial. It can always do, as well as model B plus A1, A2, and A3 to be equal to zero. All right, so when I add more polynomial terms, As long as A2 and A3 could be set to a value, a non-zero value, that makes the error smaller and therefore it will make the loss. Any questions there? Yeah, Tom, we asked you can set a you can make the loss of the road by fitting a polynomial exactly to these data points. That's correct. If you make n large enough, eventually you're going to get a polynomial. With that, we'll go through every single data point. Alright? So it is possible to get the losses the road by making a really, really high order polynomial. Which leads us to our next question. Why don't you want to do this before? That's the question. The question is, is there a ever only one solution to the least squares problem when the loss function is convex in the parameters as it is in this case. Yes, there's one solution which is the global minimum. The question is, can I clarify the notation for this class? When would I want to write y hat and x hat? There will be a general notation for the uses of hats here. Because I wanted to say that this vector of features is related to x. I want to keep the variable x, but I just chose to make it different than x. So there won't be anything consistent with these attacks. I could have called the other questions. Okay. So if you were to look at this data and I want some to answer this without using the word overfitting. Why is it not? Gets to make the polynomial n arbitrarily high? The provided data very well, but it may not fit the underlying distribution well. That is correct. Another way to say that is there could be data points from the underlying distribution that we haven't yet seen. It's just one data point here that wasn't in the training data. So the high order polynomial fit this data really well. But if we were to predict which model actually has the lowest average for this data point over here, it would be the linear model. So in general, when we build machine learning models, right, we don't want them to only work on the data that we provided it. We wanted to generalize well to data that it hasn't seen. So this leads us to the concepts of overfitting and underfitting, which I understand it again, obviously for most of you. But to finish it up on the same page, we want models that are general, that will generalize well. This leads us to first, so first I'm going to talk about training and testing data, but then in a few slides for them to just another notion called validation data. The idea of generalization can be made more formal by introducing two datasets, the training data and the testing data. The training data is data like these blue points here. There are used to fit the parameters of your models, but theta's, the a and the b. After that, testing data is excluded into training and testing data are like these red axes that weren't used to learn the model. Through which we can compute the error of our model by measuring what the law says with respect to these red x's. Again, there's also validation data and we'll talk about that in just a few slides. But at the highest level, we define this notion called overfitting. Overfitting is when a model has very low training error, which means that it models the blue points really well. But it has high testing error, which means it does not model the red axis, right? So that models are training data very well, but it doesn't generalize if you are in that scenario than your model has. One thing which we'll see later on is that more data helps to avoid overfitting. You think about polynomial that can go through every single data point. But when you have a ton of data points, that polynomial will no longer go through every single data point. In fact, if we look at the models for the polynomial is going from n equals one to n equals five, almost close to linear, right? So even for the polynomial with n equals five, even though it has x squared, x cubed, x to the third coefficients are closer to zero and the actual fitted model is linear. So more data helps to avoid over fitting. There is also somewhat more estimation underlying distribution. Distribution. So let me repeat what I said that she wanted to clarify. Another way to say this, which is having more data helps you to learn the underlying distribution better. Model the underlying distribution is better at a training and the testing data have the same distribution, then you should also generalize better to testing. Great, yeah, so Andy's question is, when I say more data, what is that relative to? Because presumably you increase the order of the polynomial, we're going to be even more me, that's good. In general, when we have many parameters as we will have neural networks, you will need commensurately more data to avoid overfitting. Right? So in addition to over-fitting, there is something called underfitting. Underfitting is the idea that we cannot think of model really simple. So there could be e.g. data that comes through a distribution that is polynomial. So this is data that comes from a distribution that is up to third order polynomial. Here's the Python code for that. If I were to assume my model was y equals a x plus b, so that I can draw a line through it. No matter even if I find the optimal values of a and B, I won't do a good job at predicting the, from the y-values, the outputs of these blue points, because this model is overly too simplistic and can never capture some of these interesting nonlinearities in the data. So going through the same method that we derived, you can fit these polynomial models, this data, and this n equals one model won't capture the curvature and this distribution and it's in these points. And so a model is called underfitting if it has both loads of both high training error and high test error. Since on those definitions of over 100. Alright, so then that leads us to one more thing which is called cross-validation. So in our model, you'll have noticed that there's something that we had to choose beforehand, which was the order of the polynomial, right? I had to set what little n was equal to, say n equals four. And then after I chose little n equals four, I did my gradient of the loss with respect to theta to calculate the F1, A1, A2, A3, A4, right? So the B, A1, A2, A3, A4 are my parameters. But this thing that I chose beforehand, little n, is called a hyperparameter, is something that I choose before I do the optimization. And this hyperparameter is quite important because it defines the model. How do we choose our hyperparameters for? The question? Yeah, great. So the question is, how do you decide that? What is meant by the training error as high as the validation testing error is high or low. So we'll talk about this more when we get to gradient descent, but it's usually a relative thing. So when we look at our loss function, and this is for training generally as a function of theta, but as a function of the amount of time that we trained for, the loss function will decrease and the slope decreases will be in front and we'll talk about that. But then for testing or validation, let me write validation because in general we won't be in this situation, although actually not having introduced validation here, let me call this test. The test error will also decrease, but then there'll be a point where it begins to increase again. So it's a relative comparison. In this regime here we would say that we are overfitting now because we haven't gotten your training error but higher, higher. Alright? So again, we were saying that a model will have hyperparameters. Why? The order of the polynomial, okay, I have to choose the forefront to the optimization. And we may be wondering how do I choose the optimal polynomial order. To do that, we also introduced this notion of validation data. And validation data is data that is used to optimize the hyperparameters of your model. So it allows you to choose what is the best little. Alright? How do we do that? What we're going to do is we're going to divide our training set, or sorry, we're going to divide our data into three sets. The first is the testing data. The testing data you should think of as with pelvic samples, Christina and set aside. And you basically get to look at this testing data once after you've set all of your hyperparameters and then the parameters and the testing data is just satisfied. You get to query it wants to ultimately scored or model. Okay, so that's what the test data is. Partitioned away from your original data set aside and use wants to just get a score on your model at the very end. Okay? Any questions there? Okay, So that's testing data, training data and validation comes from the remaining data that you have leftover. And what you will do then is you'll withhold some data and validation data someday. That's training data. Training data used to train your model. And then validation data is used to score your models generalization on data. This might be confusing, so we're going to write this out. So there's a process called k-fold cross-validation. This k-fold cross-validation is going to assume that you already have a separate testing set. And what we're going to do is we're going to take the remaining data that we have, separate it into a training and validation data set. Let's say that for training dataset contains and examples. Let's say that is 800 examples. Okay? So we have big N equals 800. Let's say we're going to do four fold cross-validation, which we often call CBD. Okay? Then what we're gonna do is report to split the data into k equals four equals sets, each with 200 examples. So we're going to have four folds, each worth 200 examples. Of this force will be used as training data. And then one of those poles is going to be used score model, which is called validation data. Alright? And then you can swap between which are the training data, which are the validation data. And what that looks like is the following. So I've drawn two examples here. The first example is very rare, so I'm always showing this to you as something that you can do. So in the case where your model has no hyperparameters, you can split your data, your original data into train and folds in green, testing fold in red. And because you have no hyperparameters, you can just train on these four folds of the data and then test on the fifth fold. And testing on this pitfall will give you a score of how good your model is. Then after that, you can make fold one, the red testing data and make folds two to five degree trended data. And you can do this five times together. An estimate of what your average test error. This is very rare. You will never see it in the class. So I've removed it off this slide. And this is what k-fold cross validation will look like. So let's say that our original data has 1,000 samples. Alright? What I will first do is I'll make it successful. This test fold will have 200. Examples in the remaining training data will have 800 examples. Now this tough fold is put aside. I'm never going to touch it until I'm done with all of my machine learning optimization. And then I get to query the test fold, wants to score my model. My remaining 800 examples. I want to make a bottle as good as possible, which means I didn't choose the best value of this hyperparameter little pen. And then you'd also do my optimization of the coefficients, a one, A2, A3, et cetera. So what I do is with my 800 examples, I'm gonna do, in this case, four fold cross-validation, which means I'm going to split this into four folds, each with 200 examples. Yeah, tomboy saying, do you stumble the data before you assign the folds disease, you could idealize shuffle your data before you assigned to e.g. if you just kept in order and this was like SeekBar time unfolds. One might contain a lot of cars and horses, but the cats and dogs mail will be important when you want somebody to sample all the different classes in each of these faults. So what we do with these oracles is this is used to learn theta. Then this is used to assess how well does it generalize to unseen data. So this is our validation fold. Right? So I would set M equals one. Then learn AND from my green training data. And then I would assess its performance on the withheld data. And my validation. Set n equals to training again, assess performance on the validation fold. I was at n equals three and then assess performance again on the validation. Right? And so you can repeat this procedure first, then choose what the best setting up little n is. That would be the model that, and to achieve the lowest validation error, let's say it was the n equals three bottle, little n equals three. So the third order polynomial, I got the best validation error, right? Little n equals three. Train a model on all of my training data. And then at the end I can query it on the test together by one measure of what the losses on this test. All right, Let's talk with them and then let's generalize it to do it. But then I didn't make us. Great. Yeah. Alright. Yes, That's perfect. Yeah, So tomboy is mentioning something that I think back then, which is choosing the best value of n when we do four fold cross-validation. But we're also going to do is we're going to swap which ones are the validation in which went to the training set. So in my second go round, I'll make fold one. The validation and then falls to four will be training. And then I could also do one where fold 13.4 are training but for two or validation. So by swapping out each of these, I will calculate it for validation errors for every single hyperparameter, and that'll give you a better estimate of what the actual question. Yeah, so the question is for doing optimization of the hyperparameters on the validation datasets. That's correct. And then after you have the optimal hyperparameters, you could set them and then just train across all of your green data. Could just get the best out of all of your training data to build your ultimate model. The question is, how's a number of both optimize? A very common thing is to do five-fold cross-validation, where you've got 80% shining in 20% test. There's also something called beat one out cross validation, where your validation is one example. And if the rest of the training data, and they'll depend on your datasets. I said, How do you choose that? There's a book. That's how much does it's a whole field called next, which is devoted to this hyperparameter optimization. All right, so I want to say one more thing before we end. Which is, students will often ask, why is this testable left out? And what do I have a different validation dataset so that your model is not Crawford demise or fine-tuned to do well on the test. It could be that if you were to optimize the hyper parameter middle n by repeatedly looking at your passport, you might get a value of n equals three. But then you would have chosen the hyperparameter based off of predicting this particular dataset. And that might not be the model that generalizes the best out there interviewing. And so the test fold again, just think that you should clearly once so that you don't learn the particularities of it. They will come back. Yes. Recording. 
All right, everyone. We'll get started for today. So we have several announcements. Didn't hear me, I think he goes. Did you have several announcements? The first is that homework number one is due tonight, upload it to Gradescope by 11 59. And for the code, this is a friendly reminder that you need to submit the code that you wrote. So if the grazers evaluated, and what that means is that we would want to print your notebooks to PDF with their solutions and clocks built in. Homework number two is going to be uploaded today by the TAs, and it's gonna be due in a week on Monday, January 30th. Upload it to grades together. A heads up for homework number two and also assignments moving forward is that the assignments are going to have a good amount of Python coding in them. And so we really encourage you to get started early in this homework number two, you are going to be commenting k-nearest neighbors as well as the softmax classifier, which we will finish discussing it and derive it in details up today. Any questions on these first three announcements? Yeah. The question is, can I clarify what it means to submit dot PY files as PDFs? So in homework number one, there are no dot PY files, but later on in homework number two, moving forward, there'll be classes that are in d2y path and just wanting to be. So ignore that for homework number one, but burrito numbers. Are there questions on the first three-and-a-half sense? Alright. So there's a detail on TA office hours notes, but I don't believe either pronounced so because we have so many CA office hours. The way we tried to organize the uploading of thumb is that the TAs are going to upload basically a consolidated office hours notes on Friday along with the discussion video for that week. So we will upload notes from office hours for the TAs, but they will all go off together on Friday. Any questions that alright. And then lastly, I'm just due to some administrative reasons, we haven't yet hired the graders for this class. And so, apologies on my end. Homework number one is going to, there's going to be the array and returning your grades to you since we haven't get fired, our graders. However, we will, of course, I put the homework number one solutions on Wednesday so that you can check your work and understand it. On Thursday. Great. Yeah, we will upload the homework number one, which is on Thursday because they'd go off after the late deadline, which is Wednesday, right? Any questions about any logistics for the course? Alright, we're gonna get back into material back. So last lecture we discussed this simple classifier, k nearest neighbors. And then we talked about some of its weaknesses. And then we mentioned that we would then look at a classifier based on linear classification. This classic buyer ends up being at the output of most classifier neural networks today. Also, it comprises something called a linear layer or linear building block, which is a key component of neural network architectures, which we should also get to likely by the end of lecture today. So we mentioned that a linear layer will implement this function if x is your input and y is your output, they'll do wx plus b. And then what we said is that this vector y, if we're doing classification, can be interpreted as a texture of spores. So y will be a C dimensional vector, C being the number of classes. So in C4, ten, that's ten classes, so it'll be attendee vector. And then the first element is a score opinion classic one. The second element is the score of being in class two, etc. We talked about how this will compute a score for every single class. And we also talked about how this wx plus b is implementing linear classifier and ends up finding solutions if you just have wx plus b. When w, because we're going computing like a w1 transpose x, right? The score will be highest when the weight wi looks most similar to x. So if you've looked at the weights, e.g. if the car cost, you can see that they kind of present all cars. Were there any questions from the last lecture? All right, We'll move on then. So after we get the scores, what we need to do is we need a way to translate them into a loss function L. Because we don't wait to say for some setting of the weights w and the bias vector b, how good is my Softmax classifier this money earlier at predicting the correct class. So I need a loss function L to evaluate how good it is. And then after that, I need to know how to make w and b minimize that loss, right? So we're going to fill in those gaps today. And so like we talked about how we're going to do this. There's something called maximum likelihood. What maximum likelihood does is it says for some observed data, what we're going to do is we're going to find a way to evaluate the probability of having observed this data given the parameters of my models. We've talked about this coin flipping example where the parameter is the probability that it comes out to parents. And then we're going to apply this today to our softmax classifier. So when we change the softmax scores, which in general can be positive negative numbers, into a setting where we're going to use maximum likelihood is going to be critical that we are able to interpret the scores as probabilities. Alright, so to use maximum likelihood, I need to be able to calculate a possibility to begin with. So that's going to be done by the so-called softmax. So we're going to do is we're going to define score for class II. I'm going to call that a as a function of x. X is my image. And then Ai of x is gonna be the score that the image belongs to classify. So this is a variable that we want or just use the I element of that vector y. And we know that the score is given by W transpose X plus some scalar bias beyond. So I want to know the score of the image X being classified. All I have to do is calculate wi transpose x plus b. Again, the score could be any number as negative if you want or as positive as you want depending on the values. Is that wi. When we define the softmax function, the goal will be to change the scores for each of these classes into probabilities. And for probabilities, we know that there are two constraints. Probability has to be 0-1. The sum of all the possible probabilities for each class at the sum up to one. The way that we do that is with the softmax function. But we're going to do is the softmax, the image X being in class. I, would later find out that this is gonna be interpreted as the probability of class I definitely image x is gonna be e to the score of class I divided by the sum of e to the scores of every single class. Alright, so in a very simple example where we just have two classes, the score for class one is going to be a one, x is going to equal w1 transpose x plus b one. I'm going to talk through these just for the sake of writing. A score for class two will be A2 of x equals W2 transpose x plus, I'm going to drop these. Alright? So those are my scores for class one, class two. The softmax function says softmax for class one of x is going to be e to the w transpose x divided by the sum of the exponentiated sports broadcaster. So it's gonna be e to the w transpose x plus e to the W2 transpose x. And then softmax for class E of X is going to be key to the W2 transpose x divided by e to the w transpose x plus e to the w transpose x, right? So you can see for every single softmax, the denominator is always the same. It's going to be the sum of all the scores. The numerator changes is going to be the exponentiated score for that class. You can see that if I sum up softmax one and Softmax two, they add up to one. All right? You can also see that these are all going to be values 0-1, right? Because I'm dividing a score by, by some larger non-negative score. And so therefore, because we know that each softmax for class II is going to be 0-1. And we know that across all classes, they sum up to one. And what the softmax function does is it turns my vector of spores into a valid probability distribution over each class. Said a lot there, I want to pause and ask if there are any questions. I said Jake's question is, what is the advantage of using the exponential instead of doing one score divided by the sum of all scores. That's a really great question. So the first thing to note is that Any function that makes the scores positive will work as a normalizing that's already normalized probabilities. If they were negative, then we might not satisfied all of the constraints. The probability distribution you could do like absolute value of Ai divided by absolute value. But then if you asked me that it has a negative score, large positive score, right? But you can choose other functions that are monotonic. So some people say like, Oh, you can do to, to the AIX instead of e to the x. That's totally valid. Also, you could do E to the two to the AIX that's also valid. All of these will create a valid probability distribution. When you use ie. This is very common, but also has some relationship to information theoretic quantities. And so that's one reason why it's preferred. Excuse for perfect gas. It's always just another thing, which is when we derived by people, like we talked about last lecture, you see we take the log likelihoods to turn all those products into sums. If you have logs of ease, we know that they canceled after just give you an exponent and it helps to make some of the math simpler. Right? Yeah, so remain as mentioning that also has a strong nonlinearity. We're gonna see how that leads to his name softmax, that this function is almost by taking the maximum of the neck, is almost something that returns the maximum score. Although it's gonna be differentiable. The question is, would I repeat the first advantage of that information theory? I'm going to leave that for beyond the scope of this class, but nobody talked about it with me in office hours. Basically, if you choose the softmax function, you'll see that several information theoretic terms fallout. Another name for the loss function, both the ride today, the maximum I can put boss watching is called entropy. Entropy is a concept from information theory. All right, we'll continue on that. So that's the softmax function. And you're going to do is we're going to define the probability of an example. So this is going to be e.g. J. We're going to say the probability that example j has a label that is I. So I will be from one of the tank classes. Given that I know the image, which is x, j, and I know what my data is. Data are my weights and biases like gave me the scores. We're going to define this probability to be the softmax applied to the image for that class. Alright, so let me just write this out in words because whenever we see probabilities, we have a good intuition over them and what they universe. So this is going to be the probability that x j, j, this is just with Jacob, my training set. This is probability that image x j belongs to class. All right? Any questions there? Yeah. Great. The question is in the data matrix, x is each column. So if we're just talking about C4, C4 can we may have any images. And then remember for these first examples could restate them to the 30 72 vectors. So each X is gonna be a 30 72 vectors. So you can imagine we might have some big matrix X of all of our data. And it's going to be 50%, sorry, big N, big N images by, by 30, 72. So then every single row here would then just be one example. This would be x superscript one. The second row would be x superscript two. So x superscript j is just the state of new tricks. Great question. Other questions here. Alright. So let me just say it one more time. This is, we're going to assign, we're going to define that the probability of your image belonging to class I is going to be equal to the softmax function applied to your image for that class already. Remember this is going to, this is going to be based on the scores for Phi-Psi AIX divided by the normalized, divided by the exponentiated sum scores for the classes. Alright? Any questions there? Alright, then we're going to go ahead and derive what the maximum by, because it was. So what we wanna do is being able to take our data. In this case, we have datasets where this is our first example, right? Again, remember X1 will be some image. Then y one will be a stable. So let's say that the first image has got a dog. So X1 is the image of the dog. And then Y one is the label that tells me it's a dog. And then epsilon would be my image and maybe white and as a cat. So what I wanna do to do maximum likelihood ready? As I wanna do, I want to compute the probability of having observed all of my data. Which means I want the probability of having observed image one. And image one was a dog having observed image and that image and was account. Alright, and this is going to be conditioned on my model theta. So remember that Theta then is going to comprise my matrix W and my biases. And those are the weights and biases that control the scores for each class. And those scores in turn affects the overall probability of my bottles. Want to pause here and just ask if there any questions on this. Just writing out the most naive. Just running up the phone. I'm sorry. Great. The question is, what does it mean by the likelihood of having seen the data? So what we're going to do is we're going to have a model, w and b, right? And that controls the score for each class, right? So let's say that image one looks like a dog and it's payables and dog. So for image one, our model would have high probability if the w's and b's assign it a highest score for dog. If the w's and b's told me that image one is a car, then I'm going to have a lower probability. Because the probability of my model saying that image one is the dog is going to be very low. So this is a way for me to write out what my model to say. Image one is a dog and a cat, et cetera. Analogously, I always like to go back to the simple example when I'm thinking about this, the data is our observations. But we're giving like the sequence heads, tails, heads, heads, tails, tails for that point where we want to make sure that our model does a good job at predicting that we see a sequence of four heads or tails. Similarly, in this case, our model parameters, but tell us that each one has a high probability of being a dog. That's a high-quality bean, et cetera. Good question. Any other questions? Great. Yeah, so Rockford is going to hit that magic, something which will bring us for our first step. So in general, when you see a probability with many terms, that is going to be something that's very, it could be something that's difficult to compute. We have to start making some simplifying assumptions to be able to actually write an expression for this profitability. Assumptions might not be true and the board, they are untrue, likely the term that we compute will be more off, but many of the assumptions that we make are not totally unreasonable. So the first assumption that we're going to make is that image is conditionally independent of dimension j given my parameters. What that is saying is that what that is saying is that I treat each data point as my prediction for data points to is going to be conditionally independent of my prediction for Data Platform image to image one. Okay. Any questions there? Yeah, from each one? Based on that. The question is, isn't that basically an approximation? Because images can have some correlated features of sorts that my model might pick up on. And being able to do an associate that might make this assumption not totally valid and not That's correct. Yeah. Given data, they are independent. Given my model. Alright, so at this point, we're not going to make our next simplifying step, which is ever going to apply the chain rule for probability. So I'm going to break this in to the probability times the probability of y given x psi. Alright? Remember that in words, is probability of y given x psi is saying, this is the probability that my image x belongs to the class. Why? Alright, and this thing is going to be our softmax probability. I didn't even pass rate e.g. the thousand samples. Samples or samples from the bulk glass. And Muslims. Tomboy asking you, what is the dependence? Hear me. So the Independents here is the identical fire sorry. The identical distribution is referring to the samples. X i, y are. When you're seeing the samples from the same class type of distribution, are you talking about p of x given y 0? I'm talking about, let's say. But ultimately, if you wanted to raining or snowing identically distributed according to this training samples. Even less, all the samples we go get the same distribution that's provided. So when we see identically distributed here, what we're saying is that p of x i comma y given Theta will all have the same distribution. P of y given x, I will always be the softmax. And I think if I understand what you're saying, tomboy, you're saying that if you have the probability of an image given its class, why I equals k or something, that this will also be a different distribution in general for each class. And that is also true. So general the distribution of exercise, which are the images given the classes will also be different, but the identical distribution refers to this term here. Actually this is a, this is a great question to make sure that we're all following. So when we apply the chain rule for this term, we do the probability of the image and then the probability of YI, the label given the image. We know that in layman's terms, this is the probability that X belongs to the class. Why? I could have also rewritten this in the following way. I could have written product from I equals one to n of p of y i given beta times this term. Next time I mentioned p of x given y i. And fair, right? I could have used the chain rule in the other direction. Why don't I do what's right? Great. So the student thinks like from a practical point of view, the goal of a classifier should go from x to y. And because we're going from x, y to y, I didn't say this, but this is kinda where the actors going. We're going to have to calculate a term that takes us from x-i to y-i is gonna be our classifier for which we have a term is soft match probability already computed. So in essence, we have this term to find because that's what the problem that we're doing. And that is correct. That's one reason we want to go with this expression. How about another reason we prefer this one over this one in terms of complexity. Wonderful. Yeah, so this student is saying, for a given class, there are many different images that belong to that class, which is true. And that's kind of getting at this answer, which is, if I were to write out this probability P of x given y, right? What does that mean in words? It's saying, let's say that the class is dogs. So let's say that the Y is the label for dogs, is saying, what is the probability of how images look like, right, of seeing this image of a dog, which is a 3,072 dimensional vector. When, what is the distribution over images that come from the class dog, right? And in general, this will be a high dimensional complex distribution. There'll be a 3,017 and I shall distribution. Whereas the softmax vector saying over my ten classes, right, which is a simple distribution with ten possible outcomes. This is much more easier to estimate or approximating this thing, which can in general be intractable. Any questions that you recently had a good following. Okay, great. Majority of class question. Intractable. When I use that phrase, music, We can't write it there. So it would be saying, well, what kind of distribution can we write down? That's 3,072 dimensional, which would model the distribution of what the dog images, stuff like. Tom Waits question was just pointing out which distributions are identical. And Tom, I was making the point that this distribution, which is if k is equal to the dogs, the distribution of but dogs look like there's going to be different than the distribution of what Casper quite et cetera. We have nothing that those aren't ethical. We've assumed that these distributions are identical. So we haven't assumed that p of x given y is identical, right? Yes. Thank you. So the student says it's a big idea, but the probability of all the pictures being classified correctly is the product of each individual image being classified correctly? And then multiplying it all come together. The answer is yes. All right. Last question. I'm sorry. You said we didn't tractable, Why would it? Oh, yeah. The answer is, are we using Naive Bayes? So yeah, if you know that I Bayes classifier, that will work. But the more troubling thing is that you will need to assume a distribution over the natural images. And probably the approximation you make there would be very poor distribution. Yes, it's almost making one last point here. If you were to actually learn this distribution, that would be very cool, right? Because then you can say, I'm going to set y to be the dogs. And if I can draw from this probability distribution that I can generate images of dogs. We'll talk about some of these generating distributions at the end, the classroom effects of games. And okay, let's move on for now. So what we're gonna do is I'm going to take this expression over here. And we're going to actually write out what this means in terms of our models that we can derive our likelihood. So I'm going to take this expression just copy and pasted from the last slide. And when we do optimization, our goal is to make w and b, which on my parameters theta, to optimize them to make this likelihood as big as possible. The first thing that I'm going to do and going from this step to this step is I am going to just erase this term. Alright? So from going from here to here, I just erased the first term. And someone raised their hand and tell me why I'm allowed us to do. So. Great. Yeah, so let me write out what, let me write out what this probability P of XI given theta is. First off, let me start off actually with a simpler question which is, can a student told me in words what this probability distribution means? You will go with, you got that. Wonderful. Accept this. The probability that you see this image or this image occurs given are setting up my w and b matrix and vector. I'm going to say that this is equal to p of x, sorry, and discuss the student answer. Because the probability that you're going to see an image of a dog, right? Or the probability of an image of a dog in a hurry. It's going to be the same irrespective of my bias is one or two, right? The been totally unrelated things like the parameters of my model to whatever I want them to be. That's not going to affect anything about how the images, images are data that are given to us in this setting of W and B have no impact whatsoever on what the images look like. And therefore, P of XI given theta equals p of x sine. This is true. Then P of XI given theta has no dependence on theta. And because I am going to be maximizing this expression over theta and this term, does it impact data? You can imagine that it comes up, that I can just remove it because it won't change what my final setting up data is. Alright, any questions there? All right, and just in general, always helpful to make sure that when we write gotten distributions, they aren't mysterious to us. For the distributions that we write down, we should always be able to say intuitively what it means in words. And that'll help people with simplifications like this. All right. Alright, we're going to continue our work here. So this is going to be equal to ArcMap data. And recall that we define this probability distribution here to be softmax side and now voting, overloading variables. So this probability here is going to be assigned softmax. I'm just gonna be the softmax score for class. Why are they? So I'm going to give us superscripts right here. So I'm going to write this in terms of my bug, in terms of my softmax is one other thing I want to do is I'm going to take the log of this so that the product turns into summation. So I'm going to turn this product over examples n into summation. So it's gonna be argmax I equals one to n. And then have log of these probabilities. And these probabilities are gonna be those softmax course. There's going to be var log softmax. That example, I belongs to class label y i. All right, we're going to then go ahead and expand this. So I'm gonna write that this is equal to arg max over theta sum from I equals one to m. And then I'm just going to plug in the definition for softmax. So this is gonna be bogged. And then the numerator is gonna be the score of the correct class. So the score of the correct class will be e to the w transpose x plus BA II Plus BYUI. Then it's gonna be divided by this for the rest of the class, it's just gonna be divided by the sum. I'm going to use, I'm Jay to index the rest of the classes. So j equals one to see. And that's going to be E to the W j transpose x plus b j. Now sometimes people get confused by this notation, y being in the subscript. So remember y is one of the numbers 1-10. Let's say that y equals 4.4 is the class label for dogs. And so this is saying, compute the score for the dog exponentiated and then divide it by the score for the rest of the classes. So example I tells me what the correct score is, dog and cat, etc. This is saying compute the score for that correct class. Any questions there? All right, so we're gonna go ahead and simplify this even further. At this point, I'm just going to go back to using that notation that we had, where this w j transpose x plus b is just going to be the score. Ha, so I'm just gonna do that. So I have plus routers today. So this is gonna be argmax theta. We're going to have a sum from I equals one to m. Now I'm just going to simplify log experts. So we're going to have log E to the score of class YUI. That's just going to equal a YI x phi. Alright? And so this AY XI says this exponent over here. Then it's going to be minus log of the sum of scores across all. It's gonna be minus log. Sum from j equals one to c, e to the w j transpose X. Score notation e to the x. Then usually one other thing that we do is we'll make this an average across example. So I'm gonna put a one over m term here. This hasn't changed the actual answer because one over m is just a constant and that won't change the state that maximizes this term. Any questions here? The question is, why don't we take involved with Chicken balls to turn all of these products into Sunday? And so in homework number two, you're going to have to take the derivative of softmax with respect to w and v easy. And That'll be easier when these are songs and our products. Raised the question is, why did we use, if I understand correctly, y in the numerator and the denominator, this is so close, has this question. Okay. Usually, you see this is the most frequently asked question. Yes, sir. Um, remember that what we wanna do is we want to compute the probability of the correct class. Let's see, let me just write it out explicitly. Let's just say e.g. all right. The correct class is at y equals four. If we look at our students are ten, we can see that the fourth example is cat. So we're saying that example, I as a cat, y equals four. And this means I belongs to class four, which is a cat. And the probability image xy being in class for, right, we'll just write as softmax. Softmax for objects. Alright? So this would be the probability that xy belongs to class board. I wanted to write this more generally then for all of my examples from I equals one to m. So instead of putting this explicit subject for, I'm just going to say whatever YI is for that example. That will be the softmax publicly. The question is, what is this probability? What is this probability? This is the probability that given my model, you observe that the image xy. We observed the image XI and its label such, such as like y equals four mini that this is an image of a cat and this is cat, Right? Yeah, so y is going to be the label that I asked. And I want my softmax to set the parameters of Theta such that y1y was equal to four. Softmax for an xy would be a large number, something closer to what? Yeah, softmax for class three would ideally be zero or something close to zero. The question is, is the probability of the incorrect class? Can you go in this expression? It is not, however, it is implicitly in there because this is a distribution that sums up to one. So the probability of cat is high. The probability of dog and everything else has to be less. Okay? So we're gonna go from this expression. This is again just plugging in the softmax. And we're going to do one last thing, which is this is maximum likelihood, but we know that in general, the convention is that we have a loss function that we want them in MRI. So to make this an RNN problem, we're going to use that property where if I have some function of Theta, now want to match it over theta, this is equal to minimizing negative f of theta over theta. So this thing here equals argument over theta, one over n. Sum from I equals one to n. Remember this over my examples and then I'm just going to flip these two terms together, my negative sign. So this is going to be log sum over all my classes. E to the h minus y. Alright? This function here is going to be my likelihood that I will desire to minimize. Honor. Yes. The question is confusion over this notation here. Oh, yeah, it's confusing over the notation W subscript y. So let me write this also more explicitly. So if I were to write, this is important because you all need to implement this on homework number two, a phi of x phi, which is this term here. This is defined to be W for the correct class Y i transpose x i plus B, y, and z. For ten, we have ten weight vectors, W1, W2, all the way up to w ten. And then we also have ten biases, v1, v2, all the way up to be taxed, right? That's an art have been here model. When I say a YUI, what I'm saying is that when you get Example I is gonna be an image. You're going to get a data example with XI YI image. I'm just going to come with a label and that label y will be labeled image. And I will be some number 1-10. So if e.g. I, they face a cat, right? And he saw before a cat corresponds to y equals four. Then a y i x i will be a four. So this will be W4 transpose x plus b for e.g. are e.g. I. Plus one. It can be that y equals ten. Then this numerator would be e to the w transpose x plus b times et cetera, et cetera. Right? Right. The student is asking, so let's say that y was equal to four, so it's a cat. What is this expression actually equivalent to in terms of thinking of like five linear classifiers. So what it would correspond to is what you can do is you could take your road W4 transpose here, dotted with x and then add your fourth price here. Or you could just take the fourth element of this Y matrix, a wide vector. Both of those would give the equivalent thing, just the score of the image being in class for. Yeah. The question is, why did I add the one over n? You don't have to add the one over n. Remember the one over m is just a scaling term. We added so that this loss is basically the average loss per example. So it's just a normalizing term that says, okay, this is going to be like physically my loss, e.g. but it isn't necessary. Alright. Go ahead and take our five-minute break. Given the number of questions, I think I didn't do the best job of explaining this, so please take some time to look over it with the five-minute break and then when we come back, I will be happy to take other questions too, hopefully. Two questions. The first one is directly given theta equals P x, y term. You say that the px py doesn't matter because even though we get, we are dealing with different Thetas. Dxi is still the same. But I wanted to ask like, we, you have a whole data set like say 60 or I could call a hundreds of patients dataset. Split. This patient's ear training set and testing set. Doesn't matter. It doesn't, because remember, this is the true distribution over images or whatever data that you have. That distribution is given to you and you get examples. Distribution. You're not going to get, you're not going to just won't look different based off of your weights of one or two to the images aren't just something true before you do any. Assuming this is the true distribution of the population we are analyzing, we are splitting Reimer with, right? Yeah, so that's the probability of drawing samples from that distribution matters here. Not for optimization. In general, it's going to be like something very complicated. But its parameters. Oh, yeah, awesome question. And you're like, what do you exactly mean by like this? P, X and Y follows the same ID. So that I can think of distribution means that this P is the same for every single bar. So it isn't that like for XI and XJ and YJ, the keys are different. What does that probability is that even the fetus and probability we are observing this hair. The same distribution across all of the sample to Halloween. The softmax distribution. We took this off. Yeah. Okay. Often. Alright. Everyone, any other requests on a softmax classifier? Right? Yes, that's correct, but it's gonna be negative effect. Remember we took the log scale because of the loss, can be any number, any number that is zero or higher. Okay, Other questions on this top path, this is super important and will be on homework number two. Yeah. Great question is, if the dataset was thought identically distributed, we happy to give weights to different problem. Well, actually let me just answer the first part. Identically distributed means that this P is the same irrespective of example. So this P was not the same. There are two answers to your question. One is we would still assume them to be the same because otherwise we won't be able to be very public and we wouldn't be authored by an analytical expression. And therefore we would just keep penalize on out for our assumption is. There might be some forums where they're not identically distributed. And you know, like they have some distributions that are different. You can define them, but when you multiply them together, they both comfortable. Almost surely a monster so-called conjugate priors. So when we say identically distributed, I'm saying that this P is the same for all the examples. And then when we break this down, that means that all of that means that P of Y given X, Y is always going to be assault. It answered the question. Oh, okay. The question is saying, if you do a good job in decoding, does that mean that there are some things are essentially valid that it was posted conditional independence. I'm not sure if that's always true, but I would strongly believe that the requisitions dependent. Sorry, I'm still stuck. The question is, what does it mean that these examples are identical? I should have just written this out given the confusion. So what it means is if I have p of X1 and Y1, X2 and Y2 given theta. I'm not saying that they have the same probability. I'm saying that they come from the same. I'm saying that the values of X1 and Y1 don't impact the values of X2 and Y2. So I'm saying that this breaks down to be the product of x1, y1 given theta times the probability of X2 and Y2 given theta. Which means that knowing that image X2 is say, a dog has no bearing on what X1 is and what it was. Last question here. The question is, why did I write this line and what does it mean? So here, what I was saying is, you could use the chain rule to break this distribution down in a different way. And we can try to optimize this, but we don't want to do that. Why is that the case? Well, in the first decomposition, this is a simple probability over ten possible outcomes. So we can model with softmax. This case. This is a distribution that's 3,072 dimensional. And it's very complex and it's very difficult to model in general because we can't model this distribution well, it's better to go with this decomposition. I still want to caution is on the independent identically distributed. So know that this is a very common assumption. They overdid examples to simplify machine learning problems. If you don't have the assumption of data being ideas generally difficult to write down any likelihood. So this is sometimes an assumption that we live with, even if it's not true, because without it, we probably couldn't simplify our expression any further. Okay? Alright, we'll move on for now. So I have a loss function, and now this is our softmax loss function. We haven't yet figured out how to get the optimal parameters, but I'll be just a few slides here. That just means it's a time I'm going to ask you to review on your own. But these are essentially, we'll say that the softmax classifier, it gives a reasonable score or loss to every example. So if you worked through just plugging into that loss function that we derived the actual values of the scores for each class. You can see that the car loss is close to zero, which is good. Remember for a lost, Zero is good, far from zero is bad, so hires worse. So in this example, the corporate car was 5.1 and it was indeed a car. So the law should be low. In this example, which is a bird. It turns out this score for car was higher than for her to associates that. But she gave us a large loss. Right. Running through, plugging in some numbers the sanity check. Then I wanted to also give some intuitions over this loss function and the softmax. Afterwards, then we'll get into gradient descent. Alright, so a few months that the softmax classifier first is where does the name actually first? Tried to break down the terms and see what happens. So if I want to look at the probability that the label is why I give him some image X, right? We know that this is going to be that term log of the probability that X is in class YI. If we're looking at log probabilities. When I simplify this expression, I get this term which is a YI x minus log of the sum of the exponentiated scores. This is the exact same term that we had on the slide. This slide right here. This is softmax for class Y-bar. What I want you to notice about this is that if we want to maximize this term in the maximum likelihood setting, we make this term as big as possible by making this, this term here, a y axis biggest possible. Alright? The reason or the way that you can see this is that if a y x produces the largest score, which means that we have the largest score corresponding to the correct class YUI. This term is almost equal to zero. The way to see that is because the exponential exaggerates small differences. So let's say that the correct class was, let's say that we had three classes. And let's say the scores were 120 and minus one. What this term is saying is take the log of the sum of the exponentiated scores. E to the 100 is a lot bigger than each of the 20 is a lot bigger than e to the minus one. E to the 100 plus 20 plus e to the minus one. It comes close to equaling e to the 100. And then what is the only take the log of e to the 100, I'm gonna get back out the score of 100. So this term right here is approximately equal to the maximum score. As long as x is core dominates the other scores. So in the case where you have the correct for us, then that means that, sorry, in the case where your classifier's predicting the correct classic, it's the largest score to the correct class label. This B will be equal to AY x and this is equal to AY AX, and therefore, this log probability comes out to be approximately zero. However, an incorrect class produces the largest score, right? Then this value will be more and more negative. And so your log-likelihood will be more and more negative and your model anymore and probably, alright, so once we do that, I need to check these received that this thing will indeed be maximized when it assigns the highest score to the correct class. That's wanting to. Another practical thing to consider is something called the overflow because softmax, so let's say that I got a vector, I had three classes. And my scores were as follows. They were 500, 404, 50. If I go ahead and start to exponentiate 500, I'm gonna get an enormous number. And sometimes these scores will be so large that we won't be able to represent them with floating-point precision. Alright? So when the score of a class is very large, each be AIX, may be very large, and that will cause numerical overflow of my five 35%. So one standard way to address this is to normalize the softmax functions by just subtracting a constant score from everything. So in this case, subtract the largest score. So I would do -500 so that the scores shifts is zero, -100 -50. And now if I exponentiate the scores, I won't have overflow. Calculating the softmax on this vector. And this vector will give you the exact same probabilities. And I will leave this since it's a very simple exercise to you all to see that this is the case that you could take your score and just add some constant to it, and they'll give you the same softmax probability. Alright? So usually, again, what we do is we take our scores and then we'll subtract the largest score from every single element to renormalize all the scores back to the same song, hence probabilities. Alright? Any questions there? All right, so there is one more classifier that we used to teach in the past called the support vector machine. And it's going to support vector machine uses something called the hinge loss. This was back in a time where maybe you might want to put an SVM instead of a softmax classifier at the end of your neural network. But these days everyone puts a softmax classifier that record. So this is relevant. That said, we've still putting these slides. To it in case you were curious to see how we teach the support vector machine and the loss function, but you won't be tested on it in an exam. Homework number two, we are going to have you play around with this hinge loss just to test your understanding of loss functions. And if some of the operations that we've talked about, but we won't test you on the support vector machine on exams. Let me scroll. Keeps going through these. All right. Okay. Any questions on softmax? Alright, so we're back to this place where we have a gain or loss functions. And we know that the parameters of our softmax are going to be that the parameters beta will be the big matrix W and this bias vector B. And we want to optimize to choose w and b to make this loss as small as possible, right? So now the question is, how do we find the setting of w and b that make this loss of small as possible. And then this will be now or refresher on gradient descent. And an after that we will get to neural networks. So this is the last from 146 are your first machine learning class. So you'll recall that in the first example that we did in class, we took a gradient. But the loss with respect to your parameters you did on homework number one to that case, you are able to set the derivative equal to zero and solve for theta. Alright? And the reason that was, was because for those problems, it turns out that if you look at our data as a function of theta, it was so-called contracts. There's one solution. And if I set the derivative equal to zero, I'm going to get the optimal value of Theta that makes that a smallest possible. In other settings, particularly for neural networks, the loss function is never look this nice. They are in general are going to have many local minima. So if you go ahead and you take this loss function, you take the derivative with respect to theta, you set it equal to zero. You're not going to get a closed-form solution because you can even just see from the image there are many such solutions where Theta equals zero and usually you won't be able to come up with something. In these cases which can fight for neural networks. We're going to get us to still get to the bottom of his class functions. And that solution is called gradient descent. So we want to minimize some function f with respect to our parameters Beta. Here we call that x. And what we know or at some setting of my parameters x. Alright. I can apply my function, a neural network or a softmax classifier. They'll give me f of x. We know from a prior lecture that the gradient tells me that if I change x by a small amount, in this case epsilon. If epsilon is sufficiently small, then the amount that f of x will change can be approximated to be linear. And that's going to be f of x plus epsilon times the derivative of f with respect to x. So this is a linear approximation. We'll draw this out in just a few slides. So this derivative, f prime of x tells me how I can make my f of x smaller. And if we are eventually able to reduce f of x such that f x is equal to zero, then we would have reached a stationary point of the loss function, which we'll talk about in the next slide. In this class, we're going to be interested in doing gradient descent, where our parameters are vectors or matrices. And we'll talk about, we've talked about some gradients already and you've done several of them homework. Number one, we'll have a few more rules that both arrive at the neural networks. Great gas a timely it's also raising the point that you might see in the blue case where you have one global minimum. But you may still need to use gradient descent because you may not be able to get a close form solution by setting for DLD beta equals. Alright, so to do this, we're going to use gradient descent. First. Some terminology that I hope is a review for all of you. If we have some parameter, I'll call this w1 for wait one. Alright? And our loss function looks something like this. The global minimum of this loss function corresponds to the value of the loss that is the minimal over every selection of every potential value of w1. So there's no point at the blue curve that goes lower than this point, then we would call this the global minimum. Will also have local minima. And these correspond to places where the derivative is equal to zero and the curve. The curve has a second derivative that is positive. And at these points, local minimum if your app is such a critical point. But the value of the loss is violet and a global Min com, alright? And then there are analogous definitions for global maximum and local maximum. Now lastly, there are things called saddle points. These corresponds to points where the derivative is equal to zero, but it isn't a minimum or maximum. So this point here, the derivative is equal to zero. If I increase my way, I will lower the loss function. And if I, I'm decreasing my weight, I will increase the loss function. So that's called a saddle point. Any questions there? Hopefully review for everyone and every call them. But we also have defined this gradient in lecture two, where the gradient of f of x with respect to x is going to be a vector that's the same size as x, right? Where each element is D f of x dx with respect to that, with respect to that index offense. Alright? We all talked about how if I were to move x but still not delta x. So let's say that delta x is comprised of these small changes, delta X1, X2 down to delta XN. Then if I know x results in a value f of x, then if I were to add delta x, Delta x is sufficiently small, like first-order approximation of how it affects what change is going to be given by f of x plus h transpose the gradient x. Remember this term here is a dot product that's doing delta X1 times d Fx dx1 plus delta x2 times d f of x dx to saying, I'm going to wiggle x one. This is how much I expect f of x to wiggle. If I change x one, I'm going to sum this across the contribution from x1, x2, etc, all the way up. Got any questions there? All right, and then what we're also going to do is we're going to do fine something called a directional derivative. This is how visible we're going to use to derive the gradient descent rule. The directional derivative is going to tell us the direction, or it's going to be this quantity, U transpose gradients. Alright, so this term looks very similar to this term. The only difference is that the directional derivative has a unit vector u whose norm is equal to one. And so it's saying essentially if I stuck in this direction, how much do I expect my loss function to change? Apply? Any questions there. All right, so if I want to make my f of x, which in general against the loss function as small as possible. What we will want to do is we want to step in directions that make f of x smaller and smaller. I remember when I step in a direction, you write the value of f of x I expect to change by U transpose gradient. Alright? So what we can do is to minimize f of x. We wanted to find the direction to step in to make f of x decrease the fastest. So the amount that will change is given by this expression. That's this term right here, with delta x replaced by you. And I want to choose the direction you to make this as small as possible, meaning that my other vaccine is going to decrease as much as it can. If I go ahead and read that this dot product to be equal to the norm of u times the norm of the gradient times cosine theta, where theta is the angle between them. Alright? Because you have norm one, this is just going to equal, there should be agreed norm sign here. This is going to equal the norm of the gradient times cosine Theta. I remember, I get to choose a direction you to make this as small as possible. This first term doesn't have a u. So all I can really control is cosine theta, which is the angle between u and by gradients. And this quantity is minimized when you points in the opposite direction of the gradient, is that cosine theta equals minus one, right? So this is a rule that for gradient descent, if I want to make my function f of x becomes smaller, I will watch a step in that direction. I will watch it changes direction as opposed to the gradient, which means I take my gradient and I multiply it by minus one. This gives us our gradient descent rule, which is to say I'm going to take the gradient on my function f at some location x. And then I'm going to change x by subtracting off. Some small number times the radius. One is called the learning rate here, and we'll talk about it in the next slide to talk about having to use their questions. Right? Yeah, so I should have actually changed this. X here is analogous to theta. And the vector is analogous to our loss function. Sorry, sorry. Oh, yeah, sorry. Rocks at this saying this Beta is not the same as district or the state of carrots angle between the previous night. Tomboy is asking the question, why did I get this optimization problem of I want to choose a direction that minimizes this, right? So remember, my goal is, this is a loss function f of x. I want to make it as small as possible. So if I change x by Delta x, my first-order approximation, I expect Exit changed by Delta x transpose gradients. Alright? So I wanted to, f of x is smallest possible. I want to make this thing here as small as possible. I can make these things as small as possible by making delta x smaller norm. But what I really want to know in which direction to take a step into make this thing as small as possible. So that's why we constrain the spectra you to have unit norm. So we're just trying to derive if I were to go in any direction, what direction would be best. And so the direction would be best mixed epitaxy small as possible, which means that this number is a smallest possible. And that's why we end up minimizing this expression. Find the derivative of a scalar. Yes, it is, yeah, So the loss function L of theta will be a scalar. And then our parameter, such as like the vector w1 will be a vector. That's correct. Right? I'm a student. I think you're replacing the point that direction that would in which you descend the fastest may not be the best direction to go into. Yeah, so there are practical considerations here. We're going to get to that when we talked about optimization for neural networks. So let's talk about different optimizers that needs momentum and historical gradients. But for now, we're going to just consider the simplest form, which is if you just ask the classic, if you just have gradient descent and you can stop anywhere, What's the direction that gets me to increase the function the most? Oh, yeah, great. So the student may be referring to oversleep. We'll talk about that in just a few spikes. Other questions. Okay, So we're going to just show you a few examples. So here, let me change these two losses. So let's say that X and Y were two parameters, W1 and W2. And then G here was my loss function as a function of W1 and W2. And so this would be a setting where we have a loft surface and our goal is to change W1 and W2 to make this loss as small as possible. And the lower you are, the more blue you are. And there's a global minimum which is given by this star here. So this is just a very simple contrived example to give you intuition. This is a view of the same function just looking from above. And now we're doing a contour plot. So this here would be W1, this y here would be W2. And then the contours correspond to places where the value of the loss is the same. And then the color blue is a better loss of lower loss, and red is a higher cost. To implement this gradient descent and by conflict would be very straight forward. You will do it in homework number two. There's a bunch of code here, but it's actually mostly just for saving values of things. There's going to be essentially in homework two, some function that you implement. This function is going to be softmax, loss and grade grad gradient. Basically, this loss ingredient will compute both the loss function of the soft max that we just derived, as long as it's gradients with respect to your parameters w. And so this returns the loss at setting of W as well as the gradient. And then to update w, you would just subtract off epsilon times the gradients. So that's all. These two lines here. Implements. So it's a very simple algorithm. I have two videos that show stochastic gradient descent where this example that we have, the loss global minimum is at the red star. And in this video, what we've done is we've started off in this bottom right quadrant. And what we're doing is we're continuing to gradient at every single point. Something negative that gradient direction. And you can see that over time, although slow, it'll eventually get to the red backs. Alright, here's one more example where I'm going to start off the optimizer somewhere else in the bottom left quadrant and just follow the gradient of the contour lines. And we'll slowly through gradient descent to the global minimum or a local minimum. Any questions here? Yeah. Great. The question is, how does gradient descent avoid local minimums? It does not. So this reminds me that there's usually an intuition that I gave on the slide that I skipped over. When we optimize neural networks are going to be using stochastic gradient descent. And gradient descent. Once you get to a local minima, right, you just stay there. Stay there. Then there's a chance that for neural networks, we may do gradient ascent and arrive at these local minima that actually have high bosses. And if they do, then we haven't learned a good setting of the parameters. In practice, neural networks usually do really well. Alright, so what's an intuition for why this might happen? One intuition to carry with you is that neural networks have a ton of parameters, but let's say we are looking at neural network with 100 million parameters. Alright, here, and this loss function, I've drawn just one parameter, w warn. Alright? But you could imagine that there was a second dimension, W2. W2 was along here, and that this was now that there was also values of the loss function along other dimensions. So maybe along W2. The loss function looks like this. So if you look at this example just going from one to Mexican see dimension. What we see is that this point here, It's no longer a local minima because even though if I change w1 only, the loss will only increase. If I change W2, there's a dimension in which my boss can decrease further. All right, with two-dimensions. For neural network, there are 100 million for ambitious e.g. what that means is when you're at a local minima across 100 million different search dimensions, there is no dimension that you can step in that would make the loss even lower. And that's pretty bad because with 100 billion stars interventions, there's probably some dimension that you can go and that will make a hospital or an order. So if you just do this very simple approximation, Let's say that for every single variable at a saddle point, there is a coin flip at 50, 50 chance that this variable moving, increasing, it will either decrease or increase the loss. If you have 100 million parameters, then the probability of you finding a local minima where there are low directions. You can go in that increased the loss becomes exponentially small. And what that means is that in neural networks, it is very rare. It's exponentially rare to find a local minima with high loss relative to the global minimum. Or said differently in neural networks, most of the local minima ends up being close to the global minima. Again, because of local minima occurs, but you can't search one of 100 million connections for another way to decrease your loss further. In that case, actually having a lot of parameters is a good thing. Any questions there? Great. Question is, does gradient descent ever stop or does it just get closer and closer? I mean, basically it will just make it into somewhat velocity decreases are decreasing by very much. Yeah. Yeah. So the question is, is it just one loop that runs forever? We will either set some number of max iterations or else we'll set some tolerance that says if the loss hasn't changed by some small amount, sorry, the loss change is less than some amount. The determinate gradient descent. Yes, it's always saying that in this class practically, you likely won't even be implementing some of these checks because they could be more time-intensive, will just stop after some number of iterations. Okay, let me get back to gradient descent and I want to do one more thing to talk about what the next step size. So again, hopefully review for most of you and I want to draw this picture just to make sure we're all on the same page on what's happening in gradient descent. So let's say I just have one parameter w and I have lost Alec W. And then let's say that I lost surface looks like the following, like this. And then let's say that my additional parameter setting is this value over here. So let's say that this is my value of my weights. I'll call that w one because it's the first iteration. So at w1, the value of my loss function. So this here is the value L of W1. Gradient descent says take the derivative of the gradient. So the gradient at this point would be the line tangent to this curve. Let's for simplicity, just say that this curve here has a slope of minus one. So the gradient of w with respect to W is going to be equal to minus one. So this is just a line of slope minus one. This is a line of slope minus one. Then what gradient descent tells me is that if I want to update to get the value W2, that's going to be equal to w1 minus epsilon times the gradient. And the gradient is equal to minus one. So in this case is just w1 plus epsilon. Consider 2 s. The first is that epsilon is big. So let's say that I choose epsilon so that one, so that W2 is equal to epsilon, so that now W2 is equal to this value, right? Because W2 is a gradient descent will be w1 plus four. If I choose a font to be large, the amount that I expect my lost to have decreased. It's gonna be given by this difference. Alright, so this is my expected decrease in l of w, right? From my linear approximation using gradient descent. However, because I took a step too large, but as the actual value of LOW, it's going to be equal to this value over here. So if we take a really large step, we're in bad shape because my approximation no longer holds. It only holds locally. So I could be in a totally different part of the loss function. And then here I have emerged to take a gradient, right? This would be a very high slope area and you can see how they're intersecting could go wrong quite quickly. Then of course, the other option is to choose an epsilon that's relatively small. So we can choose epsilon small, which is epsilon small. Maybe epsilon is now this big. So this is my new epsilon and the purple setting and this would be my new W2. Then my loss at this point would be relatively closely approximated by. Okay. So any questions on that example? We just a nice refresher on how gradient descent works. What we could do is we can look at that example that I showed you a video of. And I could set epsilon many different values. If I said x quantity larger 0.1, we can see that actually here in this case, we're gonna be starting from the top right over here. When we take us up to 0.1, we stepped too much in this direction. We go to a region where the gradient is very high. And then with this far step size, we actually suck off the page. And so gradient descent but not converge for this choice of epsilon. If we make epsilon very small, we, as in this gold color, we go pretty directly to the global minima. And you only comment would be that because epsilon is so small, we have to do many, many iterations of gradient descent, so it's going to be slower. Then there might be some intermediate values like 0.08. And this is what tomboy and the students are there customer mentioning which is 0.08, you know, you do, you do eventually get to the minimum. But because it's effect size is large, you might have some zigzagging phenomenon as you'd get there because you keep overshooting a bit in different directions. So in a later lecture when we talk about optimization for neural networks, we're going to talk about techniques that help to ameliorate some of this zigzagging. But at least for now we're just gonna go with vanilla gradient descent. The question is, if epsilon is small, does that mean that we may need more training data? I don't believe so. I think if epsilon is small, it's just the case that you will take up various though we're looking at. Alright. So why not always use smaller learning rates? This is the part that says the learning rate with a smaller on the left and larger on the right. And the amount of time it takes to reach the minima is going to be a lot longer for larger, a smaller learning rate. Then how do you interpret or how do you choose the right learning rate? This is largely an careful exercise. The axon will be a hyperparameters. These are some example characteristic curves that you might want to add phosphate you do optimization. If you see the loss may be decreased or maybe it doesn't decrease but it explodes. That's a setting where you set epsilon to be too large and you definitely need to decrease it. You see the loss decrease very quickly, but then tactile. After just a few iterations, then you probably set the learning rate too high. And what this could reflect is that the loss faced similar because you took a step in the correct direction, but now it's like yours zigzagging or rapid County. And you're never really going down to value this zigzagging around a similar level of the gallery. So this corresponds to a high learning rate. You want to decrease it by more. If your learning rate is just for losses just decreasing to slowly, then it's probably to vote and why go somewhere intermediate where the agreements, alright, so that's just all heuristic at something to keep an eye on as you are because you're looking at these loss curves here. Sorry. Can you repeat the question? Right plane along the LV. Even knowing that, what do you mean by moving along the curve? Okay. Yeah. Yeah. So rupture the saying. And we'll talk about this when we get to the optimizer structure. Instead of a first-order approximation, you could do something called Bike. Their second-order approximation. We're just approximating the parabola that fits the loss curve over here. This is something called Newton's method or you can step, you can take. This leads us to able to take more livestock sizes and not have to worry about epsilon. But we're going to see that this has some challenges because it is going to increase our computational time by a lot. So in general, we won't pay for it. Alright? Any questions on gradient descent? You raise your hand if you're following. All right? Okay, So I just have some last things to say about gradient descent. First is a question for you all. When we asked you to implement gradient descent. On homework number two, you are going to lead to compute the actual derivative, the gradient of the loss function of the soft mask with respect to w. And several of you is going to be something that takes some time. You have to do a bunch of algebra to actually derive the gradient. There's some of you in the context of thinking about this. They think, Okay, I can compute an analytical expression for the gradient, or I could just actually using numerical gradient, right? I could, on my computer, take my function f, my loss function. I could just wiggle theta by a bit and calculate a numerical gradient and use that as the gradient that I used for stochastic gradient descent to the correct direction. Alright? Why is this not a good idea? Parameters. Perfect gas. So the student says, when you have millions of parameters, you have to assess this numerical gradient and millions of dimensions. That's the main hurdle, which is that to do that will be computationally expensive. And there's another factor here, which is that whenever you evaluate this, you have to evaluate the loss function. And at that loss function, the loss function for a neural network. Evaluating S means that you have to evaluate the output of the neural network. And that can be expensive. So evaluating may be expensive. So for this reason in the homework we're going to ask you to actually compute the analytical gradient. Then you're going to actually write code that implements what you could do it on paper. You won't always have to do this. Later on, after the midterm, we're going to talk about software packages like quite fortunate TensorFlow. And really there. What are their key capabilities is that they can automatically take readings for you. But at least to begin, we're going to be doing these gradients by hand. All right. Any questions there? May be expensive. Yeah. Ok. Involved eventually. Yes. A tomboy saying that when we do gradient descent later on with back propagation will still have to make evaluations through the network. Number two here is tied to number one, which is that if we have millions of parameters, we'll have to evaluate this numerically for each of those dimensions. And that'll be many, many more evaluations of that. Alright. So a few less points on gradient descent. So in this example that we've shown you the videos on renew the loss function F Exactly. And that's like every point in space we can calculate the gradients exactly. But in terms of building networks to classify CIFAR ten, we are going to be differentiating or loss function with respect to the parameters. From these loss functions with respect to the parameters will be a function of training data. You'll see that when you work on homework number two. And therefore, the gradient that you actually stuck in will depend on the actual examples for training them to use. And therefore, you can think of each data point is providing an estimate of the gradient. So then the question becomes, what kind of data to use to estimate the gradient. So there are three distinctions based here, although we generally will be focusing on the middle one here. So let's say that we have EM images and see ***** can, one way that we can compute the gradient, but say that n is equal to 1,000, is that we could compute the gradient by using all 1,000 examples to estimate the gradient. That's a batch algorithms. That means using alter data to get one gradient. But you could also estimate the gradient from a subset of example. Let's say you have 1,000 examples, but you're going to estimate the gradient from just 50 of those examples. Alright? That will allow you to compute the gradient more quickly because you're using your examples. However, because you're using your examples, the gradient will be noise here. Alright, and so this is called mini-batch gradient descent. There's one more thing called stochastic gradient descent. And actually formally when it was defined, this corresponds to using just one example to compute the gradient. But in general, we always use mini-batch gradient descent, and we call it the classic gradient descent. So whenever you hear someone say stochastic gradient descent, they're usually doing Mini-batch gradient descent and they'll specify what that is. So don't call this the castle. That's the most common thing to do because and territory. One interesting thing is that even though you think mini-batch should do worse than batch, because they're using fewer examples and therefore your estimate of the gradient is first, it actually turns out that when you have Boise gradients, this will provide a regularizing effect for your algorithms. So actually minibatches faster and oftentimes will generalize better. We'll talk more about regularization, a few lectures from now, but that's something to keep in mind so well, almost always to mini-batch gradient descent. Any questions here? The question is, let's say k equals 50. Well, we want five images from every class. That would be the best stuff, that'd be the best thing to do as opposed to like jabbing 50 images of dogs and you might have a very poor some of the other questions. This is our last slide on gradient descent. So any questions on gradient descent in general? Where do you have this randomly sample? From positive integers? That's correct. Yeah. The first one. Yeah, that's good. So Tom way is asking, when I sample from this 1,000 on the next iteration, my sampling 50 again, so that there could be repetitions using what will happen is you want to make passes through your dataset. So you will start off with 1,000 examples with sample a mini batch. If you take a greedy this next step, you're left with 950 examples. Then you would sample 50 of those times where you would go through every single data point once before you then repeated. Alright, so that's gradient descent and, um. You will be implementing this in homework number two. There's going to be a lot more to be said about optimization. And so in a future lecture, when we talk about neural networks, we're gonna be talking about first and second quarter methods, momentum, adaptive gradients to everyone to see that this is going to help a lot when we get to the implementation of gradient descent for neural networks. In the remaining 4 min that we have, I just wanted to intro how a neuron works and then we'll talk about them in neural network architecture on Wednesday. The reading for this lecture will be Chapter six of the deep-learning. I just want to talk about how a neuron works. I've shown you here is an image of an actual physical neuron. So what we are doing is in this image where the sending it an electrode and actually listening into the activity of that neuron. You can see the neuron is connected to all these other neurons as well. And the neuron has several components, but I want to just focus on a few for the purposes of how to build connections neural networks. So the first thing I want you to pay attention to is that the neurons have these called dendrites. And dendrites are the inputs of the neuron. And the dendrites will correspond to these arborist regions of the neuron. What happens is that at these regions, there are other neurons that are connected to these dendrites. So this is an upstream neuron. If the upstream neuron has a signal, it will convey that the dendrite, and there are dendrites all over the body of the axon, all over the body of the neuron. So these are where input signals are taken. There's also something in the neuron called the axon. The axon here corresponds to this long tubule structure of the neuron. You can think of this as the output of the neuron. So you can see that in this image, what I've drawn is that this axon takes a signal from the neuron and it actually connects to the dendrites of downstream neurons. So these are three downstream neurons and the axons dendrites. There's one more component. I'm going to label this with the number 2.5 here. And this is called the axon hillock. The axon hillock is over here. And the axon hillock is the integrative part of the neuron. So what do you can imagine this is the axon hillock does, is that there's all these signals coming from the dendrites and what they do, they propagate down the neuron until they eventually hit the axon hillock. Let's say the signals where I'll call them like Z1, Z2, Z3. A very simple version of the axon hillock is that it implements a function f. And f songs all of these inputs, Z1, Z2, Z3 is bigger than the threshold, then the neuron generates something called a spike. So if f exceeds our threshold, then it generates a signal called the spike. And so it fires a spike. You can think of this as sending a signal one. And then if f is below a threshold, then nothing happens. There's no signal feminine, so that's like a zero. And the signal will be sent down the axon and then conveyed to other neurons. Alright, so when come back on Wednesday, we'll recap this and then we'll go into how to control my forgot. Let me first. 
All right, everyone, We're gonna get started for today. So a few announcements before or the day. If you are an MS Online students on good learned, we just such an announcement maybe an hour ago about the exam time. So it's going to be Saturday, February 25th, 1-250 PM. So if you're gonna sound like to me that you didn't see this massive. Please be sure to check through him as a reminder that homework number one is I'll put it in there and it's going to be due this Monday. I'll birth to grade scope. In homework number one, there is the coding components. And so to submit the code and components, please be sure to print out your Jupyter Notebook so that all of your code is because the auto-grader and the grader will look at that. Alright. Also had their solutions and Clarksville better. Readers need to see your code to give you credit for your car. So also in future assignments, you'll be editing d2y files and for any files. And yet it felt to my file to also have to print to PDF that codes of the Greater can see any questions there. Alright, so on Wi-Fi and how it affects the recordings. So we saw on Piazza that some of the Zoom recordings may not be totally reliable and that's probably because of the Wi-Fi issues that we couldn't happen. Today. We're having our MSM one TA can join the room, both be reliable Wi-Fi connection, hopefully the top of the quality. But please also remember that crew and cast upwards lectures. They're under UCLA to get reserved some Boomer. And those are also, we ran discussions for the first time last Friday. And for discussion once c, which is one of collides discussions, there is no reliable Wi-Fi connection in that room. And so for discussion once C, we will not have a simultaneous Zoom meeting for that. Any questions on this or discussions? Okay. And then the last comment is that we'll go to discussion video regarding the coding part of homework number one and that will be posted this Friday. Alright, any questions generally about logistics for this class? Alright, we're gonna get back into material. So last lecture we were talking about how to assess whether a model is overfitting or underfitting. So this is recap. We talked about this procedure called k-fold cross-validation. Where what we'll do is we'll start off with our original data, right? The first thing that we do is we split the original data into a testable than extreme fold. The test bulb is set aside pristine dataset that you clarify once at the end to score your model. Alright, but then for the remaining training data, for 24 fold cross validation. And what we've reduced, we would split it into four segments, all of equal number of trials. And then we would designate three of them for training and one of them for validation. Validation is to optimize our hyperparameters. Remember that these are the choices in the discipline, the algorithm that you as a designer get to choose beforehand. And what you would do is you would train your model on the green training folds and then you would evaluate its performance on the yellow withheld validation fold. It gives you a validation accuracy to score your model. And then you can keep doing that for different sources of hyperparameters. And then you can also switch which folders, the validation fold and which folds of the training. Just to make this as concrete as possible. Here's an explicit example that will happen in your homework. So resting for ten data set, which we're going to talk about today, is a dataset that contains 60,000 images. There are ten classes, so depth ten different categories of images and there are 6,000 images for each category. What we're going to do is with these 60,000 images at the onset, we're going to set aside 10,000 images, and that comprises the test set. And we will never touch this test set until one time at the end, when we overall score how good our model is. Right? After setting aside these 10,000 test set examples, we still have 50,000 images. What we use for training and validation. So let's say that instead of four fold cross validation study into 4.4 holes without you fight for cross-validation. Five-fold cross-validation is put the 50,000 images into fibrin. So each fold contains 10,000 images in five-fold cross-validation, one called this validation. So one fold is my validation dataset with 10,000 images. The remaining four folds are what I used to learn the parameters of my model containing the 40,000 images. Then I can repeat this training process five times, each time swapping out, which is the validation. And then we can average those five validation accuracy to determine the best hyperparameters. After you do that, then you can train one more time across all of your images for training and S-Corp once on your test set model question. The question is, is it just a coincidence that the test set is the same size of the polls that we used for training and validation. In this case, we chose them to be the same. Oftentimes, the test set should be relatively larger. So it makes sense to make it similar to an old size, but it doesn't ask the question. The question is, when we do five-fold cross-validation for each of the hyperparameters are going to have five validation accuracies. So then how do we choose the best hyperparameters? It's not a question, right? So if you are doing, actually I had this example on the next slide. So this is code that will optimize the polynomial order for our linear regression example. And you can see us a polynomial order gets higher and higher. The training set error will decrease like we talked about last time. But when you average those five validation accuracies for each polynomial order from order one to order five, you can see the accuracy, sort of validation or the validation set error increases, which is bad. So in this case, you would choose that you would use a first-order polynomial. It's the best performance of the other questions here. Wonderful. So time-wise, making the point that once you split the images into folds, those folds, you shouldn't shuffle them in between repeated validation. Otherwise you're going to be, you're going to have data-set week where across different validations you're gonna be using the same data for training or for validation. Alright, so that's cross-validation. You're free to take a look at this code, which is what you'll also incremental hallmark number one, that we'll do a cross-validation for our polynomial example. Like we were just saying. If you do to compute the best hyperparameters using the validation set with health data. You'll find that for the data that we were showing you earlier for the linear regression example, is best to use a linear form. Or one thing you need to talk about. Each of the validation sets just not profitable. Yeah, you're welcome to shuffle. What does he would say things, but then you can shuffle your examples. But you shouldn't change the examples in these folds across each cross-validation. Question is, can I explain what the difference is between the left and the right click or yes, the left finger. Sorry, I went through this quickly, is training set error. So this we should expect to get better and better because you'd expect lower error as you increase the polynomial order. Because you can use the expressive, the higher capacities at the data points better. But then this one here is validation set error. So this is how it generalizes to new data that was not used to learn the parameters. Just to keep the colors consistent. I think the training set we were doing in green and validation set we were doing in yellow or orange. Right? Any last questions on this simple example? Yes, this student is saying, just to clarify this secret clarification, we tested 25 models here, and that is correct. So every single validation fold, and we didn't five-fold cross-validation here. So there are five folds. Folds. So we're going to have five validation accuracies. Per hyperparameter, setting of hyperparameter. And then here we have five study for the hyperparameter, which is the polynomial order that goes 1-5. And so this would be five times five, and that will be a total of 25 mortals. Alright? So hopefully again, these past few lectures, these, the last lecture was a review for most of you. If you haven't had a machine learning background, please be sure to become very familiar with these. Will still talk about some things that you probably learned in your prior machine learning classes. But now there'll be relevant for neural networks. So in this example, we talked about minimizing the mean square error. It turns out for neural networks, we're going to need to use different loss function. One that is based on something called maximum likelihood. We're going to talk about this later today when we talk about and derive the softmax classifier. The softmax classifier we're going to learn is the most commonly used output of a neural network. Alright, so we're gonna be going over the softmax classifier today. So this is the topic of supervised classification. The deep learning book, it will correspond to these chapters are reading. So today we're gonna talk about supervised classification. We're talking about two classifiers. They're gonna be k nearest neighbors, which will just use as a motivational example really. Then after that, we're going to talk about the softmax classifier. And again, like I was just saying, this is the output of most modern neural networks today that do classification. And the softmax classifier we're going to train using a loss function derived from maximum likelihood. And then the way that will optimize this loss function is with gradient descent. Alright? And so these are the next steps that again may be review for some of those who had machine money back down. But this will be basically the first component of our neural network, which is the output. Right? So the motivation for this is, we're going to talk about image classification. That's because computer vision is really the field that gave rise to this past decade revolution in neural networks. And like I've mentioned before in our homeworks, are going to be looking at the image classification, which is T4 tag. And so I want to give some background on why this is a challenging problem. So we look at this image and we can clearly see it's a cat. But what does the computer C. So if you were to lose this captains your computer, you all probably know that every single pixel of an image contains three numbers. One that tells you the amount of red, green, and blue in it. And these values for red, green, and blue are all numbers I can go 0-255. If you have sympathy for red, green, and blue, that means you have a lot of red, green and blue together. And together they mixed to become white. If you have zero for red, green, and blue, then you have no read it didn't prove the absence of color is black. So if we were to zoom in on this part of the cat right here and look at how it's stored on the computer. Because it's what you would see numbers that are really close to 255 across the swath of the cat. So the computer is essentially just storing numbers there at 02:55 to represent this cat. Zero to 255. Again, just for the RGB of which there are three for every pixel. And then there's of course, a width and a height of the image. So the images are stored ultimately as a 3D array or a 3D tensor. And each of those values in that 3D texture range 0-2. Alright. So this is gonna be the input data or your computer vision algorithm for your neural network. And there's clearly a semantic gap here because we looked at this and we think a computer looks at this and sees all of these numbers. And we have to go from these numbers to determine what makes a cat a cat. All right, Any questions there? Alright, so the image is sorted array of numbers. And so you think about this a bit. So notice that there are a lot of challenges. One of the challenges is viewpoint variation. So these are different at the same object, which is a statue of David. And even though these are all the same object, if you look at what their numbers aren't any period, they're very different. So we just take the bottom left corner of this viewpoint. All of these values are close to zero because the image is comprised of black. Whereas if we were to take a look at the bottom left for these images, and the numbers will be closer to 100, 1,600% because it's not black. Alright? So just even a different angle can dramatically change the values stored for that condition. Even though they're all the same object, you point variation become awesome effect in the form of CO2. So just like the last thing WE showed, same image just zoomed in. It's still a cat. But again, the values are very different. If you're trying to decode. And cats in general, they can come in different illuminations. And here you can see again, the pixel values will be as different as they can get. Because here we're going to have a bunch of 200s or the captain bright illumination. But for the cat and dark elimination is going to be cluster two zeros. Any questions there? Alright, so then at this point you might potentially, okay. Obviously we can't use just the absolute value of the numbers making classification. We can start to maybe in trying to think of features that define what makes it. So you might say, a cat has ears, too pointy ears, two eyes, and four legs. But then there are going to be images where there's deformation. And so you see an image like here. We see three of the lens of attack, but then they're all inter, twined, entangled. Even if you were to say, Okay, it can have two eyes and image can have occlusion. And so you might have a cat hiding behind. You're here and you might only see one I get for all of these, we understand that these are the care. They can be cluttered by background. I chose this image because this cat bears or as almost as if camouflaged and the trees, and the trees have very similar values to let the capitalists like. Then lastly, we want to know how does attack. But then in fact Clement all different shapes and sizes. And we would still want to be able to classify that all of these are cats, right? So these are some of the challenges that come with having a classifier that takes an image and is able to identify it as a cat or a dog or something else. Any questions here? All right, So when trying to classify the image, there are several approaches that one could be located. One of them is this one that I just mentioned, where perhaps we can define the features of what makes the cardiac cath. And I look for those features in the image. But then this is going to require a lot of human expertise, right? Or a human has to first determine what Mason Katic cat, and then design an algorithm to look for those features with an attack. And that overall is time-consuming and not the most efficient use of resources. So the other way is to take the data-driven approach where you kinda throw up our hands and we say, Hey, I'm not exactly sure algorithmically, what makes a cat a cat. But I wanted to train a neural network, a machine learning algorithm to do it for me. And basically in the process of training based off of what the data looks like, the neural network, the machine-learning algorithm is going to learn what are the features that allow you to classify data. So in this setting, just like we've talked before, we're going to be to train an algorithm. We train an algorithm in this approach, we're given data. That data will comprise images of cats, dogs, airplanes, automobiles. And what we will do is we will take this data and train a model that takes the image and reports the class. And so this will give us in the training phase model parameters that we learn from data. Alright? So for deep neural networks, we're going to find out that these parameters result in learning. The features. Let me remove, digest learning features that are optimized to classify an image. In other words, to say that this image of a cat is in fact the cat. We might not know what the speech or talk to the algorithm will learn. After that, we'll have then our test phase. So in our test phase, we're going to deploy this model. So we start off with our model that was learned from training. So this model comes here. And now from this model, what we can do is we can put in a new image into this model. And it's going to tell me what class of students that's going to tell me is this image of a cat, a dog? It's alright. Any questions here? Alright, this is all set up. So like I mentioned earlier, we are going to be using this department dataset. The dataset is going to comprise 60,000 images, each of them or 32 by 32 pixels. And therefore, each image is going to be. I'm a 3D tensor that is 32, 32 by three. And at least to start for this lecture, we're going to go ahead and we're going to just restrict this into a vector. So our image, which was 32 by 32 by three, we're going to shape into a single vector, which is 3,070, two-dimensional. Thousand 72 is 32 times 32 times three. There are ten classes. These are the tank classes with 6,000 images per class. And then we're going to withhold 10.10 thousand of them for testing. 50,000 of them will be used for training. And I think I was looking at the homeworks. Sometimes we change these numbers and this sometimes it's gonna be like a fauvism for testing and if thousand for validation, but in the home or school. And so again, the goal of our machine learning or deep learning, will be to take in the input image and then predicts the class. I'll call this y. And remember what that we've been using the superscript I to denote an example. So why I think sample is going to correspond to the label of that image and it's going to be one of these ten classes from airplanes. Got Dr. Chuck. This is a code that will provide for you on our comforts two to five. And this will go to S4. And like I mentioned before, we're going to be taking these 32 by 32 by three images. So with this mode, seek part-time scripts. It'll give you x train, which are the images. X train is going to be 50,000 images. You took them 32 by 32 by three. Y train will then be the labels of those 50,000 images. So y train will be a number 1-10 for every single example. And class one would correspond to their plane. The y was equal to two, of course, automobile, bird, et cetera. And then we'll have our test data and our test labels, and they will be in the same dimension as the suspect there attend. These will be input data to our training. The question is, what we refit that 32 by 32 by three tensor into a 3,072 dimensional vector, we lose the positional information. Are you mean like, like e.g. like facial correlation, immunity. But when we get to later neural networks, just to motivate the first algorithm, the softmax classifier load mean of vector input. Other questions. That's a great question because there's a lot of information that spatial, spatial relationship. Alright, so we are going to talk first about a simple algorithm that doesn't require machine learning called K-nearest neighbors. Who here has seen k-nearest neighbors before? Most of you. So this should be hopefully straightforward. And dental. A different approach, which will be our softmax classifier. Alright, so consider a setup wherever using the maximum, oh, sorry. We're going to discuss maximum likelihood classifier in this lecture. We're given input vectors. These are my seat bar ten images, each of them in our 30, 72. And then for spawning classes, so each of these y's correspond to a label of that. And that'll be some number 1-10 or maybe it's Sarah tonight. Let me do one to ten just for the sake of simplicity in the center. Now, with this, we can train a classifier. And our goal is to now be given a new data point x you. And again, sorry, ignore this part. The probabilistic model, we decide to remove this from the structure based on past chairs. Feedback. We want to do it in a way that doesn't, that is kind of intuitive. And so that'll be something like k nearest neighbors. And intuitively what K-Nearest Neighbor says is, I can look at my new data point and look at the labels of the images that are closest to me. And then I'll think of both. And that'll be what I guess my images. Let me draw this out. Let's say that we had 2D data. So our examples at xy are all two-dimensional. And then this will be the first dimension of x. This will be the second dimension of x. And then let's say that I just had two classes, so I had cats and dogs. So let's say that orange circles are caps. Then we'll say that green X's are dogs. Right? So maybe my examples of each of these is a different example in my dataset. Looks like this, and then my dogs look like. So now if I were to give you a new data point that we have never seen before. And I put it, are the x values of this data point right here, this red square. And they asked you, what is the class of this new data point? What boat I could take it to be, you would say is probably going to be a dog because the things that are closest to it are all dogs. So K nearest neighbors formalizes this intuition. What we do in k-nearest neighbors is we set the hyper-parameter k. K is the number of nearest neighbors we're going to just consider. So let's say that k equals five. What this algorithm says is take my new data point and look at what my five nearest neighbors are based off of distance. And then whatever the majority or the plurality of those neighbors are, is what my class is going to be. So e.g. if my x nu was over here, alright, I will look at my five nearest neighbors. And it would be three cats and two dogs. And because of cats have three and salts have to then either classified that this point here is a any questions there or even any tenures neighbors. If you have a tie, then you can just choose randomly one of the classes that was time for the most votes. So more formally, and K-nearest neighbors, what we do is we choose a distance metric. Because ultimately we need to find our k-nearest neighbors. So we need a distance metric to assess what the distance is. All of my other data points, most commonly, we're going to use the 2-norm between two vectors, XI and XJ. Although in homework number two, we'll ask you to try also the one norm and infinity norm. Yeah, the question is on the slide, are the values X1 and X2. X1 and X2 will correspond to my example x psi. What are the two values there? So e.g. we're going to have some number of data points where each data point, data point is going to be X-i, Y-i. But then for this constructed data and I've just defined excited be some 2D vector. And maybe the values are 1.0, 0.5, which means that for this example, I would recite it one on x coordinate 0.5 and x2. Yes. We choose the values of K so that we never have to write good time, e.g. but the thing is if we have more than two classes, it can be like, let's say, added cats, dogs, and horses to this my nearest neighbor. But if you'd like to cats, to dogs and then one course and then in that case you just choose cat or dog. The question, yeah. The question is, if you make K larger, will that lead to a better classifier? I'm weren't accurate classifier. There isn't. Making cave bigger in general will help to ameliorate overfitting. But the optimal value of k will not be the biggest value of k. And that's something that you will find k-fold cross-validation. I will ask you to do that on homework number two. The question is, would you say is common to treat the distance metric as a hyperparameter as well? Yes. So in homework two, you will get back. Actually, I have a slide later on that was asking that question. Oh, yeah. Soon as asking. It could be that you have an example where let me draw this out. Maybe there's an x over here. And maybe we're doing like, let me make this. I'll do some different color. We'll call this threefold and purple. And maybe you're new example is, is over here. So in this example is much closer to the dog, but then the CAT score when by boat. And so in three, not threefold, sorry, I'm three nearest neighbors. K equals three. If you're doing a three nearest neighbors, that purple square will be classified as a cat. You can make modifications on this algorithm to, instead of doing the neighborhood kids like compute the distances, e.g. we're going to talk a bit industries size. Why not do it? Right? Perfect, Yes, a tomboy is raising the point that in this case, you actually probably want to guess that this is a cat, because in this green x here is an outlier because the screen x is very far away from the other dogs. Alright, last question. Yeah. Yeah. Wonderful. Yes, I start to write that L2 distance. In this case, this would equal the square root. And so x is a vector in RN. This distance would be the sum from k equals one to n. And then we're comparing example XI and XJ. We're comparing all n dimensions between them. So this would be x k minus x j k. This whole quantity squared. Sorry, I, right, if it's small because I'm at the edge. Standard Euclidean distance. But again, in homework number two, you'll try also the L1 norm and the infinity norms. We will also choose our nearest neighbors k. Again, notice that for the distance metric and the nearest neighbors, I said cheese, right? And so these are hyperparameters. You get to optimize over. We take our new data point, which was that red square. And we calculate the distance between that red square and every single data point that I have. And then we choose the k nearest neighbors, which have the case smallest distances. And we choose the class to be the thing that went for plurality vote. So the class that occurs most frequently amongst its nearest neighbors will be the class that we got to guess. And if there's a tie than any of the classes can be selected. That's the formal description of what we just described an image, the last slide. Any questions? Yeah. Right. Yeah. The question is, in the case of a tie, is fair randomization to choose. The time. That tells me what I just said. There were four pockets and I withdraw uniform random variable amongst four. Okay. That's great. Yes, it's one-way says that it's sometimes the case of ties. Instead of doing randomization, you can also use the actual distances to break the time escalate idea. Alright, so here's example dated at this time we'll have three classes, the green triangles, the red Xs, and the purple circles. And we're going to build this classifier. Alright, so the first question I have for you all now is how do we train the classifier? I'll give you all 15 s to think about it and then I'll ask someone for an answer of how to find the classifier. Hoping to talk to neighbors to the lungs. All right, It's disgusting. Someone told me how to train this classifier. Wonderful. That's all come back. So I just asked, how do we train a classifier? And Jacob answered, all you gotta do is save the data, and that's correct. So you were to make a k-nearest neighbor class? I need to classify by just comparing. You did appoint all of my existing data points. And then I also have to know the labels wise so that I can do the book for K Harris papers. Alright, so this is all the training purposes which is to memorize your data. Alright. What are the pros of this was 200. So it's the book. And it's also fast, right? Presumably already in memory. So you just have to make a pointer to that memory. Someone told me what a con of this training approaches as slow as well. In what sense? The testing us though, yes, and we'll come back to that. Wonderful. Yeah, so it's memory, and especially because I'm gonna write memory intensive, as your dataset becomes bigger, you have to store more and more examples. And in general, if we have large datasets like ImageNet, right, that pelvic gigabytes and that will not be a wise store. So it's memory intensive because we need to store all the input data. This will be in contrast to machine learning algorithms, where we have a predefined set of parameters. And all we have to do is store those values of Theta. So keep that in mind when we talk about the softmax classifier. How do we test the new data point? So let's say that we are working in R2 just to have a concrete example. So our data points that are two-dimensional, well we would do, is we would get a new data point by x tests. And then when I calculate this distance here, what I'm doing is I'm Kathy calculating the Euclidean distance between X test the new data points, and self-talk x train. Remember x train was my, was just my entire dataset. So if you haven't used Python before, you've likely not heard of broadcasting. Even if you use Python, maybe you haven't heard of broadcasting. This is going to be really important to make your code more efficient. So let's say that we had an examples in my training set. Then train might be, wouldn't be then a two by n array. Then my new data point is just one-sample. Z-test would be a 2D array. When I calculate the two norm, I have to subtract from every single example in the training set. Alright? One way that you could do is do this is to write a for loop. Alright? But writing a for loop is going to make your code into walk longer. And the procedure of subtracting this 2D array from every single 2D array of which there and at them in X train should be something that you can do just like a carrier. If you come from that background and you know that the function you would use DSX fun. In Python, this will be done automatically through something called broadcasting. In broadcasting, what'll happen is if I kick self dot x train dot transpose, that makes this term here. And n by two array because it transpose is the array, so it's n by two. Then X test here is a 2D array. And if you do an operation like minus, and the trailing dimension of the first variable matches the leading dimension of the second variable. Then Python will know to perform this operation minus subtracting this 2D variable from each of the N examples in which each one is two-dimensional and self-talk extreme dot transpose. Alright, so this is a quick way where as long as the inner dimensions match for your operation, Python, we'll apply this operation minus for every single example in extra. And that saves you from having to write a for loop. And it's also a lot faster time. You'll use this in the homework. Any questions there? Okay, the question is, why is x trained to buy in to begin with as explicit and bite to? It might be more common. This was just a preference for me. When I wrote this code, I defined x train to have the Fincher is being the rows and the number of examples you can call it, but it could accompanied by two, in which case you wouldn't have had to have this dark transpose here. The question is, why is X test two comma empty as opposed to two comma one or one comma two. So in Python, it's better when things aren't bacteria to just define them as two comma empty. Because then Python will know to treat it as a column vector or a row vector depending on the context in which he sees. So it's more flexible to define it as two comma as opposed to two comma 12 comma one, then a has to be a column vector. The question is, when do you define it? You will define it to be at, to come after me. You could write it that way or you could just write an array. And bettering will be like an array with two numbers, then you don't need to cite to comment. That's right. Right. Tas? Far less than I used to when I was in graduate school. So sometimes, sometimes I had to check with the TAs for syntax. All right. Other questions. Yes. The question is, isn't this square root unnecessary? Yes. That's correct. Yeah. You could have used the two norm squared because you just care about the relative distances. That would give you the exact same answer. Alright, so this is testing the testing function for our k-nearest neighbors class. The pros, again is a simple. Let's go through these lines, but these lines will just sort the distances and then choose the k nearest ones and then do the plurality. What's the chronosystem? Our students already said it. It's really slow because whenever you want access to new data point, you have to compare its distance to every single point in the dataset. And that is going to be slow. And not only is it going to be supposed, going to scale with the amount of training data and not of training data. This is really bad if e.g. we're looking at real-time applications. If you're building a neural network that isn't a self-driving car, right? It has to be able to process the image. And it just is receiving a real fun and make decisions, right? And it cannot be waiting a long time to process a new image. X tests. Alright? So in general, for machine-learning, and this will motivate our next algorithm, softmax. We want to flip this. We would tolerate the training phase being very long. Instead of a second here, or like Toby, microseconds here, nanoseconds. We're fine with the training process being wrong. What's talking about neural networks later on, where even with many GPUs they take over three weeks to train. But then after you train the neural network, the testing phase is very fast on the order of milliseconds. And that's really important because then it can be used in real time. Alright, so we kinda want the flip the situation. K-nearest neighbors, sir. Any questions? Alright, so this is what the solution looks like for one nearest neighbor. So basically what I've done is in this code, I've iterated over every single point in a grid like fashion, and I just computed what the k-nearest neighbors are. And you can see that there are some of these islands over here, where from my outliers and X, these points are all nearest to actin, so they get colored red. And that can be a sign of overfitting. This is ameliorated when you increase the number of nearest neighbors to three. And you can imagine, again, interesting this further. Ultimately then the question is, how do you optimize these k-nearest neighbors? Well, it wasn't hyperparameter, as well as the distance metrics. So the answer to this question, what are the hyperparameters of k-nearest neighbors? They are K and the distance metric. And then how would you optimize them? Well, you would do that with k-fold cross-validation and go do that again in homework number two, you might have this question, or maybe you don't have to specify. What might you say nearest neighbors not be a very good idea for image classification beyond what we've talked about, about protecting time, being very long. High-dimensional data, there may not be explicitly boundaries between classes. Perfect gas and the students said they're collapsing into a vector, so busy spatial information. And then lumbar drain k-nearest neighbors we're measuring and the neighbor distances based off the values. We've already talked about how the pixel values aren't. Good enough semantic sense, because e.g. cat can be in a light image or an image or a dark image, and it would still be a cat. To that point. We have this image where we have an original images image. And then if we do three modifications on this image, what is boxing out? Part of the features shifting and another is just by thinking everything. All three of these images will have the same L2 distance to the original image. Getting again at this point back on pixel distance does not equal syntactic or semantic understanding. Alright? And then there was also a student who mentioned difficulties when it comes to high dimensional data. So there's something that you've likely heard the Machine Learning class called the curse of dimensionality. And when we have a C4 TEM image being a 3,070 two-dimensional vector. This is. A point in a 3,072 dimensional space. While k nearest neighbors makes sense when we're drawing into d, distances or intuitions that we have in 2D break down when we go to a very high dimensional space. The first thing about a high-dimensional space is that every single dimension that we add increases the volume of that space exponentially. So in going from 2D to 3D and 2D still have the same number of data points, but they are in an exponentially larger volume. And this means that there's a lot of empty space in that volume. The first thing is, it's implies is that the nearest neighbors may not be served there. Then within this really large volume, there are 3,072 dimensions that you can compute a distance across. Some distances may matter more than others. So our intuitions over distances of proximity in 2D and 3D really break down what we get. So high dimensional data, making k-nearest neighbors, not a great algorithm for high dimensional data. Right? Any questions? Right? Yeah, so the city is asking, can you do some feature normalization, e.g. to whiten the features so that the features are hopefully more uniform space. Still our intuitions of distance breakdown because we have soaking points in this really high, really large volume space. Mr. Mention that it's very small. Great. Yeah, so tomboy said, when you go to high dimensional space, because there's such a large volume, the variances between data points actually decreases. And so the distances become even less meaningful than they were with the word dimensional space. So these are just the intuitions. We won't test you on the curse of dimensionality, but these are institutions to carry with you guys to why. This is not a great idea for image classification. So perhaps a better way would be to take an image. And we know that the image and see if our time has to be one of ten classes. So what we can do some particularly image and then basically develop a score for each image onto a class. And so we'd have an image. Maybe the score dipping an automobile is ten for it being in the airplane is 200th birthday. Attack is negative five. And we would take the highest score, which would be for the airplane classy cheese. And we're going to do this base off of linear classifiers. We do this for two reasons. First, linear classifiers aren't major building block for neural networks and so it's important for us to understand them. In particular, we're going to see that each layer of a neural network because of a linear classifier followed by non-linear function. Then lastly, like I mentioned at the start of fracture, the classified ever going to talk about which is called the softmax classifier is going to be the most calming classifier at the end of the neural network. And so in any, in most applications where you are going to be doing classification, I drove the a softmax classifier at the output, and that's what we're going to derive next. Alright, so let's go ahead and take a five-minute break. When we come back, we'll do that softmax classifier, right? Right. Yes. If this is because of the plaque operation of broadcasting, because I want to take this subtracted from every single 2-dimensional. Yeah. So when you do security as a row vector apartment, just want also this one yet subtracted for every single example that I just had a question. Yeah. Terrific axes which are right back. Yeah. I see reconstructed warrants and the ones after that. Let me ask Tom way rocks get right now. Thanks. Hi. Generally there for these things. Hello. Alright, if I had a couple of questions, questions asking about podcasting. If you can hear me, you go. Student questions about broadcasting. So I want to say broadcasting is something specific to Python. So if you try to do that operation, that subtraction operation that we did in a different language, it would fail. The Python is programmed to know that, but the inner dimensions of your arrays naturally to do the broadcasting operation. Great. So tomboy says that the TAs are going to cover broadcasting, encoding video to go out, not this Friday but next Friday. Alright, so we're going to now do classical classifier, the softmax classifier based off of linear classification. And here we're going to imagine that we're still in the classification problem. So we have data is n-dimensional. So in C4, ten middle Andrew here and 3,072. And then we're going to have some number of classes. So first CFR, they're going to be the numbers 12 all the way to ten, because we have ten different class seconds. We're going to do is we're going to define a matrix, big W. Big W equals the spring. So big W is going to be c. C is the number of classes. So MC4. See what equal ten. By the snappy big and this should be little n. Little n is the dimension of my inputs. So what we're going to do is we're going to calculate y equals wx plus b, where b is a vector of bias terms. Alright, so let's just go ahead and draw this out. So we're gonna have a matrix W, W. I'm just going to copy down. It's gonna be w1 transpose, That's the first row of w. W2 transpose the second row of w all the way down to WC transpose the case of CIFAR ten equals ten. So this would be a ten by dimension of X matrix. So this would multiply x. This is w times x. That's this part of the operation. Let me just put the dimensions for C4. Ten would be ten. X is 3,070 two-dimensional. So this vector here is 3,072 by one. Why is gonna be a number 1-10? Actually, no, sorry. Why is not? Why here is gonna be a vector and arctan y is gonna be a vector in R2. So c is equal to ten for C4 ten, which means that this W matrix is gonna be ten by 30, 72. We're going to add that matrix B. So this is going to be b, and b is going to be at ten by one matrix. B is or ten also, there's a bias for each class. Then that's going to boom. When we do them this operation to wx plus b, we're going to get an output vector y. And y is simply going to be this vector of ten numbers, w1 transpose x plus b one, W2 transpose x plus b2 all the way down to WC transpose x plus b, c. And this is going to be a ten by one. Alright? And then what were bunch of zeros? In this case is we're going to interpret the scores or each element of y as the score for that class. So this is going to be a number which is going to be interpreted as the score of being in class one. And then this is going to be the score of being in class C. We have a score for all of our sea clocks. This is the part that would be ten scores corresponding to ten classes. So therefore, our predicted y. For some example, I maybe it looks like respect your 300, 500 minus 150 dot, dot, dot. Let's say that these are all negative numbers here. In this case, then we would say that the score for the second class is the largest because it's 500 and it's bigger than everything else. So then we would predict that example I belongs to class two. Any questions on that linear setup? Yes. Great. So Tom, we asked a really great question, which is, this looks very similar to the linear regression we did for the housing prices. What is different here? Great, yes. So in the housing example before, our output, Y, in that particular example was a scalar value because it was quantifying and rent and rent it to be any number. Whereas here our output y is actually gonna be a score for every single class. And the way that we choose the correct classes to take the argmax of the scores. Just see how the space to lead us into a different loss function. Because you can go for the housing example we used to be square error loss, right? Where we compare the predicted versus the true rent. Let's say that in this example. The answer is that example I belongs to class two, alright. So that's our prediction. If it was actually class too, and we use the mean square error loss, right? We would have like a two minus the true class. I say the true class, I'll red and orange was too. If we take the mean squared error, that equals zero, and that's what we did. You actually will see really quickly that if I use this encoding scheme for the crossover breaks down quite quickly. Because maybe two is the class for automobiles, one is the class for airplanes and tenants of class for trucks. So let's say that this was actually an airplane. If I do two minus one squared, that would be an error of one. But if this was actually a truck which was class ten to minus ten squared, this would equal 64. In this case, you can see that this is already not a good boss competency to use because if I predicted that this was a plain class too, and it was a sorry, I think I said clusters and automobile, but it was actually a claim class one versus the truck past, we've had very different losses, but we shouldn't because the Wasserstein are different by this much. So we'd definitely at least some new loss function, which we will be shortly. Great, yeah, so this student says, can we apply a one buddy, buddy, It's the classes and then you use a squared error loss, you could do that. So that's, that's one thing that you could use it. We're not going to talk about this, but what this is saying is we could make y hat attendee vector where everywhere is zero except for the class with the highest score. In this case, it would be 01000 is the reserve natural one month after representing the correct class and we wouldn't have this carbon. I would talk to me about you didn't follow that. Don't worry, we're not going to do that. Okay? Any questions on just the linear center? Great question is in this case, we really need to know these vectors W1 to WC. We also have to know these prices, B1, B2 to BC. How do we get them? We will show you how to get them. So we'll have to tell the loss function and it will have to know how to optimize it. Which will be, the loss function will be introducing maximum likelihood and then optimizing it won't be as doing gradient descent. Let's move on for now. So because what we're comparing in this course is essentially w transpose x plus b, right? And W transpose X is a dot product that measures the similarity of two vectors. Then there's actually an intuition for how you should interpret each row of W. Let's say that w, I'm going to actually try to get in class two is automobile or a car. If W2 transpose times picture a, W2 transpose times pictures of cars has a high score in W2. So book like what a car looks like. So you can think of for cars, automobiles here. If you take that 3,070 two-dimensional W2 vector and you reshape it. You look at what it looks like. You can actually see it kinda looks like a car, right? Like this is the hood of the car. This is the one intuition is that you can think of each row of w as a template which captures how, which captures the average book of the examples from that class. Any questions there? All right, and then just be a refresher for most of you saw it, go through it a bit. I'll go through how we get an intuition of what we're doing here is linear classification. So that's again, consider the case that we're gonna be working in TV because that's all I can draw. So that means x is going to be comprised of scalar components, x1 and x2. If I have a 2D space where X1 and X2, when I do my linear transformation, I'm computing y equals, let's say Y1. The score for class one is equal to w transpose x plus b. I'm just going to jump the biases for now. Just in the name of simplicity, let's say that we had y equals w1 transpose x. W one is going to be a two-dimensional vector. So let's say that w1 is this 2D vector that looks like this. Then let's say our data point x is going to be some data point here. So let's say that this is the example X and it resides right here. When I continued w1 transpose x, this equals the norm of w v1 times the norm of x times cosine theta of the angle between them. Theta corresponds to this angle right here. Then for this intuition example, I'm just going to make this assumption. Assume that the norm of w one is equal to one. Right? If I didn't assume that we said we just scale my scores. So this simplifies to norm of x times cosine theta. If we look at this image, I've drawn, right, what I can do is I can draw for drop a perpendicular line from excited to w1. So this is a right angle. To draw this, even though it doesn't look like a right angle to me. Actually let me redraw this dotted line so it's more like a right angle. Okay? So this is a right angle. If we're taking the norm of x, norm of x is the length of this vector. Cosine of theta is going to be for this red triangle, the adjacent divided by the hypotenuse, right? So this is hypotenuse adjacent over hypotenuse, which means that this value y will be the length of the adjacent side. So this distance from the origin to here is my value y, so that is my score. What you can notice then is that for every single point x-i that lies on this dotted line here, all of this normally access times cosine Theta are going to be equal to this point. Alright? So this dotted line corresponds to when w1 transpose times any point along this line x side will be equal to some constant, I'll call that constant K. Any questions there? All right, so then let's say we have our second class, W2 transpose x. Let's say that W2 is over here. For the same exact reason, there's going to be a line perpendicular to W2. I'm going to draw that over here. And this perpendicular line defines a line where W2 transpose XI is equal to some constant. Let me call this x j. So it's just a different example. So now we know that if we were to put in a new example, I'll draw this as an orange square here. I wanted to do it in class one or class two. All right, just from this image you can tell me what it is. Because if I look at its core for classic one is going to be larger than K, right? Anything along this line is a score equals k. If you're lower than the spine and the score will be less than K. And if you're greater than the five is called the period m and k. Whereas for W2, if you're vivo despite your score is greater than K, And if you're above this line, discourse less than k. So this orange triangle, we will have a higher score in the red class one than the blue class two. I can therefore classify it as being in class. And so when you think about how these spaces are essentially partitioned by these lines, you can see that what the linear classifier is doing is it's going to partition the space into linear boundaries. And so this is going to be a point of indecision where the score for W1 and W2, class one and class two are going to be equal and there's going to be equal to k. There are also all these other parallel lines. So this will be when w1 transpose X transpose. So smaller value than k, we'll call it p. And then there'll be a similar line here for W2. And where they intersect will be another point of indecision. And if you connect all of these lines, this becomes, this becomes a linear boundary separating class one and class two. I can't draw a line very well from that purple line, but that's supposed to be a line that separates class one and class two. Generalizes as you go from two classes all the way up to ten classes, you're going to have essentially ten separate lines have been nearly cord that had partition the space within your boundaries. Any questions here? All right. Hopefully again, review for most of you. Back there. The question is, what are the blue and red dashed lines? The blue and red dashed lines correspond to points where any value on these dashed line. Um, we'll have the same score for the class W2. And any point on this red dashed lines will have the same score for class one. So there are the lines of the same z-score and these lines will always be perpendicular to. This. Question is, is why just a scalar in this case? Yes, that's correct. So y will always be a scalar for each class. We're going to get the score from classical and the score for class two, et cetera. And just comparable. Tom I says, do these w i's have a relationship to the mean of the feature vector belonging to class I, e.g. does this wi have a relationship to the mean automobile? I don't know off the top of my head. I think the answer is no because it'll be something that general to standard via an imperfect optimization process. But I would say that the mean feature is a really good initialization for your gradient descent is to find something kinda close to the mean feature. Austin. Yeah. Yeah, totally raise a good point, which is several of you won't follow this. Nowhere is, this is not going to be covered in this class. But if you're doing maximum likelihood with Gaussian conditional Gaussian distribution, the answer ends up being the beads and they put their answers in this distribution. In that case of the poem is also convex, whereas in this case is because no question. Oh great. The question is, in this slide, what should be the values of k for the blue lines and the lines is arbitrary. I could have picked by, this could be like k equals five. And then, then for this one it might be like k equals four, e.g. so it's just a line of where everything on here has the same value. That value is arbitrary. So I knew somebody said, Oh, yeah, just select that senior. Thanks everyone. So why don't we also normally lineup that we along with a picture. I see. Yeah. So the student is asking, you know, there could be an image that is highly aligned to W1. Let's say it's just example in purple here. And then there could be another image which looks a lot like or which is more aligned with another vector. But maybe it's over here. And it has a larger score on w1 and the purple. Yeah, so there are two things to say here. I said proceeded with an asking, you should be normalized. So two things to say here. The first is, while the score for this blue point will be higher than the purple point for W1. Remember that when we're classifying, we're not looking at the absolute value of the stores scores for a class. We're looking at how this compares to the score in another class. For another vector, this course will be higher than it was for W1, whereas for this purple one, it will only defies the company one. That's the first thing. The second thing is, yes, it is a good idea to normalize and we'll do that in the big sets. Yeah, I think the question is, is this fine for different XII or the same excite? It is for a different one, yes. So I I kinda overloaded. I had this would be for x j. So this would be like for a point like this. Next question. Sorry, can you say that again? Yeah. So the question is, if the score for two classes is the same, what happens? So the image, I'll just say for now, the image can be classified as either a class. You're going to see in a few slides that we're going to change these scores to probabilities. And so if they were equivalent scores would be saying that the probability of any of these classes. And if you were to do like e.g. text generation, you could sample from that distribution. And some percent of the time you get the font-awesome. And after some time you get to the other. All right. Last question here. Yeah. Yeah. In response to Tom was a question where Tom Waits asking what these conversion to me. So just some practical optimization. It won't converge to the mean because we're gonna be doing gradient descent. However, because we have this intuition that, you know, the features should look like the images than it would be a good idea to initialize these features. Will have, you will have him Clark this on homework number two. You're going to have to change the Softmax classifier there. Alright, so let's move on. Um, where my McCarran classifiers fail. This is pretty straightforward, also probably be kept for many of you, but if you have problems where the boundaries of the medically cannot be linear, then the tail. So one example is the XOR problem. So maybe our data points look like this and there's no linear boundary that can separate the red from the blue loads ordered. You might have radial data. So maybe your data looks like this red axis in a circle and then blue O's in another bigger circle. And there's no linear classifier that you can build this data. Hello, can I give this example? Because several of you may see that if you just do a change of coordinates from Euclidean into polar coordinates, instead of x and y, we're going to have r and theta, right? Then the red axis will reside here, and the blue circles will reside here. The reason I'm showing this example is to say data may not be linearly classifiable. But oftentimes there's gonna be a transformation that you can make such that they are linearly classified. And I told you that a neural network usually has a softmax classifier, which is a linear classifier at the output of the neural network. And so one way to think of a neural network, knowing now that is output is linear. Even though it's doing a very highly non-linear task, is that the earlier layers of a neural network performed the non-linear transformations to make the data is linearly separable for a softmax classifier as possible. I just intuition to keep in mind as we continue on. We have now a way to transform our input data X output scores. Why? And what we want to do now is we want to be able to know them what a good model is. So machine learning recipe, that means we need a loss function. We need a way to tell us how good asserting W and a BR. And then we need a procedure, a learning method to learn W and make this loss function as small as possible. So this will now hearken back to what we were talking about earlier or what some students raised in terms of what the loss function is here. So we're going to optimize this via procedure called maximum likelihood estimation. Maximum likelihood optimization, optimization. The idea is that you're going to have data like x's and y's. And we want to maximize or wanted to be able to compute the probability of having observed data. This can be kind of a tricky to understand. So I always motivate this with the simplest coin flipping example, actual softmax classifier. So let's say that we have a coin given to us. And the coin has a parameter, which is the probability that it comes up. Alright? You don't know what the probability of this coin comes up heads is that you're allowed to flip the coin as many times as you want to determine that probability of coming up heads, right? So when I phrased that to you all, do you know the answer? To get the probability of coming up heads, you flip the coin 1 million times and kept him to the portion of tax, right? How does this come from? A more rigorous mathematical approach? So this experiment where you flip the coin 1 million times or what's the state eight times? Because I don't want to write 1 million heads and tails. Let's say you flip the coin eight times and you get heads, tails, heads, heads, tails, tails, heads, tails. That's your data from a one plus, and you want to determine what the probability of coming up heads, right? And so we need a model for this. And our model is that our data are exempt or our samples X, which represents the outcome of Bitcoin. These are either going to be heads, which I'll assign the number one, or tails, which I'll assign the number zero. So this is a Bernoulli random variable and it's gonna be heads with probability theta. And I'll write this Theta in red. And then if it's heads with probability theta, then it's going to be tails with probability one minus theta. Theta is my parameter. And I get to choose Theta or get to optimize for Theta to best explain the sequence of observations, heads and tails. In other words, I get to choose theta to maximize the probability of having observed this experiments occur. So the way that we do this is we have to write a problem, ability to expression because the likelihood, the likelihood is simple. It's simply the probability of having observed or data given your parameters. Okay? So let me just start off by saying, let's say I give you three models to choose from. In model one. We have that your parameter theta is equal to one. In model two, model three, theta is equal to 0.75 and model three is equal to 0.5. I've given you these three models to choose from. I want you to tell me which one is most likely. The way that you would do that is you would calculate the probability of observing your data under these models assumptions. So let's start off with model one. Model one says the probability of coming up heads is one. And if that's true, then there would never be any tails. So the likelihood should be zero. Let's confirm that the model one likelihood. The probability of observing this sequence would be the probability of heads. The probability of the first hedge is a one. The probability of tails is zero. Heads, one head. This one tells us their hotels is Sarah edge, this one tells us around. And if I multiply these altogether, the probability of model one being correct or what I want to claim this data is there so I can reject model one. It definitely does not explain this. For model2, this would equal 0.75, the probability of coming up heads raised to the fourth because I have foreheads and then 0.25, the public coming up tails raised to the fourth. And this equals 0.0 0124. Then for Model three, it would be 0.5 raised to the fourth times 0.5 base to the fourth equals 0.0, 0239. So we can see a faucet models 12.3. The probability or the likelihood of having observed this data is highest under Model three. So if I asked you to choose at these three models, what is the best model you should choose? Model three, because it has the highest likelihood of explaining the data. This is a really critical concepts. So any questions here? Alright, again, hopefully review for most of you. For me, you know, sometimes perceive any problems getting really complicated and it's helpful for me to come back and pick up the simplest example. And this is the example that I usually returned to you for maps provided in a tomboy says, I'm sorry, can you say it one more time? Columns on the right. So tomboy is saying, essentially, we were going to make an assumption. You'll see this later on that P of D given theta, every single trial is going to be independent. The question is, is P of D are those child's independence also? To get to observe the sequence? Yes, that is true. Yeah. So that is not an independent. Yeah, thank you so much. Alright. So this was just sourcing from three models. In general. For optimization, I wouldn't ask you to choose between three models. I would ask you to choose what is the best value of Theta. All right? So in this case we would actually write out the equation for the likelihood. So the likelihood l, This is the likelihood would be the likelihood of observing your data given your parameters Theta. So this would be theta to the fourth times one minus theta to the fourth, because I have four heads and four tails. And if I wanted to then just solve for the best data. Then we'd have to do is take the derivative of the likelihood with respect to Theta, set this equal to zero. And if you do this, you just solve for Theta equals one-half. I'll leave that as an exercise. It's all. One quick note if you do this exercise is that generally in machine learning, oftentime for going to take the log likelihood. So instead of looking at just L, we're gonna look at Lagerfeld. Maximizing the log likelihood is equivalent to maximizing the likelihood because log is a monotonic function. If the employment for longest larger, the output will be larger. But then the nice thing about taking a log is it turns all of these multiplications into summation. So generally I makes our life easier, so we'll usually be working with log likelihoods. So tomboy is raising another point, which is that optimization data, because it's a probability is constrained 0-1. And so you could go in, and you said, do this with overdrawn two multiplier. I believe that for this example, if you just solve this unconstrained data, so pumped up to 0.5 because he shook me. But yeah. The question is, the data here is just one experiment. Why are we trying to fit Theta to just this one experiment I gave? You might have had it, maybe too little data to infer data reliably. So in general, yes, more data will lead us to a better a better estimate of the underlying fabric data. But sometimes we just won't have that much data and so we kind of have to, I'm going to figure it out and probably the next form. A tomboy also raised the point that this is a independent coin flips. It can be thought of as a experiments, not one. But the point is, just more data will be better. All right, other questions. Yeah. The question is, we don't have the equation for the likelihood. How can we solve this? So you need an expression for the likelihood to be able to use maximum likelihood. And we're going to derive the expression for the likelihood that a soft classifier if you're in this situation, but if you can't derive the likelihood and that's sometimes arises, then you may use other machine-learning interests. And so this is truly a preview of what comes into play in week eight, OB GYN, we're going to learn about an architecture called the variational autoencoder architecture. The likelihood is intractable, it cannot be written down. However, what we'll do is we'll be able to write down a lower bound and we can optimize the lower ends up working. So there are a few other tricks that we can use in the case. We can write out the likelihood. For both classification problems we do with neural networks. We're going to use a softmax classifier at the output, and that will give us a fair likelihood. We're not something that is going to correspond to something called the cross entropy loss. That public who here is the cross entropy loss for like half the class. Some of you have heard of that before. Alright? Any other questions? Alright. That's maximum likelihood. I'm great. So there's one more thing. I wanted to go over this, which is the chain rule for probability. I don't over this because I want to make sure we're all on the same page, but this might just be me. I took several probability classes and it wasn't until I took information theory, but I really understood the chain rule for probability. So I'd like to just go over it to make sure you all know. Well, basically in probability, there are two really critical roles. When is the chain rule and what is the law of total probability? And with that, you can do a lot of manipulations for probabilistic expressions, which will be important for machine learning. The chain rule for probabilities as follows. In this class, we have a random variable we usually denote with a capital letter. So let's say I have a random variable, big a. And I want to know the probability that it takes on an event. Event we'll denote with the lowercase letter a. In this class or in other constants. And you may seem this is denoted with a little p. And then the random variable that this little p defines a distribution over is a subscript. And then the event that it takes on is in the parentheses. Then oftentimes the events are matched to the same letter as the random variable big A's. So oftentimes people will just write this as. Appear there, and that's what we're going to do in this class. There's also a probability that big B equals there'll be, this will be denoted in the following ways. And then we can have the probability of 20 events so that a equals little, a, big B equals the Hopi. And this one, we could denote the following ways. But in this class we'll usually just write this as P of a comma b. Alright? So there is a chain rule for probability. When there are two variables. This is Bayes rule. And what it does is it tells me what is the probability of x. Let me write this out with a probability expression first. Let's say I want to know the probability that a equals little a. So the answer is the same thing as this comma and big B equals B. The chain rule says that if I want to know the probability that big a equals little a and big B equals Adobe. I can break this down into two sub-problems. The following. First, I could find the probability that big a equals little a. Then because what I want is big N equals little a and big B equals B. After us continue the probability that a equals a, i need to compute the probability that big B equals b, given that big a equals with all. And what this means is, in notation, the probability of a and B is equal to probability of a. Given, we draw with a vertical bar, the conditioning bar. This will be the probability of B given a. The probability of getting both a and B is the probability of getting little a and then fixing that you got and you got it. All right. And that's the second term right here. Because of this, you could see up to read this in a different way. I could say that this is the probability that I get. That'll be alright. And then conditioned, or given that I have little b, what's the probability that I get a little area? So this would be probability of a given little. Any questions there? Okay, so let me write the chain rule now for three variables. So I might have a P of a comma b comma c. I can write this in many different compositions, right? So I can write the probability that first get little c. And now all the probabilities will be conditioned on getting velocity. So after I get little seat, I can write this as the probability Getting a given I got little c, little c, I have a, I fix those, and then all I have to do is get little b. And so this would be times the probability of getting little b given that I already have little a and little c. So this is a valid decomposition, decomposition of a, B, and C. I can write this in another way. I took the prelim this as the probability of a times the probability of B given a. So I have, AND now, now I have to get a seat. So probability of C, given that I have little a and little b. Alright, there's one more way I could have written this as a probability dot a and C. And then what's left to do is be, given that I got AMC. And so this is going to be probability of B given that I got fancy. Again, these are all valid decomposition of an intuition to have is that basically every single random variable gets to go in front of the conditioning bar once. So here C goes in front of the conditioning Clark, there's a conditioning bar. And after that, in every single term, C is going to be behind the conditioning part. And this term a data governance of the conditioning bar. And so for the remaining term, it goes. Any questions there? The question is, does this require independence between variables? The answer is, this is always true irrespective of indulgence. Russia says that there's independent conditioning. So you may know that if a and B are independent, then I could write that as p of k times P of B. Because a and B are independent, I can remove it from the condition because a, it gives me no information about our apps. So just to make sure everyone's following two example questions. So I'll write this out. Let's say that I have. Billy a, b comma c given d and e. This is equal to something in the numerator that I want you to find, divided by P of D times P of E given D. So take 20 s to figure out what that red box should be for the city correctly. Feel free to talk to your neighbors. I want to raise your hand and tell me what the question mark is. Perfect. Yeah. So this student says that the red question mark is probability of B, C, D, and D. The way that you arrive at this is that you know that p of t times P of E given D equals P of D and E. Then if I take P of D and E and I multiply it by P of B and C given that have no deal middle. Then these two by our chain rule, just equal P of B, C, D, and E, right? Does anyone have questions on that example? Yeah, great. So let me repeat that. P of D times P of E given D is the probability of P of D and E. That's the chain rule for two variables, just like here. Then what I'm saying is to get this red question mark, I'm gonna take this denominator and multiply it by the left-hand side, what that would be as P of D and E. And then times the probability of B and C given a and little a. Because this first term is the probability of D and E occurring. And then the second term is conditioned on that date heard. Then their product is the probability of all thermocline B, C, D, and E. All right, so we'll do one more. And that will be the following. P. I have P of D comma E times a question mark equals P of a, B, C, D and E divided by P of a given B, C, D and E. And I take thirty-seconds topic top of the neighborhoods and try to figure out what the red question mark is. And then after that, we will we will move on. All right. Does someone want to tell me or take a stab at what the exact boxes. Yeah. Yeah, that's exactly right. So the students says At the answer is probability of B and c given d and e. All right, here's the way that we will see them. I'm going to multiply the denominator with the left-hand side. And if the denominator times the left-hand side equals P of a, B, C, D, and E. And I know that this expression, right? This thing has to multiply P of a given B, C, D, and H. It gives me the probability of all of them. So this expression here has to be equal to P of b, c, d, and d, right? Because this thing times this thing gives me the joint probability and everything. Here. I wanna know what multiplies p of D and E to give me a P of E, C, D and E. And the answer for that is what we just did above here. It would be P of Vc getting the right. This is your first time or you're not as comfortable between both voted, please just take some time and we have several questions on homework number one, color go over this. When we come back next, people derive them. 
To get started for today. Our announcements, our one I reminded you that homework number two is due tonight. Upload it to grade scope, and please budget time for submission will be done a lot of sabbatical about problems with submission. Compiling or printing PDFs. Or I'll put into grade scope. So please just pleasant time if there's an issue for the homework, and then be sure to submit your code violence Notebooks in your doc PY files as well. The TAs will be uploading homework number three later after class today. And instead of it being due a week from today on Monday, we decided to give two more days on it, so it'll be due Wednesday, February 2023. Any questions on course logistics? The question is, will this affect the due Jacob future homeworks? The answer is yes. So I believe in conversation with the TAs. Homework number four will also be due the Wednesday after that, and then homework number five will now be two after the midterm. That's how we started clearing. Alright? Any other questions? Alright, we're gonna get back to material. So today's lecture is going to be on backpropagation. And this covers Section 6.5 is 0.6 and the deep learning textbook. And you'll recall that at the end of lecture last Wednesday, we just finished talking about the neural network architecture. We've talked about the activation function f that we would use for it. And we've talked about how for classification we would use the cross entropy loss. So that's great. We have our model architecture, we have our loss function. So we know that at the end of a neural network, we will pass this to some cross entropy loss to do classification. And now we're basically at the last part, being able to actually train and deploy neural networks. Which is we need to learn how to set the weights of the neural network. So remember that in-between every single layer we have, which is there'll be new and biases B. And so in this three layer neural network, we would have three sets of weights and biases. And we need to be able to know how to update these weights and biases. And we'll do that via gradient descent. But to do gradient descent, you know how to update the weights and biases. We need to compute gradients. Dl DW three, DL DW to, and DL DW one. And because this network looks a bit more complicated at least and other things, it's still an open question as to how we compute these gradients. Alright? So this lecture is going to be on backpropagation, which is the algorithm that we're going to use to be able to take the gradient of the loss with respect to W1, W2, and W3, basically any parameters and network, we're going to be able to take the gradient of the loss with respect to a few things that nomenclature. There's something called forward propagation. So forward propagation is just the act of starting at your inputs x and from them doing the forward computation. So linear layers then followed by your ReLu during the forward computation through the network to eventually calculate your loss. So for propagation goes from your inputs to calculate your loss. And this term backpropagation, is essentially the opposite. So at backpropagation, what we're going to do, we'll see is we're going to start off with a gradient of the loss with respect to our output. Then we're going to develop rules to basically calculate all of these gradients by taking our gradient at the end and basically back-propagating it all the way until the end. So that's where the name backprop comes from. Any questions on the motivation of why we need that competition. Alright, and so after we get these gradients, all we have to do is use gradient descent and we'll be able to change these weights to optimize this neural network. Alright, so these slides here, just putting it into text, what we, what we described on the prior slide, as well as the nomenclature before it and backpropogation. There is a question that you might have, which is, why do we need backpropagation? So if you look at the neural network architecture and you'd write out this equations, there is conceivably a way where you can analytically compute the gradients of the loss with respect to the weights, just like you did in homework number two for the softmax and for the hinge loss. Alright, so why did we do back propagation? The first thing is that back-propagation will find out is computationally efficient. Because before every backpropagation stuff, we're going to do a forward propagation step. We calculate all of the values and the number on route from the input to the loss. And if we just pass those values, will find that we can reuse them when we calculate backpropagated gradients. And so you don't have to do extra computation. Let me do back propagation. And then another reason that you might want to do back propagation is basically operationalizes the gradient computation procedure. So while you could write out the equation of the neural network and analytically calculate ingredients. At least for me, it takes quite a bit of time and it takes a bunch of mental effort. It's possible to do. But when you do backpropagation, but we're going to see is that we want to break down this gradient into many sub-problems, each of which are simple to solve. And so just like how decomposing code into functions is a good idea. So two, is decomposing your gradients into these small complications. We find out that when that happens, it also operationalizes gradient calculation, which means that it gives us a very simple algorithm to compute gradients in a neural network. And that is what makes software like PyTorch and TensorFlow is so powerful because there'll be able to compute gradients of any functions that we pass them using this backpropagation algorithm. Alright? Any questions before we get into the mathematical details here? Alright? So we're going to start off with just the simplest example where you can calculate an analytical gradient. But we'll show you, you how backpropagation works in this novel graduate to tougher examples. So we may want to compute the gradient with respect to our inputs x, y, and z, where f implements this function, alright? And this is not difficult to do, right? If I want to compute DS DZ, alright? We know that that's equal to x plus y. And we can also compute ds dx, that equals df dx and df dy y both equal to z. You can just do that by inspection. So this is super simple. We're going to use this example to motivate back propagation. So before backpropagation always comes forward propagation. So what I want to do is I'm just going to assign some dummy values to x, y, and z. There'll be 23.4. And then if we were to do this computation, x plus y times z, what I'm want to do is I'm going to draw what is called a computational graph. Alright? So basically what I do is I take this operation and I break it down where the sub operations are these nodes in the graph. And then our variables are the inputs. So here we have x plus y being implemented by this part of the computational graph. And then the result is multiplied by Z, and that's this multiplication here. And then that gives hopefully straightforward. If you plug in the values 234, you can go through these operations and get that at people's. What happens in backpropagation. Backpropagation always happens when we do it or some settings of x, y, and z. So what we do in backpropagation is we're going to start off with the upstream gradient. What do I mean by upstream gradient? I just need a gradient at the very end of our graphs. So here I'm just going to assume that DL df equals one. And I'm going to draw what the gradient of the loss with respect to each wire and the graph is. The graph as something in red. The wire. Alright, we'll talk about why we do it this way. You may say, where does the LDF come from? So at the output of a neural network, you'll have a softmax classifier. And from homework number two, you already know how to differentiate the loss with respect to the softmax classifier parameters. And so that would be like you're starting gradient at the end of the graph. Alright? And then what we wanna do is we want to take this gradient backpropagated to compute the gradients with respect to every other node in the networks. So we just assume for simplicity that we're going to have our upstream gradient equal to one in this example, but we will always be starting off with some gradients at the output. Any questions so far? All right, let's start to do the operations then to backpropagate. So that propagation really is the chain rule for calculus or the chain rule for derivatives. So let's say that I want to backpropagate the LDF to compute the gradient with respect to DC. If I want to compute the LDC, right, what I would do is I would use my chain rule for calculus, where DLD z would equal D LDF times df. Dz. Remember, DL, DFT is known to us, is the value of our upstream gradient that's just gonna be equal to one in this example. What is the FTC? This is something that we're going to call a local gradient, and I'll have a slide later that describes that more. But DFT is a gradient that we can compute because if we look at F, F is equal to z times the value on this wire over here. Let me just call this wire w. I'm gonna go ahead and call this wire w. Alright? So we know that F equals w times z. That's what this part of the computation graph tells us. If I want to compute d, f, d z, I would differentiate this equation with respect to w, and that will give me that ds z equals w. Remember w is a value of disquiet. If I want to compute DLD, I take my upstream gradient whose value is equal to one. And then I take the value of d f d z is w, the value on this wire, and that's equal to five. So this is equal to five. And therefore DLD z equals Any questions. Alright, and so similarly, I keep backpropagate through this wire by computing DL DW. So if I wanted the gradient at this wire, this would be DL DW. Dl DW would equal D L ds times ds dW, df dW equals z, right? Because remember, you put w times e. So if I differentiate this with respect to w, I just get x0 out. So in this example, the LDF is equal to one and d l, sorry, the FTW equals z and the value of z is equal to four. Alright, so then the gradient here DO w equals same operation as Bob for DLD z, but this is just for the other wire. Finally, backpropagate the gradient to find what DLD x and DLD YR. So if I want to find the L dx and dy l v y, then what I would do is I would write up the computation of this node, which is that w equals x plus y. Therefore, dw dx equals one, and d w d y equals one. Let's say I wanted to get dy dx. So let's say I want to get this gradient here. This is dy, dx. The gradient that I go at. This is DL DW, right? So this here is DLT W sacrilegious DL DW times dw, dx, dw, dx, I know from here, is equal to one. This thing equals one. And then DL DW, this thing equals four. So this equals four. And therefore the gradients, the L dx is equal to four. And for the same reason, DLD y will also equal four. All right, so you see just by using the simple chain rule, but doing this procedure where we just focused on one wire at a time and write the chain rule for that wider. You see that we were able to backpropagate this gradient to compute d l with respect to every single input variable x, y, and z, as well as intermediate. Any questions there? Rupture. And then that button. And so Ross's question is, can rewrite this as DLD F times d FTW. Like to swap these two because these are scalars in this case you can. But later on in this class we're going to talk about the factor multivariate chain rule in which case the order matters. And that's why I'll generally write the chain rule going from right to left. Because you'll see for the nominator layout, our chain rule for vectors and matrices will go from right to left. All right, you guys have had to be followed this example. Alright, so this is the idea of back propagation. We did it for a very shipboard example and now we're going to make it a bit more difficult. But first, some intuitions. The basic idea, as you've seen us play it out, is. We're going to always take whatever computation we're doing and we're going to break it down into a computational graph. That's this drawing over here. The forward pass, we will plug in the values of the inputs to compute our output. And then the backward pass. We'll first take the derivative of the output with respect to our last wire. That'll give us a gradient. In this case, we just said that radius is one. Then basically at every single node, what we can do is we can backpropagate the derivative by using the chain rule associated with the computation at this nerve, which we call a vocal. So let me just tell you more explicitly what I mean by local gradient. So if I start off with an upstream derivative CLTS, so the identity of this wire is, if I want to compute d L dx, so this graph, X and Y are these wires. And so d L, dx would be the gradients. Under this top input. We know that dy, dx is going to equal our upstream gradient. That's the gradient on the wire that's ahead of me. So this D LDS is called My upstream gradient. Then there's going to be a gradient. Dy dx, df, dx, and df dx is going to be called a local gradient. Because df dx is the gradient of this vocal computation or the football function f. So when I refer to a local gradient that refers to df, dx, and that reflects what the gradient is. For this function. In the backward pass, I can always compute the gradient of the wires just ahead at the input. As long as I know what the upstream gradient is and I know what the local gradient is, e, I know what the derivative of this function is with respect to its end points. Any questions there? Alright, so this is a slides just talk to you about what we talked about. We take a derivative. And as long as I know how to differentiate this function with respect to its inputs, I can always backpropagate through that function. This is how it around you'll see pie charts and TensorFlow operationalized gradient calculation. Basically whenever you call an operation like multiply or even something like cross-entropy. Pytorch or TensorFlow will always store what the gradient is of the function that you're calling. It will know how to compute this gradient. So that if you want to backpropagate, it knows to just take your upstream gradient, multiply it by the gradient of d f with respect to its inputs and then you can backpropagate the gradient to it. These foreigners question. Yeah. Yeah, the question is, is the order of these gradients because we used the denominator. So when these are multivariate quantities, yes, it will be because we use the denominator. Other questions, and we'll talk about that in more detail with Tom way says, are there, are there any roles to drawing a computational graph like e.g. x plus y times z. You wrote three independent variables as XYZ and then decompose. All right? This one way, another way. I see, yes. I'm always asking basically when I drew this computational graph, we draw every single operation, the addition and the multiplication has its own nodes. This is the way I think that you should all do it. You could conceivably no, ignored this phosphine and have some other way to draw a computational graph that confusing and it'll probably be two mistakes. And so when you draw these computational data, at least for this class, it will be most helpful for you to really write out every single operation. When you, when you draw these computational gas. That's gonna be a large part of getting, getting things ready. Alright? So just to nail it, the basic intuition of backpropagation, right? Is that we break up the calculation of a gradient into small and simple steps by focusing at one node at a time in the ultimate computation that we're doing. Each of the nodes. And these graphs correspond to a function. As long as I know the derivative of the function with respect to its inputs, then I will always be able to take an upstream gradient backpropagated to the inputs. Alright? And if I just keep composing these operations, I'll be able to take the gradient at the output of a neural network and back propagated to every single layer, the layer that occurs before it. So composing all of these gradients together then returns the overall group. Any questions there? Alright, so let's go ahead and talk about some operations where we're going to see this so often. And there's basically a role for backpropagating that makes it really straightforward. So if you have two inputs, like x and y and they are added together, we know that the local gradients, so in this case, equals x plus y. So the local gradients will be df dx and df dy y. And these things are able to work. So because these things are equal to one, and the gradient here is the upstream gradient times local gradient, which is one. When you see a plus sign and you're doing backpropagation, what that means is that a plus sign just passes through the gradient. So if you see a plus sign, you can always just write that this gradient here, dy dx will just equal d LDF, the upstream gradient, also equal TO. So if we think of these as gates, basically the AG gave the add operation and the neural network will distribute your gradient, the LDF to these two wires. We also will have multiplication gate. So let's say that we have two inputs, x and y. And the output is going to be S equals x times y. And we know the local gradients are df dx equals y and df dy y equals x. Again, since that propagation tells me to take my upstream gradients and multiply it by the local gradient. And the gradient at this wire is going to be the local gradient, y times dy LDF. So this is going to equal y times dy LDF. And then the gradients on this wire is going to be equal to x times DL DS. So when you see a multiplication gate, you can think of it as like a radiant couldn't put switcher where switcher just means I'm going to take my upstream gradient backpropagated to y. Then I multiply by x, by backpropagate to excellent, I have to multiply by y. So whenever you backpropagate through, you're just multiplying the upstream gradient by the value on the other wire. Flushing said, alright, let's continue on then. Um, we're gonna do this, the max operation because we're going to find out that max is going to come up a lot in our neural networks because we are going to be using the ReLu operations, which is max of zero comma x. So in this case, let's say that df dx, sorry, must first day that ash is equal to max x comma y. Then df dx is going to be an important one. If x is bigger than y. Because if x is bigger than y, then f equals x and dx, dx equals one. Then if y is bigger than x, then x equals y. Differentiate with respect to x, I get zero error. So this is going to equal one if x is greater than y and zero. Otherwise. We can write this as the indicator function, that x is bigger than y. So then when I got off it through a max operation, I'm gonna get what is essentially a gradient router. So one of these conditions, x is bigger than Y or Y is bigger than x is going to be true. And so x is bigger than y. Then the gradient comes here, has DLD f, and the gradient on this wire will be zero. If y is bigger than x, then the gradient d LDF comes onto this wire. But over here it's going to be equal to zero. So the max operation will route the LDF to whichever wire as a bigger value. Daniel. Daniel's question is what happens when x equals y? That gets asked every single year. So I always say, practically, first-off, x will not be equal y in our networks because we practically implement them because they'll be like double or single precision. If x equals y, then it depends on how you define your max operation. So if you said S is equal to y, if we return x, then that, then you could assign the gradient to be one when x is bigger than or equal to one. Question. Other questions. Yeah. I'm not sure I understood the question. So you said when we say DY DX before. When I say that, sorry, so I'm not making any assumption about independence of X and Y. Just looking at the function, you differentiate it. You get an x and y depend on each other in some way when we take the partial derivatives of the assumed that the other variables are all constants. The question is, when do we use the max case? When we have ReLu, we know that ReLu of x equals max of x and zero. And so in our computational graph, we're going to have a bunch of bees, or we can have a max where the inputs are x and zero. Yeah, the student's question is, we don't want to backpropagate to zero, right? Yeah, this case it would be useless to backpropagate to zero. But we still need to know how to backpropagate to x. And that depends on the value of X being bigger than or less than zero. We're going to continue on with this. What we're gonna do is we're gonna do a more involved scalar example. The scalar example we're going to be using a bunch of big clinical case we talked about already and multiplication and addition. So I'm going to be using a bunch of other gates as well so that we can make sure that we understand this operation. So what I have here is the sigmoid function. We have 1/1 plus exponential of minus WE 00 plus W1, X1 plus W2. And so I've drawn out the computational graph. So all the way up until this node, this node em, implements this sum here. Then I multiply that by minus one, That's this node. Then I exponentiate the results, you get this. So that's exponential. Then to get the entire denominator, I add one, that's this node right here. And then to get f, I take the inverse of what's in the denominator and I guess the one over the denominator. And so that's this computational graph that we see. And this is where we've drawn out every single operation again, just for the sake of being careful and it'll it'll unsure a reduced headaches and mistakes later on. So what I've done here is I've just arbitrarily chosen some values of w zero, et cetera, et cetera. I've done the forward hopper for propagation through this graph. So if you assign these values and you do all these operations, you will get that f equals zero points sudden the root. Alright? Now we're gonna start with backpropagation. So we're going to assume that we're going to start off with an upstream gradient. Dld. Yes. That is equal to warn. And the first thing that I want to do is a backpropagation through an operation we haven't talked about yet. So what I want to back propagate from one to the gradient over here. I'm just going to call this wire. Okay? So let's say the value of this wire is a and in this particular instantiation, a equals 1.37. Firstly, I want you to think about it for 20 to 30 s. Feel free to talk to your neighbor as to how do I back propagate from the LDF here to the LDA, which is backpropagating through this inverse operation. Alright, if there's some discussion, does someone want to raise her, someone raise their hand and tell me how I back propagate through the inverse operation. So this student told us the answer to exactly. Remember that I break down my computation into my upstream gradient, which is DLT F. And that equals one times my vocal gradient, which is ds, da. Alright? In this case is the inverse operation. So f equals one over a. And therefore, if I want to compute my local gradient, which is the gradient of just the inverse operation. I would get that DF da equals minus one over a squared. Therefore, if I want to compute what the LDA is, all I have to do is I had to do the LDA equals my vocal gradient by local gradient in this case is going to be minus one over the value of a. So a is 1.37 squared times by upstream gradient which is born. And this thing simplifies to be equal to -0.53. So the gradient over here is gonna be -0.5. I guess that's just taking gradient, which is the gradient of f with respect to a and then multiplying it by my gradients of one. Any questions? So does anyone want me to go over this guy? Alright, we'll continue on then. I have, this is the word for the one that we just did. We have the gradient -0.53 here. Next we're backpropagating through a plus sign and no for plus sign, the video just passes through. So if I want the gradient at this node here, it would be just -0.53 times the gradient passes through a plus sign. So we have -0.1 by three here. Now we want to backpropagate through this exponent exponential function. And to do this, we would do exactly what we've been talking about. So if I call the values on this wire, if I call this C and I call this one D wire, which has the value of one, that will also get minus point. Yeah, great. I see a tomboy saying that the gradient at this wire would be -0.53. So if you were to just follow our rules, that would be true. But really those gate rules apply it. This is like some variable because really DEF d1 is equal to zero. So if this were a variable that is gradient would also be -0.5. Alright, so we're going to backpropagate through this exponential. Here. I will write up the computation. So if I call these wires b and c, Then the local computation is that d equals EXP of C. And if I want my local gradient, right, I need to be able to compute DB DC. And the derivative of this function is exponential of C. So if I want the gradient here, I would take my upstream gradient, which is -0.53 and multiply it by the local gradient, which is exponential of c and c is equal to minus one. So the gradient over here as -0.53 times exponential of minus one equals -0.2. Any questions on this local gradient? Backpropagation? Alright, if we've made it this far, the rest is easy. Because the rest are just multiplications and additions, or for those we have our, alright. So if I want to backpropagate to this value over here, we know when you backpropagate through a multiplication, you take the upstream gradient and you multiply it by the value of the other wire. So this would be -0.2 times minus one. That gives me 0.20. And then 0.20 here, I can start to backpropagate through these plus signs, which just pass through the gradient. So the gradient DLD W2, It's going to be 0.20. This is going to be 0.20. These are going to be 0.2, 0.0, 0.20 plus signs are super easy. And then the multiplication for just a tiny bit more complicated. So backpropagating from 0.20 to x one, I would multiply 0.20 by the wire value here, which is two. So this gradient would be 0.40. This would be 0.20 times the value of X1, which is two, that's 0.00 would be 0.20 and DL DW zero would be -0.20. And that's using the rules that we developed further multiplication gates. Here you've seen that by using this backdrop roll, we've taken the LDF. We have compute all the gradients with respect to the inputs W0, W1, X1 and believes you. Alright? Any questions on this example or any step in this example, and then use it as a proposition. So yes, this is scaling up when we're differentiating whether you're doing. Great. Yeah, so his question is about pi torch and how it keeps track of what the gradients are. So if you're multiplying two matrices and Py Torch, right, you wouldn't use Pi torch, his version of matrix multiply. Because pi torches version of matrix multiply, doesn't just multiply two matrices, but it also knows what the local gradient is so that it can backpropagate through that operation. So basically in Py Torch, whenever you make a computational graph, every single node in our computational graph is going to be a defined by torch function. But only that, not only does the operation, but stores or local gradients, and then you backpropagate through it. And then if you want to implement a function that's not implied works, you can define your own function and all you have to do is pass it what the local gradient is. So then pie charts will still know how to backpropagate through it. We'll get more into this in week seven or week eight when we talk about the deep-learning likers. Any other questions? Yeah, question is, can I go over the inverse again? Yes. So for the inverse, remember that we always, when we calculate the gradient at the input of the function here, we take our upstream gradient, g LDF, and we multiply that by the local gradient of this inverse function. The inverse function I'm calling f. So that would be the FDA where f equals one over there. All I have to do to get the local gradient is compute the FDA. And the gradient of y over a is minus one over k squared. So now I know that the LDA will be DLD F, which is one times df dy, which is minus one over a squared. A is the value of this wire. So a here is the value 1.37. And so my overall answer is gonna be one times -1/1, 0.3 s squared. Any other questions here? Yes. Thank you. So the student is asking, when I call somebody the upstream gradient, is it different for every wire? Yes. So when I back propagate e.g. through this exponential function, my oxygen gradient is going to be -0.53 and my local gradient is the gradient of this exponential functions. So basically, I always look at just one operation. The upstream gradient is the gradient at the output. The local gradient is the gradient of that function. And then I want to get the back propagated gradient, which is a gradient as the important. Thanks for letting me clarify Other questions. Alright. Can you raise your hand to to follow? That's awesome. I think that's almost the entire class. So given that, that's how backpropagation works at the scalar level. Now we're going to move on to do back-propagation at the multivariate model. Oh, sorry. Before that, there's one more rule that we have to talk about. So first off, I want to say what backpropagation? As long as you can draw this computational graph where you know the local gradients, you can take the gradient of anything. Alright, so I think on question number two of the homework, which will be a homework number three, which will be optional for C1, 47 students will be mandatory. Proceed to 47 students. He took his paper from Europe's in 2004. And I remember reading this paper. And they just wrote down some gradients of the loss function with respect to weights. But oftentimes in textbooks and in papers, they don't show you all the mathematical steps to get from D L to do just to complete that gradient. And so as a grad student, I was working on this gradient for an entire day and I can never get the analytical solution correct. And then I remembered, oh wait, I could do this with backpropagation. And that's what you're gonna do in homework number three, pushing to wherever you read the computational graph can do with backpropagation, you get the gradient fairly easily, alright? So that's one advantage I talked about where formally for me it's just easier to think of gradients when you break them down into these bite-size chunks. Alright, there's one more thing that we have to talk about, which is what you do when you have two paths converging on a node. So I'm gonna call this wire x. I'm going to call this wire one. I'm going to call this wire Q2. And let's say I have upstream gradients. I know what DL, dq, then I know what DL, dq2. And then in this case, Q1 equals h of x, equals h of x. What I want to know is, if I know my upstream gradients, DLT, dq, q1, and q2. Now there are two of them. I want to compute what dy, dx is. So here's a new situation. We haven't been captured where to gradient paths converge on one question for you all is, what is dy, dx in terms of DLD Q1 and DLP T2. So this is a tricky question. I'll give you 3 s to think about it. Feel free to talk to your neighbors. Does anyone have the answer? I think it happened yesterday. Danielle. That's exactly tracks. Yeah. So Daniel's answer is that this will equal, um, so I'm gonna write this as just a general sum for the general answer, or the system will just be over two terms. So I'm gonna write a sum from I equals one to n. Little n is the number of converging passband. And we're going to have the upstream gradients D L, D Q Pi. And then we have the local gradients dq dy, dx. So you may recall that this is the law of total derivatives. And the reason it makes sense is because x changes through Q1 and Q2 because they are functions of each other. And so if I want the total change of the loss with respect to x, I have to sum up the contributions due to Q1 and Q2. Alright? So if the summation of this with I equals one is how much the loss changes if I wiggle x did acute one. When I equals two is how much the loss changes with respect to x when I recall Q2. And then a total change of the loss with respect to x will be the sum of all of these contributions, alright? Yes, a tomboy saying that in this case h is the same. So d2y, dx is going to be the same for every single eye, and that's correct. In fact, usually, even though I've drawn this example in almost every single case where we're going to see this happen, h of x, this is going to be the identity function. So usually in this class, or we're going to see is that x is going to branch out into multiple different paths that eventually lead to a loss function l. And all you have to remember is that when you backpropagate, x is granting two different paths, when you compute the L dx, you sum the gradients across all of these apps. Alright, so here, D L, dx is going to equal DL, dq q1 plus q2. Alright? And that's the general thing that you have to remember. For neural networks, which is, again, if a wire splits off, when we backpropagate, the x is gonna be the sum of all of the wires, all the gradients on these wires that go back to x. I mean how she's there. Alright, so given that there is one more example here, I'm going to skip it in the interest of time. Yeah. I feel like a battery life of these microphone, usually five or six vectors. Anyways. So in the interest of time, we're going to skip this example. It's just another scalar example that should be valid. Or we're going to be asked for here so that you can follow. Okay? Alright, so now we're going to get into how you do backpropagation for neural networks. And neural networks we're no longer multiplying, are doing operations on scalars. We're doing operations on vectors and matrices. And you know that for backpropagation we are using the chain rule for derivatives. We have vectors and matrices, the order matters and so we need to know what the multivariable chain rule is. For gradients. You just looking at the time crook radical point for a five minute break. So let's use a five-minute break. When we come back, we'll do a gradient. Awesome. I love, like I say, you don't have to follow the way that. I believe that this is a Boston. I want the LDF doing something like a song. So the question is, did my best to share with you some of the pages on their due diligence. Sorry. What did he submitted the works and the graders didn't submit everything. Drawn. Like this is a function. Yes, we could do that. Yeah. It does take some practice. So the answer implementation because the vectorization Professor. Yeah, Could you just say some other resources like that? The question about like someone asked me like that's just a picture like that. And having a more complex. Always break it down to prompt you to find more complex. Basically, if it's going to be pumping, right? Yeah. Yeah. Okay. All right. Everyone. There was a question that I want to clarify, which is that one of the students asked, diol df equals one. Is that generally going to be the case? In general though, gel DF will not equal one. But we assume that this is given because basically when we have a neural network, the output here is going to be the scores that then go into a loss, like the cross-entropy loss. And in homework number two, you all have already learned how to calculate e.g. d. L with respect to the scores DCI. And so because our output layers will be made by cross entropy loss, so we know how to take it. We will always have an option gradients that we can start to backpropagate. Alright, so the examples that we've given, we just said DLP API calls one because we assume that you will eventually know somehow between gradient and I'm just back propagate that gradient. Go back to that last year. When I Is that right? Yeah. Yeah. Yeah. So tomboy is asking, essentially we're going to have DLD Z for one example, right? If I could have done this back propagation where we have particular values of what the wires are and we can back propagate for those particular buyers. So then what do we do when we have many examples, right? How do we sum them together? Well, for that, we will actually use this rule that we derived here, right? So if I have, let's say I have ten examples, x, so I have X1, X2 dot, dot, dot Johnson x ten. Let's say they just go through. A simple architecture is a simple neural network where we have our inputs and they go to a layer, and this will be w1. And then they go to another layer that gives softmax scores. That gives the loss function, right? If I put W1 into this network, sorry, not W1, X1 into this network. I'll calculate the loss for that example. But the weight w1 would have been the same. And if I put x two into this network, it will lead to a loss. But that would have been for the same w1. When it comes to DL DW one, I can think of this graph has happened to replicate in ten different times, where I have ten different losses, but that all share the same w1. And so the gradient would actually be the some gradients across all of my examples. If you didn't follow that, that's okay. We're going to talk about this more explicitly when we do the backpropagation through something called stashed normalization. Alright? I want to get to this multivariate chain rule. So the first thing I've done is I've just put these two slides here that you've seen before. These are just the gradient of a scalar with respect to a vector and a scalar with respect to a matrix. This is stuff from chapter two. Alright? So we know the derivative of a scalar with respect to a vector and a scalar with respect to a matrix. Now we're going to do the derivative of a vector with respect to a vector. And this will lead us to something called the Jacobian. So let's say that we have an operation, Y equals W, X, Y and X are vectors. So let's say that y is n-dimensional and x is n-dimensional. So that w is n by m. Alright? What we can then do is we can look at our vector y. So let's say my vector y has samples or has entries Y1, Y2, down to why. And what I can do then is think about what the gradient of y with respect to x should look like. But first, looking at how wiggling x changes each individual element of y. So if I want to look at the change in y, y1, right? By wiggling x. All I have to do is I have to compute a gradient that we know how to do, which would be the gradient of the scalar y one with respect to x. And that's a factor, right? So if I want to know how changing x changes y one, I will take the gradient of the first element here, y one with respect to x. And I would do the dot product. So I should have a transpose here. So we know that the dot product of the gradient of y with respect to x times x will give me the change in y one from linear approximation. So what do I want to know how all entries Y1 through YN change? Well, I'll just stack these into matrix. So what do I mean by that? If I have this vector here, or sorry, sorry, if I have this matrix here. What I'm going to do is I'm gonna say this matrix is going to multiply some change delta x, right? So this year, Delta y one, sorry, gradient of y with respect to x is gonna be a factor. And if I take this vector and I dotted with delta x, this has got to tell me the change in my first entry of y delta y, y1. This is going to give me a delta y, y1. And then if I take the gradient of y with respect to x and I dotted with delta x. That's going to give me the change in the second entry of y, Y2. And then similarly this is going to give me the change in y n gets smaller. What is this bathroom changes in y at each elements. We'll just call this the overall change in my vector y. What this tells me is that if I create a matrix, were along the rows, I have the gradients of the first row by one with respect to x, right? Going from X1 to XN. Then this thing times delta x is first element will tell me how much one changes. The second element will tell me how much Y2 changes. Alright? So this matrix that tells me how wiggling x changes. Every single element of y is called the Jacobian J. And it is defined to be this matrix over here. Let's just do some dimensional checking. So if x is in R n and y is in R n as we defined above. If I want to know how to change the x, leads to a change in y, then that means that our Jacobian should be R n by n. And we see from this definition or the Jacobian that this thing is indeed n rows by columns. And overall, again, the Jacobian tells me how a small change delta x raised to a small change delta y. Sorry about this monitor flickering. Overhear me just try to turn these connections. Any questions there on the definition of Turkey? Alright? So this is where we need to really careful about our denominator and numerator. So this is a definition of the Jacobian. Let's see how that relates to the gradient of y with respect to x and denominator layout. So in denominator layout, right? If I compute DY DX, DY DX, the gradient of the scalar y one with respect to my vector x. This is going to equal, we know this vector of the same size as x. I take the gradient of y with respect to every single eliminated x. The thing to remember and denominator that is that the thing that we differentiate with respect to. In this case, x, will be the variable whose elements change in a column. So in denominator that the thing we're taking the gradient with respect to x will be the variable that changes along a column. So it'd be X1, X2, XN, the first column. That means that into your layout, this first column is going to be d y one, dx. The second column will be D Y2 dx. And then the column over here will be DY DX, where each of these is an n dimensional vector. So that's just the definition of the gradient of y with respect to x and denominator layout. You can see clearly then that the dimensions of this gradient and denominator there will be m rows by n columns. So it'll be m by n. So another way to remember denominator is in denominator layout. If I have the numerator being n-dimensional and the denominator being m dimensional. The dimensions of the denominator of what we take the gradient with respect to x always go first. So if x is m dimensional and y is n-dimensional, but they'll be m by, I will see more examples of this later on. So by definition, this should be an equals, equals triangle for defined to be and denominator layout. This is the definition of the gradient of y with respect to x. You're going to notice that this matrix is simply the transpose of the Jacobian. So for the Jacobian, the x dimension changes along the columns and the y-dimension changes along with the rows. But Jacobian is n by n, whereas this gradient here is n by n. So in denominator layout, we would say that the Jacobian is the derivative of y with respect to x transpose. Enumerator layout, there is no transpose here. So this is why some people prefer numerator over denominator for various reasons, but one of them being that the Jacobian doesn't require this extra transpose. Just definitions, any questions there? Alright? So given this, let's go ahead and take the derivative of a linear layer. This is going to pop up in our neural networks. So we're going to take, we're going to take y equals w times x. And here we're going to define Y to be an H dimensional vector and x to be an n-dimensional vector. If this is the case, then w is going to be an H by n matrix. And we're going to take the derivative by using what we defined in the last slide. So W times X is a matrix Y. And I'm thinking this gradient with respect to x, sorry, W times X is a vector y and taking the gradient with respect to x. So the first thing that we should do is we can compute w times x. So W times X is a vector y, and that's simply the operation of matrix vector multiplication. So that gives me these entries for this vector y. And so this thing here is equal to y one. This thing here is equal to Y2. All the way down to this quantity here, which is equal to y. Now, after we have Y1, Y2, y h, the gradient or the gradient of y with respect to x will be differentiating y with respect to X1, X2, all the way down to the last element of action. So I would take this Y1 and I differentiate it with respect to x, which would give me an n-dimensional vector. So if I take this E1 and I differentiate with respect to x, this here is going to be DY DX. So what is DY DX? Dy DX is gonna be a vector where for the first element, I differentiate with respect to little x one. When I look at this term, little axon only appears once and it multiplies w1 one, so I get w11 here. The second element is differentiating with respect to little x, for which I get a W12, etc. Similarly, if I differentiate y with respect to x, I will get this column over here. And then if I differentiate y h with respect to x, I will get this column over here. So what you see here is the gradient of W times X, which is a vector with respect to x, is just equal to w transpose. And so this is something that we should remember because we'll see several times in this class, if you have y equals wx and the gradient of y with respect to x is simply equal to w transpose. We can do some dimensional checking just to make sure we've done everything right. So we know that y is an RH, x is in R n. And so DY DX should be an R. And remember the denominator layout, the dimension of the denominator goes first. So should the n by n by w transpose, we can see is the transpose of a matrix W that is h by n. So w transpose would be n by h. And so indeed, this thing dimension dimensionally matches what we expect the gradient. All right. Any questions here? Yes, Tom way, I just wanted to point out that some degree of y is complete. Without doing any definition that should be about new content and match the hunches I'm competitions. Sometimes dimensions that pedigree is wonderful in some ways saying something that we will talk about. The hope everybody on the cost. Which is that the dimensions, the dimensionality checks, oftentimes are really distended the checks to get your transport is correct. So Tom, I was saying if I differentiate y with respect to x, I would think you would might be W. But because I know the gradient should be n by h and h by n, Then ingredients is like w, but I should put a transpose X, make the dimensions work. That's actually an intuition that we're going to use commonly in this class to make sure we get our derivatives right. And what we'll see that more powerfully in our next matrix vector, in our next, in our next pregnant. Any other questions on this example? Alright, there's one more thing called the Hessian. We're not going to be using the hessian very much in this class in terms of it's gonna be something that will come up when we talk about second-order optimization. But you're going to see it more frequently in like an optimization class, but I just wanted to find it. So if we have a scalar f of x, and x is a vector in R, n, and f of x is a scalar in R. Then what's the Hessian is the multivariate generalization of the second derivative. So we know that if I take the gradient of f of x with respect to x, this is going to be a vector that is the same size as x, is going to be our m. And now we know that if we take the gradient of this quantity with respect to x, that's going to be like the Jacobian of this gradient. And the result is going to be a matrix that is n by m matrix is defined here. And again, this is a generalization of the second derivative to higher dimensions. You can imagine interpreting this as telling you the curvature of your function along these different dimensions. You won't be tested on this. This is just something that you will see in later classes. I'm also talked about it briefly when we talk about second-order descent methods in this class. Alright, so now we need to derive the multivariate chain rule. We'll do this in the same way that the scalar chain rule is right. So the scalar chain rule, we have some variable x and we want to know how it affects the z, alright, but accept x, z in the following way. X is y to the function f, and then Y if X is X0 through the function g. Alright, so then we know that d z d x will be just to make things consistent, I'm going to rewrite this as d z d y times DY DX. How do we get this? Well, we can look how wiggling x equals y in the scalar case. We know that a little bigger than x will change y by approximately DY DX. And then if we look at how changing y, z, we know that's true GCD Y. And if for delta y, we plugged in delta y equals d y d x times delta x. Then we arrive at this chain rule. We're going to do for the multivariate case. Let me just go ahead and write this out. The player side, we'll have written this out formally. We're going to have x, that's the vector in R n, y a vector in R n, and V, a vector in R. We'll call this P. And then we're going to have that y equals f of x and that z equals g of y. Alright? So I'm gonna have to relationships for how wiggling x affects z. So in the first one, I could go directly from a wiggle and next to a bagel in z. And in the second one, I'm gonna go wiggle x, see how much that wiggles y. And let's see how much we're going, why we go see, and this will give me phi multivariate chain rule. Let's do the first example. If I want to know how wiggling x, z, we know that this is just the definition of the Jacobian. So I know that if I wiggle x, the amount that is z wiggles is gonna be given by my Jacobian matrix. And my Jacobian matrix is just DZ, DX. And remember, because I've denominator layout, we have this transpose here, but we have to remember, okay. So the Jacobian tells me how much we're going x, because the Jacobian d z, d x transpose. Remember, x is n-dimensional, z is p dimensional, and the Jacobian will then be p by n. Then we're gonna do number two. So we're going to see how delta x first because delta y, and that's going to be through the Jacobian from x to y. So this is going to be delta y. I should have approximately is here. Delta y is approximately the Jacobian DY DX transpose times delta x. Then if I want to see how that wiggles Z, I will see how my wiggling of delta y, because z. And we know that this would be delta z is approximately the Jacobian DZ DY transpose times delta y. And then what I'm going to do is I'm going to plug in my delta y being DY DX transpose delta x into this equation. So this is going to equal DZ DY transfers DY DX transpose x. Any questions? Sorry, I missed. The question is why approximately? That's because it's just a first-order derivative approximation. And so there are second-order, third-order terms that we're going to copy here. But if we just take a linear approximation this question, Are there questions? Alright, so in red, I have a relationship that takes me from delta x two delta z. And blue. I have another relationship that takes me from delta x and delta z. So I'm gonna go ahead and equate these. If I equate these, I get a dc dx transpose equals blue DZ DY transpose times DY DX transfers. And then to solve for dy dx, I'm just going to take the transpose of both sides. And this transpose is going to flip the order of these two, which is why our chamber goes from right to left. So after I take the transpose of both sides, I get that dy dx equals DZ DY times DY DX. And this here is my multivariate chain rule. And then check the mentors really quickly. And then after that, I'll take any questions. So DZ, DX we know to be a p by m matrix. So this is an AR, p by, sorry, d z d x transpose is p by m. So d z d x is n by p. So this is n by p, d z d y will be our denominator layout. Just remember that whatever is in the denominator, the first dimension. So why is n-dimensional and z is p dimensional? So this is going to be R n by p. And then for DY DX is going to be R and then m-dimensional and why it's n dimensional. So it's going to be n by n. And thankfully we see an n by n times an n by p matrix equals an n by p matrix. And silver in good shape here. Okay? Alright, any questions on disqualified area chain rule? Great. Daniel's question is, what happens when we take the derivative of a vector with respect to a matrix, giving us a 3D tensor. We're going to do that actually exactly. Next question is, will this to hold? The answer is yes. So this is always true. Chamberlain, always true irrespective of the dimensions of z. Alright? So the following slides here are just some of the formal knows that go through exactly the same algebra that we just did for this multivariate chain. Alright? I want to pause again and ask any questions on this multivariate chain rule. Because if not, the hardest thing for today's lecture, which is a tensor, derivative. Okay, can you raise your hand if you're following? Awesome. So let's go ahead and do the tensor derivatives. So in a neural network there, right? We're going to have a linear operation, y equals wx plus b. And y will be m-dimensional vector. In this case, x will be an n-dimensional vector, and then w. Going to be an m by n matrix. And we know that in the case of neural networks are parameters or these big. So I need to differentiate vectors with respect to matrices, because ultimately I need to get gradients with respect to big W's so that I can change the weights. And so this means that there is going to necessarily be a derivative of a vector with respect to a matrix, right? So I want to say the following at the offset, this is going to be challenging to many of you. And so this is something that you may just have to review. But at the same time, it's actually not necessary to know to be able to do optimization and neural networks. So after we do this tensor derivative, we're going to see that there's actually a shortcut that you could have used to get to the answer that uses the metric that convoy was mentioning. And actually when I talk to people in deep learning, especially like those in industry, this is how they say the derivative of this comes along. But it actually isn't a rigorous answer. What we want to do here is the rigorous answer so that nothing is magic. But after we do the rigorous answer, so that you know that we've done this rigorously, you can use the shortcut to react. Alright? So let me just say that at the outset and then moved on it. So I haven't do this example. So if I want to compute the gradient d y, d big W, This is going to be the derivative of a factor with respect to a matrix. And denominator lay out the dimensions of the denominator go first. So w is going to be m by n. And then after that the numerator goes, y is going to be m-dimensional. So we're going to expect that d, dW is going to be a 3D tensor. And let me show you intuitively what that looks like. So if y is equal to some vector of values Y1, Y2, YN. And let me just color these different ways so that you see what's going on. So let's say that one is in red, Y2 is in blue, and then y, n is in green. Those quantities are scalars. And so if I were to calculate D1, dW, we know what this gradient is because we define this in backyards here, right? So this dy1dw is going to be a matrix that's the same size is W. And it's going to be the derivative of a scalar y one with respect to every single element in the matrix W. So this is going to be an n by n matrix, m by n matrix. I'm going to draw it as this matrix over here. It's m by n. Then I could look at my second element Y2. And I can compute D Y2 dw. That's also going to be a matrix that is m by n dimensional. And what I produce, I can take that ending the n by n dimensional matrix, stack that right under my red one. And then this keeps going on until I have my last GYN dw. So this vertical stack of matrices of which I have in total m of these matrices is going to be d y d y, d y dw is going to basically look like this 3D cube exactly might not be symmetric and all the natural is just gonna be like this cube matrices where every single slice of the cube is the derivative of one of my elements of y with respect to w. All right, any questions there? The question is, what did I say about this topic before? So what I mentioned that for a cube, the cube isn't fun to have the length, width, and height to be equal to each other. But this 3D tensor of length, width, and height and general rule for specific cases but not in general. So we're gonna go ahead and do this operation. Again. This is primarily so that when we actually get to the answer and then you see the shortcut that we take to get there. It is not magic. And we know exactly where that short clip comes from. We're gonna do this once and you may need to review it because there's a bunch of 3D tensors times one of the vectors and sometimes it takes time to process. But in the end, we'll get to a shortcut, that book that you'll likely just take on and use for the rest of this class. Alright, so we are going to do this in the context of this example, where we're going to look at something that we know the answer to. This is what you did in homework number one, I think was questioned for what the optimal w is in the case where you have x, i, and y being vectors and least-squares problem. So. The way that we're going to motivate this, I'm going to write this out as equal to one-half. And then there's going to be the sum from I equals one to big N examples. And then we'll write y i minus W transpose y minus w xy. And then we're going to use a multivariate chain rule here. So what I'm going to do is I'm going to define GI. Gi is going to equal y minus WX xy. And this means that ultimately the loss is equal to one-half I equals one to big N Z transpose Z. And so you can get the gradient by saying that DL DW is equal to d L, d z times d z, d w, and d LDC is really easy because it's a scalar with respect to a vector. But then this d z, d w is a more complicated thing because z here's a factor and then w is a matrix. So in the context of this example, we're going to learn how to calculate dw, because z is linearly related to W. This will be the same brilliant about actually using our neural network layers. Alright. Hi, my question is, where did this summation go? Oh, you mean like over here? Yeah, so this is a good point. So what I'm going to do is let me actually write this here. I'm going to, for the following work, I'm going to omit the superscript I. So I don't have to rewrite this summation so many times and the superscript I. So basically we're just going to calculate the loss with respect to the weights for one example in this summation. And then the overall gradient will be the sum over all of my examples. Let's do some dimensionality checks to make sure everything works. So in this case, why is m-dimensional? X-i is n-dimensional and w is n by n. So DL DW better be an n-by-n matrix. G LDC is a scalar with respect to a vector z. Z has the same dimension as y. So z is m by one. So z is n by one. And then d z, d w is going to be a 3D tensor, like we talked. But I mentioned the big W go first. So big W is m by n, m by n, and then the dimensions of Z, Z is m-dimensional. So this will be times M over here. And if I take this 3D tensor that's n by n by m, multiply it by this n-dimensional vectors, vector, sorry, these m's cancel out and then the overall result is going to be n by n by one. But m by n by one is just a 2D matrix that's n by n. So indeed, the dimensions work out this example. Great. Daniel's question is do the rules of matrix multiplication applied to tensor products? Yes, so the same rules generalized to three dimensions or higher dimensions. You just have to have these inner dimensions match and then they'll cancel out, right? Yeah, tomboy says, you could just think of a matrix if it to detect, sorry. Alright. Any questions? Alright, let's go ahead and do this. So this is, these are just the formal notes that talk about what we just discussed. And remember, I'm going to drop this superscript. So this is also the stuff we just discussed. Let's go ahead and do the actual gradient. So here we're going to have our z equals y minus w times x. And what I wanna do is I want to compute DZ, dw. And we know that this is going to be m by n by m. The way that we're going to compute this gradient is just like we drew this example here, where we're going to take our vector z and just differentiate each element of z with respect to big W, which will be a matrix. Alright? And then we're going to stack all of these matrices together. So I am going to come back to this slide and we're going to differentiate. I'll call this the k'th element of z with respect to w. And we know that this is going to be some n by n matrix. And because z has n elements, I'm going to have n of these matrices. I simply stack them all together. And that's going to give him got a 3D tensor agreement, right? So the first thing that we have to do then. See what is Z k. For that, I'll just write out this in terms of the sum notation of the matrix vector multiply. So z k here is going to equal the k'th element of Y minus the sum, the sum over j of the kth row of w, dotted with my input vector x. Okay? So that is my scalar, which is the k'th element of my vector. Let's see. Any questions there. Alright, we're gonna go to the next step. Now I'm going to differentiate and try to find d, c k dw. So we're gonna do d z k dw. And what this will be, is this going to be a matrix of the same size as w where I'm going to differentiate my GK. Let me copy that down with respect to every single element. So we've already used the subscripts j, k, and then n. So what I'm gonna do is I'm going to take the derivative of z with respect to every single element of w. I'm going to index W by I and the columns by t. So I want to find the derivative of z with respect to the IP element of w. So WIP is just gonna be a scalar and the I throw in the pth column of the big matrix W. Alright? So let's go ahead and differentiate. If I differentiate this expression with respect to w, middle IP, this is a scalar that doesn't appear in y k. So we're going to have a zero there. And then we're going to have this minus sign, minus sign. And then it's going to be the gradient of this thing with respect to WIP. So this is going to be dy dw IP of the sum over j of w k j times x j. Alright? Now some of you maybe able to see this kind of thing is by inspection. But for me I always finite a mistake, so I like to write these out. So let's go ahead and write this out. I'm going to write minus d, d WIP, and then w k j x j. This summation is going to equal W K1 X1 plus W K2 X2. All the way up to x wasn't RN. So this is going to be w, k, and x. Right? So far just applying again definitions from linear algebra. Any questions so far? Yeah. Were you doing essentially two zooms here, but first zooming in on the kth element of the vector Z, and then we're zooming in on the i-th element of the matrix W. So we zoom in on Z and W zooming on w. So this is actually a scalar derivative over here. Alright, let's take this gradient. So there are two cases to consider. Case one is when I does not equal k. So this I does not equal k. This gradient is zero, right? Because all the w's and this term lead with a subscript k. So when I does not equal k gradient d ZK, DW, IP equals zero. On the other hand, when I does equal k, right? Let's see that. So I is equal to k. So now this is w k p. Let's say that p was equal to four. So this would be dy, dw K4, right? The only term that would have a WK for would be the term that multiplies x four, right? Or if p was equal to two, dy dw K2 would give me x two, right? So when I equals K D ZK, the WIP is going to equal. Remember there's a negative sign here, so it's going to be negative sine and then whatever the value of exit and DXC. Does anyone want me to go over that again? Alright, so I've now differentiated my one scalar ZK with respect to every element in WI and the matrix W. And I can write out what this looks like. So we have that dy, dw is I'm an m by n matrix. And this matrix is largely filled with zeros. So basically this matrix here has all elements equal to zero when I does not equal k. So basically I'm going to draw a bunch of zeros over here. But then when we're looking at the kth row, here, at the kth row, d w, d z k GW, k one will equal minus x one. So the first column and this row will be minus x squared. Dy dw K2 will equal minus x to the second column here will be minus x2, and so on until minus x, d, c k, d w ends up being this matrix where all the rows are zero, except for the kth row. For the kth row is just minus X transpose. Any questions there? Yeah. This is for a single example. That's correct. And then this is also for a single entry of z. None of them will have entries and z. So we're going to stack a bunch of these matrices where at every single step, for step Z1 is gonna be the first entry that's minus k. For the second DCT, DC to dW is gonna be the second row that has gone zero. Right? Okay, so this is from the formal note that go through this more formally, but they're the exact same thing that we just derived in the prior slide. And now we know that we have these DZ case, dw is, and this is what they look like. All right? Now what we want to do is we want to compute d z d w times. So I changed that L to epsilon because I'm just doing this first simple example. So this was previously my Albert. I change it to x one. I want to compute this tensor product, which is my chain rule. So I'm going to take, let me write this out here. I'm going to try to compute d Epsilon TW. Let me just go back to this slide so that everyone's on the same page. It was on this slide where I'm just replacing out with epsilon from my chain rule. I know that D at finding w dw times d Epsilon DC. So I'm going to finally do this tensor product where I have d Epsilon dW is equal to D epsilon. Sorry, that's not right. D epsilon d z times d z d w. And then the generalization of a matrix vector multiply two tensors tells us that this is equal to the sum from k equals one to n, because V is an n-dimensional vector of d z k dw times d epsilon d z k. So this is just like writing a matrix vector multiply, except now it's going to be a 3D tensor vector times a vector. Alright? So this is the same formula for the matrix vector multiply except now instead of this being a vector is a matrix. Then this d epsilon d d epsilon d z. We know that this is going to be the derivative of a scalar with respect to a vector. So d Epsilon z is itself going to be some vector with terms d Epsilon dc1, the epsilon dc2, all the way down to d epsilon d c m. Right? So now we're going to write out what this summation is. So this foundation, sorry, sorry, I didn't say it correctly. This is a matrix and this is a scalar. Because this is the epsilon d z k, which is just one of these elements. Are there questions? The question is, how do I justify this thing here? So this is the generalization. So if I have a times x, right? Like my matrix a times my vector x, if I want, if I want to know e.g. the first element of that, right? I would write this as a sum of a i j times x j. Sorry, this is a one j times x j for j equals one to the dimension of back spam. And so what I'm doing here is I'm just generalizing this to the 3D tensor case. Where this matrix here turns into a vector or dishonest A1 j, because this is now a 3D tensor, it turns into this matrix CCK. Just in the interest of time. I'll, again, this is complicated. I'm more than happy to answer further questions in office hours about this. I just wanted to make sure I get finished it in the next 6 min, as you can all use it for homework number three. So here we are going to have d epsilon d. So I'm going to write out the sum now, going to remove the summation. So I'm gonna write the k equals one term. This is d epsilon d z one. And then it's going to multiply dc1 d w d z one dw is this matrix here where the first row is minus X transpose and everything else is there. So this is going to be minus x transpose. And then everything else who's around. And you'll notice that I took d epsilon d z one and I moved it from the right-to-left. The reason I'm allowed to do that is because the epsilon dc one is the derivative of a scalar with respect to a scalar, a scalar, a scalar on the left or the right. Then this will be plus d epsilon d z2 times dc2 dw, which is this matrix square zero. And then in the second row we have a minus x transpose. And then everything else is Sarah. And then viscous, a plus dot, dot, dot. You have these terms all the way up to the DCM and then DCN, dw. So when I multiply these together, I will in the end get an expression where I have here a matrix where everything is zero. The first row, the value of the first row is minus dF font, easy, one, X transpose. So this is going to be the first row minus d Epsilon dc1 x transpose. This term contributes just a second row to this matrix, which is d epsilon d z two times minus x transpose. So there's going to be d minus d Epsilon dc2 times x transpose. And if you follow the pattern, you're going to get this here, minus d epsilon d z m times x transpose this matrix here, I could have just rewritten as minus d epsilon d z times x transpose. And dimensionally this works because d epsilon d z here is an M by one vector. X transpose is going to be a one by n vector. And in some, that's going to give me an n by n matrix. And this is the derivative of a scalar with respect to an n by n matrix. This gradient should have been also an n-by-n matrix. So the National way, everything works out. All right? Okay, So I just wanted to set you up with a few things for the homework. And I have office hours right after this. I'll be happy to take even more questions there. And we'll also start off next lecture. We're doing this, but in case some of you are starting early on the homework, these will be slides that go through everything that we just derived. And if you go ahead and apply the chain rule once you get the formula for b squares. So I wanted to give a few notes on tensor derivatives. These will be the entity that actually gives you the practical how do you do things? So we know from a prior example, if I take w x, I differentiate with respect to x. This is going to give me a matrix W transpose. We derive that earlier in lecture. So you might have thought that if I take w x and I differentiate it with respect to w, this thing look like x transpose. Alright? In reality, we know that this is a 3D tensor, but just like following the rules of how when you multiply two numbers together, the derivative with respect to one is the other number. This thing is probably something that looks like x transpose. When we did this derivative out, we did see that it was like taking our upstream gradient and multiplying it by x transpose. So how do people generally do this gradient in machine learning? We did the 3D tensor, which is the Windows example. What people do is they say this thing ought to look like x transpose. And then just like tomboy I said, we're just going to play around with the dimensionalities to make things work. So what people do is the following. They'll say, okay, I have a d epsilon d z and this was m-dimensional. And then renew that overall our gradient d Epsilon dw is N by N, Okay? Dx1, d W, we know, should equal some upstream drilling. Dx1 dizzy. And df, dy, dw again is m by n and d epsilon d z is n by one. And then this should be multiplied by dW according to my chain rule, dw over x transpose. So what we do typically in machine learning is we just try to mask the mountain. So X transpose is one by n. And I noticed that if I were to multiply here by x transpose, then I get an m by n matrix. Alright, this is totally not rigorous, but it's what people did. So by matching dimensions, you would come to the conclusion that this might be your derivative. And actually that is the correct answer. But if I were to just tell you this, you shouldn't believe me, right? The way that we get through this is by actually doing the rigorous 3D tensor matrix derivative and showing that indeed equals Jet fun d z times x transpose. Alright? So I've hit the time. I know I went through that quickly will definitely start off by reviewing all of this. 
All right, everyone for can it get started? We have two announcements today. The first is that homework number two is going to be due on January 30th on Monday. I'll put it to grade scope. And this homework has a considerable compliance. So be sure to start the homework early if you haven't already. And then like greenhouse last time, there's going to be a delay and returning to homework number one grades because they're still working out some admin. But just fix the higher the gleaners. Alright. Any questions on any quickly? Correct? Yeah, any other logistic questions? Of course questions. Alright, we'll get some material. So today we're going to cover the neural network architecture, some design choices. And we're also going to get started with backpropagation, which is an algorithm that's used to train neural networks. We talked again elastic there. This will be the reading for the deep learning textbook for these components. We talked about briefly at the end of last lecture of how neural networks are inspired from biological neurons. And last lecture we pointed out three key components of biological neurons. One is that they have inputs which are these arborist like regions of the neuron called dendrites. Dendrites receive inputs from other neurons. There's this thing called the axon hillock. The axon hillock. What it does is it integrates all of those inputs on the dendrites. So the inputs are in red. You can think of the axon hillock as summing up all of those inputs and that, that sum is above a threshold, then the neuron generates what is called a spike or action potential. And that's the signal that's conveyed down the output of the neuron, which is the axon. And eventually this axon will connect to the dendrites of dams thing. Alright. This is a very high level overview of neurons. If you'd like to know more about how neurons work, like the company I teach. Next quarter we'll go into this important detail. Alright? Any questions for this brief recap from last lecture? Alright, so again, the key things are that there are inputs. The inputs are summed and then pass through some nonlinearity, like comparing to a threshold, e.g. at the axon hillock. And then they're propagated along the axon. Neurons come in all diverse shapes and sizes. But when you boil down a neuron into its x and y components, those are the three things are dendrites, the axon, the axon. And if you were curious it too, the output signals of neurons look like if you just focus on this image right here, what we're doing is we're starting an electrode in and measuring from the axon of the neuron. And you can see that basically the signal, if we plot this voltage is going to be at some resting voltage, VR. And then sometimes it'll occasionally spike. And so you can think of these spikes as conveying something happens like the second one. Whereas when it's just sitting at rest, just all zeros. So that's the absolute potential of the neuron fundamentally communicated through this all or nothing. Spike. The spikes from neuroscience are very complex. And so another thing about spikes is that probabilistic. So what you can do is you could record from a single neuron and you can repeat a stimulus. So you should be recording from a neuron showing someone a picture of a cat. And even though you showed them the same exact fat on five different trials. If we were to plot across time when the spikes occurred, which are these red vertical lines, despite train would look different from trial to trial. So usually in neuroscience. So instead of looking at these spike trains, we often look at something called the firing rate of the neuron. The firing rate of the neuron tells you how many spikes you might expect to see given window. And so maybe the firing rate at the start of a trial. So the x-axis here would be time, my start-up around 50 spikes per seconds. So on average, I would expect to see 50 spikes, no 1 s window. And then maybe that rate decreases over the length of the trial. So usually when we think of neural networks, we think of them as their values reflecting what is analogous to the rate at which a neuron is spiking. Alright? Any questions on anything? We've made it to the biophysical neuron. Alright? So this is what the artificial neuron that we're going to use a neural network looks like. So kind of similar to the biological neuron. In fact, I'm actually just gonna go two slides ahead, but we do comparisons for the artificial neuron. What happens is that you're going to have inputs that come from oxygen neurons. And so these are X1, X2, X3, X4. You can, these are the outputs of prior artificial neurons. And then you can think of X1, X2, X3, X4, and then neuroscience analogy as the activity on the axons of the upstream neuron connects to this current neurons dendrites. After that, you take your inputs and you multiply them with base. And this will reflect how different neurons will have different types of effects on the downstream neuron. They could strongly activated or they could even inhibited if the weight is negative. And so, while this process is very complex and biology and the artificial neural network, what we do is we replace those connections with just scalar weights, W1, W2, W3, W4. And then just like the biological neuron has this integrative center for the axon hillock That's Psalms, the inputs and the artificial neuron. We will have this component, which will sum of the inputs. So it's going to keep W1 X1 plus W2, X2 plus W3 X3, etc. And then after that is going to pass it through a non-linear, non-linearity. Just like how biological neuron, I've got thresholding operation. However, when we go through lecture today, we're going to talk about some of the design considerations into how to pick up and what's the best practice. Alright. So that's the artificial neuron and kind of how it was inspired by the biological neuron. Any questions there? All right. So some people, including my own lab and use artificial neural networks to study mechanisms within biological networks. But there should always be caution when comparing these artificial networks to biological neural networks. Because of biological neural networks. Many more complexities as stochasticity, dynamics that are not model that own these artificial neural networks. So the way we think of these artificial neural networks is that their architecture was once inspired by these neurons, a very crude approximation of these biological neurons. But they are far from an adequate and perfect model of these biologics on their arms. So we should always use caution when comparing artificial neurons to biology. Nevertheless, those principles from biology led to these neural networks, which we know performed very empirically now. Okay? So if you're interested in talk to you about any of these differences stomach, which I read it on the slide. Feel free to stop by my office hours and I'll be happy to talk about some of the differences between biological and artificial. Alright? Any questions then before we start to talk about neural networks? Artificial neuron. The question is, when we drew this artificial neuron here, was this output just any real number r? That's correct, yes. Okay, So we're gonna start off with just some naming conventions. So typically a neural network, we'll start off with an input layer that's going to be drawn in blue here. And this input layer we usually denote with the variable x. You can think of this as far, alright? So x corresponds to the actual image that we're going to put into the neural network. The neural network will also have the very last layer called the output layer. And if we're doing classification with C bar Tanya, remember the output layer corresponds to the scores of each class. So this is the output layer that might get you the scores that go to e.g. a softmax classifier like we discussed. And then everything between the input and the output layer are called hidden layers. So this right here is the hidden layer. And you've probably heard of things like the depth of the neural network. A neural network that is deeper is going to have more layers. So this neural network, this would be a two layer neural network. We don't call the input layer since that's like the input. And if you don't call it that a layer, but all the hidden layers, as well as the output layer, are included in the count of how many layers of neural number of gates. When I draw this image, you're going to see connections between each of these nodes. And each node corresponds to, in this case, a scalar number. Each of these will be an artificial neuron. And if I had these connections, e.g. from my input to each one of these are going to have a weight. So we'll call this weight, weight W1 one, W12, and then W1 three, where the first number denotes the neuron identity that's going to H1, so it's index one. And then the second 123 correspond to these three efforts. So H1, just like we've talked about in the prior slide, would you do this computation where it would son my inputs W0 and 11 x1 plus W12 plus W3 X3. And then there's also gonna be a bias, I'll call that b1, and then passes it through some function. And this will result in a scalar, that is the activity of the first hidden unit of that hidden layer. Each one. Any questions here? Alright, and then if you want to know the activity in the entire conveyor, right, we can write this out not just for H1 but for h12 3.4. So let me write that right now. So if you want to write the activity of H1, H2, H3, and H4. What that would be is gonna be a matrix vector. Multiply. This vector here is gonna be the input vector X1, X2, and X3. We know that we get H1 by doing this transformation. And so this would be a w1, 1X1 plus a W12 x2 plus W3 X3. Then there will be a bias term here. Also, I'm running out of space. I'm going to really try to squeeze this in. B1, B2, B3. And then for the second neuron, H2, right, would have connections W2, W2 to W1, and W2 three. So that would be W2, W2 to W3, etc. There'd be a W4, W4, W4 three. And therefore you can see that to compute the value of all the artificial neurons and enrolling a bricklayer. Oh, I have to do is apply a backline transformation and then pass that through a nonlinearity. And so this pain can be written more succinctly as oh yeah, thanks Tom. Like before. This can be written more succinctly as H. This H here would correspond to this specter of the four H's. H equals F. And then we'll call this the vector of w's a big W, the inputs and x and then this bias. So here we see that linear classifier that we've talked about. The components of it shown up again over here. All right? And then one more thing to note, when I write f applied to a vector, right? So this, do you have a plus B is gonna be afforded the vector. When I write a function f applied to it. This means that I apply f to every single element of that vector. So there'll be f applied to the first, the second, the third and fourth elements. Any questions there? For this architecture? Is this not work acyclic? Yes. This network and everything that we do up until the midterm will be strictly before, which means that they're not even any feedback connections and therefore there are no cycles. Later on we will talk about recurrent neural networks, which will have cycles. Any other questions? Alright? So this first layer then, which computes the hidden activations of these units, we write h equals f of w and x plus b1. So these weights here would be my W ones. And then my second layer, which is the output, I would write as W2 times the inputs, which are this vector H plus another set of biases, v2. Okay? And then again, we should think of these. I put, at least in the case of c far as softmax scores. They tell me how likely in class one and class they told me the score of the image, any class one or class two. So these would go into a softmax classifier. And that's softmax classifier would produce some loss function, which is the loss function that we derived. Last lecture. We have Macs and it's the one that you'll implement on homework number two. One thing that you will have noticed and we'll talk about this later, is that generally we don't apply this function f at the output layer. Which means that after you do dependently or transformations, that will be a linear classifier. And we'll return to this in 10 min or so. But it's not a tight with the bebop. Great. Yeah, tomboy. I say softmax quarters. These are the un-normalized scores or the normalized. These are the un-normalized scores. So they can stay there before and go inside there before going to this. So maybe I should actually thanks Tom. Wait, I'll just call these scores, I suppose emphasize that there are normalized. Let's make sure that we have everything dimensionally correct or dimensionally correct in our minds. This w1 is going to take us from our 3D inputs to our 4D hidden units is going to be a four by three matrix. W2 is going to be a matrix that takes us from our four d hidden units to our 2D outputs as two-by-four. And then we'll have also biases, D1, there'll be applies for each hidden unit, so there are four of them and then also be output for each bias. And so that's going to be an art to nomenclature wise. We would say that this network has two layers. And then these two layers, there are six neurons. Neurons are these four and these two. And then we also calculated the number of weights it has. So the number of weights would be the number of weights and W1 and W2. So that's four times three plus four times two. That's 20 total weights. And then there are a total of six biases. And so in total, this neural network with 26 total learnable parameters. Any questions? Yes. The question is, is the softmax function presence in z? Yeah, So these, these, these are the scores that are then transformed into softmax probabilities. So there is not any additional money earlier. This is the linear transformation part that gets you softmax, that gives you the unnormalized scores. And so the Z is what you apply the softmax to. So eventually you'll do g of z. And this will give you softmax probabilities. Where the GSR softmax function, right? Oh great. So the question is, does this comprise another layer? So this softmax operation isn't thought of as another layer. Generally a layer is thought of as things associated with these linear transformations. But even if we had like several linear transformations with no nonlinearity, nonlinearity between layer. That's a great question, thanks. Other questions. Alright. Yeah. Reveal example. Discretion that we do not need. The slope. Actually be able to leave that space Destiny. Tom ways, making a really great point. I'm going to reserve that for just a minute. She's like to combine all I'll be emphasized. Alright, so I'm just showing you here now a three-layer neural network. And so in this case we would have two hidden layers. Our input would be three-dimensional, but then we would have two hidden layers. This one would be four-dimensional, this one four-dimensional. Then again, our 2D output. In this case, the transformations that describe these neural networks are straightforward generalization of a lever to talk. So H1 would be our nonlinear function f. Our function f in general applied to w1 x plus V1. So that if w1 and it'd be one here, and then H2 would be f of W2 times H1, the activity in this way, plus some biases be two and that would give me the activity H2. And then similarly, for the output, there'll be a W3. And if D3, and again, we don't apply that function. And these networks are called fully-connected. We usually abbreviate them with FC neural networks. The name is pretty self-explanatory, but it's because there's a connection from every single neuron in one layer to the neuron and the next layer. So because there are all of these connections that can be learned there called fully-connected. Then another name that you might. These cold is a multilayer, perceptron and LP. So if we say MLP or Fc network in this class, that's referring to these types of neural networks. Alright? So this slide has something works out with the best exercise for yourself. You can make sure that you can compute the number of weights and biases for this three layer neural network. All right, these are very straightforward. Yes, great. So Jake is just confirming that if we were to look at any of these weight matrices, the number of rows will be matched to the number of outputs and the number of columns will be asked the number of inputs. So W2 here would be our four-by-four, where the first floor reflects the number of neurons and H2. And just for reflects H1 and actually vote had been more care should be W1. W1 would be four by three. We haven't slide here to show you that these neural networks, as you see them, they look really sick for two implementing data, but also straight towards implementing code. So this, these four lines of Python code implement this neural network that we see over here. If you're not familiar with this lambda notation in Python, what this lambda notation is saying is that we're going to define some function f. So that's equivalent to saying define S. And then the input is x that corresponds to this x over here. Then it's going to return the value of this. And so this is going to be back times x greater than zero. This function might look kind of mysterious to you right now. Don't worry, we're going to talk about in this lecture. But this will be a common nonlinearity. But we'll talk about it when we would've made the nonlinearities. Alright, So then in Python you would just do each one. The activity of these neurons would be w1 times x plus b b1, and then apply that function f. Same thing for H2. And then lastly, we have our outputs. Alright, hopefully straightforward for the architectures. Any questions? All right, so moving on, let's start to talk about the function. So the first thought is what it is linear. This even simpler example, we'll say at this the identity. So the identity, then these are the equations. So each layer of the neural network. And the point of this slide is to say that is that it is identity or f is linear. That the entire overall neural networks tasting here. Alright, how do we see that? Let's look at H2 right here. What I'm going to do is I'm going to write and I'll rewrite it up here. H2 is equal to W2, H1 plus B1. What I'm going to do is I'm going to plug in the value of h one into this equation. So from layer one, I know that h one is equal to w1 x plus distribute two plus b1 plus b2. Let me now extend my terms. This is going to equal W2, W1, W2, B1 plus B2. Now what I can then do is I could say that W2, W1, right? It's gonna be a new matrix, I'll call it w tilde. And then these two vectors together, I'm going to call a new vector B tilda. What you can see is that H2 is also a linear function of my inputs. If you were to propagate this through all the way to Z. The point of this slide is to say that if f here was identity, or more generally, linear, even though you have many layers in the end, Z is still just an affine transformation of x where WE is gonna be the composition of all of these individual WAS. And there's also a bicep and composition of this extension. So you have a linear classifier. We know that it's limited. Anybody could use newly drawn lines between facts and it can't solve more complicated problems. So if f is linear, then we haven't really gained two bugs or we haven't gained any capacity or watering. Alright, any questions there? Homeless question is, is the dimension of Z always less than x? And many applications that's the case, but it's not always true. I'm actually going to take this as a jumping off point to then say, are there any cases where f might be useful, even though, I'm sorry, what were ef thing? Linear can be useful even though it's linear damper. And the answer is yes. So it could be useful in theoretical settings where you want to simplify the problem and therefore you want to study the linear version of a neural network. Another example which will come up in homework number three, and that's what I'm going to talk about it here, is that you can use a neural network where f is linear to still do some pretty cool things. And one thing that you can do is called dimensionality reduction. I'm going to talk about an architecture that you'll see in homework three is called an autoencoder. And the idea is that you start off with some x. Let's say that X is four dimensional. And what you wanna do is you don't want to work with a four dimensional input because maybe you want to visualize it and it would be really nice if it was, say, two-dimensional instead. Alright. So what you might do is you might design a neural network that looks like the following. I have my four-dimensional x. I'm not going to make my hidden layer just two-dimensional. So she's going to have two units. And then I'm gonna make my output be four-dimensional. And then this will be a fully connected network. So there's gonna be connections between all of these. I won't draw all of them. But hopefully you get the idea. This is a really interesting architecture. Because what we can do is it can make a low-dimensional representation of your x. If you ask the following, you create a loss function L. You take the loss function is z minus x squared. So what you're saying right now is I started off with an x and I want my output z to be as close to X as possible, right? So z is also four-dimensional and hopefully it takes on the same values of x. But to reconstruct z, you have to go through a two-dimensional bottleneck. Alright? So in this architecture, if it were to work, what your neural network has to do is it has to take your inputs x, squeeze all that information into just 22 dimensions here, engage, and then try to read out that information to reconstruct x at the output as the squared loss is going to penalize to make sure that it's being extra close to each other. Alright? So you could do this with non-linear neural networks. You can also do it with linear networks. And that's an example where even if f is linear, so you could do useful things with it in this case, dimensionality reduction. Any questions there? The question is, what is the relationship between or how effective is this method compared to other dimensionality reduction methods like. So. Actually in the case where these are linear, there's a relationship between this very simple than your auto-encoder and PCR. Great. The question is, for an autoencoder, would it be better to use a nonlinearity then to have a linearity in general? Yes, because with the nonlinearity, you would sell debt for power in terms of the nonlinear or the hidden representation, the dimensionality of the dimensionality reduced representation, a non-linear function of your, of your original scapes, Right? Yeah, The question is, is it useful part of this being able to look at the two-dimensional hidden states. And the answer is yes. So this is a form of so-called unsupervised learning where they're trying to find structure in it. And basically these two, these dates are going to be to the states that capture the important information in X and Ben could be e.g. visualized. The question is, when working with neural networks, do we have a choice of the number of neurons and number of layers, or do we not have a choice? We do have a choice. You can choose those as our hyperparameters that you'll find via cross-validation. And I believe it's homework three where we started asking people to certain accuracy classifier or homework floor. And later homeworks you're going to have that freedom to adjust the number of neurons to classify C bar as well as possible. I'm going to take one more question here. The question is, does it help to make the autoencoder symmetric? So actually the students are there constant I mentioned the relationship between autoencoders and TCA is when they are symmetric, but it can be viewed as incremented PCA. In general, you can make that decision if e.g. your data concerning, you don't want to overfit. But if you have enough data, then you will have more modeling power. If you don't constrain these, these what are called encoder and decoder matrices. Alright, let's move on. So it is linear. We haven't increased the power or the capacity of the neural network. So to do so, we should choose f to be non-linear. Alright? And then this gets us to one of the major design choices in Northern Alberta start, which is, how do I choose this? Because I have an infinite number of non-linear apps to choose from. All right, a few notes. We talked about how these are called feedforward neural networks. Fully connected neural networks are multilayer perceptrons. F is usually called the activation function. It's applied element-wise to every element that is applied to the vector. And we talked about this also. There's no activation function on z, but we'll talk about absolute applications after we've talked about this already. So let me get to what Tom was saying, which is Conway was mentioning that we talked about how at the output of a neural network, we're going to have a softmax classifier. Alright? You all know a softmax classifier is linear. So why is it that we're doing this high-powered non-linear, non-linear neural network and then put in a simple linear classifier at the output. This is intended because the way that you can think of neural networks working is it takes data that is not linearly separable. But then through the actions of layers one to n minus one, It's applying all these transformations, f of w plus d. That's one neural network layer supplying all these nonlinear transformations essentially to unravel this non-linear, Non-linear data so that by the time you get to a softmax classifier, they are linearly separable. Alright? So again, you should think of a neural network. We have this example of this could be like polar coordinates that change this non-linear classification problem into a linear one where you could draw a line to separate the points, right? But you can't draw any line that separates the points on the left. So in this very simple example, right, renewal transformation that makes these linearly separable. But then in general, what the neural network is doing is this Getting to look at your data. And then through the machine learning algorithm is learning what are the kinds of features. I want to transform the inputs and two, so that by the last layer. And I can linearly classify them with a softmax output softmax classifier. Then these features don't have to be handcrafted because they're learned entirely through the learning process. In this example, this is a handcrafted feature of saying we're going from Cartesian to polar coordinates. Okay? Any questions? Yeah. The question is, I said that softmax is linear, but isn't the softmax function incorporating the exponential, so it's non-linear. So the softmax function incorporates non-linear functions in terms of the exponentials. But the overall softmax classifier is still a linear classifier. One way to think about this is when we have the softmax classifier, right? The thing with the highest score is going to be chosen as the correct class. We talked about when you're comparing spores through that wx plus b is just drawing lines. Linear hyperplane. In this case, the softmax function has a nonlinearity, but it isn't going to change the order of the scores. The highest scores still wins. It's just turning those words into a probability so that we can optimize. And so in the end, just until a linear classifier, because even after the softmax function, the highest score has the highest probability. Sorry, Your question was, is the nonlinearity to find different than what? Then those linearity in? Are you asking because I'm going to just take video and offline, particularly. Yeah, so this is some of my more vigorous here. So when I say linear classifier, I mean that the boundaries between different classes are linear hyperplanes. And I will sometimes refer to an affine transformation as linear, but you're right that I should be referring to it as math fine transformation. So this is formally an affine transformation, nonlinear transformation. Did that answer your question? When I say that, you mean Like e.g. when I was saying like ethane identity, e.g. here, it still makes it linear. The hyperplanes may be different because if you optimize this with gradient descent, you might arrive at local minima that are different. However, the point is to say that you don't get any increase in capacity or modeling power. Because if you want to separate two classes, that fundamentally cannot be with lines, you will never be able to separate them even if you add more layers. That's great. Yeah, So Tom way was addressing the question about how the soft classes of non-linear function. Tomboy said you can also think of the softmax function as teacher normal, normalizer, but that doesn't change. So ultimately in your classification for the same reason seven. Right? I didn't have an example here of the XOR problem, which I'm going to ask you to review on your own just in the interest of time. If you don't know, the XOR problem is one where you have two classes and they take on these points in a plane. You can't draw any line that separates this. But what this example goes through is if you allow there to be a nonlinearity, then you can perfectly classify them. So this is just one example of one neural network that can do a non-linear classification tasks, the XOR problem, alright, so please just review those on your own. Just really support plug-and-play. Alright, so there are a variety of activation functions and we're going to discuss some of the most commonly encountered ones to address this question. How do I choose? And in doing this, we're going to make good use of this good felon quote, which I will explain a bit in two slides from now. But the closer the following. One recurring theme throughout neural network design is that the gradient of the cost function or loss L must be large and predictable enough to serve as a good guide for the learning algorithm. Alright? That probably doesn't make sense to anybody right now. We'll talk about it and just two slides. So let me first introduce our first nominee, arity, which is the sigmoid. So I've used this notation because it's commented describe the sigmoid activation for neural network. You would say that f of x is sigma. X sigma would take the place of the African the neural network. This is a sigmoid unit. You've all probably seen it before. On the y-axis it goes 0-1. And it's nonlinear because it has these curves in it. And around x equals zero. The slope of this line is close to one. Alright? So she started, the slope of the line is not close to the slope of the line. The, sorry, what I meant to say is that this function is approximately linear. Alright? So let's come back to this quote that says that basically, when you are choosing to design neural networks, you want the gradients of your loss, right? So we're gonna have, our loss is gonna be a function of the weights. We want the gradient of the loss with respect to the weights to be large and predictable enough to serve as a good guide for the learning algorithm. Alright, so let's extend that this names. Let's look at our sigmoid unit. And our sigmoid unit. If I were to apply sigma two w. If I were to have a neural network layer with a sigmoid unit, well, we would have is, we would have our wx plus b. In this case, I'm just going to assume that we have one output unit. So instead of a big wx plus b, I'm just gonna write a w transpose x plus b. So this is the activity of just one neuron. I'm going to pass that through a sigmoid. Alright? So I take my neural network repair, that's w transpose x plus b. I push it through a sigmoid, and that's the output activation for this one neuron. This thing will be a function of the weights. Because remember, when I built the machine learning algorithm, I get to make the weights or do optimization to make the weights as good as possible to minimize the loss. Alright. Alright, so ultimately we want the gradient of the cost function to be large. So I want the gradients of loss with respect to my weights to be large. Why do I want this? Well, later on, we know that we're going to be doing gradient descent. Update the witness, right? And in gradient descent, what we do is w is going to be my old w minus epsilon times this gradient, w dw. So if this gradient is close to zero, then I'm not going to let that, right, because gradient descent will say, okay, this is close to zero, then w is just equal to W. W is, and I stayed at the same value. Let's try it out with DL DW is for the sigmoid. So to do this, I'm going to use my chain rule from calculus. I can write this as DL, DW or d Sigma w, sorry, times d Sigma w, d w. You noticed that I will have the chain rule from right to left, which is probably opposite of what you're used to. There's a purpose for me during this time, we're going to try to keep his confession in class because even though it doesn't matter for this example, because Ellen sigma are scalars, right? So this is just a scalar number and this could have gotten on the left or the right hand side. But we get two matrices and vectors for denominator lay out the chain rule always goes from right to left. So please just remember that for this class and I will always write the chain rule going from right to left, right? Tom was question is, when I write this out, am I assuming that L is right after my nonlinearity sigma? I'm not. I'm just saying that we know some gradients of DL DW with respect to d sigma. And then to get the gradient of the loss with respect to w, I would have to use this chain rule. Okay, here's where I want you to recognize. Something interesting, which is if I take d Sigma w dw, there are regions in sigma where the derivative is zero. So if the input to the sigmoid is very small, then the gradient is equal to zero. And if it's very large, gradient is equal to zero, right? When the input to the sigmoid is close to zero, then the gradient will be relatively large. Alright? And that's good. But if the input to the sigmoid is small, then the gradient will be equal to zero. So one potential con, of the sigmoid is, it could have been the case that the input to the sigmoid, right? This is w transpose x plus b. That's going to be some score, right? There's gonna be some dot product and it can be very negative or very positive. If it is, this will be a very small number. So this in the case where the input is very negative or very positive, will be a very small number. And if it's a very small number, then even if this gradient is large and multiplies a very small number, DL DW will be small. If DL DW is full, I don't take a big gradient or by weights will stay exactly the same and ultimately their stock can be anybody that occurs, right? So we want to avoid these situations where our loss functions, there are places where the gradients are small because those small gradients overall, Kilbourne. Any questions there? Had to be following? All right. Great. So Tom way saying there might be another reason if I can be interpreted as plus and there might be another reason the gradient is small, which this ad you're at a local minima and other local Min at no gradients are small. And that could also be true. The way to practically disentangle this is to be looking at their loss function over iterations. The question, is it Daniel? Janice question is, do I techniques like batch normalization or other regularizations ameliorate this at all. In general, they can help. With sigmoid units are attendant or whatever you get is better validation accuracy. But they won't solve this problem of if you're in a rural part of the gradient here. Alright? So let's get into talking about the sigmoid unit. Prose. Around x equals zero, the unit behaves linearly. And another pro about it, which actually isn't really important, is that it's differentiable everywhere. So some empirical evidence that this differentiation being actually doesn't really matter. But, but people like that, these are differentiable. Alright? But what are the cons of the sigmoid? So the first is that when we talked about at extreme values of the inputs, the units. When I say the unit, that's another way of me saying the activation function. So these are interchangeable terms. The activation function saturates and has zero gradient. And if it has the road gradients, and we've already shown that DL DW will also be zero and therefore no learning occurs. Alright? There's also something about the sigmoid unit that results in so-called zigzag ingredients. So I'm gonna go over the following. The following is really more to make sure that we're understanding gradients and category the center of these activation functions. But it turns out that this is the exact ingredients solving that problem that I'm just about to talk about. It's actually not that concerning because they still do well even if the gradients zigzag. All right, so the sigmoid unit is not zero-centered and actually more importantly, the sigmoid unit is always, is always non-negative. And this is going to lead to a problem called zigzagging gradients. But you can practically it's not that big a problem, but you should understand it. So what's going on here? Let's say that I haven't neural network. The neural network is going to take an input x. And then it's going to apply a weight vector w1 and the sigmoid unit to get half the patients H1. And then there's going to be, and not tell you 1 hz is W and H. Then maybe it goes through another weight matrix and sigmoid to get W2. Let's start H2. And then there's another weight matrix to get some output the city. And then, sorry, I should have, I'm going to update the slides to say these axis are going to become Hs, h1h1 and H one. Alright? So let's consider a scenario where activation function f is the sigmoid. And we're looking at the sigmoid applied to this affine transformation of taking H1, dotting it with the weight vector, adding a bias and then I'll give you a H2. Okay? Actually, sorry, there shouldn't be an issue. They should actually just go to, sorry, there should be an H2. Let me call this output z. I'll put Nazi, but why? Because I do see the next. Okay, so I'm gonna do a few things. I'm going to define z to be w transpose h one plus b. That means that this function over here can be written as f of z, because z is equal to w transpose h one plus b. What I want to do in this example, it's a wanna compute the gradient of the output of the sigmoid activation with respect to these weights w. Alright? The way that I'm going to do that, this is the answer, but we're going to do the work. The way that I'm going to do that is via the chain rule. So the chain rule, I'm going to have a DF Thomas asked me to add another page. I'm going to keep this time way too, so I can reference other things on the slide. But if things get a bit messy, paper be done and I will go to the next page. Let me, let me just erase this always non-negative and put it on top and that all may be hopefully clear up. Some space is non-negative. Okay? So you have TW. I'm going to compute this gradient by doing DFW. I'm going to use my chain rule. So I'm gonna do DZ. I'm going to have a DZ, dw, again, where z is defined to be this w transpose h one plus b. Alright, let's go ahead and compute these gradients. So the first thing that I'm going to do is I'm going to compute the gradient d fw, DZ. Alright? So here, F is equal to the sigmoid function applied to z. So f is equal to sigma of z. And we know from this previous slide, or you can take me at my word here, but you can derive this. The derivative of the sigmoid function is given by Sigma of z times one minus sigma is the derivative of sigma of z with respect to z. Is this, alright? So this derivative here of f with respect to z is going to equal Sigma of z times one minus sigma xy, right? And that's the gradient in. The next one I want to do is I'm going to do this GZ, DW. Let me do this in a different color. I'll do this one in green. Gradient of d z, d w. We know that z is equal to w transpose h one plus d z equals w transpose h one cross B. We know that when you take the gradient of this with respect to w, It's just going to equal h one. And that was from when we did back later. So d z d w equals h one instead of this dG dW term here is equal to h one term over here. So that's the gradient of the output of the sigmoid unit with respect to the weights. Any questions they're using under the following. Okay. So why have I done this? I want you to notice something which is owned by the way, the reason that these things could change orders, like I have green times through here and then this is really tiny screen is because the blue thing here is a scalar, right? So the scalar in front or behind. So sigma of Z is sigmoid applied to something. Sigmoid applied to something is always bigger than zero because sigmoid is non-negative. One minus sigma z is always bigger than zero or bigger than or equal to zero because the maximum value sigma is you can take on is one. So this term here is also greater than or equal to zero. Then what is H1? H1 is the output activations after applying some sigmoid activation in this neural network layer, right? So H1 is the output of sigmoid units, all of which are greater than or equal to zero. So H1 is also non-negative. Okay? What this means is if I want to calculate the overall gradient of the loss with respect to the weights, right? If I want to know in this corner here, the gradient of the loss with respect to the weights. By chain rule. This is going to be the gradient of the loss with respect to DFW. Then we'll have this term here, d f w dw. And we know that df w dw, this expression. Let me use a different color. Now I'll put this in purple. This term in purple is always greater than or equal to zero. Its terms are always greater than or equal to. What is DLD I felt with you. If I look at the LDF, WE DO DFW is gonna be the gradient of some loss with respect to the gradient of the output of a city water. The reason I say that is you should all recall Abbas the scalar and the output of the sigmoid is also a scalar. So this thing is a scalar. And if it's a scalar, it means that it's going to take on some value. It could be bigger than zero or it could be less than zero. But it's going to just be a single number that's bigger than or less than zero, okay? Because dF, dw is always, is a vector which has the oldest element is greater than zero. What that means is that all of the, all of the terms in the gradient are either less than zero or greater than zero. Now what that means is that you imagine having to wait. So I'm gonna draw a space where we have two weights, W1 and W2. My gradient descent step is going to be in the direction of DL DW, right? But if DL DW has all positive values, that means is that if it's all positive, that I can always just change w1. And W2 together to interests together. Or maybe they're both negative. In which case, if it's less than zero, I can change W2, W1, both in the negative direction. But I can ever stepped in these quadrants because they correspond to a W2 being positive and at w1 being negative or vice versa. This is Barney some settings. So maybe our optimum, w is the top right hand quadrant right there, right? If it is, then I can take some gradient descent pathway where I increase both W1 and W2 to go to the optimal w. Like this, like remarks. However, if your optimal W isn't the bottom right quadrant. But I can only take steps in directions where I increase both W1 and W2 or decreased both of them. Then the way that I would get to this optimum would be to zigzag like this. Because remember, I can only ever both increase w2 and w1 or decrease them. And this leads to a zigzagging pathway to get to my office. Tom way of saying, I drew this graph with H2, H2S on, in this problem said, alright, so I didn't need to have this H2 there. All right. Any questions there? Sorry. The question is, is this offer two layer network? So this actually happens whenever there's a sigmoid anywhere. So this would be true for any layer neural network that is easy activation that we would always have zigzagging presents. An optimizer. You said? Yeah, the student pointed out that I have my green and my pink slips because you stopped in the direction of the negative gradient. Statements are correct there. Yeah. Alright. I sort of walk there. I also looked at a time and so I think it's time for a five minute break, but during the break, please think about this and then we'll come back and answer any questions about this exact ingredients. At this point. When you see interpreters Clark, did you like the values of the scores? Once they both put together? Yeah. Yeah. So the scores you could just think of as the higher you are, the more likely it's intact mass. And it's always a relative, James. So the highest score is the best way that the spores are. In terms of like the linear hyperplane. To say the following. If you have one more upstage, 300, et cetera, and then you'll have with your Dr. no other partners have different colors. It's like sounds like it might be like or the composition of those will give you the linear separating places. Yeah, because at this point here, 3.4, 400, yeah. If I hover over my score for 75 for, for class 125, okay? This is not in dispatch. So softmax is this is the linear classifier and nobody can stop softmax. We need a way to find a job. So we need to make this into a loss function. Softmax is a normalization of working for a public document a lot. Like a transformation is solvents. But this makes me feel good job. Yeah. This is a classifier. Renormalization. So surfactant. If we normalize, it doesn't change the order of the sports I want to follow. Whatever the highest score will still be really nice distinction. I can't. Yeah, it's, it's something that you would have to live with and could lead to four. So we will actually not. We yeah. Okay. Yeah. So really it just take a particular researcher will have to do. Okay. Yeah. Alright, everyone will get back to this again because we did a lot of parts here. And for some of you might be the first time that you're playing with radiation thinking about chain rule. So I want to ask if anyone would like me to explain any part of the work here. Again, this might also be something where at least when I took this class, I had to definitely review it if you've taught on my own for everything to click. Those shoes, don't worry about that. That's, that's totally natural. Questions. Great. So Nathan's question is, can I explain this on the left where I started? Dl DW is, has all of these elements, either positive or negative. The way that we get that is we realized that dF, dw is always non-negative. And that came from this step over here. So these terms are always greater than or equal to zero. And then GL, DFW is gonna be a scalar. So because the scalar, it's either a positive number or negative number or cookies. Or if it's a negative number, then it's a negative number times a vector where everything is positive, so every entry will be negative. That's fine too. That's pretty impressive. And I see the student is asking, there could be contexts where f of w could be a vector. So this would change this problem quite a bit because if f of w was adapter, then they W would be a matrix. But then if you worked through all that, you'll still see the same problem. The question is why H1 always greater than or equal to zero? That's a really good question. It's because each one was the output of a neural network layer where the nonlinearity was a sigmoid. So H1 or all values after a sigmoid sinus sigmoid is always bigger than or equal to zero. That's a great question. Yeah. The question is, why did I overwrite x with each one? Is because actually try. I'll put up a neural network bear. So X will have both positive and negative values. And so I wanted to point this out for the layers of the neural network that they will have. The exact ingredients requires it to be the hidden layers. Because actually this student's question, H1 is the output of a sigmoid and that's why it's always positive. Great. Yeah, Tom way such that I had earlier because for CPR is 0-255, it could have been. So C bar, it's also all positive values. However, in general, x could also be negative. So it's just a city this with each one. Great. So this student's question is, for the first layer, you may not zigzag. Is that right? This is actually correct, but none of you should know the reason why yet, because we haven't derived back propagation. So it would be for these to help us later on where it's exact gradients occur. But we'll see that when we derive that. The question is, can I re-explained the connections of the zigzag? I'm from the bottom left here, right? Yeah. So if DL DW is, say, all positive, that means that when I take a gradient descent step, I can only change W1 and W2 to both increase. And actually this should be decreased because of negative signs, but I'm just going to increase clarity. So DL DW has all at least positive. Then it will say if you increase W1 and W2 together. And so that's why you can only take steps that go into a bank might go in the first quadrant where W1 and W2 are both positive or negative, it can only go in the third quadrant. Clients want to be stuck for W1 and W2 both change and put decrease. The question is so it's just the opposite. Yeah, So because the updates are always positive or always negative, that constraints the directions that you can update it. The question is, what is l in this context? Healthcare, so boss function. So it'd be whatever loss function is that the output of your number, e.g. the softmax loss that we derived rocks are all negative. But the question is, when I say all positive or negative, is it referring to the weights in a given layer or the entire neural network in this case and just didn't give them better. Take one more question here. The question is, how is the derivative of the loss function with respect to the sigmoid calculated. Oh, sorry, How's did a scalar 0? Because the output of this sigmoid will be a sigmoid. So in this construction, f of w is sigmoid of a dot product, which is a scalar plus a bias b, which is a scalar. So that's why this DFW. It is a scalar in this example. Great gas. So the student is saying, I shouldn't this be like a big matrix W times a times the vector h one plus eigenvector of these. And so the reason I haven't done it in that case, I just did it for the simple scalar and vector example is because we haven't yet learned how to do the chain rule for derivatives on matrices, the derivatives of scalars, but we will do that next lecture. This term is a scalar and this term is a vector. All right, I'm going to move on for now. Again. You may have to contact the slide, I think is a good example to play around with to be sure that you're understanding chain rule and had it been different gradients, alright? Okay, so not the sigmoid, you, we might often use a candidate activation function, which is just a sigmoid that goes from minus one to one. Alright? And sigmoids are rarely used. If you want to use something that has a sigmoid shape, you can gauge the tan h has the same prose as a sigmoid. Around x equals zero. It's behaves linearly. But the other thing is because it's, the tangent can be positive and negative. It avoids this zigzag ingredients problem, which again, empirically speaking, is not actually a problem, but people always teach it and talk about it and it's good for understanding. So it's really not that big of a deal, but it's good to understand. Great student tomboy set is to ask why is it good for a unit to behave linearly about the origin? For this, I'm going to just give the answer from this slide, which is a line has a relatively high radiant compared to these sections where the gradient is. And so as long as this line of high slope, our gradients don't die off. Going back to tan h. Tan h is just like the sigmoid. It saturates. And so if you give it that is too high or too low, then you're going to have zero gradient and other learning. That's the ten is this. The ReLu unit will be the unit that we will use for this class. And it is by far the most widely used activation function and deep learning. So the Rayleigh unit is this function that I defined before, basically max of zero. And so this is what it looks like at the input to the ReLu is positive, then it just returns that value. And if it's negative, it returns right here stands for rectified linear unit. This is the raven function we talked about. We're going to have to be able to take its derivative. And some of you may notice that ReLu has a place where the derivative is not defined with Sarah. What did we do in these cases? Because I don't know that the derivative at zero for the radius undefined. The following I'm about to say is beyond the scope of the class, is just giving you the term. So if you want to look into this further, but if you were to take a class like convex optimization step in to 36 years, See, that's the class that introduces a concept called subgradients. And subgradients, our gradients that you could still do gradient descent breath. So instead of doing gradient descent with the derivative at x equals zero, which is undefined and we can choose a sub-gradient of it. What that means practically for you all is at x equals zero, we can just define that the gradients is either zero or one. Both of them will work. Alright? So basically derivative of ReLu, of x, it'll be one. So the slope is one when x is bigger than 00, when x is less than zero and zeros, you can choose it to be zero or one. It doesn't matter. Both are valid. For gradient descent. This is another thing which will be in these slides, which is sometimes people do it as a pro that a function is differentiable. But empirically it doesn't make a difference and actually it incorrectly, but non-differentiable ones end up doing better. Okay? Alright, So the retailer units prose. So the first one is that in practice, learning with the railroad, you didn't converge faster than the sigmoid and tanh image. So we're going to talk about this paper in 2012 called the introduces an architecture called AlexNet. This is the first ImageNet competition, whether that's a neural network and it significantly improve the performance of classification. And they reported that paper that ReLu Was six x faster to train with then tan h. So empirically is a lot faster to optimize with David and Tammy. When the unit is active, it behaves as a linear unit and its gradient is equal to one. So over here the gradient is equal to one, right? No, that's good because if we take a GL ReLu, times of d ReLu way, right? Because we know that this gradient is the derivative with respect to x and put it hasn't written equal to one. You'll only die off if the input to the rabies is make it. So when a unit is active, meaning the input is bigger than zero. I wasn't being the derivative at all points except x equals zero is zero or one. And that's very simple to implement. And then there's no saturation as long as x is bigger than Sarah. Columns. Relu, like the sigmoid is non-negative. And so therefore, it also has this exact ingredients problem. And this is where we learned in therapy that it's okay, There's something is exact because effectively does best. Relu is not differentiable at x equals zero. However, like we said, you could just set the derivative at x equals zero to the zero or one and everything works just fine. Then again, the more rigorous explanation for why that is will be in 236 is it BRC Tom, why would they do somethings to 36 ft? We'll talk about. And that's beyond the scope of this course. So you won't be tested on any subgradient from this course. Alright? One con is that if you are in the place where you have zero activation, if your inputs, their baby was negative, then you have zero gradient, gradient and it doesn't occur. So that's one of the questions here. The question is, is there a reason why all of the activation functions are monotonic? That's a good question. The output then you would probably want it to be monotonic because higher score should be higher probabilities. The activation functions do not have to be monotonic. So I'm gonna give an empirical answer here. What's the TAs have a better one for the monophasic knowledge necessary. Do you like? Yeah. Yeah. You could use a non-monotonic function if you wanted to. If I just doesn't perform as well. Alright? Textbook wants the gradient in this case. Why don't the maximum of zero? Well, what property with some older floor. Oh, so tomboy saying, why don't we put an Alpha here to scale the gradient where outfit object. You could do that as well. But it's just a scaling factor. So yeah. Yeah. Yeah, so tomboy is asking, why can't we make this alpha x, in which case the gradient would be alpha and you can make alpha larger because good fellas, I said you want ingredients to be large. So good. Hello statement is a relative one. You wanted to, like in the tan h you wished it was always Sergeant to do sometimes decrease. But then doing this by alpha would have the same effect. Essentially, it's like scaling a weight by alpha. So what matters here is the relative. Yeah. So the TAs are just basically saying there are a bunch of dogs that can control the scale of these gradients, e.g. also the learning rate. Okay. So with Raymond coming out, shortly after came out, people were like, Oh, well, instead of ReLu, let's use the soft clustering it. The soft clustering it kinda looks like a ReLu, but it's an actual function that doesn't have a discontinuity. The gradient is everywhere defined. And therefore it's a version of the railroad that is differentiable everywhere. But when you run the actual experiments. This is a paper by Zagier, Laura. And I want you to focus on a few columns, a few rows, sorry, this row here. So performance of ReLu. And then this here is the performance of soft costs which is everywhere differentiable. And what you can see is that when you look at error rates, so the lower you are, the better ReLu, outperform stock plus everywhere. So even though soft costs. It's differentiable everywhere. That doesn't really help for you. I really was still better. Question. The question is, what about the performance difference between ReLu and thank you Bailey? So let me first tell the rest of class we're thinking it is a leaky ReLu is this function where instead of when x is less than zero, everything being Sarah allows it to be a slightly sloping negative line. So in this example, I wrote at this line here is 0.1 times x. This is the alpha here. Even though I wrote this is 0.1 times x. That's just a visual thing. If I made this 0.01, you wouldn't be able to see it. But usually alpha is on the order of 0.01 for the leaky. And so in this case, ReLu is not saturating on the left and that doesn't go to zero. There's still gonna be some small gradients there. And then there's also something called prelude. Prelude is a parameterized leaky ReLu, where this alpha, instead of being set by us, can be learned through optimization as well. Okay? So that's what it is. Alright? Any questions on ReLu, soft costs? You maybe prelude. Right? Great. If I could put your question into my own words. I, because I think I had the exact same question when I first learned this stuff. It looks to me like tan h is quote unquote more non-linear than ReLu, right? Relu is like it's linear or zero. Whereas Canada has all of these curves. And one might think that because tan h seems quote unquote, more non-linear than ReLu, it should work better. Is that basically what your question is? Yeah, So that's a good question. I would say that our intuitions about like what is more non-linear breakdown when we are composing and many of these within their own effort. There's something that we won't talk about it as classical theory of deep-learning class, which is that important neural networks have something called the Universal Approximation Theorem. You can actually show for an infinite width neural network using even just a ReLu, it could, it could implement any functions. So the mere act of adding a nonlinearity, even if intuitive, it doesn't put quote unquote that common here gives us the capacity of the model, any function back there? Sorry, can you repeat the first part of the question? Yeah, Great. Question is, I've been saying performance. Does this mean classification error or does it mean the amount of time it takes to train it? I've been using performance to mean classification error. So the ultimate train network will reproduce, will have better class, will have lower classification, classification error. But it also happens to be that babies also the fastest. I think one more question. Yes. So Tom Weiss, that is one pro of ReLu is that it has this on-off property that either active or snack. Alright. So I'm going to show there's a lot of people have made their move units. This is the exponential linear unit. You can look at what the function is. I have this result from this 2015 paper that compares ReLu, leaky ReLu. Relu is a shifted ReLu, a mood is that exponential exponential rule. I mean, the point of this is to say, you can probably get some increase in performance by using a leaky ReLu or exponential linear unit. But oftentimes those differences are marginal. And so typically in practice, there's also a Mac side units that they did fellow invented, it says in the textbook, but we won't test you on that. And practice. The Rayleigh unit is going to be the thing that you could start off with. You should probably not use sigmoid. And if you're going to use sigmoid, you should use tan h instead unless you have a reason for activations to be non-negative. If your thing is, if your network is doing well, it's going to be worth trying out. Vicky ReLu, preview. One of these more, one of these hire, one of these things, but additional hyperparameters are, alright. Any questions on activation functions? That's leaky ReLu avoid is exactly yes because it can be negative. But needed a click. Event. We can have linear approximation. Okay? Yeah, yeah, So a rupture the same non-linear decision boundaries can be approximated by piecewise linear decision boundaries. Which is also a really good point. We're going to see that when we talk about adversarial examples. But you can pick up the railroads giving you interrogate this piecewise linear boundaries which are nonlinear. You didn't follow that, no worries. Okay. That's the neural network architecture we've talked about. Now there's one more thing which is an output activation. When I say the output activation, I mean that we're going to take our scores Z and then we're gonna do something to them. So in one case we might take GFC where g is the softmax function and then do a softmax classifier. Or do you might take Z and you might compute the mean square error. Whatever this is, I'm going to call this the output activation. And the question becomes, what output activation do we use? I want to talk about this because I think it's often overlooked thing that it really is important to choose functions at the output wisely. So we're gonna just do an example that I hope convinced you of that case. We're going to consider a neural network. The neural network outputs one score at the output Z for binary classification. So it's going to output a single z. If z is large. We're going to say something is in class one. And if z is actually, let's say negative, negative Z. The more negative it is. We're going to put that in class, Sarah. This is large positive c. And this would be large negative z is in class there. So there's just one score. If it's positive, classical I knew it was negative class. So this is different than the softmax or we would have a score for each class, compare them. But can actually be shown that softmax for two classes reduces in this class one, we're going to say that our output, I'll call it y e.g. I. Would be equal to one. And if this in class, the road and wife, e.g. I. Will be equal to zero. So one way to build a binary classifier is to get the score. And then just like the softmax, turn it into a probability, but passing it through a sigmoid. So we're gonna say sigmoid of c is equal to the probability that X is in class one. Because this is binary for the probability that excising classes are only just see one minus sigmoid. So this is a binary classification problem. And then the softbox is the multivariate generalization of this binary classifier. Alright? We're going to consider two loss functions. So the first loss function is mean-square error. What I do here is I take my Y-i. So let's say that the correct class was one. We put a one here for y. And if my classifier was really good, and z I should be a large positive number. So then sigma z I would be close to one, all right, and then vice versa. If y was equal to zero, hopefully Z i's are large negative number. And so then as sigma of z i would be plus. Alright, so this seems like a really reasonable loss-function. In fact, a few lectures ago when I was talking about what loss function to use for the softmax classifier. One of the students raised that they can take a mean square error of one-hot representations. And this is what that case. One byte in a few seconds asking you said there'll be a one by n Here. Yeah, we can put an n here also as a normalizing factor. There's also the other loss function which is cross-entropy loss. And this is the loss, the same max likelihood loss. We derived for soft max, just simplified for two classes, we derived for softmax. What I hope to show you through this example is that whenever you have a sigmoid or a softmax activation of the output, you always want to use cross entropy and you never want to use mean-square error. Alright? Why is this the case? So let's draw a picture for intuition. We're going to have on the x-axis, our score, e.g. I. Saw call that z on the y-axis will be the output of the sigmoid. E.g. I. Will call this sigma xy. This is what our sigmoid looks like. And here is one, and here is what the sigmoid looks like. If I want to compute the mean square error, just e.g. I. So the mean square error, e.g. I. Am going to call this MSC superscript I. So it's just for one example, this is gonna be the value of y of Pi minus sigma T of I. This whole thing squared. That's the definition of mean square error. And then just like we've been doing before with gradients, I can write what the derivative of the loss, which is mean square error, is with respect to d z by using my chain rule. So I would write this as d mean squared error, e.g. I, d sigma z, and then d sigma z, d z. Alright? What I want you to notice is the following. Remember that sigmoid has these points where the gradient is zero. So this term here, it equals zero when z is very negative or very positive. But let's say that Z i is some value like negative 50. So DI here is equal to -50. And let's say that for this example, it happens to be the case that the true label y is equal to one. So y is equal to one. I want my network to change by the base to make ZI as big as possible. But because of our initialization, we started off with z I minus. What you should see here is that this network will never learn to make this bigger. Because my gradient is going to be zero. Because the gradient is over here, is there. Alright? So because d sigma d z is equal to zero when z is -15, even though I want the I to increase to a positive number because y equals one. Alright? Why is this in terms of an intuition? Let's say that I were to try to make it change my network ports to say, Okay, I'm going to try to make the eye from -50 to -40. Alright? This case, let's calculate the mean squared errors. So if I compute YI minus Sigma of -50, here, y is equal to one. This is approximately equal to if I try to change z to be -40, right? Well, the sigmoid had zero spoken, so still sigma of -40 is still going to be zero. And so one minus zero squared is still also going to be approximately equal to one, right? That's just another intuition to see that we're in bad shape here because the network has no incentive to change the parameters. Because when it starts thinking the parameters in this area, my loss remains the same. So if we were in this situation where I started off at a bad CGI and I want to increase the eye because I have zero slope. I would never burn any questions there. Okay, So I've drawn this picture or I, I've written this more formally by taking the derivative of the mean square error, e.g. I. With respect to DCI. And this is what the derivative looks like and so on the function, sorry, I'm just pardons. The x axis is the value of C and the y-axis is the gradient. This is what the gradient looks like, which is this function. If I plug in y equals one, alright? So if I look at this plot, I can see that there are a few regimes. If I'm in this regime, things are splendid because when y equals one, I want c to be large. And if z is large, like gradient is close to zero. And so there's no learning because I already have. Even if I'm in this range, things are fine because here my gradients are not zero. So the gradients are not zero. And I'll go end up changing the weights so that eventually z increases. Alright? It's in this regime over here that things are really bad. Alright, dad? Because here is very negative. I want him to be positive. But here the gradients are all close to zero. So no learning. On the contrary, if you were to differentiate the cross entropy, e.g. I. Will leave this as an exercise for you all on your own. With respect to z, this is what the gradient looks like as a function of z. And this is really good because when we are close to the correct answer, we have less learning the gradients go closer to zero. But the further away we are from the correct answer. The more learning happens. In fact, as G becomes more negative, I'm going to make larger changes to my network to try to make it more possible, right? So if you have at the output of your network a sigmoid activation or a softmax. You definitely don't want to use these square error. You want to always be using a cross entropy loss or the max likelihood loss, because it has the desired behavior that when you're far away from the correct answer is gonna give you big gradients to change your. Alright. You raised your hand if you follow that. Awesome, That's most of the class. Any questions on this? Great. So the students question is, could you address this? Let's say that you're in some situation where someone's like no, you have to use the square error loss and they don't want to listen to reason. What's something that you could do? You could change the initialization to be in a regime where hopefully all the starting to, the eyes are small and you would, you would hopefully have your age. They're going to later find out that initialization will matter a lot for these neural networks. Great question. It's better. Z always is bigger, is always better here. Only because I defined that Y equals one. So in the case for y equals zero, we would want smaller, say, Oh great, nasa mission is asking, Would there be a problem if y equals zero being in the green region, the answer is yes. So this polymer also works in the reverse direction. If y equals zero and you start off with a high Z, you won't have anybody because of Surette. Yeah, that's a great question. Right? The question is, is this the case? In general when you use softmax or sigmoid with mean squared error and the answer is yes. So basically like the, the key thing about cross-entropy loss is that it'll have this log that and does that exponent, which then leads you to not have this saturation. Alright? So other types of output activations be on the sigmoid. You can just have a linear or identity activation. I wrote something here which is related to statistics, but you won't be tested on it, so don't worry about that. Most commonly our output activations are going to be the softmax outputs. We're gonna be using that for the rest of this class. And we're going to use that in tandem with the cross-entropy loss. I have a question here which I'll, I'll let you all dry it on your own, but you should see that if you were to have, let me just draw it out really quickly. This function here is an output that looks like maximum Sarah. And then Min of one z. So it looks like this. And this would not be a good output to use. Again for the same reason as a sigmoid, that if you're caught up in these regions where there's not going to add. Okay, So again, in this class we're going to be using the softmax output activation in tandem with the cross-entropy loss. Cross-entropy loss is the same thing as a max likelihood lost that we derived last lecture. Alright, in the final two slides here, I just want to motivate, but we'll talk about next time through it's the backpropagation algorithm. So basically, we have our neural network architecture. Now we have chosen what the activation function is f will be and what the output activation function g will be, which goes to Softmax. Softmax uses our cross-entropy loss of power. After we've defined this entire architecture, we know all of the functions being used. We know the weight matrices, W1, W2, W3. And we have our loss function that tells us how good our model is. In our machine learning problem. We know that we're just missing one more thing, which is, how do I change W1, W2, and W3 to make L as smallest possible. For that, we know that we needed to compute things like DL, DW. Right? Now the problem is that this function for the neural network when you impose everything becomes pretty complicated. And w1 is pretty far away from Tau. And we need to be able to get this gradient to know how to change W1 to make Alice small as possible. So the algorithm that we're going to begin, then next lecture will be called back propagation and it tells us how to exactly compute these gradients so that I can change the w's to optimize. So you get that on Monday. Individual function when x is negative. 
Alright, let's get started. You only have one announcement before we begin. And that is that homework number three is due a week from today. And please be sure to print out your Jupyter notebooks as well as your code for that assignment. All right, any questions on NH4 statistics? Okay. I went through this tensor derivative last factor and it was a bit fast at the end and we didn't get a chance to ask too many questions. So I wanted to come back and recap this and I'm happy to take any questions on this tensor derivatives. So we were in this study where we have y equals wx, That's right here. Here, y and x are both vectors and w as a matrix. So if we have d y d w, d y d w is a derivative of a vector with respect to a matrix. And we talked about how you can think of this as a 3D block. So what we can do is for each element of y, we can take the derivative of that scalar element with respect to a matrix, which we know is a matrix of the same shape as W. And that's this red matrix here will, would therefore be the derivative of the first stellar element of y with respect to the matrix. Now we can do this for every single scalar element of y and that all these matrices together we get a 3D block. And that is what the gradients dw will look like. It's gonna be a 3D tensor that looks like this block right here. Alright, Were there any questions about that? Russia. Music, pop music. Oh, I see. Okay. I see what you're saying about it. Yeah, so we talked about how in denominator we add the dimensions are m by the thing in the denominator. The W are the leading dimensions of the resulting tensor. So it's going to be n by n, the dimension of w, by m, The dimensions of y. And so if we're thinking about this block is having the height in the first dimension, the second dimension, and the depth d The third, then you could rotate this 90 degrees if that helps you to visualize what that cancer is. Thank you. Other questions? Okay. So then after that, we did this problem where we have z equals y minus WX. And we ultimately wanted to first take the, sorry, I wanted to first take the gradient of z with respect to w. And so we did the operations for that. Following the exact same logic over here. What we did is we broke down into its scalar components, ZK, and we wrote what each of those dk are. And then we differentiated those ZK with respect to w. And that gives me one matrix. Alright? And then I stack all of these matrices for k equals one to the dimension of Z, which is N. And that will give me a 3D tensor corresponding to the gradient of z with respect to w. So this was the derived matrix, which is the gradient of a scalar element of Z, Z k with respect to w. And we saw that it was a matrix that's everywhere zeroes except for the kth row by x minus x. And what we then saw is that by just changing Z1, Z2, Z3, all the way up to Z, right? If I just stop all of those and I have the 3D gradients, but 3D tensor gradient d z, d w. Were there any questions from this slide? Alright, so here's the last part of it. In our question that you're trying to solve for, we had a scalar epsilon. Epsilon, I believe was equal to something like one-half Z transpose Z. And I want to compute d Epsilon dW, where then I would use my chain rule, which we derived last lecture goes from right to left. So I would take d Epsilon DC and then multiplied by d z d w. This is going to be that 3D tensor that we just derived. Times a vector. And we saw that the operation of this 3D tensor times the vector will simplify to a matrix. And if we went ahead and wrote out this operation, we said, we see that this thing here, d z d w times d epsilon dc, mathematically simplifies to d Epsilon DZ. That's this term right here. Then the effect of multiplying on the left by this 3D tensor d z d w. Simplified to take the d epsilon d z and actually write multiplying it by this one detector. So the Chain Rule runs right to left. It's a, formally to compute this, we have a 3D tensor times a 1D vector, but when we simplify all the math, it turns into a 1D vector times another 1D vector transpose. So this is the result that we get from the Chain Rule by following these steps precariously. Alright? Are there any questions over here? The question is, is the same as well? Yes. So in this example, if you look at the slide from last lecture, the loss of epsilon was just one term of the loss for one example. But the loss is the sum of all of these epsilons. Other questions. Okay? So I also talked about how the last lecture, the 3D tensor that we went through is really to show you all that everything works rigorously and nothing is magical. But really there is a much simpler way to arrive at this answer. Kind of just using intuitions about what gradients to reflect and also naturally dimensions. So I'm going to redo this because this is quite important and it'll be the way that you usually end up doing these ingredients for backprop your assignments. So we had derived this result earlier last lecture, where if I take w x, which is a vector, the product of these is a vector. I differentiate with respect to x, I get this matrix W transpose. Alright? So even though I know that this gradient of a vector with respect to a matrix is a 3D tensor. If I kinda follow the rules of how when you differentiate with respect to a variable, you get the other variable, right? So if I differentiate w x with respect to w, actually get something that looks like x transpose. We can say that the gradient of w with respect to w looked like x transpose. Because it's not rigorous because this gradient is a 3D tensor. X transpose is a row vector, but it should look like that because of this trend that we see in the gradients. And so what people usually do is they say, Okay, if I know d Epsilon dw is an m by n matrix, I know that the Epsilon DW, right from chain rule is going to look like a swan. Dz times a DZ detail for you. What I can say is I know that this dw is going to end up being the epsilon d z times the gradient dv, dw look like. So this d epsilon d is the I copy down here and that's an n by one matrix. Sorry, I meant by one vector. And then I say this gradient of w x with respect to W. So let me change all of these years to i w x's bit more clear. Wx, wx, this gradient dw, dw soup look like x transpose. And so if I fiddle around with dimensions, X transpose is one by n. If I take this m by one and multiply by one by n, I get an n by n. Alright? And by doing that dimensional matching, I can come up with this as an argument for what this gradient should be. It's not rigorous, but you can use this in the future. We have already shown it is rigorously true by doing this 3D tensor. But now moving on to the future, you can just use this results and just try to match the dimensions to get your gradients. Alright? Again, this is not rigorous, but it's a really handy trick moving forward. Any questions here? Yes. We verify this by using the numerical gradient. Yes. So in homework number three, we will have to implement backpropagation and there'll be a numerical other questions. The question is, does this gradient here denominator layout? It is. And I think I know where the clustering comes from because in denominator layout, we need to multiply. We need to go from right to left. But here in the final answer looks like we're going from right, from left to right, right. So how is this denominator layout? It's because this expression is a simplification of having done correct denominator layout where DC dw is indeed on the left. But this is a 3D tensor. And when you do this, calculation out, simplifies to multiplying by X transpose on the right. So we did things rigorously denominator layout with the chain rule going right to left. It's just that when you happen to simplify it, it ends up as the same effect of multiplying X transpose on the right. That's really important to remark. Any other questions here? Great homework. Question is, does this simplification only hold for 3D sensors or does it go for 4D tensor or higher dimensional vectors? And it does hold for higher detectors. So given that, I'm going to go ahead and the next slide and just summarize everything that we need to do back propagation for all of your homework questions as well as for the coding. Alright? So the first thing is you will have a neural network layer. Neural network layer. We're going to multiply a matrix W and some input x. And that's going to give me some vector hidden activations. I'm just going to call this y for simplicity, since we've been using Y, X, and W and these examples. So we're going to have y equals w times x. And in backpropagation, I'm going to have some upstream gradient, DLD. Why? This is gonna be a vector derivative of a scalar loss with respect to the vector y. And I want to know how to backpropagate to both w and tax. So here are the rules. If I want to backpropagate to do to x first, I want to compute dy dx. Then we derived this is just gonna be del dx equals DLD y on the right. And then there's going to be a DY DX over here, which we derived already is w transpose. So backpropagating to D L dx will be w transpose y. The other more challenging one that requires this tensor derivative. But from here on out, you can just solve by saying the gradient should look like a transpose and then match dimensions. We would have that DL DW, He's going to be DLD Y times X transpose. All right? And again, you can kinda see the symmetry and b's when we backpropagate through this multiplication, right? We're multiplying by the thing on the other wire transpose. So backpropagating to DLD x, I multiply it by the thing on the other wire, which is w transpose. To get some DL DW, I multiply DLD, why my upstream gradient by what's on the other wire transfers, right? And this is the thing that we've rigorously derived. Capture any questions there. Okay, so then we're not going to show the following in class because it would involve doing 40 tensors. But this pattern that we see the generalizes ever want to use that dimensional matching tricks that I talked about calcium back there. The question is, why is X transpose on the right? It's because when you go ahead and you do the gradient using the chain rule, when a 3D tensor is on the left and you simplify through all the math, it ends up being multiplication by x transpose on the right. So we did do the chain rule correctly, where this 3D tensor is on the left. It's just that when you mathematically simplify everything, as it turns out to multiplying by a row vector on the right question. And that's something again where I encourage you to just tilting to you. Please just go back and watch the lecture or watch the segment on this 3D tensor derivative. And I'm also happy to take any questions here. The question is, why is this w t, w transpose on the left? This is chain rule. So the chain rule is ideal dx equals DLD y times DY DX and DY DX, we derived last factor is equal to w transpose. So this is just straightforward chain rule where this simplifies to w transpose. This is also the chain rule, but it's sympathizer, right? Multiplying by X transpose. With this, you can back propagate through a neural network layer. I was just next thing also, we're going to write down the backprop rules for if you have a maintenance funds, a matrix. So let's say that we're in the setting where we have a big matrix, Y equals W times a big matrix x. And here I'm going to say w is, sorry, I'm going to say x is n by p, w is n by m, and then y is therefore n by p. So if you tried to take the 4D tensor derivative of a matrix Y with respect to the matrix W. That gets carried very, but the same kind of intuition that I mentioned before, it happens in that case. And following these rules can be used here and that's all you'll be responsible for for this class. We're just going to write out what these gradients are. If you want to backpropagate to D L, dx, and you have some upstream gradient DLD. Why? Backpropagating through this will actually be the same one as this one. It's going to be multiplying by W transpose on the left. And then if you want to backpropagate DL to get DL DW, you would take your upstream gradient DLD y, sorry, I shouldn't have renewed far away. You're gonna take your upstream gradient d L, d y and multiply it on the right by X transpose. And so you can see these two look eerily similar to this, where instead of x and y vectors here, they had some big wire matrices. And dimensional way everything works out. Any questions there? Alright, so that's all you need for homework number three, these backprop rules should allow you to solve all the three nm paper cautions that we gave you for solving backprop. And then these are also the Bangkok rules for neural networks. So I'm also just going to take this opportunity to buy what backpropagation neural network there looks like. So let's say that this is our neural layer. The neural network layer we know comprises first a linear operation, w times the activations are the input, right? That layer, I'm going to call this thing H1. So it's clear that this could have been e.g. the output of a neural network layer. We add a bias to it. And then after that we pass them through a non-linear activation, Wally. And that gives us, we'll call this thing actually, I'm here NH2. So this is your next layer. And eventually this thing just gonna go to a softmax classifier. And that's going to give you some loss. Okay? So in homework number three, you're going to write the backpropagation for the neural network layer. It's going to start off by getting some gradients with respect to your softmax. And so this is exactly what you implemented in homework number two, a softmax, loss and gradient. So this is going to give you the derivative of the loss with respect to your Softmax parameters. And then we're going to assume that we backpropagated this. So we have some upstream, upstream gradient, DL, DHA. And now what I wanna do is I want to back propagate DLD A12 through all these operations so that I can get the gradients with respect to weights and oxygen bears and neck and depleted. So the first thing that we're going to do is we're going to backpropagate DLD CH2 over here to the LDA. So if I compute the LDA, that is going to equal my upstream gradient, DL dA2 times but local gradients of the ReLu operation. Remember the operation is a map of my input a when the zeros. And so know that the maximum function routes the gradient. So whichever item is bigger. And so what I can do is I can represent this routing via indicator that a is bigger than zero. So this is going to be here are the same size as a, where a is bigger than zero, that element, sorry, that one is bigger than zero, then the first element is one. If A2 is less than zero, the second element is zero. So the scar to be the same size vector. And if a is bigger than that element of a is bigger than zero, then the gradient should pass through. So I'm going to just multiply these element-wise. Alright, so I do that with this notation here. This is called the Hadamard product. So DLT H2 is an n dimensional vector. Then this indicator is also going to be an m-dimensional vector. This Hadamard product just means take the first entry of this vector and the first entry of this vector and just multiply them together. Alright? And that's going to give me the LDA. So that's backpropagating through the reboot. Any questions about that? Great, Yeah, because every element of this indicator vector will have, will be either one or zero. It's not gonna be that all of them are ones are oligomers. Yeah, So in general, it'll depend on the value of the a in that element. And as long as some are positive and some are negative, you'll have ones and zeros in this factor. And basically what this is saying is that the LTA is going to be equal to DLT H2 whenever a was bigger than zero, right? Right. The question is, where does the chain rule come in here? So in this case, we didn't have to use the chain rule. This is because let's say that let me actually just write this out. There'll be better. Let's say I had my vector a and I just have three artificial neuron. So if I say the values for like 52, negative one, and then three right here then goes through a ReLu. So after it goes through a ReLu, which is comparing these to zero, I would get 5203 because the ReLu is applied element-wise to each of these components, right? And this ReLu was doing max of zero. And whatever element in a row. When we backpropagate through this, we have from last lecture this gradient where if we're backpropagating through a maximum, this gradient d LDF is just going to go to whichever wire with. So in this case, for the first element of a 52 is bigger than zero. So the gradient DLD H2 in that first hour in the first dimension would drop back to the first element of the LDA. But the second element of a was negative. And so the gradient here would be zero for the second element because, because it was not bigger than Sir, our intuition is if I take negative one and I would go a little bit, not much change my output at all because my optimal voice stays values and you can write, Tom way is giving a better answer to your question, which is that the chain rule does apply our next sorry, I should have said that also worked fairly. The chain rule applies because that's how we ever backpropagate through anything. I was doing it element-wise, but tomboy was saying and other ways that you can view this is the chain rule applies. But TH2 da is a matrix that is diagonal and the diagonal elements are one or zero based off of that element, if a is bigger than zero. For all of these, if you're having trouble following this, don't worry about it, but there's something where again, just try it out on paper, looking at each dimension individually and hopefully everything will make sense. The question is in lecture and discussion, Yes. You did find this indicator function as returning all zeros or all ones. So I'm not sure what that okay. Yeah, The TSA, they didn't define it that way. Let me tell you how the indicators used here. So let's say that. Let's say that this is our vector a here. If I do indicator of a greater than zero, this equals checking each element and comparing it to the first element is greater than zero. Okay? So the TAs are saying that in discussion, the argument to the indicator with a scalar, in this case, our a is a factor. So when I say Indicator a greater than zero, what I'm saying is we're going to look at all the elements of a 52 -1.3, compared them to zero. And for each one we're going to say whether it's true or false. So 52 is greater than zero, that's one minus one, not zero. That's alright. Any other questions on this? Okay, let's continue on to do the backprop. So now I'm going to back up to my biases as well as the squire. So this is just backpropagating through a plus sign. So if I backpropagate through a plus sign, the gradient just passes through. So we can say that DHL DB gradient of my boss with respect to my biases also equals DL DC. That's the gradient at this wire for C corresponds to the value of this wire. And this is just equal to the LDA, where D LDA is this quantity. Alright? Back propagated to this plus sign. The last step is to backpropagate through this multiplication, which we now know the answer to, because we did both of them. So if I back propagate to the LDH one, then Dio di H1, we know is going to equal w transpose times d L d, c. Alright? And the LDC put the LDA, which is equal to this point. I can plug everything. And then similarly we will have to backpropagate to here. And so DL, DW, we know, will equal LDC, the upstream gradient. And then it becomes a, right multiplied by x transposed by H1 transpose. So that's backpropagation neural network. With this, you can compute the gradients of the loss with respect to all the weight matrices in your neural network. And if you have all those gradients and you can optimize via gradient descent. Okay? Any questions here? Customers, is there an easy way to remember what does all that work is on the right? So I'm gonna give an answer, which is, I don't remember. The reason is, oftentimes, when you look at code, it depends on how big matrix of examples. Exclusivity is N by the future, because this is what we do in the homework. But sometimes it's features by n. So in the end, all I remember is that if I go back to hidden activations, I'll have to multiply by something like w. If I'm going back to DL DW often multiply by the hidden activations. And that's where then I put a break point and my kid, I do x dot size or gradient dot size. Choose the transpose of the orders so that the dimensionalities correct. Alright, that's how, that's how it's being done. And of course for the gradient checking to make sure that that's why I mentioned in that strip is a very frequently that said on the exam, if they asked you to do a backpropagation question, you will expect you to get these dimensions correct. So make sure that for the exam room will give you a cheat sheet. So you want to write down how to backpropagate through these matrix vector or a matrix matrix multiplies. Or you can also take the dimensionalities of things. Okay? Other questions. Okay. Yeah, so sometimes they're giving some tips to remember things. I won't repeat that one because I guess like students may think of different tricks for how they remember things. So yes, this is the answer. And one way again to always check it is to match or dimensions. Other questions, Yeah. Alright. Can you raise your hand if you feel that you generally understand how to back off through the neural network. Alright, I see like maybe that was 75% of the class. I want to ask if there are any other questions on the back propagation steps. I'm happy to answer them. Yes. So the question is, what are the dimensions of DLD X1? So let's say that in this example because I didn't give dimensions, we'll see, we'll see H1, H2, m-dimensional. We'll call H1 n-dimensional. And then if H2 is m-dimensional, that means that w has to be N by N D 0 T H one will be m-dimensional. Tld top view will be n by n. The LDA will be, I'm sorry. Each one is n-dimensional. So this is the LDA will be m-dimensional. Db will also be n dimensional. You can go ahead and verify all of this and introspect yourself well, so, all right. Any other questions on this path problem? Alright, cool. This is something where, you know, I also had an electrode like this and it's for me solidifies when you actually put it up in the homework or do a trick question. So please come to office hours if you're still struggling with anything there. This is our last slide on that truck. So now that the gradients DL DW, that we can now compute the neural network. We can go ahead and apply our learning algorithm or gradient descent. And gradient descent will tell us how to update the weights W to make our loss, cross-entropy loss or smallest possible. On homework number three, you're going to find out that when we do this naive Lee, their performance is still not going to be great. That's because in addition to just the loss function, how to calculate the gradients, which is backprop. And then gradient descent, which is our naive learning algorithm. We're gonna find out that actually there are a lot of specific trips for neural networks to get them to train well. So this is going to incorporate several regularizations as well as initializations and then also use optimization techniques. And that'll be the topic of the next two to three lectures. We're gonna talk about some of these specific tricks for how to train neural networks file. So in this lecture, we will probably get today through two things. One is, how do I initialize the weights in a neural network file? And we're going to find that, that absolute intensity difference. And then the second thing we're going to talk about is something called batch normalization in this lecture. Then next lecture we're going to talk about other types of regularization is including something called stomped out that you may have heard about. And then the lecture after that, we're going to talk about optimizers like using momentum, RMS prop, and Adam, who here has heard of being Adam optimizer despite the appearance. Yeah, so several of you may know the Adam optimizer and know that it's much better than just vanilla. Gradient descent will go over the details of why that actually helps to train these neural networks. Right? So regularizations and specific train for neural networks. These are the chapters to look at in the Goodfellow textbook. We're going to start off by talking about initializations, which is something that we may not usually give much thought to. This is the initialization of the weight matrices. W1 is worth the price is P1, W2, and W3, V3. So the first thing I want to get in your head is that initializations matter. And what that will do for this is to say, maybe you might start off and training neural network by just be something surreal but something close to zero. And I say, okay, maybe the neural network will learn to like bigger. So what I've shown here is Python code, or a ten layer neural network. And each layer will have ten layers. Each layer is going to have 100 hidden units. Alright? And then I'm going to write a for-loop over my ten layers. I'm going to do something simple. I'm just going to write the neural network, which is going to be my waist times my hidden activations. And then after that it's going to pass through a ReLu. But when I first do my neural network initialization, I'm going to initialize each weight matrix so that its elements WIJ in a weight matrix W come from a normal distribution with zero mean and a variance equal to 0.01. So this is a really small distribution, or the distribution that will give you a relatively small values. All the values will be around, centered around zero and their variances is gonna be zero. What happens if we go ahead and do the neural network and run it forward? So just the first iteration of a forward pass with these small weights. What I'm going to do is I'm going to plot the following. I'm going to look at every single layer. So you remember that there are ten layers. So this is, Hey everyone. This is layer two. All the way to layer ten. Within each layer, there are 100 artificial neurons. I'm going to do is I'm going to take the mean of all of these artificial neurons. So if I take all 100 neurons in layer one and I take their mean, I get 0.04. And then I'm also going to take the standard deviation. So this plot on the left is the mean in each layer, and on the right is the standard deviation in each layer. So what you'll see is that as I go to layer two and I get a smaller number, and layer three. And beyond it eventually approaches zero. This is not that surprising because the weights are small numbers. It's a small number of times activations to bake you smaller and smaller, and that's why you go torques around. The standard deviation also goes towards zero. And what that means is, if we were to show you the actual distribution of the unit activations in each layer. So this is layer one. Layer two. The x-axis is the value of the activation, the value of artificial neurons. And the y-axis just counts. And so this is giving a distribution, a histogram of my activations. We'll see in layer one several or non-zero. But as you go deeper and deeper layers, all of the activations are just zeros with neural networks. Alright? This initialization leading to later layers having several activations. That was there a question? The question is what is on the y-axis? The y-axis will be counts. So the more, the better way to have written this would be to divide by the total number on the y-axis. And it will look like a distribution. But basically, if the y-axis is larger, it means more units had that value. So this is run across many different epoxy. So after how many different examples? So after having run our examples, I look at units that had a value between zero and let's call it 0.1. There were 67,000 of them. There are 50,000 with a value between 01100. Again. But just to show that in, as you go to a deeper and deeper layer, you have to pay six becomes there. I'm going to tell you, it's really bad for learning if the output activations or posters, or you might think, this is not our problem because when I do learning, the learning is going to make those weights bigger and bigger. But intelligence and someone tell me why? Yeah. Because it kinda just go so heart rate. So once it says kinda like the sigmoid will be in areas where the gradient is equal to zero. So there won't be any learning in the first place. That is correct at the high level. And then I want a bit more detail on why the gradients are zero in this case. Yeah. Perfect. Yeah, so let me just write what the students said. We know that. Let's say that we're at the output layer and we're computing our scores Z. So the Z is going to be our last layer, weights w ten times the activations from the pirate layer H9. Alright, so this is the computational graph. We're going to have a weight w ten inactivation H9. I'm just gonna ignore the biases for now and it gives me some z. All right? And then we may have an upstream gradients DL, DZ. And now I want to do WE tag. So I'm going to backpropagate to DL, DW ten. And we know that when I back propagate, the rule is I had the LDC times what's on the other wire, H9 transfers. Here's the problem, which is that H9 are all zeros. So if H9 are all zeros, DL DW ten is all zeros, nobody is going to happen for DL DW, for wait time for w. And so by virtue of having all of our activations equal to zero, we're actually not going to have any learning occurred in the number. The same applies if the same applies for all the other weights earlier that purpose. Okay. Any questions here? The question is, if we have a non-zero bias, does that mitigate this? It does not. So if you recall, for the bias, what happens is that the upstream gradient will just pass through a plus sign. And so if you look at the equation for DL DW, nothing about the bias appears. It's just an upstream gradient times the oh, I see Gesso, I'm saying. But then in this case, if there was a bias here, like a benign, than H1 will not be equal to zero. That is correct. So in that case, there could be some gradients on the P1s are the biases are also crystals. They're out if you initialize emphasis around, they'll also be yes. So, yeah, I'm always asking when I initialized to zero, are they all positive or are they Gaussian? And some are negative and they're Gaussian and some are negative in this example. Alright, so it's bad if the weights are initialized to zero. And here I've just written the code to do the backpropagation is look at the gradients. And what you'll find is that if I plot the distribution of the gradients look like. This is what they look like. And the key thing is that all of these are numbers times one minus seven. So all of these gradients are eclipsed is thereof that the gradients are close to zero than the goals aren't in. All right? So small gradients aren't the answer. The other thing is if we consider a very large weight initialization. So if you had done WIJ comes from a normal distribution with mean zero and variance one. If we were to go ahead and look at these networks using the same thoughts and said Here I'm showing the mean of the artificial neurons in each layer going from layer one to layer ten, as well as their standard deviations. You can see that in this case, the wafer to large, all the activations are going to explode. And if they explode, we know that this is bad for the gradients because we know that the DL DW is, I'm going to multiply the activations. At the activations are hundreds of millions or tens of millions. Those gradients are going to be really big. Big gradients are not good learning. So another, but you might have is, well, I was showing you this for Rayleigh's, but maybe you can mitigate how much the gradients explode by using cat age. However, if you use tan h, Alright, this is just asking you about tonight to general these candidates for the small initialization. The small initialization will still send all of your weights to zero. And then if you use Kenny to the large initialization, the activations for exploded ten age is bounded by one and minus one. So if you actually look at the distribution of the activations for a tan h network with large initialization, you will see that most of them are between -1.1. We also know that when the input to the tan h is very large or very negative, right? The contaminates, we're going to be in a region of zero, the gradients. So those learning is going to happen. Alright? So basically all of these scenarios, so not all the scenarios of a small initialization and a largeness realization both lead to learning difficulties. The question becomes, well, if not small, not large activity is gonna be intermediate, but what the intermediate value do we choose? So we're going to derive an initialization or an intermediate value of the weights that will lead to stable training. And this is quantity called the Xavier initialization. And we're going to have the following setting. We're going to have a neural network. And this is going to be the first layer. And then there's gonna be a weight matrix, W2, W1 here from inputs X to our second layer. And just for notation, I'm gonna use something that we had introduced when we first did neural networks. I'm going to call all of these units h one comma 11 comma two, h one comma three. Let's say that there are 100 units in these, so each one comma 100. So the first number tells me what later on in the second number tells me which student in that layer. So these would be H2, H1, H2 to Dan, to H2 comma 100. For the Xavier initialization, derive it. You are going to make an assumption which is first that all of the units within a single layer are going to have the exact same statistics. And so what I'm going to do is I'm going to simplify the notation even further. I'm going to say H1 here is a scalar. So this is a scalar that refers to any unit within the first layer. They all have the same statistics. And so if I see that the variance of H1, the variance of each one, this is going to be a scalar number that reflects the spread of one of these units in this layer. And that spread is going to be the same for all. Good I think. Alright? So I write here variance of H2. That's going to be the variance of just one of the units in this layer. But all of the layers have the same parents because they have the same statistics. So what does obvious idea was, was when I showed you the e.g. the small initialization, right? We saw that the standard deviations, which are the variance, the variance is the standard deviation squared at the initialization is too small. The variance goes to zero. And if the initialization is too large, the variance does explode. Simply. Xavier's idea is to say, we're reporting to take the variances of units in a given layer. And we're going to say that they're going to be approximately equal to each other. So the variance of neurons in each one will approximately be the variance of neurons and H2. And this is all the way up to the variance of neurons in our, let's say we have L layers to be bearing HLR. Then. Now we're going to make another assumption, which is not only are the variances of units and beta one plus unison layer two all the way to layer l. But if you backpropagate gradients with respect to loss, derivatives or gradients of the loss with respect to these waves are also going to be similar across all the layers. So he wants that the variance of the gradient for the weights and layer one are going to be approximately the same as the variance of the gradient of the loss with respect to the weights in layer two, all the way up to Layer. So now we're going to make an assumption that we want this to be true. And given this is true, we're going to try to derive what the optimal setting for the initialization is. Sitting on a questionnaire. Identical distribution or their means are. Great. Tom voice says, we're asked for clarification. When I say that H11 has the same statistics as H12, what are we assuming here? Are we assuming that the distributions are exactly the same? All we're going to assume in this case is that the expected value of H11 equals the expected value of h B12 all the way up to 81100, as well as the variance. Variance of H11 equals variance. Each one comma two equals dot dot. And that's why I just simplify it to the variance of each one. So if you have all the sensitivity, so remember this H one could represent any single unit and just maybe not for this derivation that need not be correct. Alright, let's take a five-minute break. When we come back, we'll show greatest masterpieces to in terms of initialization. So, okay, so the mathematical reason, second sentence to zero, we're trying to understand that all the weights are set to zero. Then the argument is that since the DL DW or whatever w, which would be DL with hh3h is just It's useful. This would be very small. Well, yeah. But then the thing is that W2, this is a thing that simplifies to multiply the other side by h. So the rule where, where when I have, when I had this multiplication, so this would be like a W3 and this would be an issue for you. When I back propagate in orange to w, I'm going to get multiplied on the right side by H2, right? And in this case stage two. Right? Okay. That makes sense. Yeah, In the same question, would he only had really when bitmap we can get them and advocate Florida's new helper function that we can derive an analytical form and can be ambiguous and very easily find out that matrix. Yeah, so if you can't get a derivative and you can't backpropagation. But as long as we can write down the function. Whatever this nominee or fingers hoping able to calculate the gradients or else something called a subgradients. In those cases, we need to have defined radius the backprop. But in those cases we can backprop with someone I say, like really doesn't have a derivative because it has a peak at zero. Bogey could just define it to be zero or one. That's a subject in and establish, oh, I didn't mean that the derivative doesn't exist. What is the current format analytical columns so as to begin this night in the simulation so that I can just get the points or the matrix. I don't know what the setting would be though, where you wouldn't be able to write the analytic put the record is because this will just be like a scalar function. You'll always be able to write the analytical gradient or subgradients at that expression. But if you think of an example that I can pick up just talking about again. Let me just get back to your timer. At the beginning when we did it in their shoulder, like a specific firm. So that's a negative term in front of them. Disappeared and that's because I should have been there about this also. In this example, we were doing z equals y minus w. Okay, so that minus sign comes to this morning, right? Yeah. Great. When you pull the cord, we have done with small initialization. Yeah. How do they began to speak in respect to this? Yeah, so when you have a small weight initialization, the variances, we'll quickly decay to zero. So this part here, the very specific conditions where you have hedge one-and-a-half just derived from, I mean, you just randomly. But how did you get the standard deviation for HTTP? Calculate it from his jacket. In this case, I just did it incorrectly. So I just looked at all the activations. I actually took their ********. So to actually compute it, will need to make some assumptions and we'll show that in our software initializations derivation. You move to the next slide. The last slide. That's fine. Yeah. So when you talk about the statistics of these isn't like Don name, Let's have to stick. So this involved even the statistics of detail tonight. Yes, they will be a function of the data because ultimately they're all, yeah, one is a function of W in order to get the proper initialization of the dates. Yeah, they should have some information about data, isn't what that means. In this case, we will be able to derive something you're definitive data. If the data statistics were scaled and W1 is independent, then the variance of these. So if the variance of x was like say 100 or two things, you say firstly, z-score. So the variance of this would be one. But even if you weren't then variance 100, then that will scale up all of these variances, but they would still be approximately equal to. Okay? Just because of time syntactic, pass a five-minute break. So can we talk about that now? Great. Thank you. All right. I will get back to it. You're talking about how we are going to want the variance of the units in every single layer. The approximately, another case that the units won't explode or vanish. And then that will allow us to do training. And so this leads to the Soviet Union Square edition. This is just a slide that formerly says what the book we just discussed there. So what I'm going to do is with the Zagier initialization to do any math to kind of increase your risk for what industry. You are going to make a bunch of simplifying assumptions that are not true. But the assumptions will get us to an answer. And if that answer allows us to save a general about books, that's great stock and it failed because our assumptions are there. But we're going to make these assumptions to just try to get at least to an answer. And we'll see that even with these assumptions, things will work out. So we're going to be in this setting, we're gonna be using the same notation as on the prior slide. And in this case we're gonna be looking at layer I minus one. Going to layer. Remember we're going to have this notation that this unit is going to be H and layer I minus one comma one. This will be h in there, I minus one comma two, et cetera. Then there's gonna be a total of n n units. So this is going to be h of I minus one comma n. So n is going to be the number of units in this layer, i minus one, we call it in because you can pick up where I minus one as the input layer I just bought me care about analyzing. And so what I'm going to do is I'm going to look at some dinner and where I, this is still a scalar. This is just any unit. And later on, I'm using this notation. Little HIT to reflect the fact that all these units have the same statistics. So I'm just going to call this HR here. If I want to compute the value of HI, right? We know that all of these units have some weights in my neural network to HI, so it's gonna be a dot product with that weight and all the units in the preceding layers. And so that's the summation over here. All right, What I wanna do is I want to try to get salvia condition. I want to assume it to be true and let's see what that means for my waves. So remember the variance of HI will equal the variance of h of n minus one under this IV or initialization. So what we're going to do is I'm going to simplify this expression. I'm not going to simplify. I'm going to take the variance of both sides. I'm going to take the left-hand side, that's gonna be bare. Hi, and then I want to take the variance of this summation. Alright? So if I take the variance of summation, I will need some rules to simplify it. So in general, if you take two scalars, W and H, W corresponds to w I j, and this H corresponds to h i minus one j. If you take the variance of their product, then there's this variance expansion formula. And then we're going to make a few assumptions here. We're going to assume, again, these may not be true, but they're necessary for us. So rather than Massive, we're going to assume that the weights w, as well as the activations in the prior layer h i minus one have zero mean. Some of these students are going to notice that this assumption that h minus one has zero mean is really, really not true because if the output is right, then the minimum value in the HIV can take on its surroundings. But then remember in this example, we're assuming the units are going here, so there can be positive and negative in this case. Alright, so if the W's and the H Price have minus ones have zero mean, then this term equals zero. This term equals zero. And we're just left with just the variance of W times the variance of h. Alright? So now if I take the variance of both sides, I'm going to get an equation where I have on the left-hand side the variance of HI. And then on the right-hand side, I will have a son of the variance of the W's times the variance of these Hs. And we assume that all of the units in layer i minus one had the same statistics. So I'm going to take that variance of h minus one and take it outside of the summation. Alright? Any questions on the again, this is just like a rough hard to give us an answer. That remember that Xavier initialization says the variance of all the layers is approximately equal to each other. So the variance of HBr and HI minus one are equal to each other. I can divide both sides by doing so at h i minus one. And this is going to lead to an equation that we're going to have the sum from j equals one to n of the variance of w I j is equal to one. And then further, I'm going to assume that all of the weights themselves have the same statistics. So that is the case. So this is going to assume which have same statistics. That means the variance of w I, j is the same for every single value of j. And so I'm just going to have n of them. So this is going to lead to n, n times the variance of W, or just call this i. Now, the weights going from minus one to i equals one. Then this tells me that the variance of the weights should be one over. And any questions there? Yeah. The WIJ in this case are the weights that go from units in there, I minus one until later on. So I had the WIJ, this would be wi, wi, wi Wait for one particular unit later on. Great. The question is, is this always to me that we have a multilayer perceptron? Yes. So it's assuming that the form of how the artificial neurons and where I might want to go to layer i is through this linear conformation. The question is, is there an analog for other arbitrary architectures? Yes, So there is also an initialization rule for recurrent neural networks. We will talk about that when we get to the RNN after his death. What is the n? N? N, N is the number of artificial neurons in layer i minus one. So there were 100 years than n, n equals 100. Tomboy. We're assuming that the expected value of the weights is still zero. Yes. There are going to run other functions here that may not be true, but it gives us the service to get the linear case. And we'll see if it works practically in the nonlinear case. Recognition that's going to make that distribution. Nasa Tom way is asking if n is large, isn't just a small weight initialization? That the variances are small, the answer is yes. So it's all relative to the size of the networks. So for whatever network I showed, whatever industrialization I chose was smaller than one over n. Right? Perfect. Yeah, so this student is asking the question that gets me to my next slide, which is, if the waves are identically distributed, the variance of the w's is one over n n. But then Xavier initialization had this other condition that the backpropagated gradients to pop the same barriers. And so we follow the exact same type of argument. You'll find that the variance of the weights, if you look at the backpropagation condition, should be one over and out, where n is the number of neurons in layer. So this is number of neurons in layer. I'm just going to send n. Number of neurons here is an R. So what do we do? What we do is we just take the average of the n average, right? Will be the average of n in and out. So it's just an n plus and add over two. And then usually what we do is we initialize the variance to be one over an average. So it'd be two over n plus one thing that you can do. Oftentimes, people don't even consider this and they'll just set it to one over n. And that's generally fine as well. But if you want to take into account the backward condition than you would set it to two over n plus. Alright, let's go ahead and just see if it's empirically works. So I'm going to run the exact same experiments where now the weights here are gonna be drawn from a distribution whose variance is going to be two divided by the variance of an plus an hour. Sorry, the variance is two over n plus n. Now I see this bird here. So that means that this strand and function takes the standard deviation or variance. So this is the Xavier initialization. And if we look at 108 numbers and we looked at this same plots, where we look at the mean activation in each layer, the standard deviation, and then the histogram. You can see that the units are starting to go towards zero, but they last for much longer time. So all the way up until Mayor, Can you still have a reasonable distribution of non-zero activations, which means that it hadn't exploded or banished to zero, and therefore it will have reasonable gradients. So we can do, alright. Any questions here. So the expression that we used in the answer, actually use that expression with all. Great, yeah, so Abishek is saying, when we derive things, we derived this initialization. It was with certain assumptions like no nonlinearity. And all of these equals statistics between weights and activations. These are not true in real life and real life. We also have nonlinearity. Even so. It's good initialization. And the fact that in Cherokee still works, even though these assumptions are not true in this thing work. What each layer, we have particular values for the weight distribution, as opposed to the previous example. All three plus in this case also be scandalous smallpox. And he jumped, they're wiping the example that we showed earlier when we were all going live. So as you're going deeper than you are composing functions, monkey password, it's going over there, specific. Maybe that's right. Yeah, so if the layers have different number of units input and at the output, then the variance will be different. Because the variance will be a function of the number of inputs and outputs. So one point of time I made him a fish layer could have, will have a different initialization. And that's also true. That makes sense like if you have few units, e.g. you'll want your weights to be initialized to be larger than if you have many units. If you have 1,000 units, are doing a sum of 1,000 inputs versus if you have two units is just the sum of two of them, right? So to make them have approximately the same scale at the output, you'll want the weights to be initialized differently. You should have made the number of units, correct. Yeah, yeah. So Tom ways mentioning this is especially relevant because in neural networks oftentimes we will reduce the number of units as we go deeper layer. So these would be initialized with different theories. Any other questions here? The question, is it a necessary condition? For, is this a necessary condition? This condition you're talking about for the neural networks to train. For it to be high performance starts. There are other initializations that could work, but this is a reasonable initialization that keeps her activations alive by the end. Excellent. Right? Yes, It's always saying, and by the way, I want to emphasize is the spread of industrialization. It's not for training the vagus initialization at the start so that our gradients won't die or explode. But as training occurs, the training can alter the wage, so they have very different variances amongst players. What we wanted with initialization is just to get training kicked off. But then after training, the variances in each layer will definitely be different. Question is what is the difference between ending in and out? So in this construction, and n is the number of units in layer i minus one, and n is the number of layers, number of units and b are. Any other questions? We'll take these last two normal by, Sorry. Oh, you're saying, is there a particular value for these studies? Emphasis should be equal to 0. Like would it be better if this thing was constant? Yeah, it would be, it would be better if these all look like this. But it's hard to like very quietly to the initial weights so that everything set up a standard deviation of excuses not to pursue. But still after ten layers, it hasn't gone through. Said that last step you start training. Now, what's the homework? Yes. So the students verifying, we haven't adjusted by that these assumptions are true. We're just using them. And then we rely on the empirical results to see a reasonable kind of like the ends justify the assumptions. That that's true. For a more complicated setting where you include the nominee area, e.g. we won't be able to come up with simple derivations like this. And so we make simplifying assumptions. Getting the answer, we try it. And there's profit of works and it actually personally, alright, it works for tan h. But then this is a nice follow-up to that student's question. If you use a Salvia initialization with bamboo, it fails. So if you use this audio initialization with rabies, we see that by the end, we also had traditionally assigned to zero. And because most units are most neural networks for using the baby activation, this is important to address. And colleagues in 2015 suggested a bridging this one over n, n that we derived to two over. And then using just the heuristic intuition that many units will be close to zero. And so because ReLu might kill on average half the units and this is not true. It's like usually no text 20% of the units that are zero. But they said because rarely kills off the units less increases by a factor of two, right? And then there's also this other initializer uniform also by Zagier. And this is often used in neural networks. But returning back to her, the Xavier initialization doesn't work. Probably because the ReLu has several units that are going to zero that reduce the variance. So her size to multiply that by two. And if you go ahead and you multiply that varies by two, all of a sudden things work. Alright? So the initialization is the Xavier initialization with an extra factor of two. However, sometimes something as simple as a factor of two mixes, very big different. Initialization is a very important aspect of training neural networks and it remains an active area of research. This is the plot where I believe in blue is ReLu, and in red is tan h. And if you use this audio initialization, they said that you don't ever get better error, but if you use that Dr. to initialization, then all of a sudden you take care. Alright? So initialization matters in this class moving forward, this always confuses off your initialization. And this is just a table from a paper in 2015. The papers called all you need is a good admit. That's not true anymore than just good at it. But this is showing different initialization methods like Zagier and SRA. Here is the initialization and it shows that with different initializations, you can get as much as what's like 3% improvements in some of these particles is Claudia. Any questions on initialization? I got the question is can I clarify the two graphs? So I believe that sorry, I think I misspoke. I'm 90% sure of this. Tas check this for me because I haven't looked at this paper in a while. I believe that this is for tan h networks and this is for ReLu networks and tennis networks. If you use the initialization which differs from Xavier by a factor of two, they both train, but in ReLu, zombie or doesn't train and Rabia traps. So thanks for that. Archean rocks and facts on the x-axis, what is the epoch? These are training to be false. So this is a gradient descent. Oh, I see. Yeah, so in gradient descent, you know that we have mini-batch gradient descent. Let's say we have 1,000 examples named mini batch over say, 50. So 20 iterations of gradient descent will be one path for our dataset and that's called one. So one epoch is one entire path of the receptor. That's great. So the students asking in the firefight, I also wrote this initialization. So in this paper by his Amiri Baraka in 2015, they discussed the math WE went over for giving the one over n in initializer. They also motivate a different initialization where you draw from a uniform distribution. And then this is the range of the uniform distribution. So sometimes when people say solve your initialization, they need this uniform distribution. But in this class we'll refer to the one over n, n. And I'll refer you to this paper if you want to learn more about what that initialization is. Alright, so that's the initialization. The next thing that we're going to talk about is batch normalization. Batch normalization is a really important technique to help to make the neural networks practically be less sensitive. So some hyperparameters like learning rate, you'll find that training is a lot easier with batch norm. So in homework number three, you're just implementing neural networks. But in homework number four, we'll ask you to implement batch normalization as well as from it, which we will begin the derivation of today and we'll finish the derivation of backprop through batch normalization. And Monday's lecture of next week. You'll see that once you implement batch normalization training and choosing hyperparameters and the neural network will be a lot easier. Alright, so then another thing here, which is that if you look at batch normalization was first introduced in a 2015 paper by I'm not sure I'm pronouncing these race, but IOC and security. In 2015. And they motivated the need for batch normalization through something called internal covariate shift. And that's also Bella uses in the textbooks. So we're going to teach batch normalization from this motivation of solving something that we'll call internal covariate shift. But there's been some more recent research which is asking why does bashed arm really worked? And then maybe for other reasons. Alright, so this is also an active area of research. Okay, So let's first motivate batch norm through internal covariate shift. So we're going to imagine we have some neural network. We have our inputs x. And then we're gonna go through some weights w1 and B1. I'm not going to write the d is just for convenience. We're going to have activations H1, H2. I'll draw one more, H three. And then we'll have wage W2, W3. Eventually these go to some loss function L, right? And we know how to backpropagate to get all of the gradients. So we're going to have gradients by DL, DH three. Backpropagating from the loss will have a d L, d, sorry, not H3, and then W3, W3, PL d W2 to W1. So these are all gradients. Tell me how to change W1 and W2 and W3 to make my law smaller. Does anyone see a problem with the following, which is when you're going to implement, I'm gonna compute all these gradients, W1, W2, and W3. And then I'm going to exchange all of them together by doing my gradient descent steps. I'm going to do W1 is my old w1 minus epsilon DL DW one. I'm going to do this also for W2 and W3 all in the same batch of data. If someone told me a potential problem with that. Yeah, Perfect. Yeah. The students said let me just reiterate what he said. So let's see that we're looking at the weight W3, right? Dl DW Three is a gradient that tells me how do I change W3 to make my boss molar, right? Mr. Changing only W3 in a gradient descent step, Change W1, W2, and W3 together. I may be doing something unwise because calculate delta w three, this tells me how to change W3 and the absence of anything else changed. But now W1 has changed. Now W2 has also changed. And so now my H2 might be wildly different than what this layer was expecting. So how do I change W3 might be totally different now because H2 is different since W1 and W2 also changed. All right. Does anyone want me to repeat that? People do. Because I like my dad said, most clearly, DL DW three is telling me, how do I change W3, assuming everything else stays the same to make my losses smallest possible. But when we do gradient ascent, we change W1, W2, W3, all at the same time. Alright? So now that I'm changing W1 and W2, when I use DLT W3, which assumed that W1 and W2 are constant. This change to W3 may be nonsensical because now H2 has entirely different statistics than the A12 when I initially took my DL DW. So basically said even simpler. Dl DW three tells me how to change the W3 with everything else stayed the same, but I'm changing everything else. So how do I know that the only WPA3 is actually the question, does this problem? So the question is, does this problem still arise? If I'm just going to say more generally, the second thing you said, you take the gradient of L with respect to older rates at once. This problem does not arise. Mathematical relationship between each one is a function, right? So tomboy is saying, essentially all these things are linked. White. H3 is a function of W three, H2, and H3 itself is a function of W2. So H3 is going to depend on both the values of W2 and W3. And H3 will also and affect the loss. So I take DL DW Three, Let's give a concrete example. Let's say that H2 has values that are 1-2. All right, so those are the statistics of H2. Abs mean is 1.5 and experience is like one, right? So W3 is doing an update assuming that my H2 is going to be 1.5 and variance one. But now DLT W2, W2 much, much larger. So now H2 has a mean of 20 and a variance of tech, right? Then the update that I do to W3, which assumed that H2 would have much smaller mean and variance is less. So that's the internal covariate shift problem that we're going to fix with batch normalization. Batch normalization is that you're going to take every single layer, H1, H2. And we're going to normalize them to have unit statistics. What's this unit statistics mean? So if my layer I is a ReLu of excide, we're going to assume the unit since it's 600, ended up before the activation function, the axis. So it's going to mean that the expected value of x dy is equal to zero and the variance of x psi is equal to warn. All right, so intuitively what is that saying? It's saying after every single layer, we're going to put something that normalizes back to lease their own variance one. So in that example that I gave, where W2 mean H2 has a much higher and much higher variance. And W3 was changed expecting issue to have much smaller venomous barbarians. Fashion on layer comes in-between here and says, your statistics are going to be normalized back down to mean zero and variance one. So hopefully my updating W3 would have made more sense, right? So basically, after every single layer, normalize the mean and the variance is 0.1. And then hopefully mitigate this column of internal covariate shift. Any questions there. Alright? So you'll notice I talked about applying the mean zero and variance one on the thing before the railroad. Sorry, you said at the beginning, HA, HA, minus one times wi minus. Oh, yeah. Just to be clear, I'm just going to say that at psi is the product of a wi times and HI minus one. These are multiplied together to give an XI, which is then given to to give HR. In this class, we're going to apply batch norm on the axes. And it's gonna be in the following way. In our vanilla neural network without a batch norm layer. What you'll see in homework number three is that we're going to define this w times h to be an affine layer that I find where is gonna go down into a ReLu Bayard. And then we'll have multiple layers of a neural network. So then that would go into an affine and we'll go into a raving. When IOC and Saturday first proposed batch norm would apply the batch normalization before the railroad on these oxides. And so in the homework, we're going to have an architecture that looks like that fine. This will be homework number four. We're going to use a batch norm layer. And then we're going to use a re route. And then we'll repeat this offline. Batch norm, re-root, et cetera. Since this paper in 2015, people have done more extensive experiments where they do the batch norm on H's instead of the axis. And this actually helps performance. So today you would do affine, ReLu, been, bashed, norm, et cetera. Alright, but our assignments will go off of the original IOPS. I didn't taper. When we derive the backpropagation for batch norm, it will be for this architecture. So in this class we're going to use this. Let me make that clear. Their star we will use we will apply batch norm before rabid. Yeah, right? So telomerase is saying, when I'm motivated it, I said I want H1, H2, H3 that have statistics. But if I were to actually apply this bashed on before the railroad and the output of the Ravens does not have enough statistics. And that is correct. And what is statistics? Well, it's not easy to write down because this will be a nonlinearity. And that's why it probably makes sense to apply batch norm after review and w's to higher performance. But again, I'm just going to go off the original paper. The question is, what is the affine? Yeah, fine. Is w times h minus one plus BI. Question. The question is, did the original paper have a rationale for applying dashboard before baby daddy didn't get any rationale. The question is, how is this different from feature normalization? Is it different from feature normalization? Like z-score in e.g. so the scoring is this idea of normalizing your input. You didn't have the zero and unit variance of people didn't know what the scoring was. Batch norm is essentially doing the same thing, except with a few key differences. One, we're going to see that bachelor actually allows you to, to, to, to learn parameters that allow your needs. Not these are undergoing asked to not be one. So we'll talk about that. The second thing is that feature normalization is typically done offline, but officer data and then you use that in that form. This is part of the computational graph and we'll have to backpropagate through. And this is actually a very critical prior to this paper, people had tried to do feature normalization without backpropagating through it. And if you don't drink and the bartender, that picture that normalized. And so it actually didn't know how to essentially his book where he talks about this, it's like use with effort. So it's important that we backpropagate through this normalization, also. Normalized and ask this question is because we are normalizing the activation. Does that make? Well, we talked about the weight initialization a bit redundant and the answer is yes. So when we applied that foundation will be far more improved robustness to weight initializations. You also see that in, I think will be accentuated to comment number four right there. Well, even if he doesn't do the contestant yourself and you'll have more reluctance. Has the question is, is batch norm and affine transformation or does it introduce non-linearity? So let me just go to the slide where we talked about with dash. Dash normal look like the Z-score and operation. We're going to take our activation, right? And we're going to subtract off. So actually let me, let me first draw what I mean by XOR and whatnot. Since these are always things to keep track of, we're gonna be looking at just one layer. That layer, we're going to have activations. Artificial neurons are so x i refers to just one of the artificial neurons in this layer. So batch norm is applied element-wise to every single artificial neuron. We have 100 units than we would have I going 1-100. And what we do is for that. Artificial neuron. The artificial neuron. We subtract off it's mean, and then we divide by standard deviation. Now, instead here do square root of the variance plus Epsilon. Epsilon is just a very small number to make sure that you don't divide by zero. But if your, if your variant is not close as irrelevant, this is just dividing by the standard deviation. Alright? What are mu and sigma squared? Ys are the means and the variances of the unit XI and the computer the cross-examined. So what we do is we look at x-i a cross N training samples. So this M here is m training examples. So far ten, I will show my networks 50,000 images, e.g. so in this 50,000 images, I will have 50,000 values of X1. And if I just average them together, then I'll have the average activity of this first neuron. So this is m training examples and this is for the artificial and unit. Great, yeah, so tomboy says, and this is a batch size, not the training set size and that's correct. So whatever your batch size is, maybe you have a vacuum 200 examples and your m would be 200. So let me say I'm the training batch examples from what you compute your mean and your standard and your variance. Any questions on the first bullet point there? The question is, we've been using H for the oxidation. So should this be HI, because we're going to follow this convention, but we're going to do it before the ray tube will call the thing before the ray Bu Zai. And that's just following the notation and the 2015 paper. So they're basically a longer fast and it's not a long decimal. Then another method that they use, but it's great. So remaining is pointing out that the computation of the statistics is along the examples in the batch, the m examples in the batch. That's why it's called batch normalization. We're being as saying That's also something called mayor normalization, where you average across these neurons. You're not going to talk about that at all in this class, but that also does exist. Alright? So that's the normalization procedure. After that, batch norm has two other important parameters. And I know that you will all have questions about this. The parameters are Gamma, I and Beta. And these are learnable parameters. And we know that if we were to take Psi scale it by Gamma I and then shifted by Beta I. The result which we'll call y I will have the following statistics. So we know that x, sorry, has mean zero and variance one. However, if I multiply excited by gamma and then add beta, and I call the output y. We know that the expected value of Y equals beta i and the variance of y i equals gamma i squared. And this is a part of the batch norm beta. And Gamma I is not equal to one and beta I is not equal to zero, then the output of the batch norm no longer has units statistics. It's no longer means they're out there, it's one. Alright? So this is a critical part of batch normalization. I know the question that you have in your mind is, why did we do all of this normalization only to undo it by this beta i and gamma i term. So what I would think of this, think about this in the following way. What we're doing with batch normalization is that we are normalizing the statistics to have zero mean and unit variance to address that internal covariate. However, you are significantly limiting the capacity of your network. Every single layer has to have insurance with the same statistics. We know from looking at neural networks. The features computed in each layer, well, in general, will generally be dramatically different and maybe they need to have different statistics. So batch normalization then gives you these two knobs to say, okay, if you need to have a mean that's different than zero or very specific, the button will allow me to do this, but this kind of biases the network to try to find solutions with mean zero and variance one statistics. And if they need to be different, the network has the capacity to change those things and those prices too. Yes. So Julie's question is, I'm sorry, j, is this the activation of the artificial neuron e.g. the second one, when fed the J image, that is correct. The question is, what happens with different layers? So remember that there's just going to be a bachelor applied at every layer. So that's why we don't have like another subscript white guy I for the layer. This will be an operation that's what applied just on a single layer. Since the question is computed over the entire set of training examples. So it depends. So I believe in the training phase in homework number four, we'll ask you to calculate it over all of the examples from the training set. But when you run it in real time, you will have to compute a running, the inner running variance. I'm not 100% sure that shoe, so it's either going to be over the entire 20 centered over the batches in your training set. You know, TAs, if it's batches, our entire training set. Well, it's going to be one of the two. If he's entitled instead, you'll have a better estimate of the mean and variance. This whole thing, right? Yeah, so this student is saying when we run it in real time or when we run it in France and testing. And we're only class, you have one image. What do we do? Because we're not going to get a good mean and standard deviation from this one and h. So usually we will initialize the mean and the variance to be what was found in training. But then we will keep passing it in many examples for inference. We're gonna keep a running tab on what the mean is of the few examples coming in and what their parents, it's not perfect. We call the running and it's running periods and the only implement that in homework number four. This right does not remember, right? The student is asking, if you put it after an activation and deactivation has a bias. E.g. there's something called a shifted array route where ship the ReLu, up or down. Oh, great. Yeah, so the student is saying right after an affine transformation where we have a WHI minus one plus BI, BI, and this data I essentially be done didnt the answer is yes. Any other questions? Alright, so that is the batch normalization layer for the forward pass. We insert this into our neural network. We definitely have to backpropagate through one word before we, before we show the computational graph, which is, you'll notice one thing. In batch normalization, we are Applying the mean and the standard deviation or the variance on to each unit in isolation. Alright? But you may know from your statistics class these units may also have a covariance. And so you can clinical better normalized or whitening your data by normalizing by a covariance matrix. And it is possible to do this. But we practically don't do it because this computation is much more expensive than doing it. Doing it unit by unit, as we've drawn here. For computational reasons. Efficiency, we won't do this normalization by covariance. Alright? And then I just want to reiterate that we do have these Gamma I and Beta parameters, which could make the statistics not mean zero variance one. And that will actually be important for the network and his wife. Several people also believe that doesn't really help with internal covariate shift, but it is the motivating reason, but it's usually taught for batch norm. Alright? So what we will do is we'll call it a lecture actually for today. When we come back on Monday next week, we'll try the computational graph with batch norm, and then we'll start to do back-propagation and derive the steps of how to back propagate to all of these operations. 
Good morning. Can you guys deal with what we talked about in prior lectures from last quarter? Alright? Yes, Daniel. Daniel's question is he's been copying Dr. White House their cells instead. Okay. Fantastic. Yes, they're vomited TAs or equal to its documentation. Able to read that before. Okay. So last lecture, you're talking about batch normalization distraction. I'm going to have to convince me, ask you, so please don't distract other students. We're talking about batch normalization factor. And we talked about batch normalization is going to help with this problem called internal covariate shift. Where the issue is that we're going to be calculating gradients of the loss with respect to all of the weights. And for each of those weight updates, what we do in the training that we take. Sorry, I see a video there also. Please guide students here are paying tuition to take these classes. I know that this might be for some YouTube video or something, but please for the students I'm going to ask you. I'm working. I'm sorry. Can you please all right. I'm just asking why we didn't use it. Like Wow. All right. Sorry about that. Everyone wanted DMCA, YouTube. They do post it and it's not respectful. I appreciate it. Alright. Because he was asking you a question about not being a full tell you right now. If you took the fact that the TAs are very helpful. Alright, that was 10 min material. So we're talking about batch normalization and batch normalization. What happens is we're trying to help this problem with internal covariate shift where we update all of the weights and redo it, unfortunate or we do it at the same time. And what that means is that I may be making a change to W3 to try to make my loss folder. W2 might also be changed. And maybe when I compute this gradient, DLD or W3, all the values H2 or 0-1. It's on adjusting to try to handle this and the scenario where each switch 0-1. If I go ahead and I also changed W2, maybe W2 is such a large change and other values of H2 are determined by ten. And now my W3 doesn't matter that much. So what we wanna do is we want to reduce this problem of an internal covariate shift by normalizing the statistics. These hidden activations to be in the range of having zero mean and variance. And therefore, even though we're updating all these bits together, even though we're updating all these waves together, hopefully those changes that we did. So basically the more relevance to decreasing force. All right? Any questions there? Alright? Okay, so this moves the conversation. What we do is we take one unit at psi. So there's going to be both a subscript and superscript in these examples. Xii, try here is the neuron upper layer. So if we take a hidden layer and there are 100 units, that's ten, or x2 would be the second unit in this layer. And then the superscript j, x i superscript j is going to, the j is gonna go over exactly. Alright? So you can think of looking at this layer in the neural network. And we're going to have a mini batch of examples. They're going to be images from C4 prime images. And let's say that there are 100 examples, right? So we pass all those images. And for each example, we're going to have a corresponding activation of what this artificial neuron is during that exam. So that you can compute the mean and the variance. We can go ahead and across all of those examples. That's why there's the one over n sub j equals one to m average the activation and inactivation. And then the standard deviation or the variance is going to be the sample variances and across all of those examples. All right, any questions on that operation? And then lastly, even though this will normalize the means and variances to zero mean and variance one. We are also going to scale and shifts the normalized activations. This allows fashion occupies an additional degree of freedom where it says, you know what? My copy. A good idea to make all the activations have zero mean and unit variance, have different needs or different variants to capture different features of the image. And so it is better for the activations that have a different bean or different variants. We allow there to be two of them learnable parameters. So for every single unit, alright, we give it a gamma and beta are. The Beta is going to be how much we shift the mean by Gamma I is going to be how much we scale it by which we'll scale the variance by gamma xy squared. All right? So we're going to have a high level of batch normalization is going to first make the mean zero and variance one. And then after that is going to have to learnable parameters that could make them being different than zero and the variance difference of one. So desire. Any questions here? Great. So the question is during training, do we need to compute a new view of sigma i for every four tasks? And I forgot to look at the homework to see what we expected there, but we do make the trait and fast one UI and sit nice spread across the entire dataset. Or is it for each batch? Okay, so tomboy says for each batch, so for each back to the computer do view and a Sigma I squared. Any other questions? The question is why the label here, know why? That's a great question because I've used that before. Why did I here is the scale that the patients. So you can imagine that if I take this layer and I pass it through a batch norm, what is going to return is the same sized vector. But these are now all the Y-i. So this is y one, this is y two down to 100. So this will be the re-scaled batch norm. Pitchers are the more ions. Question is, how do you know what value to shifting scale it to? We don't know if that was going to be burned by the algorithm. So these will be displayed Iceland Gamma i's are going to be fit during stochastic gradient descent. The algorithm will in a data-driven way to get your losses gets possible. The question is, can I talk about why we then normalize and scale of the data? So you might ask this question because you think that is something that peaceful purposes having done cache warm. Because now the teachers can have different meanings in different areas. And so the reason or one way I think about this is what this operation does, is it biases and that works towards 01 activation saying, can you solve this problem with a nice or ovarian just want activation. And then if you can't, if it really helps to make the bearings different than what it can mean, different things in one than zero. But if the question is, do we initialize the beta and the gamma eyes? Yes, I believe that the TAs can check me on this, but we should initialize them to Beta I equals zero and gamma equals one, which means that it would be mean zero, variance one. The students question is, can we think of this as a two-step process which is first to normalize and then scale and shift. Yes. Alright, so with that last lecture, we left off with drawing out this computational graph of the batch operation. So here is the denominator of the Z-score, which is square root of sigma phi squared plus epsilon is done by this here. So the Sigma i squared plus epsilon I square root, I square root it and then I put it in the denominator, that's the inverse x I minus mu comes from here. And multiply these together and that gives me x psi hat. So that's this. And then I multiply that by gamma i. That's this node right here. Beta i, which is this node right here. Alright? Remember, in this computational graph that I've drawn here, there are. So remember that this is for just a computational graph for one unit. You didn't ride, right? Is just one of these artificial neurons in layer. And then just for one example j. So one example j, and that example will be one. This is a computational graph for one unit and one example j. What you'll notice in this graph is that everything has a subscript pi. So this graph will look the same for every single other unit as well. So on the next slide, I'm going to drop the subscript i for convenience because the subject I will always do that. Yeah. I'm looking at something up front, right? Yes. That's correct. And the axon is just a small number that prevents that the variances are owed. You don't want this to be infinity. So the epsilon is a small number that prevents that. Other questions, right? Yes. So Daniel asked when we talk about X-i, is that input or is that an actual neuron doesn't actual neuron. So let me ask you this, have this be for next year. That notation, we're gonna be using the exact same notation as in the IOC Saturday paper. So usually Zai was reserved for inputs and HI, in our class has been the given units. But here x, y will be the artificial neuron activity. And then y, instead of being labels are going to just be the output of the DashCon later. Alright. Any other questions? Okay, Let's go ahead and do this back propagation. So you're going to assume that we have some hops, Srini gradients, this upstream gradients, I'm going to call d L, d y. And then I'm talking to the subscript, I'll just keep the superscript case. So DL DYJ, right? And when we do backpropagation, we need to know how to backup to the parameter, my parameter to the beta and a gamma. And then we also need to know how to back propagate all the way to begin, right? Because these inputs, the x i's, are the actual values of the artificial neurons. In layer. I need to know their greatest so that get back up to earlier layers. So we're going to start off with are up-to-date. And the first thing that we're going to do is backpropagate one step. So we have a plus sign here, which means that gradient just passes through, right? So the gradient at this wire or at this parameter beta i, it's just going to equal d L d theta. Sorry. There's gonna be DLD y j. That's the backpropagate. Then you're going to notice something, which is, if I read the gradient DLD Beta, I get, I've just dropped the eyes for convenience. I have a sum for j equals one to n over all of the DLD Wij's. Whereas if I'm doing it this graph for just one example, I just have a DLT YJ. And someone tell me why the total gradient for DLD beta here is gonna be the sum across all my examples. This is not an easy question, so please take it to the comparator. Object does have to do with Watson of derivatives. It does. So remember that what I'm doing is I'm drawing this graph for one unit and one example, cherry, let's focus on the fact that I'm drawing this for one example. So let's say that my dad had 100 examples. I'm going to be running this computational graph, these computations for j equals one to 100. So because this is a graph of just one example j, but I have 100 examples. I'm going to have 100 replicas of this graph. So let's say I'm going to draw another one. Let's say I'm going to just replicate this graph. So I'm going to draw exactly how it looks, but not label everything. So what I've done here is I've just drawn a replica of this graph. This graph is going to also exist for unit I, j plus one example. And this is the output for the j plus one example. This graphic, this also for the j plus one example. However, when we look at this Beta I, write this Theta I is just one parameter that is the same across all examples. For all examples, I'm going to multiply by gamma i and then add beta, which means that this beta i, which is added at this node, It's also going to be added this node. So this is going to be the same exact data that comes from I, j plus one example to shift the activations. So there's also an upstream gradient, d L d y, j plus one that brought back propagates here to be beta i. And then remember we talked about how when you have a converge onto one parameter, other gradients, sad. So that's why DLD beta is going to be the sum of the losses across all of my example. These are, sorry, the sum across all of my examples of these gradients, DL, DYJ. Quick reminder letter I and j represents is going to be one unit in the network, meaning one unit in a layer. Meaning I is going to reference e.g. if I equals two is going to be the second artificial neuron. And then J or a trust my m training images. So if I have 100 C part-time images, j goes 1-100. Indexing does images. Yes. So Daniel is saying across the batch, is it the case that the same upstream gradient is obligated to pay the high? That is correct because that's going to just be the sum across all of my examples. Whereas when I back propagate, we're going to tell us to derive what the backdrop for the x j's the plank wholesome. And then just to be clear, moving forward, we don't have to worry about the price because there's a subscript eyes, because there's going to be the exact same graph for everything. But I'll defer it on this one slide. I said we're going to drop this off to apply for convenience. But indeed there is a Beta, a Gamma, there's an IBC be on everything. That's fine. Let's keep going on oh, actually, before I do that, any questions on this. So I'm told me to make sure that you all understand these because this has been, uh, there's been a barrier. If somebody asked you about this. Yeah, The question is, can I explain what task converge again? So basically, let me draw it in the following way. There's going to be a beta i parameter. Beta i parameter is always going to be added to gamma xy prime irrespective of which example RON, there's no j index. So this beta i is basically a parameter that sits here and it's the same data I added here to every single activation across my examples. And so the conversion paths are that beta i, which if there were 100 examples with branch out to all hundred of these graphs, is going to sum up the gradients DL DYJ from j equals one to j indexes the data instances with this index. Indexes which artificial neuron in our layer of yard. So we can just go ahead and set it for the rest of this class. For the rest of this lecture, I is going to equal to, so we're going to be looking at batch criminalizing. Just a second neuron in this layer. Split it up. All of our data is that we started off all of our data to every single word. So the question is, what is the difference then between the neurons? Because neurons in general have different weights, the upstream to the earlier observations will be different than one another. So because of the weight matrix has different weights for every different you're on the right. My question here. Here's the individual neuron with an array or just to, just to make sure that's clear. What is the batches of students question? So the Batch refers to if m2 equals 100, then I have a batch of 100 images. That means that I have 100 images of cats, dogs, airplanes, whatever. And for j equals one to 100 for each image, I'm going to pass that into the network, and that's going to cause x2 to have a different value. So u to the main activity of this second neuron is going to be the bean across my 100 examples, and that's the variance is gonna be the sample variance across those 100 examples are saying, then that means that each different batch will have a different view and a different Sigma squared? That's correct. Yes. Yes. Oh, sorry, I'm sorry, I misspoke there. So batch norm doesn't question, does dashboard happened after the non-linear activation? Nowhere. So today the answer is yes. People have done experience and experiments and they see that it's better to put back on after the acquisition box. In the homework, we're going to follow what IOP and said that he did in 2015, which pushes, which puts batch norm right before the non-linear activation function. Great. So Tim's question is, well, the weights and the biases will also change the mean and the variance of the data. So isn't this kind of redundant? The answer is that while in theory it is possible for the weights and the biases to have it set so that it learned mean zero variance one features. In practice, we know that the weights and the biases or just be updated by a gradient descent. So when do you can think of this? As? You can think of this is, you can think of this as a technique that biases or network towards variance 01. Because it's highly improbable that stochastic gradient descent would find that solution. We're going to see this comes up later on in an architecture to call residual network the price net. And that's another thing where even though in theory a neural network because of residual layer by adding it really helps to train the wall. Alright, so that's the backdrop to DLD data. The question is, why don't we learn the betas and gammas and then fix them as hyper parameters for future training. You couldn't do that. But it seems to be more constraint than if you just let the network to learn two betas and gammas. And so I would expect you to achieve much better performance if you let it in Canopy or the sound optimal, sub-optimal. Okay, I'm gonna move on for now. But if you have a question about this, please feel free to come see me during the bridge. So DYJ backpropagation these theta i's. And then because again, this beta I is the same beta i for all of my examples, then the gradient is gonna be the sum across all my examples of the beast DL DYJ is the gradient over here is also going to be a DL DYJ backpropagating through a plus sign and then we can back propagate out to gamma i. So let's do this in a different color. I'm back propagating out to TLD Gamma. And we know that when we back propagate through multiplication, you take the oxygen gradient and multiplied by the value on the other wire. The gradient here is gonna be my upstream gradient, which is DL DYJ. And the value on the opposing wider dispatch had j, right? So it's going to be DL DYJ times x hat j. That's where we get this term. And then again, because of Gamma I is the same gamma i across all my examples are going to be summed from j equals one to 100. These are all scalars. Yeah. So, yeah. You may have also wondered what the outside j on the left or the right, because they're scalars. It doesn't matter if you go on the left progress. Alright? That gives me this gradient over here. So this is the purple gradient, my gradient DLD beta. Now I'm going to backpropagate to this wire right here. I'll draw this one in bike loop. I'm going to put the gradient over here just so that this case is encoded. And this is going to be DL DYJ times what's on the other wire, which is gamma i. So this gradient here is DYJ gamma hard. I'm sorry, I'm going to drop the subscript sides, like I said. So the gradient of L with respect to d x hat j is going to be DL DYJ times Gamma. So this is my blue gradient. Any questions about the purple? Alright? And then after that, we're going to do one more back propagation step. So we're going to backpropagate and I'll do this in green to this right here. If I want to compute DL DHA, alright, I'm gonna take my upstream gradient, which is this blue one. I'm going to call this D L dx j. So this is going to be PL d x hat j times the value on the other wire, which is b to the k. So this is b j. So when you see me write it out right here, I have the x hat j, that's this term. Then b j here is going to be the value of this wire, which is gonna be one over the square root of sigma squared plus, alright, and so that's why this term here is equal to DJ. And I've just expanded it out. Any questions on those pretty Let's do one more actually here, I'm going to back propagate to DLD BJ also. So my two D, L, D DJ, I'm going to take my option gradient, which is dy dx that J, it's going to be DLT x hat j value on the other wire, which is here a j. Right? So those are radiuses that propagated all the way to these workers. Any questions on any of those gradients? The question is, do we care about the wire that's coming out from the J? Which wires that yes, we do care about these. We're gonna do them on the next slides. Are there questions on dissenting of these gradients? Next five years, next time. That is correct, yes. So we will get to that in later slides. So we will have to write a DUDX I turn. Okay. Other questions like that. Right? So if I understand rocks this question correctly, it says that theta i and gamma i are shared for all the J computational graphs. Xy is not exciting as a superscript j plus one and epsilon I j. So those are not shared. The question is, what is the proper order of are you talking about the terms that multiply? Right? Great. So the question is, here I put BJ on the right-hand side, but here VJ is on the left-hand side. And the answer is because they're scalars, you can put them on the left or the right equipment. Yeah, so the question is what if you were doing, was doing this for the matrix that you would have to get the ordering correct. I'm going to follow the denominator layout. But we won't do it with a matrix because it turns out that if you try to do this normalization with the matrix, you need to compute a matrix inverse across or parameters. That's very expensive computationally. So that's why we always break it down into just a scalar computations. All right, we're going to move on then. So we're going to start off then on this next slide with DLD BJ. So that was this gradient in green that we calculated on the previous slide, DLD BJ, That's right here. And this thing was the L dx j times a j. And a j is just x j minus mu. So that's why I write that. Djs x or x j minus mu times d L dx factor. Okay? Now we're going to finish the back propagation along this part of the graph. So the first thing that I'm going to do is I am going to backpropagate from here to here. Can someone tell me what that backpropagating gradient will be? Someone else other than Daniel. Welcome back today. I know that's perfect. Yeah. That's correct. So remember that when we backpropagate, right, to take our opportunity gradient times local gradient. So this operation here is saying that d j is equal to the local operations inverse of d j is equal to one over CJ, right? And therefore, if I take the vocal brilliant, I'm computing d b j C j. And this is equal to minus one over c j squared. What that means is by backpropagate here, it is going to eat both. The gradient is gonna be d L d b j times minus one Over the value of C j, this whole quantity squared. So I've written that expression out here. What is CJ equal to C j is equal to the square root of sigma squared plus epsilon. And if I go ahead and square that, I get a one over sigma squared plus epsilon, and then there's a minus sign here. Alright? So this minus sine one over sigma squared plus epsilon is my minus one over c j squared. Then this is DLD, DJ is this term. So this is my upstream gradient. This component here is my local gradient minus one over some cases there. Any questions? Alright, now we're going to backpropagate through the square root symbol. So, well the square root operations. So if I back propagate here, what do the exact same computation of a local gradient. So here the operation is C j equals square root of EJ. Alright? Therefore, D C j d e j equals 1/2 times the square root of this gradient here is gonna be my upstream gradient, which is my purple thing. Minus one over sigma square plus epsilon times this. And I'm going to further multiply that by 1/2 square root of e j 1/2 is over here. The square root of e j is equal to square root of sigma squared plus epsilon. And so that's why then she Min squared plus epsilon goes to sigma squared plus epsilon to the three-halves because I'm adding here a square root. That will be my gradient. Backpropagated to d L, d e, j, k. We're doing a bunch of stuff here. Any questions or anyone want me to explain any of these treatments? Can you raise your hand if you're following so far? Awesome, that's good. So let's go ahead and do one more thing. Which comes back to the student's question over here about backpropagating through. So they also knew the gradient here, DL DHA. Dha is what we computed over here. And it is equal to dy dx times dy j. So we know that this gradient DL DHA. And what I wanna do is I want to backpropagate to D L, D view. Alright. Do DHA. Let me write it out here. The LGA j was equal to d L d x hat j times DJ. I remember j is equal to 1/1 over the square root of sigma squared plus epsilon. All right? So DLD a J equals one over sigma squared over the square root of sigma squared plus not fun. That's this term here. Times dy over dx j. So the first thing that we're going to do is we're gonna take this gradient and back, propagate it to you. Let me do this in a color red. When it back propagates to view you as being subtracted from x j. You haven't minus sign. And so this expression here, which is equal to this gradient is also going to get a minus sign. Alright? So backpropagating DL DHA here gives this expression here. But this is not the complete derivative. Because new comes in somewhere else in this graph. And this is therefore incomplete. I see red. I need to read without the summation. Yes. So this ambition here though, will be the same argument as before. And so that's why I'm just going to be summation there. Right? Yeah. The question is sigma squared also will apply to every example. Jason will also have a sum j equals one to m. Yes. Yes. Okay, So there's one more thing that we're missing here, which is that when you look at the operation of batch norm, Sigma I squared is also a function of you are. Alright. So your eye is going to go through this function. I'm going to call it the sample variance function to tell me what the value of Sigma I squared is. So there's also going to be a function called sample variance. Sample variance. Compute Sigma I squared. And it has inputs x, j. As well as to do so because this view is the same, right? To get the total gradient DLD in you, I also have to backpropagate to this new and then sum those gradients together. So I need to also compute a plus d L d sigma squared and then a d Sigma squared d nu by my chain rule. Again, that's because sigma squared is also a function of. Alright, so to do that, I'm just going to write up the equation here. So the sample variance equation tells me that Sigma squared is equal to one over n. And we have a sum from j equals one to n. And we have an x, j minus mu squared. If we take d Sigma squared DMU, what we're doing is we're doing d Sigma squared DV view. And this is going to be taking the derivative of this expression with respect to mu. So I'm going to get the two to come down. So I'm going to have a two over m. And then sum from j equals one to n of x, j minus mu. And then I have to differentiate with respect to D, which is gonna give me just a minus one-half. So this is going to give me a minus sign over here. What that tells me then is that this gradient here is going to equal DLD sigma squared. And then for d sigma squared d new, I'm going to plug in this expression right here. So let me erase this. And I plug in d Sigma squared DU, which is going to be this times two over n. Sum from j equals one to n x j minus. And then that is my gradient. Questions here. Daniel's question is, isn't new, something computed as a result of the data? Meaning it is calculated from the data. It's not really a parameter. I'm not going to do gradient descent on it. So what do I have to calculate DLJ? And you constructed the student's point earlier, which is that in the computational graph, you depends on x j. We need to compute a DLT view so that we can back propagate it to x j, x j. We need the gradient for because that's going to give you the greatest upstream. Other questions here. Alright, so that's actually brings up a bit to get to more babies here. So we have all these gradients. Now, what we wanna do is we want to ultimately backpropagate to DLD acts. Like you're just answering for Daniel's question, influences u and Sigma. So I also have to know how to backpropagate the gradient DLD due to D L dx and then DLD Sigma squared dy over dx, which means I have to compute what DLD Sigma squared. So these are gradients, DLD a and DLD that we already know the gradient. There's a question here. For the DOD view. The customer, is there a reason that what the question is, is the reason for this summation the same as for the beta i's summation? Yes, it is, yeah. Because u is going to be the same view for every example. Alright? So now we need a backpropagate to d sigma squared. We know DLD EJ, we did that here, gradients. So let me write that in blue. We have DLD j, that's the gradient right here. And what we wanna do now is one of that property, that is sigma i squared. So this is just a plus sign. So the gradient passes through. And then for the same reason we typically keep talking about the Sigma iceberg serves every example j. So it's gonna be the sum of the DL ij's. And therefore, this is a backpropagated gradients for DLP sigma squared. This expression is exactly what we calculated on the prior slide in blue. And now overdoing here is something that across all of my examples, j equals one. All right, so DOD sigma squared is basically for free because we already knew this gradient calculated through a plus sign. Alright, now we come to our last thing, which is we need to get dx j. We need this gradient right here. So for this gradient, recall we had a DL, DHA that's in green over here. We're going to have a blue gradient, del d Sigma squared. We'll have a purple gradient, DL d u. And remember that all of these are function of x. So u is computed from X bar. The sample mean, sigma is computed from X by the sample covariance, or sorry, the sample periods. And so what we do here is we're going to write the chain rule. So first off, D L dx j is going to be a component from the green backpropagating gradient that's propagating through a summation. So it just comes through, That's the steel DHA. Then for the Sigma squared term, remember that there is the sample variance function that is affected by x JMU. So to backpropagate here, I needed to do DLD sigma squared times the local gradient or the sample variance is sigma squared x j. That because that's going to give you this term. And the last thing is the backpropagating front of you. So mu goes back through a sample mean function for which x is also an input. And so to backpropagate through this view, I'm going to have to calculate DLT view of times d Mu dx j. Alright? All of these are gradients that we've already computed or ones that you can see. So I'm going to just, I'm not going to compute d Sigma squared dx j, but please do that as practice on your own. That would be taking this expression right here and differentiate it with respect to x j. And then when you add all of these together, because these are all convergent pass on the state x j. So the law of total derivatives apply. Mp get the total gradient d L, dx j. Yeah, So you're saying you appears here and remember, you also has XJ affects this new shouldn't there be another cluster, the new term there isn't in this case because this GLP view term came from this slide and this slide DLT knew already took into account the effect of both of these pathways can do. So. We've already talked about gradients. So great, yeah, so Anna's question is, when I propagate, my back propagated through this view, I've got a minus sign. When I talked propagated through, backpropagated to the x j, I didn't have it. And that would just be because right here will have a j equals x, j minus mu. So because the minus sign is on the new, then that's why that minus sign come from the view, but not for the extra question. Any other questions on that? We have seen that should be right. Back with some great boxer. This pointing out that this deal dx j is for one example of j, right? And so what's going to happen when I back propagate to train the parameters of my networks. Well, in that back propagation code, you're going to have a dy, dx j. Let's say this is let's say that there are, I'm sorry. This is, this is a scalar, so this is an R. But let's say that we had 100 units. So we would have a big X being a, a matrix, which is the number of examples, which was little n by the number of artificial neurons in that layer. Let's call that 100. And then this would multiply a big dealt with you. So you would not sum them together. It would be concatenated as a batch, just like in homework number three. And you've got complicated. This is great. The question is, is this just be seen as an example of continued back propagation? Or do you need to know this for the exam ladder? So you need to be able to backpropagate through these types of operations as well. And there's good chance that there might be an exam question on that. Alright. Thank you, Tom. I said Tom, I said that they're going to do this example in discussion as well. So this is something where it went a bit fast for, you know, already the TAs are gonna go over it again and discussion. And you can ask some more questions here. Alright, let's go ahead and take a five-minute break and then we'll come back to continue on. All right. Yes, correct. All right. Sure. All right. Hi. We'll get back to it. So we're gonna follow the convention used by, oh sorry. Actually before we do that, last questions on any of the gradients computed or normalization. Alright? So the batch normalization layer, we are going to follow the convention of IOPS IgD, and place it before the non-linear activation functions. So we're going to have it w times h plus b. That's my linear layer in a neural network. And instead of passing back directly to ReLu, I'm going to first passage the batch norm and then Turabian. Alright, So in homework number three, you implemented these new layers. So in homework number four, you're also going to write a batch norm later. And that DashCon layer, you're going to write the forward and the backward pass. Those fees instead of just asked find Bailey, I find really find batch norm, ReLu atropine batch norm. Alright. Any questions? Alright? And so there's gonna be one more thing for homework number 42, more things that we talked about, which are gonna be dropped out and difficult to misers. And hopefully we may get to drop out today and then we'll do optimizers on Wednesday. All right? So this gets us into the topic of regularization. Regularization is, are things that all of you have heard of before and a fire machine learning class almost surely. So the first thing is that we're going to talk about are going to be a recap for many of you ever just going to go to them if they parked at the end, you also implemented some regularization. So like, like L2 regularization on your homeworks, right? Before even getting to that, if I asked you to define regularization, think about the definition without reading the slide here, I didn't ask permission. It's interesting to think how you would define it because this is a term that we throw around a lot. Regularization incorporates many penalties. Dataset augmentation. So the first thing that we're going to do is we're going to make a working definition of regularization for this class, which is set regularization is any modification we made to the learning algorithm that is intended to reduce this generalization error, but not its training error. In other words, if we have a validation and testing error, regularization is something that's going to make those things go. Alright. This is a definition taken from the deep-learning textbook, like the fellow which, which I wrap it in orange. Definition is pretty wide and it may incorporate things that you hadn't thought of a regularization. So one thing that incorporates is early stopping. Imagine we had a situation where on the x-axis we have number of training epochs and on the y-axis we have our loss function hour. We know that if you train with stochastic gradient descent, you're training loss in general will go down and down. But your validation loss might go down and then start to increase as you overfit. We talked about this before, right? So this will be our validation loss. The act of stopping your training at this point. And using the bottle at this epoch is a form of regularization because it's an intervention that makes her generalization error, your validation error better. Alright? So if it's talking early under this definition is an example of regularization. And then there's this really interesting discussion. You won't be tested on it. But in the deep learning book on pages to 42% to 45, Goodfellow actually gives a really cool insight that early stopping could be viewed as a form of weight regularization with you hold on the homeworks. Yes. I was saying in this setup, you would need to evaluate your validation loss on every inbox. And that's correct, but you could also evaluate it up for you, you know, ten bucks. But then you would enter your resolution would be stopping at the regions that are multiples of it. The question is, will the validation set? The different for all the epoch? Depends on how you set up your batches. If you're randomly sampling at every possible different batches, then it could be different. Actually started. Let me walk. In general. Yeah, we will just reserve a separate validation set which will be the same. That will be like my one fold, that's for validation and that'll be the same across all of the same validation set has to be used for that folded cross-validation. Alright, I'm gonna go quickly through the next few questions are several parameters because all of these already, I see them. So a parameter, norm penalties. And for the next few lectures, I'm going to use the letter J to represent loss instead of l. Just to be more consistent with the textbook, people will use J and L together. But for these phi j, remember that j is lost. What do you do when you have a parameter norm penalty is you have your original loss. And then to that you add some function of your parameters. Here we denoted this by omega. And one thing that you've implemented under homeworks is L2 regularization, right? So if w is a vector, then omega, which simply be the two norm of that. Vector squared. And on the homework, right, the w's are not vectors, but they're matrices and so on the homework, you all have done that this regularization term is one-half. And then we take the Frobenius norm of the matrix. And then you also know from the home birth and from how we define gradient so that you do D L, w or not, the L is going to be d sigma d omega. This is just going to be four. Because this thing is added to the loss function. This thing is absolute loss function and we want to make the loss function as far as possible. The effect this will have is to make our weights smaller. One interpretation of L2 regularization is what is called weight decay experiment that comes from. So let's say that I were to do L2 regularization. So I have my original loss J and I have added to it. We're just going to work in the case where w is a vector instead of a matrix. W is a vector. So I'm going to kind of analyze the norm of the vector squared, which is w transpose w. And I'm going to call this my new loss function, which is J Tilden. So if this term is in here, right? If there is no regularization, we know that when we update our parameters, we will do w is going to be my original w minus epsilon times the gradient of J with respect to. Remember that j is the original loss. J tilde is the one that adds parameter norm penalties. So what happens when we actually have regurgitation? The case where there is regularization. Now my update is going to be w. And my new loss function is g Tilda with this parameter norm penalties. This is W minus epsilon gradient with respect to w. J will be to do is we can go ahead and expand out this gradient with respect to J tilde. So if I take the gradients of both sides of this equation, that gives me the gradient with respect to w. That's going to equal the gradient of J with respect to w term over here. But then I also differentiate this term with respect to w and I guess being out there. So this equals an update row where it had w minus epsilon. And then we have w plus gradient of J with respect to w. I'm going from here to here, I've just substituted what I do simplification on this. This leads me to a new update rule. W is one minus x one alpha w minus epsilon gradient of J with respect to w. Now if you compare this weight update rule, which comes from parameter norm L2 regularization with the case where there's no regularization. The only difference is that this W changes to one minus epsilon alpha times. And that's why this is sometimes called interpretation of this L2 regularization is I were taking the gradient steps, but every iteration we're also decaying w by one minus epsilon. Yeah, So Daniel's question is our understandings of epsilon alpha where this could go the wrong way. And that's the case, then that's fine. I'll better set so large that the waste actually scale up. Almost surely your gradient descent isn't going to converge because you're always full. I will explain it. Instead, you would want to pick up, alright. I hope this slide, I wrote not tested. This is just if you've seen different types of regularization for Gaussian distributions and other classes. There are connections to this L2 weight regularization on the slide for those of you who want to participate, but you won't be tested on this. Alright? While we usually just regularize the norm of a squared are there for PBS norm. I want to point out that there are other things that you could do with this L2 regularization. So when I penalize just the norm of the weights squared, right? I want the weight to be smaller and smaller. What I can do is I can penalize. Norm of the weights minus some vector b. And what this does is instead of taking the waste was smaller and smaller, It's going to make the weights closer to this vector v. So if you have prior knowledge that the weights should have some value close to something that you could use L2 regularization to try to push it to that value. Then maybe in another setting, you might have some prior knowledge that two sets of weights should be close to each other and valleys, you can go ahead and just kinda lights are different. And L2 regularization will cause the W1 and W2 to be closer to each other. Alright. One more thing that you have likely seen your fire. Yeah. That's wonderful. Yes, a tomboy saying, you may have taken an optimization class before, but maybe convex optimization, but he didn't say optimization. And in constraint optimization and he would take your constraints if I didn't as an additional withdrawn came factor and that's exactly what this is as well. Did you haven't taken an optimization class and what I just said doesn't make any sense, you know? Okay? So there's one more form of regularization called L1 regularization, you've likely seen before. And all the changes with L1 regularization is instead of using the 2-norm, the square root of the sum of the value squared. We're using the one norm, which is the sum of the absolute value. And this has a really interesting empirical property, which is that if you apply the L1 norm to a set of weights w, a lot of them end up being zero. So this leads to so-called sparse weights. And I'll just give you a rough intuition for why this is, if we have a 2-norm, write a two norm is a quadratic. This is our 2-norm. And a one norm is the absolute value function. So the one norm here is our absolute value function. If I am changing my base, my 2-norm smaller, right? What that means is that I want to make the two norm as small as possible. You'll notice that as I get closer and closer to zero, the gradients become smaller and smaller because the parabola curves in. And so as it curves in these gradients here getting closer to zero. Whereas if I take the gradient of the one norm, they're always large and the same irrespective of if by waves are large or small. The 2-norm, as you get closer to zero. Gradients or rapid change less with the one norm. When you're away from zero, the gradient is always the same. And so empirically the speech to the one norm resulting in both sexes. Alright? So sometimes instead of the waste being sexist or you might want to parse representation, you might want your artificial neurons themselves to be sparse, has many of the occupations that are over. And so it's interesting to regularizing the weights. You could go ahead and just regularize the one norm of the activations. And this will lead to many of the occupations. Alright? So those are general things that you have seen. Some neural network specific regularization. And the first thing that we're going to talk about is dataset augmentation. It's really straightforward, really intuitive, but actually thinking about the performance. So let's say that you want to classify that this cat is a cabinet. Cat is still attack if I flip the image, if I crop the image, if I do some type of ripeness or distortion on the image, right? I could do a lens correction. I can even rotate the image if I wanted. All of these are images of cats. You'll notice that these images, right, the pixel values could be very different because if I cropped, this pixel, which was originally white in the original image, is now a dark color. So the pixel values can be very different because all of these need to be classified as cats. I can, in effect, make BI dataset on larger by taking different flips and Fox advantage. And using those additional data. More data helps to avoid over fitting. And therefore it can have a regularizing effect. Any questions there. Alright, so this is a paper from an architecture called Google neck. We're going to talk about what you will get is exactly, but in 2014, it wants the ImageNet competition, this one where you have to classify the images that we've talked about. And what I want you to do is just focus on this part of the table. And if you read the description here, what you'll see is that for Google Maps, they made up to 144 different replicas of the exact same cat and your input image by taking many different crops. And what I want you to notice is if you just don't do any dataset augmentation, that's one crop does a pretty good job. The error rate is 10%. But if you take these 144 columns, again, just taking the same image and copy it in different ways and they're flipping it, the error rate decreases to 7.89%. This is a 20% reduction on the base error and that is a significant performance improvement for all you did was take the image and crop it in different ways to get many more examples. So this has a really strong and positive affect. The ultimate question, right? Yeah, so the question is or statement rather is we have to be careful about the types of augmentations that we do. They have to be reasonable ones that lead to that first one to Bowlby would want the algorithms to be able to do so like e.g. if I didn't augmentation where like I blacked out a part of the image outright copy what I want to do it because I wanted to say that these are cats, but trumping is reasonable because each of these crops is still a specific order. I can just prompt that. Right. Yeah. Great. Question is do they do different prompts depending on what images quite maybe you want to call caps and cars in different ways. So this paper did not. They applied the same exact copying out with them. So all different types of images there. And that led to a great performance. You could conceivably think of a specific Crawford and classes. And I'm not sure if that would help or not. Student asks, what is the number of bottles in this case? We're talking about that in just a few slides, but that is the number of ensemble model. Alright, so this is for integers. We took this idea and we'll try to finish interfaces as well. So I'm going to show you a video from my grad school days where what we're doing, if you're trying to build robust brain machine interfaces. And we perform these experiments where there's a monkey who learns to control this cursor just by thinking about it. And the left side, it's same day is the state of New York decoder. Up until this point. What we realized is that over the course of time, so let me pause this video. Over the course of time. Neural data also fluctuate. Over the course of an experimental structure. Sometimes neurons you're recording from just disappear. Sometimes as a reward for doing this child to one of these get juice and the juice has sugar in it. So sometimes they become anchored by experiments is that activation of a neuron toiletries. And then sometimes the monkeys will become more drowsy and there'll be a decrease in global decrease in the neural activity that we perform dataset augmentation is that modeled these increases or decreases and also loss of neurons. And you also trained a neural network on the right that can take advantage of all of these augmentations. And you can see that even when the state of the art on the left, the RNN, which the neural network that has these augmentations, does our data set augmentations, oftentimes make sense. Many more settings and this is a figure from a paper. I'm going to skip it for now. Talking about his work, feel free to drop by my office hours. Type of augmentation is called labels to the game. So in the very first lectures of this class, at least one student who asked this question when we showed the ImageNet dataset. We have an image and we assign the label. Those tables are given by humans and sometimes with labels are not correct. And so what we could do is something called labels moving. So. In your homework right now for C4 ten, you develop what are so-called one-hot representations of the data. Which is a factor that is zeros for all the incorrect classes and one for the product, one where this is, you know, Airplane, car, cat, etc. The idea is you say, Okay, Are, they will be, aren't perfect. So instead of using this as my true label, instead what I'm going to do is I'm going to make my true Example. If I need to guarantee the following, I'm going to say instead of one for the correct class, that's going to be 0.9. Then I still have ten per cent left them. I probably distribution, I'm going to distribute that amongst all the other classes. So all the other classes will not take on a value like 0.01, 10.01, 10.011, all the way down to 0.011. So this is the distribution that sums up to one. But if we now use this in our cross-entropy loss, this is saying the correct classes cat with a probability of 0.9. It turns out that using these labels also has a regularizing effect. Alright, so within this table from a different paper, I want you to compare these two numbers. This is the error rate for a network called Inception V2. By the way, within two or three lectures, you'll understand what inception and Google that the end here is batch norm, right? So you understand what all of these terms mean in a few lectures. But if you just focus on inception B2 versus inception v2 with labels smoothing, you can see that there is a small bumper of 0.6%. Performance improvement. Regions depending on the particular class, give me one region versus another. So this time but maples ArcMap does affect the bathroom supplements. Thomas question is, how does this affect the back propagation through the softmax? Depending on why did you want to look at this image? Great. Yeah. So I'm always pointing out that when we did the softmax classifier, remember the numerator of the likelihood? It was just the exponentiated score for the correct class. So that wouldn't apply here because we no longer have a correct class. And so that's a softmax but has to be updated. Alright? You other things that are helpful. One is called multitask learning. It's because they did the following. Maybe if we were training at work to do one task for me, it would be really fun to, to that past. So instead, why don't we have a neural network, many related tasks, and I hope we identify features that are helping all of those tasks. So let me write that up here. Let's say that we have three different tasks. In this task, the goal of the neural network is to label, label the class. Every, right, so all of the purple ones would be labels for street. All of the red ones are labeled for people into the table. Alright? Another task that you could do is called instance decoding. And so this is to label all instances of a class. So in this case, we told the network for this example to label all instances of people. And so it goes and picks up all the people and in colors, but maybe it makes them. It also enables this bicycle as a person, but it's not. Alright. And then maybe another task that you may want to do. A task that we do frequently is a definite decoding. So tell me how far in the z-plane each pixel is. These are all things that we as humans are able to do, right? So it stands to reason that in our brain are developing a core set of features that allow me to do all of these tasks together. And so the idea of multitask learning is to say, let me take many tasks at bottleneck by saying I need to use a set of features given by a neural network that work. To do all three of these tasks at the same time. The way that I would do that is that each task will give me a loss. So maybe the first task could be L1. Second task is the L2, this task is L3. And then I was something together, L1 plus L2 plus the L3. And I can back propagate all the way to the start of the network. And that would then transfer features don't work on all three tasks and those features may be better explained on one task. For me. The question I believe the question was assign different weights to these different losses, right? Yes, you can. If some tests are more important than the others, maybe, I'll, maybe the first task is more important. You can give it a weight of 0.5 and maybe these are 0.25. So you have freedom as the experimenter to change. The question is, is data augmentation as subset of regularization techniques? Yes. Because remember here we're saying regularizations and anything that increases that makes my generalization. Question is, are there four numbers here? It depends on how you define a network, but there'll be neural network layers for the encoder. And then actually I have it drawn out here. So you might have three different tasks. And here this could be a neural network with many different layers. But then yeah, you would have separate neural networks with its own parameters that then branch out for each task. The question is, that's when the intuition of why did we sum these boxes together? Because if I want, if I didn't have L2 and L3 here, then my backup to get it lost with only changed the encoder or the earlier layers to find features that are good for just the first task I wanted. If I wanted to also learn features that are good for tasks 2.3, they have to be in the loss. For the other part of question, I didn't follow it entirely. Abandoned office hours. Alright, so another really important thing is called transfer learning. This is really useful in the setting where you don't have that much data. On a bunch of PhD thesis committees where students are trying to build neural networks that classify medical scans like MRI images, PET scans, CT scans to try to be coded into another prokaryote. Then here you're working with doctors who had very limited number of stems, but they don't have as many scandalously have images in ImageNet. And you know that if we don't have that much data, then numbers are very prone to overfitting. So how can you still build neural networks and the contexts where we have the middle? So the idea is there's something called transfer learning. Let me draw what this is like. Actually, let me first explain the intuition. The intuition is that we're trying to classify these images. When a Dr. looks at the medical image, right? That Dr. is using the same brain that he uses to look at a picture of a cat. So prior to assist would be that the images, Sorry, features in a network that are good for classifying that a cat, a cat or a plaintiff must also be useful for identifying features. An image that says, okay, because there's a malignant tumor, tumor. And this is, so the idea is that the first train a neural network on a large dataset. But the initiative, and then we fine tune it for this MRI or CT scan data set. So the idea is the following. We have the neural network and it has some number of layers. Let's say it has 157 layers. We're going to talk about this neural network in a few lectures called the ResNet with 157 layers. And then after that, remember at the end of every neural network is a softmax layer which comprises write a linear operation. And then that goes into my softmax function. That gives me a lawsuit. Softmax does get me a loss L. So what I can do is I can train this entire network on Commissioner for a lot of data. And then transfer learning is the following. To fine tune this network to be able to classify MRI images. What I'm going to do is I'm going to take that network trained on ImageNet. And I'm going to fix all of the parameters in my 157, right? And then what I'm going to do is for this new dataset where I have, let's call it MRI images and corresponding labels like a medical diagnosis. I'm only going to use those examples to retrain or fine-tune. The softmax classifier at. What this says is that each one to 8157, this is a neural network components that is going to identify the features that are good for classifying images. And I'm going to use those features which I can now get from MRI images, but MRI images through this exact same neural network. And then after that, I'm just going to tune the linear classifier at the output to try to reduce the loss like accuracy of diagnosis. And turns out that this works really well in many settings when you don't have that much data. And this allows you to translate these large neural networks to small data centers. Any questions they're saving of the features in that you mentioned that they just don't eat much different than the screaming of the features. And affects NRC, right? It says, are these networks learning is scale invariant features. Likely they are because the ImageNet dataset would be able to classify objects even if they are of different sizes. And in some ways point is, maybe that might not be the case for scans or CT scans. So if there are features in these Petr CT scans are different than the ones that Verde's for classification that in those cases in my transfer as possibly. Other questions. Okay, with that, we're going to get small sample methods and methods will lead us to truck out. So ensemble methods are another significant boost in performance without very much cognitive effort. And here's the intuition of ensemble methods. I could trim one neural network to do CFR ten classification like you've done in homework number three or will do in homework number three. But what if I have the following instead of one network, I have ten numbers. These ten networks make independent errors, right? If you have one network, if the error, you're dead in the water. But if you have ten networks and you show an image, maybe two of the networks make an error, but the other apes get it right. So what I can do is I can pull the knowledge of these ten networks. And because there may be a dependent errors, even though some networks may get some examples wrong, the majority will get them right. And that should boost performance. And that idea works. And so the idea of ensembling, It's for the same dataset to train multiple different models. Then you average the results together when the testing here. And this almost always increases the performance by a substantial amount. Intuition for how this work is. Work is probably something that you've seen in your very first statistics class. If we have k independent models, right? Then if I want to know the average error across those k models, so epsilon I is the error for one model. If they're independent, we know from statistics that if I were to compute the expected value of the error on model i and the error model j, because they're making independent errors, epsilon, epsilon j are independent and therefore, this simplifies the expectation of epsilon i times the expectation of Epsilon. If we go ahead and we carry forward this expansion, I'll get one over k squared and then an expectation of epsilon, I square it. That's applying. If you do out this, this quadratic, this is what it simplifies to. And therefore you see that epsilon I is the error, sorry, expected value of epsilon I square is the average error for one model. The average error when you average the ten models together will be decreased by 1/10. In this case it's the variance decreases by one over tangent to the average error with decreased by one over square root of two. So this is why it's good to average the outputs of a different models together. Now you may say, well, you know, these models are not ever fully independent. So if they're not independent, this is what the error is. And the worst-case scenario. Is that your models are totally correlated. Epsilon i equals epsilon j. In this case, ensembling doesn't help me because if you plug in epsilon I equals epsilon j equation, it just simplifies to expected value of epsilon I square. But as long as they are not perfectly correlated, It's obviously have some independence about guns, some degree of independence. You will reduce the error by averaging the output. I'm sorry, the question is, here we are multiplying the errors together. In the original expression. We're taking the average of the errors. And when you do this amplifying that, you get some of these terms here. Because she just has this first equality holds. I'll leave that as an exercise to you all, but it holds from, from simply this expansion here. And also in this proof, we assume that expected value of epsilon i equal zero. So all of these cross terms are going to equal zero. So when you, even though this is like an epsilon one plus epsilon, epsilon, epsilon 100. Yeah, you square it before taking the expectation. All right? Okay, So hopefully that tells us that it's a good idea to build any model outputs together. There are multiple ways that one could do ensembling. Outside of deep learning. One common way is called bagging. Bagging stands for bootstrap aggregation, and the idea is really simple. We start off with a dataset with an example. Let's say that we want to make k models. So from this example, from these examples, I'm going to make k different datasets. Each of these datasets, I'm going to draw N examples. With replacement. This is the context where I want k models. So they get up k models, I'm gonna make k different datasets will have lot of overlap. But because I'm drawing with replacement, these data sets are going to be different from each other. I can then train one model on each of these K datasets. They'll give me k different models and then I can average their outputs together. I can ensemble them together. And that also gives me hopefully the benefits of ensembling. Alright. This hasn't really done for neural networks because we're going to learn that some of these neural networks take a really long time to train. But just for now we're going to talk about a number called VGG net, and that took three weeks to train. So you don't want to train. Don't want to have to make leaps for every single model. Even if you run them in parallel, that would require a lot of computational effort. Another thing about neural networks is because they are generally higher capacity, more complex models that some simpler things that you might have learned in applied machine learning class. It turns out that even with the same exact data center and examples, if you just initialize neural networks differently because of loss of surface is. So, so I guess Com class. It turns out that these different initializations tend to lead to partially independent particles. So to get your k models or K neural networks from any examples, you could really just do k different initialization. And those models are often sufficient. We are different, right? But then again, neural networks take a long time to train. So usually you don't want to train different models. You want to do this some other way. One way that this paper from ice CLR 2017's decided to do it was to say, on my x-axis I have the training epochs and on my y-axis I have lost. And what I can do is I could just train this standard way. That's this blue curve right here. And I can wait many epochs to get a best model, right? That's one way that I could train. This paper said, well, we know that ensembling is really good. What if I just do this really wonky training where I take large steps and I get the error north to the minimum but to some respectable level. And then I basically turned back up the learning rate. So I have my local minima and I go to another local minima. Turn out by learning rate, gets to another local minima. This will give me six models where each of these six models on their own is probably worse than just one blue model at the bottom. But if I take the six models and average ensemble, their outfits together actually do better than this one model. Again because ensembling. Yes. So Tom weighs point is It's stopping early for each of these epochs. And then using this as one of my six models. The more standard way to do this type of ensembling or this bootstrap aggregation and neural networks with just one training class is something called dropout rest. 03:50 P.M. so on Wednesday we're going to start off by explaining exactly how dropped out for externalities. 
So a few things before we begin. First is a reminder, homework number three is due tonight. And please be sure to upload the code with the assignment as well. We've been receiving a few e-mails about late. We've been receiving a few e-mails about ME days and I just want to remind you, oh, you're welcome to use your late days but got to the foreign cars and you have three free rate. Alright, so there's no penalty for guessing. Here. We're going to upload homework number four today. And so that's an assignment that the TAs asked me to tell you all to start early. Because in the assignments, we will also ask you to do some hyperparameter optimization to achieve at least 60% accuracy, punk boy, yeah, 16% accuracy. So that optimization will pick something. So please be sure to start early on homework number four. In homework number four is going to be due Friday. Not this Friday, but the Friday after that. He taught boy, that's 17. Regarding the midterm exam, all of the past exams that we have ever given for this course, our ability to grow and learn already under modules. And for this year, the exam is back in person in prior years, but the pandemic, it was remote. So for the in-person exam, the exam is closed book and closed notes, but we will allow you to bring forward. She cheats shipping one standard 8.5 by 11 inch paper. You can write on both sides. So you'd have to pay tools sides, and you are allowed to put whatever you wanted to see shapes. So I have friends who would, you know, they would write cheat sheets and they would also like to put my lecture slides on there. And then after they made a cheat sheet, they would scan it at like 60% size and put it into a corner. And then this led them to tissues that are extremely small materials. You can pick whatever you want to print out, retain any questions about any of that until after. So this question is, what will the midterm cover up to? It'll cover up two covers up to material a week from nasa covers up to, and including Wednesday's lecture on February 15th. And therefore the midterm, you'll have a week after that to study further material quite. The question is, will the midterm review session have a Zoom? Often there'll be a Zoom Room and the TAs will record on Zoom how clever they won't be monitoring the chat so they won't answer your question builder. And then the TAs will put effect session. The question is, you need to write matrix could put the brakes on a PC. Now, there is a gradient that would come out of the matrix cookbook. We'll provide that gradient. Other questions. Yes, it's almost that'd be what remained. I'll provide simple, in fact, the gradient of its two norm or the ones that we derived in class by the backprop gradients. Stop pumping through a matrix vector multiply or a matrix matrix multiply. You see if I've got under teaches other questions on admin. The question is, will the exam cover homeworks for Biden? Will cover homework four. We're going to finish the material for homework number four today. Homework number five is on convolutional neural networks, and we have pushed the due date of homework number five past the midterm. So the lecture material up to an including next Wednesday, we'll have some convolutional neural networks, but it won't have all of it. That's pretty hard to do homework, number five, so it'll just be whatever we get up to understand and swipe this one. Are there any questions? Sorry, can you repeat? Your question is, what do you need to be prepared to use knowledge from the homework to solve questions on the midterm? Yes. Will there be any way to tell you? Yes. We may ask you to write code on the exam. It will be handwritten. Yeah. I got it. So Tom ways terrifies. We're not going to be testing on it'll just be on more likely concepts around how you'd like to add something. Other questions. Are you allowed calculators? You're allowed to bring calculators, but we typically design the exam so you never have to use a calculator. Other questions. Alright, I just have one last bullet point, which is a word of thanks. So we all know last factor didn't start off in ordinary way. And while I was up there, I can confess to you that my mind was totally flooded and I wasn't sure exactly what was going on and how to respond. And therefore, I wasn't even process what was said in the audience. And when I went back to look at the video, your support for me. And that meant a lot to me because I think that honestly that's probably what's most effective at helping the pranksters to eventually leave. So I just want to say, Well, where did they so all of you for your support. And it's just really appreciate that. And I think that that was what's effective to meet you all. Alright, we're gonna get back into material. So last lecture, we were continuing to talk about things that are tricks, they should be regularizations, I think helped to increase neural network performance. We talked about this concept of ensembling, which at a high level is to say, instead of training one model to do a task, bot, training them, or some number and average the results together. And the basic intuition is, for a given example, if those ten models make independent errors, if one model gets it wrong, then it's wrong. But if you have ten models and only two or three of them and get them wrong, wrong, but the other seven get it right and you average the results are probably gonna get it right. So as long as the models are making independent errors, if you ensemble them together, you will get better performance. And we should last lecture some tables that talked about it showed that compared tree. So this actually motivates a technique that is very frequently used in deep learning to regularizing network called troponin. So why dropped out? Well, we also talked last time how we could build them ensemble by essentially training ten separate neural networks. But train a neural network to be extraordinarily expensive so you don't want to devote the computational time discontent networks. Dropout is something that will basically allow us to get some of that regularization effects of ensembling. And while we won't cover it as much in this lecture, Ian Goodfellow in the deep learning textbook. Significant space to, to talk to you about how dropped out is approximating, ensembling through bagging. Let's talk about them with a diesel to drop that arm. So what happens in dropout is it starts off with a hyperparameter p. Alright. Let me first give the starting, let's say that we have 100 artificial neurons. What we do in dropped out is you randomly a given neuron and we set it equal to zero, meaning that we chop it out of the network. So if I had 100 neurons are, what I would do is I would draw 100 Bernoulli random variables with probability p. So this probability is essentially going to tell me how severe by drop-down. He told me the probability that I keep a neuron. So what I would do is I would draw 100 of these random variables. And so because Louis, they're going to be zero or one, it's going to be one with probability p, and it's going to be zero. With probability one minus p. I'm going to take, what I'm going to do is I'm going to take this mass m. I'm simply going to multiply the activations H of my 100 neurons by this mask pattern. So that goes around the activity where we drew a zero for that Bernoulli random variable. All right. Typical values of p are 0.8 for input units of the network. And then at t equals 0.5 might be a bit severe. I think in the homework we give you, P equals 0.6 for the hidden units would be that you would keep 60% of your units. And then on a given iteration of training, throw away or set to 0% confirm. The picture that you have in mind is that this is your neural network, shown here. This neural network has 13 artificial units. And what we would do then is for each of these artificial units, we would make a mask. The mask is also 13 dimensional. The mass is going to contain zeros and ones. And so I multiply this mask by my artificial units. And the ones that multiply is zero or effectively dropped out of the network. And so for a particular mask, this might be the network on some training iteration I is instantiated since all of these other units hoping trump card. Questions on the mechanism of what's happening here. The question is, does a massive change with every iteration? Yes. Over the course of training? You'll notice. So first off, if we have capital N units, you'll notice that there are two to the n possible configurations. Since every single unit can be in one of two states, there are dropped down. And so we're going to talk a bit more about why dropout it is a good idea. But at least the first idea that you can see is that if we're changing this mask, every iteration of training, what we want is that this neural network robust and works well in many different configurations to work out well when despite units or dropped out, it has to work out well if you go another subset of units or talk down on the bench for a given me. Any division can be wonderful. That's a good question. So what do we do in homework for author? Always putting out, I'm using the word iteration here and that could be vague because it took me back. They're doing a different mask on every mini-batch or you're doing a different task on everything. I'm going to guess that in homework number four, we tell you to do at every thought, but I'm not sure. Yeah. Yeah. So it's almost surely will say it will typically use the same probability for each layer. So usually you would want the probability to be quite high for the input units, meaning the first layer, because these are the ones that are capturing information about your images. So usually we might the first P for the first side, The Chief of the first layer, something like 0.8. And then these ones, you can be potentially more aggressive. People do hyperparameter. Values between 0.6 is 0.9 are typical. In the teal shirt. First, we define talk in general or the whole network with one exception. You might define it for the implementer separately, but generally it's to the floor. Yeah. Yeah, missing in this example, p is the probability that we keep a neuron. So here, P is the probability we keep a neuron. And we're gonna go with that convention for homework number four. But then the student mentioned that in the applied torques dropout layer can use a public IP dropping neurons. So instead of passing and p, you would ask them one minus p. So, but for the implementation, but we're going to do with p be the probability of getting it wrong. Does this also work? We basically have different mass for every single one. That's right. Yeah. It's asking if instead of doing dropout on the pocket into it on a batch, does that lead to better performance? I'm not sure if the answer doesn't, TAs or anyone else. It's something that we can talk about homework number according to my lab experiments, but I don't know if there's a consensus. One can be approximated by making the line. I see lots of people. Yeah, that's a rocket ship. And Tom y are both essentially stating that it may be too aggressive to change the dropout batch. Let's take these three participant aboard. Great. The question is dropping values, especially normalization. You've done it before or after batch normalization should be done after that. So the question is, when we multiply the activations by this mask, is it before or after? Oh, yeah, just send me the testing phase. You will get to that. You do something else in the testing phase. So we'll have that repeat just like wonderful. The question is, how does this affect the backpropagation? So when you do the backprop, you will need to backprop through multiplication by t. That talking through a simple application should be something you are all comfortable with. So you're not going to talk about the backprop lecture, but you'll implemented on homework number four. Alright, so that's trumped up. This is what dropout, so ******* good. So here I have my affine layer, wx plus b. It goes through Rava and that gives me they didn't do this and they are one. Each one. What I do is I just draw this mask of the same size as H1, where it's going to be one or zero with, it's gonna be one with probability p and zero with probability one minus. Alright, and then after that I just multiply each one by M1, and that will give me the drunk units in there. Alright, so this is a super simple operation to implementing code. It's just an additional two lines of code for each layer. Alright? So that's how the forward pass changes when we implement Dropout in one message. Remember that for each net, right? Yes. Oh, yeah, so and this other student questions, tomboy, staying here, I defined one mask for the entire network and then multiply this mass by all the units. And here I am doing it per layer. They're equivalent as long as the parameter p is the same. But this is how we wrote it out for layer in this example. Alright, so that's struggling code. Now we're gonna get back to this question, which is, What do we do during testing time, right? Because and training time we just kept generating all these random. Nasa gave us different configuration to the network. So what configuration do we use intestine. We're gonna give an intuitive argument for what they should be. And then the argument is that let's say that we had four units, H1, H2, H3, H4. And these are their weights and the rest are just gonna be W1, W2, W3, W4. And this leads to HR. So let's say that for this example, we're going to have a dropout parameter, p equals 0.5, right? P equals 0.5. Then for these four units, Let's say I do training iteration, one. Iteration what I dropped that mask on these four units. So the mask will be for the first four units, H1, H2, H3, H4, and it's gonna be 1010, right? Sorry. This means that HCl is going to equal ReLu of W1 x1 plus W3 X3, because those are the only units true mammal. It's the same for every example that alright, and then let's see, I'm training iteration to redraw a new mass m. And now this mask here is 0101. And so for training iteration to the network was using the second and the fourth unit. So the ReLu of W2 H2 plus W4 H4. Alright? Now let's say that in the test phase, we decided, let's not use any configure any particular configuration. Let's just use all the units. So let's say that in the test phase, or at least to start, we're going to abbreviate this. The test phase. Let's say that we just did h equals ReLu of W1, W2, H2 plus W3 X3. And then WE, for each four. If I were to do this, how would the statistics of H out in the test phase compared to the statistics of HR in different periods. Right? The skill will be different because in the training phase, we throw away half arguments. And so on average, this h out and the training phase will be two times less than if I were to use all of these units together. Alright? So what we then do is for the test phase, equalize the scale of h out. We're going to take all the units together and simply multiply it by the value p. And therefore, the scale of HM, the test phase where I use all of my units will be similar to the scale of HR in my training days. Another way of doing this is that over many iterations or training, the contribution of wi, HI. So the contribution of unit one wasn't always just like HI. W1, x1 only existed in the training iterations. So the contribution that WISMDI to each out was P times wi times HI. Alright. So this rule of scaling these values by P captures the fact that these units only contributed P, percentage, or proportion of the test phase. What we do is we don't use a particular configuration. We use all of the units, but then we scale down how much they contribute to factor p. Any questions? Yes, here is the p is the probability of other questions. Alright? So what that then means is that at least in this current iteration is written. Sorry. There's one more thing to say here, which is this row here of scaling the piece. You can think of as scaling these weights, W1, W2, W3, W4 by the value p. This is something called the wage scale inference rule. And although we haven't very intuitive argument for why it's reasonable, like we just discussed. This is from the good Philip book. There's not get any theoretical argument for the accuracy of this approximate inference rule in deep non-linear neural networks. But this is another one of those things. It seems to be the captain, but we don't have a theoretical reasoning. And then instead of scaling the weights in this class, we're going to scale the activation. That's typically how that dropout rates and productive. So instead of multiplying this P within the ReLu with these grapes, what we actually do in this class is we can feed the baby. When we put the p outside, we multiply the activations h out by a homeless person is it will be after the batch norm? That's correct. The question is, why is it putting it? Why is it that putting it outside the ray moves makes a difference? Because if this number was greater than zero, then the p would come through anyways. Yeah, that's, that's correct for the rating occupation or other activation. Alright. Okay, so because in the test phase, we take RHEL, we multiply it by P. This is what the testing phase looks like. So we have the testing phase where we're going to use all of our units. Do I find later than our ReLu? And then we multiply by p. We do the same thing for layer two, and then we have to type it here. This is W2. So that's this inference rule. Or from now, there's something that we don't like about this, which is we generally don't want to change the text of our neural network class. It would be great if we could just add dropped out. The training and not have to have like a different test versus test underscore dropped out that has this P here. So the way that we can get around this is we use something called inverted dropout, where we kick this p factor that would normally be in the test phase. And we move that key factor into the training phase by dividing each one by p. So p equals 0.5. If I take each one and I divide it by P, Then when p equals 0.5, that would be like multiplying these things by two, right? So now if I multiply these things by two, the scale-up H AB is the same as this h Tau. So inverted dropout says to get rid of the P and the test phase so that I don't have to modify the testing code. I'm going to in the forecast, simply divide my mask or my activation to call it b by P. Then my testing base, I don't have to have an HP and the test case. Any questions there? All right. So that is trumped up. The question is, is dropped out to the inputs the same as profit? It could be, but if you crop an image or do you like black and, or abuse or allow parts of the image? Then for fully connected network, it could be, I think that's an equivalence, but like for a convolutional neural network, that will be too many units in the next layer having a spatial zeros, which wouldn't be the case if you just draw on a random mask so that there are differences? I would talk ever at Foundation. Do you mean does our population change when we incorporated dropout layer? So because basically non computational mask, right, we're going to have an H1 and now it's going to multiply a mass m. And that's gonna give me, I'll call it H1, capital superscript D for dropout. So there's an additional multiplication operation here. And you're going to have to backpropagate through this multiplication operations. Question is yes, that's correct. Other questions? Questions. And so that's the verdict drop that. And this is how you're going to implement drop that on the homework. Alright? So you might have been the question, how is this a good idea? It's really funny because I'm Jeff and his group are the ones who discovered dropped out. And they were presented once in a talk because they said we did this pain. They have been by accident. I did this thing called dropout and they found that lead to increased to performance and 2%. And the audience are like, That doesn't sound right, but it's often a simple enough that the audience, some people tell anecdotes of how they were coding it on the fly. By the end of the talk, they saw a 2% increase also in their data. It's a very simple and quick way to get performance. But the fact that they tested it brings up this question that we all have, which is dropping out. So again, I'll refer you to the Goodfellow textbook for more details. But Goodfellow talks about how approximates bagging, where when we have dropped out, overdoing it. And every single key part, we are using a different network configuration. Alright? Our total task space, use all of the units. It's kinda like adding all those numbers together. And so, because you have many different configurations that must be good at predicting the output and can predict the output when you combine them all together and you're testing phase, you have an effect of ensemble as a result of dropout. Other reasons why he dropped out might be a good idea. You can think of dropout as regularizing each hidden unit to work well in many different contexts. The context being different masks. So a unit has to work well. If it's an active in two consecutive epochs, that would be two consecutive, two different architectures, effective architectures. I'm asking that you didn't have to worry about in both of those contexts. That's what number two. Number three is trumped up may cause units to encode redundant features. So maybe if you want to detect the cat, you need to get features like that. It has 20 years and the network has to learn that there's only one artificial neuron remembers that learned it might not be robust if the activations on that neuron are in any way altered as a result of some other factor in the input. And so when you have dropped out, many neurons have to encode these features, especially if their vital. And so these two redundancy in the network. One more thing to note is that when you run dropped out, typically you have to make them larger. So if you have a network with 100 neurons and it does well with, without Trump got to have good improvement with dropout. And we'd have to increase that to 100 units to something darker. Which also makes intuitive sense because when we dropped out or we're going to be throwing away a bunch of pounds. Any questions on any of these intuitions? Copying off some units. Dropout rates are set to zero. Tom was asking when you do drop that drops out. We all the ways that companies that needed or sexes there are. Tom was asking, does that have an effect of something similar to that L1 regularization? Which bases are always read. The answer is no because if it was always trumped up in gas, but because on some interval it dropped up, those will lead to great effect change but switched to be nonzero. We apply. I really only some delays. You can trump. I've just talked about layers if you want. You certainly see in the homework, we will put Trump on every layer. There are some layers, but he wanted to, particularly for companies and big fire information, regularized some boring display the chalk out. The question is, how would you determine which layer you should not do the drop that arm. Yeah, that's not something easy to know, a priori, which is why people just generally quite tired. Hi, The question is, are redundant features a good or a bad day? Redundant features can have a quote unquote con in that invaded more than expended to do the same computation. But if you can tolerate that, but it's generally a good thing because then you aren't reliant on just a few numbers. It's kind of like the idea about sampling, if you have any that perks that will look for this future appointment. Yeah, so even if one neuron, as is often the others, are there. Is it generally the case that we had better testing versus training accuracy? Let me do drop that. You mean like an absolute value of testing higher than the training node. Generally, the testing error will still be, the testing accuracy will generally be worse than the training still. But it'll reduce the gap. The training error will, the training accuracy will go down and testing accuracy both. So that's the regularizing. The question is is the drop-off I'm asked if he didn't each batch or is it saved over the entire talk? Actually, I'm going to just ask the TAs to look at homework number four. We were saying you bought because we think it might be a bit excessive for each batch. I haven't I haven't written dropout code in awhile. So the TAs will give us a firm answer in a minute. But the question is, if we do it for a whole, you talk with at the poem doesn't kind of definition for it. There'll always be epoch, which is that you pump is just one facet of your chain. I see. Right. Okay. So this to me is asking a question which is beyond the scope of this class. But if you're a deep RL context when you're turning from a replay buffer and you're not going to go over all the samples from the replay buffer, or you're going to sample for every iteration. I don't know what the best heuristic is, but you may want to keep a mask or a few, a few videos to saplings from the takeoff it, that would be my guess. Music, any last questions here on top up? Alright, so that concludes this lecture on importance of various regularizations, augmentations and industrialization is that improve the performance of neural networks. So remember initializations thought here is unfortunate. Regularizations. We have things like dropout, batch norm, and then did augmentations. We've talked about how we can just take different costs of images. And that's a way to get a 2% of Africa is considerable. Okay? So the TAs are correcting that. Crafting what I've been saying incorrectly, this vector, which is that the masks are drawn for bash, not premium fonts. So they are changed every single batch. All right? Okay, so in addition to these tricks for improving the performance of neural networks, you also know that there's one more component of our machine learning problem, which is the optimizer, and that's the cast of gradient descent. And are there particular ways we can improve this for deep learning? I'm going to say one thing at the top also, which is that even though all of these optimizers were developed to deep neural networks, they're also available outside of deep neural networks have just general machine learning algorithms. So that's gonna be the topic of the next set of slides. This is gonna be optimization for neural networks. And we're going to talk about ways that we modify stochastic gradient descent with momentum adapted, brilliant and adaptive moments. And this is something that probably a lot of you have heard of called the Adam optimizer. And so we'll talk about exactly what. We're also going to add these at a high level, introduce you to the concept of what's in order. Gradient descent methods are. Although they are rarely seen in neural networks for reasons that we're going to talk about later on. But if you would like to learn more about these than the optimization classes to 36, BMC will cover them. The associated reading with this topic are components of chapter eight. So what we know, going back a few weeks, we now know how to design neural network architectures for loss functions and the activation functions to choose. We know what the hyperparameters and cost or loss functions for these numbers should be right, for classification would be, we should be using the cross-entropy loss, which is the negative log likelihood of the softmax classifier. We know how to calculate gradients of this loss with respect to all the parameters in the neural network. That was the backpropagation of lecture. And then we talked about how to initialize the weights and regularized and networks and ways to improve the tray. That was the beginning of this lecture. And just like we were saying, now, we know how to optimize these networks with stochastic gradient descent. But can sarcastic gradient descent the improved and because the sector, the answer is yes. So let's get started. So this is the destination will be using for this factor, the cost function. This is just the loss, is what I'd usually written as L, but we will use the variable j as well in this vector. And then our parameters are theta. We know that then stochastic gradient descent or Mini-batch gradient descent, I'm using them interchangeably, is that we update theta by stepping in minus epsilon times the gradient direction. This is a slide just from a prior lecture, which is just recapping the differences in batch, mini-batch and Catholic algorithms. But remember in practice we always use the dash. We call that mini-batch gradient descent, stochastic gradient descent. Alright? So this again is what we know. We'll just do this because when we introduced the other ones, we're going to follow the same structure. So let's say that we have an initial parameter setting theta. We have the minibatch with ME examples, which means that we have, this is C4, ten images, x and m labels. Why do gradient descent? We compute the gradient of the loss function with respect to those parameters. Remember that's the softmax loss and grads function. You didn't hover number two as well as the backpropagation homework number three. And then after we get this gradient g, we update theta by just typing in minus epsilon or gradient direction. We know that the way this works is that we have some function. This function is like softmax, loss and grad, which you implemented in the code that I'm going to show here. These axes are the weights. Softmax, loss and grad is going to return to you the loss function and the gradients and then update the width. I'm just going to subtract off epsilon times, right? We're going to return to this function called Beall's function that we talked about in which the capsule gradient descent lecture. And then we're going to optimize it or we're going to update it with our new optimizers. So here's just a reminder. This is the video of what stochastic gradient descent looks like on this fault surface. So starting from initialization, this bottom right-hand corner, stochastic gradient descent will follow the gradient which is perpendicular to the contour lines and go to the optimum. And here is a video of that and Cassian gradient descent, but starting from another location. And you'll see that it's moving slowly. Let's talk about the first thing that we're going to do, which is momentum. So we talked about how the cost of gradient descent, we have to set the learning rate epsilon. And sometimes when epsilon is a bit on the larger side, we'll run into problems like this green descent curve where we get into a valley, we start to zigzag back and forth, right? So this zigzagging back and forth along this valley that researchers to think of an idea called momentum. And momentum works in the following way. Let's say that we weren't exactly. So we start out at a location Studying of our legs and our first gradient points in this direction, I'll call that G1. Then we get another gradient and that points in this direction, G2. Do a third gradient, G3 like this. And then maybe a G54 like this, right? So our average class grade is to get the overall direction that we're going in and cancel out these zigzags. Alright? How does it do that? It sets a new variable d. This is our momentum. It's initialized to zero. And then we have a hyperparameter called outlet. And it also is essentially going to be how much of a running average to do. This will become more clear to write it out. So let's go ahead and write out this momentum update step. So first we compute the gradient of g, and then what we do is we update this momentum vector. So let's start off at the initialization. We have that's at the very first time step to the substructure will be initialized to zero. Alright? At the second time step, we compute a gradient. And that gradient will be equal to g1. Alright? So that's my gradient at the first time step by dy is going to be alpha times the prior B, which is zero minus epsilon times by gradient, which is G1. So b1 is just going to equal minus epsilon G1. Then after that, we're going to calculate a V2. V2 is going to equal alpha and alpha. Let's say it's gonna be like 0.9 e.g. it's going to be equal to alpha d1 minus epsilon times the gradient at time step two, which is G2. And if I go ahead and expand what D1 is here, I'm gonna get that this is equal to minus alpha, epsilon one minus epsilon g2. Let's just do one more. We'll do v3. V3. I'm gonna get that this is equal to alpha v2 minus epsilon G3. This is equal to minus Epsilon. And I'm gonna do some simplification. This is minus epsilon times alpha squared plus alpha G2 plus g3. So basically what you see is that our momentum is going to be this effective running average of the past gradients. Gradients further in the past are weighted vest because alpha is a number less than one, so alpha squared will be smaller than our drew. The picture to have in mind then is that in our momentum step, what we will have this we're going to have, let's say that we're trying to step into direction before. So we're trying to compute the momentum for V4. V4 is going to have on D4 is going to be equal to minus epsilon alpha cubed plus alpha squared G2 plus alpha G3 plus t4. And what that means is that we're going to have a vector like this. This is alpha cubed G1. Then we're going to have a blue vector. This is going to be alpha squared G2 bit longer. And then we're going to have a third vector, this one here, this is alpha G3. Our fourth vector here, this is G4. And if I add them all together, the overall sum is going to be equal to this vector. Respect to your here is before. Alright? So that's how it works. Basically, we're averaging or has gradients and it gives me the direction in which the gradients don't cancel out. And that's why this is called momentum, because essentially we're capturing the momentum of us pulling down ago. So another picture to have in mind is that if we're zigzagging and gradients in this valley of loss function, the momentum is going to compute this red arrow over here. Alright? Any questions, sir? Alright, let me show you one more quantum number. This is a picture to have in mind with momentum. So momentum we introduced this new variable v. Remember v is the momentum vector and at every single point in time. So if we are at this point in parameter space, compute a gradient. And that gradient with a minus sign minus G is shown here. Ingredients or momentum step computes data updating according to V, where V is alpha v minus epsilon t, right? So V here is our blue vector, Alpha v here is in purple. Our new momentum is going to be alpha v minus epsilon j. So I take this minus epsilon g and just add it to tail with Alpha of p, that gives me this vector here. This is minus Epsilon g. So my overall step, the change in my parameters is going to be somewhere in-between the momentum I was in, the direction of the momentum Io is going in and the new gradient at that point in time. Any questions here? The question is, is this running average computed trust mini batches or inbox minibatches. Other questions? Exactly. Right. Yeah, Jenna is asking to clarify. This means that if we are having six, I can gradients. Momentum has the effect of damping out that zigzagging and just going in the direction where the gradients aren't consistent. That's correct. My time has other benefits we're going to talk about in a few slides. So close to one. Okay, great gadget question is, we are setting alpha equal to 0.99. What's the point of having it at all? Well, through training, we're going to run many, many, many batches, like oftentimes on the order of millions of iterations. And so maybe at the million iterations we still don't want to be influenced by the first one. We only want to be influenced by that collapsed 2000s. And so alpha controls that. So the question is, will momentum help us to get to the optimum faster? Or what was the second inconsiderate? Or is there a local minimum? That's a local minimum. That's a really good question. I'm gonna get to that in just a few slides. So let me write out what momentum looks like. This code I called momentum p because that's what they call an in physics. So let me just actually changed at all to be. So momentum is just an additional line of code to update your momentum vector. Then after you update your momentum vector u, I simply stopped in the direction of the momentum. So let's go ahead and see what that looks like. So We're gonna be looking at the same wall surface now and now we're going to include an optimizer that has momentum and the momentum is important. Alright, so the only difference between good oranges that we added momentum, you can see it does something kind of intuitive, right? So momentum in this loss surface is going to the left, right, that's where the gradients are first calculated. Momentum overseas relative to food because it still has that momentum of stepping down this surface. But then because that Alpha is 0.9 or something less than one, eventually corrects course because now it's getting the momentum of the more recent gradients, which point pop into the right towards that star. Let me show another example. This is momentum from the other initialization. And you can see, in this case, momentum actually goes to a different local optimum because it has so much momentum going up into the right. That's, it kept going in that direction. And by then the gradients, we're pushing it to the land. And so that's why you converge to a different local optima there. Alright, let's go ahead and take a five-minute break. When we come back. We'll talk about why momentum helps and then we'll talk about other, other piece optimizers. Yeah, so for each of these fumble, don't get older. Exactly. Yeah. Just put it off. Yes, yes. Yeah. Here's what's your question. Sorry. Hello. Excellent. Opacity or after you take your overall assessment. All right, everyone, back from the break, any questions on the momentum algorithm? Alright, so later on when we talked about convolutional neural networks, we are going to learn something really interesting, which is that when we talk about CNN, one condition you have competition. Almost all of them. Just use the Casio gradient descent with momentum. All right, why is this the case? Well, their acuity. Questions that I believe that momentum is really good at finding local minima that are good local minima for generalization for performance, right? Why is this? A few questions we have here. One is that momentum optima, that's actually a pretty poorly worded questions. So let me just draw what I actually mean. Let's say that this is our loss surface and these are our weights. And our loss surface looks like this. So when I see this momentum help with local optima, I mean, does momentum help us to avoid being trapped in Babylon? The answer is yes. So if I just do stochastic gradient descent with momentum and I start here, we know that we're going to take steps down this hill until we get here. And then at this point, our gradient disturb. So it's a custom gradient descent. And it's fair if instead I'm doing SGD plus momentum as I'm coming down this hill, right? I am keeping momentum. So when I get to this point, even though the gradient at that point is zero, update if not zero because I have the momentum of my prior vectors, which means that I'm going to keep updating my parameters by increasing W. And if I have enough momentum, I will actually escape this local minima. And therefore all continue to keep going down this hill over here. So if there is a relatively local, a local minima that is somewhat steep, then our momentum can carry us past that local optima, local minima, right? So then this leads to our second question. What kinds of local minima does momentum tend to? Think about that for 10 s and I'll ask someone to answer. Right? Can someone tell me the type of local minima? Really flat, shallow local minima. So basically, momentum is going to find these local minima where it stays flat for a while, because it just stays flat for a while. Then when I get to the local minima, even though I have momentums, right? Because it's shallow and talk for a while, my momentum eventually dies out. And then I said, if you think that about what it means to be a shallow local minimum. So it's shallow, we'll call this a shallow flat local minima. In contrast to, let's say another loss function has a local minimum that looks like this. And so this is a steep local minima, right? What do these mean? So, cheap local minima. Remembering that the x-axis is weight and the y-axis is lost. A steep local minima. It tells me that even if I'm over here, if we go my waist, just do it all. But I lost gonna get a lot of purse, right? So these are poorer local optima because they're more sensitive to the settings of the parameters w. Whereas Uranus, shallow flat local minima. This is saying I could change my waves over this entire range and my boss stay small business, the better local minima to be in, because I can tolerate noise added to achieve good performance. The question is, can you make an argument that deeper local minima is overfitting? I'll rephrase that and say yes in a positive way, which is that in general, shallow minima, better generalization. So it turns out that all the ImageNet winners, even though we're going to talk about RMS, Adam. And those are really by the settings that you want to be working in. A prison, setting up conditions where you have this really big dataset. And you have such a large loss surface because there are a number of parameters are in the millions. And pure basement found that using SGD plus momentum is what is best and is believed because it binds the shallow local minima. Any questions there? The question is, can I explain why the shallow optima to better generalization? So, um, there are more rigorous theoretical arguments for this. There was actually a QC to every side Cambridge. I think he defended his thesis on in 2018, He wrote a paper analyzing the Hessians, these local minima to try to make the argument that they generalize better. So there are theoretical arguments, but I'm just gonna give the intuition that one reason it might generalize better is because in a test set, the test set data is different. The loss function is also a function of your data. If you are in a region that is really shallow and flats, right? That tells me that if my waist we're to change a bit, I saw achieved good performance. So you can think of it as I have some tolerance within this large region. For there could be noise which could, which also is related in a way to noise and the activations. And so because I can tolerate so much changing of my weight and still hadn't been lost. It's more robust. It's a general right to the center. All right, So the question is, do we in general prefer this shallow flat local minima to the steep local minima. And the settings of image classification, which is neural networks. Yes. I see. Yes. Yeah, The question is, in general, again, in the setting of computer vision with neural networks, we prefer to find a shallow flats. The answer is yes. There are sort of two-dimension that there are these other algorithms where when you're looking to call you my search a bit more to your left and right to see if you can continue. At least for the networks that we talked about in class. I haven't seen that applied. But my guess is done in the setting of a flat local minima. Reply staying the same. Saddle points are all like they are in general, more fecund minima. Right? So how does that work over there? So Tom was question, is, does the momentum also help in the case of saddle points? Well, if there is a saddle point that there's still a direction where you could decrease for loss. And so right? Right. I have extra energy, right? Exactly, Yeah, So kind of the same team as we said here. Momentum will keep your gradients alive longer. Or the ketone Barger for longer. And so if you didn't want to get something to saddle point momentum, but it gives you a better shot of bacteria. Remember it stops only when it's in a flat valley where there's no other direction. Alright, so that's momentum. There is another thing called Nesterov momentum. Momentum is very related to momentum, but it's actually a bit better in performance. And in the study of convex optimization is actually better in some senses. Nesterov momentum books, very similar to momentum was the following. The classical momentum was to do that d is alpha v minus epsilon times the gradient at the current parameter setting theta. Okay? The only difference between classical momentum and Nesterov momentum is that instead of evaluating my gradient Theta by current parameter setting, I'm going to update, I'm going to start, I'm going to compute my gradients at the parameter setting plus the moment. Alright? This might be confusing, so I think it's best explained as to why this is a good idea. I picture. So you remember that in momentum, what we do is add our current parameters setting. We compute a gradient that's minus g, and we have our momentum direction. That's absolutely right. And then the momentum, the classical momentum step, can be interpreted as we're stepping into alpha v minus epsilon g. So that means I'm going to step into the direction of my momentum and then I'm going to sit, step in the direction of negative gradient computed from this point. All right? Nesterov momentum says, well, in momentum, the direction that was some good interaction. So why am I calculate the gradients over here? Why don't I take one more step along the right? Why don't I first step lumpy because that note, I know I'm going to stop there. Alpha B. Instead of calculating the gradient here, I calculate the gradients after I've taken my momentum and Nesterov momentum says, instead of calculating my gradient here, because I know alpha V brings me closer to my local minima. Stepping Alpha v right here. And then here I'm going to compute by gradients. And maybe here, instead of my gradient pointing straight to the right, which it wasn't a classical momentum. So my gradient points down to the right. Okay? So that's the intuition for any questions that you guys have had to back substitute. So maybe half the class on a question here. The question is, this could be a disadvantage in the setting where your momentum takes you to a point where the gradient doesn't. Essentially your momentum is, you're further away where you're reading this morning accurate? It couldn't be the case. But if you have a good momentum, momentum is CPU towards the local minimum, well, you should get a more accurate gradient. So in convex optimization, this is provably better. And empirically for neural networks, this is also better. So in the homework number cards you'll see in therapy that problem that tumbled out. I see the question is particularly talking about the example that we gave before. Where in this example the momentum takes us to a different path into this local minima. So that's true for this example. But remember that this is just a 2D loss surface. And in general, for the neural networks are going to be a much more complex surface. And so, and also that as we increase the number of parameters, we have exponentially minimas but poor loss. And so even if for a contrived example or a simple example like this, momentum takes us to a slightly worse local minima then this one over here. And the more complexity and grown-up books. Tom Wisconsin is in Nesterov momentum. The momentum calculation is exactly the same, right? It is exactly the same. The only thing that changes is the gradient instead of being evaluated and my current parameter setting theta is evaluated at the parameter setting, right? First step in my work, I first started my momentum direction. So I first go to this location and then I compute the gradient here. So the only thing that's different is this gradient calculation. The question is, is this moment is Nesterov momentum better than classical momentum in all cases? Though it won't be better in all cases, but it generally performance better. But I'm sure people could find edge cases where it doesn't do as well. Okay, great. The question is, when I say that Nesterov momentum does better, so I need that it converges to a better local minima. Where do I find that it converges faster? When I said better I met that they converge faster. All right, let me show you. Oh sorry. There's, there's one thing here, which is you may look at this expression. And if you're like me, you think I don't want to deal with this, right? Because how do I write this into my stochastic gradient descent algorithm? I just want to be able to take a gradient on my current parameter settings. So the way that we typically implemented is we do a change of variables into this coordinate system. Beta tilda. Beta tilda equals beta plus alpha times momentum. Alright? And then it's fairly straightforward to show that if you do this change of variables, this is what your gradient step turns into. And your gradients doesn't have this change of variables. You're taking the gradient of your data till the space with respect to wait until that. So you don't have to worry about this being where you need a first step in and take ingredient. But the change of variables, this is just a gradient computed out of parameters sudden. Alright, Just in the interest of time, I'm going to skip this. But if you can't figure this out, just come to my office hours and we'll go through it together. So that's how we implement momentum and Nesterov momentum in practice. We do this calculation of the new and the old according to these equations. And then the update is that datanew is our old theta plus d nu plus alpha, or momentum parameter times v minus the old. That's just a change of variables to allow us to compute this easily. So this is the code for Nesterov momentum. We get a gradient. We compute our new momentum p, and then we update the weights according to this equation, which is d plus alpha momentum minus momentum. That's this equation right here. So let me go ahead and show the video of what Nesterov momentum looks like. Nesterov momentum will be in green. In this video, you're gonna see from momentum, momentum, at least for this. Simple example, very similar paths. You can kinda see that Nesterov momentum degree course, course-correct a bit earlier. And that makes sense because remember it's computing the gradient after you take the momentum, sir. So it's getting gradients. After you take the momentum step. Up until about the gradients are pointing towards the right. So that makes sense that the green course corrects the earliest it goes to the right because it's getting bigger. Rod and the first initialization, the second initialization, Nesterov momentum look almost like if you go to the same local minima. All right. That's Nesterov momentum overfitting. Because he upon inspection, yes, and because they haven't maximum interruption. Alright. So that's momentum. Momentum is typically called the moments. We're going to see that when we get to add them, there's gonna be a first moment and second moment and talk about the second moment. The second moment is motivated from this idea of maybe we don't want a constant learning rate. Epsilon. Generally at the start of learning is greater than epsilon is bigger, right? But as we get closer and closer to a local minima, you want to take smaller steps. So maybe we should make epsilon smaller. There's one way that you could do this manually called annealing. Annealing is a scheduler. So what you do is, you said annealing rate, where after 1,000 epochs, e.g. you multiply down epsilon by a factor, maybe by a factor of ten. Or you can do manual and healing. They're basically whenever you have a tax on the loss function and you see a chat to, you, saved your training step. And then what you do is you start a new initialization from those parameters with epsilon scaled down by two are accurate. So these ways of either scheduling or manually making Epsilon smaller or cold and dealing animal, right? And it's kinda like another design choice that we have to make. But there's actually another way to adapt the learning rule through a relatively reasonable intuitions. And so this leads us to the first adaptive gradient method called added grad. This was from John duty in 2011. And the idea is as follows. I am going to change by epsilon in the following ways. Taken on a large step in a certain direction. Then I've probably done quite a bit of optimization. That direction. And that direction I want to make my step size is smaller. All right, So what we're going to do is we're going to scale down the gradients are still dumped. The Epsilon directions that have already taken a large tariff because I've already made progress in that direction. Alright, let's write this out. So let's say that we have unrolled network and it has n parameters. So if we have n parameters or GI, our gradients is an n-dimensional vector. And in this case, I'm going to use the subscripts of g to denote which parameter. So the subscript two adults no time date to know which element effector. So G1 here is the first gradient for the first parameter, G2 subgradient for the second parameter, all the way down to g n for the nth parameter, we're going to have a Hadamard product, G, G, right? All this is, is element-wise multiplication so that G had them are, G is going to equal one squared, J2 squared, all the way down to g n squared. And then an undergrad. What we're going to do is we're going to set a equals zero to start. And then we're going to accumulate these gradient squared. Alright? So if we've stepped, let's say along dimension one with a large gradient, g x1 squared is going to be really big. So one squared is also gonna be really vague. And now when I take my ultimate update on my parameters theta, I'm gonna divide by the square root of a. So in that first dimension vibrator too large because a is being, the first dimension, is going to scale down the step size, that first dimension. So let me write out what this a Hadamard product is really quickly. Just to be clear, this Hadamard product. So epsilon over square root of a plus new. The new is just to avoid division by zero or g. This equals epsilon v1 divided by square root of A1 plus mu. Then we'll have an epsilon G2 divided by the square root of A2 plus knew all the way down to the parameter. Alright? So again, quite large in G1, if one will be paid. Because a one is big and now it's in the denominator here. Then it's going to scale down my future steps in this direction because I'll have a large value here. Any questions there? Alright, let me show a video of how added grad does. So. That was just two lines of code, right? I have a slice to zero. I accumulate that a is equal to itself plus the gradient squared. And then I stepped in this direction of the gradient, where I divide by square root of a plus, plus a small number to avoid division by zero. Alright, so let's do the optimization with added grad, added that is going to be just gonna be here in red. And who, sorry, this is the wrong one. By the way, all of these are linked. And I, The link is in the lecture. So I said if you want to ever revisit these videos, or you could just go to this directory. So read it is going to be added grad. And what you'll see here is that the graph does something reasonable. It gets to the minimum. You'll notice one thing first off into those rights, sorry, I'll be to the left. More so than the other algorithms. Can someone tell me why that is added grad, step up into the left relative to the momentum and the vanilla stochastic gradient descent. All right, Perfect, yeah, so that was exactly right. If we started at this example, sorry, we started this parameter setting. Remember that we have two directions. Let's call this x-direction W1 and this y direction W2. So my first gradient points in this direction. This is the first gradient. So it's component and W2 is this big and it's going to put it in, w1 is much smaller. So my first step a lot more than I do for the left. And then remember what added grad does is this is the gradient for w one and this is grading for W2 because I stepped more in the W2 direction, right? The next iteration, A2 will be bigger than a WaterMe. So I'm going to step left, the W2 direction, I'm going to scale down movement in the upward direction. Which means that on the next iteration, I'm going to step more left than I do. And that's why I added red goes up into the left relative to these other optimizers. So intuitively, it's doing exactly what we told it to you. I want to make one more caveat about these videos. You'll notice that they're like these, these distinct changes in directions. This is because when I generate these videos, I actually only took the location of the parameters every ten iterations. So I'm skipping over like ten steps in between here and here. Ten steps in-between Karen, Karen. And so that's why it looks a bit more character because it sounds like the question is, are there theoretical explanations for why having Brad is better in certain situations or in general? So the scope of this class, but John Dewey developed at a grad in the context of convex optimization. So I'll refer you to his 2011 paper for the theoretical arguments, facts, the context of non-linear neural networks. It's just a good idea in terms of if we believe that intuition that if you've stepped in a direction than I did progress, then I can take smaller steps in that direction. Let me show the other video. So for the other initialization, which is again dump into the left grad, shown here in red. And you can see that I made very good progress toward minimum and, and generally perform better than stochastic gradient descent. All right. Any questions on the cooperation of undergrad? He does anyone see a problem with adding grad? Sir problem? Would that be correct? Yes, there's a problem with that. And that is exactly as you said, which is that added grad. And undergrad is only ever going to increase, right. So because a in the first mentioned equals A1 plus g1 had a marked G1. G1, G1 times itself, right? G12 squared. U1 square root is ever only positive. Because it's only non-negative. It could be zero. So it's gonna be zero or positive number. Because that is the case. A1 will either stay the same or if it only increases. What that means is that our learning rate in a certain direction is only ever, always going to get smaller. This can be bad because remember, our phosphorus is really non-linear. And therefore we might have a larger mountain. The first direction might not have been good stuff. But now, for the rest of the training, where constraints to take small steps in that direction because my initial gradients are so large. So we want to fix this by, instead of making this an accumulation of gradients, making this a running mean of brilliance, where it's running some ingredients only looking at the recent history instead of the entire history. And that algorithm is called RMS problem. So RMS prop is exactly like added grad. Now we're going to do this running sum, where a is gonna be attenuated by beta. Beta is 0-1, so it's like a value like 0.99. Alright? And you can see then in this way, alpha, sorry, alpha a can decrease, right? Because now is multiplied by beta. And so if we're taking small steps along direction one, right, then the direction one a is going to spike, is going to repeatedly get decreased by discovery of Beta and it can go down back to zero, e.g. so that's the RMS prop, very simple modification to add a grad. Let me show the video of what RMS prop looks like. So I believe pharmacy practice this one. So harmless puppies is gonna be in purple and it won't be as clear as this one, but we'll see it more clearly in the next video. And add a grad going the same direction. And then the red added grad takes smaller and smaller steps, go into the local minima. But then the purple RMS prop, which allows the age to decrease, will overtake it. It'll be more clear in the second one. So I take the second one again. Remember, purple is going to be RMS prop and reg is not a grad. Having Brad has a good read. But then you're going to see RMS prop overtaken. Right? And again, that's because that I'll play that one more time. So remember red tick, small entourage, that's everything. Any questions there? Tom Waits says, come we ask, are these level sets handcrafted or is it for a particular loss function? Yeah, So I generated these for something called function. So the OLS function is, is something that is a predefined function. So that's where we take our gradient with respect to. But it's just a simple teaching example, obviously much simpler than the neural network. The question is, can the values of a ever be negative? And the new RMS current formulation answer is no because a is twice as thereof and it's only added. The only Beta is also 0-1. So because g squared is always positive, we're only adding positive numbers. So it can never be negative. I'm sorry, say it again. Right? There's no square root issues because all the values of a argon. The question is, in the visualization that I showed, the student observed that I just read jumps out faster than purple and purple over patient. When I, when I did these, I also varied interests and try to make each one as good as possible, so I didn't have to get a larger learning rates I could further ahead. But, but that's why, that's why it starts with a bit further, right? The question is, to be clear, this is different than learning rescheduling. The answer is yes. So learning rescheduling, you just scheduled epsilon to decrease every number in the box. And that can be done with RMS prop. The question is, can I explain what data does it and how that can make things like negative, you said, right? Yeah. So I just want to be clear. They will never go negative. A will always be non-zero or larger. The intuition is as follows. Let's say that we have a 99. 99 is 99 was our sum of gradients on the timestamp. So here this axis is time. I know I'm overloading and vaccine indices. And this would be if there was no, if there is no RMS prop, it would be like J1 squared. Actually, sorry, No, never mind. What it does is it does a equals. So I say that beta was 0.99. So this is like saying a one-hundred equals 0.99 times 899 plus 0.01 times g hadn't, had them RGS g squared. So basically to our gradients, right? Sorry, to our second moments are A's are still accumulating gradient, right? We're still adding gradient to what are some of the squared gradients are. But let's say that in the first dimension of g, The gradient was equal to zero is the smallest. This beta then will decrease a in that first dimension from, let's say, that first imagined having just equal to ten, that'll be equal to 9.9. And if there are still small gradients and that will continually decrease the sum of the squared gradients and active mentioned until that'll allow stick. Right? Sorry, you were saying this is like talking to how much history of a, how many, sorry, how much history of the squared gradients sum up into a. That's correct. I don't want to put it between both G, so it's non-negative. Get exactly what we want to look at, like how big the gradient direction, okay? But then we undo the square root, will lead you the square root over here. Right? Right, rocks what this is asking, can I write this down? Because I wrote like divide by a vector and then Hadamard adapter. So I'll just refer back to the slide. This term here is equal to this factor here. Other questions. I see. The question is can we said beta equal to zero so that we only care about the green sides of this. You certainly couldn't do that, but I don't think it would perform well. Why? Because because it's gonna be noisy, right? Maybe you took a lot of steps along direction one, right? But then on the 100th iteration in direction one, your gradient is like one times ten to the minus ten, right? So you've took them a lot of more steps. But then the dimension, once you have such a small gradient, you divide by square root of one times ten to the minus ten is gonna be a huge number. Two is seven tickets. Two-step reaction. That's why you want to be accumulated gradients over the course of training rather than just have a single platform. Means now that adding a constant in front of our loss function will make a difference. The question is, because we're taking a Hadamard product of our great dance. Because adding a constant to our loss function to make it behave differently. It doesn't, because if we do plus C, but she's not a function of Theta, then this will equal zero. So the gradient, the gradient from the comfortable. God Daniel is pointing out that because we need to compute and keep this vector a and we needed at every time step, this is going to double the number of parameters and voltage gradient descent. That is correct. The question is, are there any ways to alleviate this? Not that I know of. Alright, so let's move on. That's the RMS prompt. Now, you might, do, you might have an idea which is, what if I combine harm as momentum, right? So I have a slide on this. You can combine arm as momentum. So I'm going to skip this because really our most helpless momentum is what is called Adam. So Adam is the adaptive optimizer. Adam is basically R and S plus momentum with a few wise corrections, we'll talk about what that means. So first it says adaptive moments. What are the moments? There's the first moment. Actually, I'll just do this on the next slide. There's a first moment. And the first moment is momentum. Because the first moment refers to the first one woman at G. So we're just taking values of g and averaging them. That's momentum. The second momentum, sorry, per second moment, refers to g squared. So the gradient normalization g squared is the second moment. And that is like adding grandma or the RMS. So all Adam does is it updates the first moment, which is computing momentum. And then updates the second moment, which is doing you're RMS prop on your gradient squared, the running average of your gradient squared. Then you take a gradient step where you're doing the normalization with the adaptive gradients in the direction of the momentum. So this is different from RMS prop in that, in that farm has taught here, had a G, right? So it's only scaling the epsilon. But now Adam takes a step in the momentum direction. So it's a combination of momentum and it got any questions there. Alright, so this is Adam. Adam has something else called the bias correction. So, Right, right. Tomboy saying y is g squared called the second moment. So Tommy was also saying in statistics, the second moment is the expected value of a variable, a random variable squared. So as G, a random variable here. And the answer is yes, you can be thought of as a random variable because the gradient is Boise and it depends on your particular data. So what we squared, just think of it as a second moment. Okay? The bias correction in Adam is, oh darn, I meant to write this is on the ruler and flies by your Dr. Jonathan. I have a link to just the Coursera video where Andrew talks about what this bias correction does. Essentially, all it does is, well, let's first look at the following. As t goes to infinity teaser number of iterations, right? Beta-1 and beta-2, which are numbers less than one, go to zero, right? So as t goes to infinity, detailed and a tilde equal DNA, so as t goes to infinity, you don't have to worry about despise corruption moments. When t is less than infinity. All these bias correction moments do. All these bias correction equations do, is they give you a more accurate measure of the knee, of the presence of elevated. So when you have build data calculating a mean, it doesn't really make sense because you don't have enough samples. And so the bias correction compensate for that lack of samples and both sensory scale up those values. And please take a look at the docent grew and learned. That hasn't been to the Coursera video. If you want to just watch someone actually do an example with numbers and show how this helps you to estimate the mean better. Absolutely iterations. Okay, so that's Adam. Adam has a few more lines of code, but it's still quite straightforward to implement. Then let me shoot a video of what an atom looks like. This. Alright, so it's really getting credit now, Adam is going to be in pink. And for this video, like Adam harmless copy all the local minima around the same time. In this video over here from the other initialization. We can see here, Adam starts to fit behind RMS prop in undergrad, but then over time, like RMS prop is going to surpass. All right, so that is Adam. And then you will all be implementing this in homework number four. All right. Any questions on that? Okay, well, in our remaining 6 min, then, I just want to talk about an intuition of what's happening order until R. And then I'll point you to a bunch of things if you're interested in learning more, but you won't be tested on. Disorder versus second-order methods. In this class, we're only going to be using for order methods. This is really to tell you that second-order methods exist. So why are they called first and second-order methods? Well, we do gradient descent, right? We're making a linear approximation, which is called a first order approximation because when we write out the Taylor series for only, we're only keeping the gradient term. So for a first-order method, we're approximating the line in red as J of Theta equals j evaluated at my current parameter setting theta t. Theta t is going to be the setting of theta right here. Then plus the first order or linear approximation of the loss function better. So that's just going to be my Taylor series expansion. Theta minus Theta t transpose times the gradient. This red equation corresponds to this red line. I remember that tells me if I change my data, how much I expect my lost it changes. So if I change my theta by this amount, right? That would be saying, so let's say that this is Theta t plus one. T plus one updated via gradient descent. Did at t plus one is going to eat for my data, my prior time step minus epsilon g. Right? Now if I go ahead and plug this into this equation, that will give me that J Theta t plus one equals j at Theta t plus this is going to be theta t plus one. And for that I'm going to plug in theta t minus one t, theta t minus epsilon g minus Beta t transpose gradient. Right? And then if you look at this expression, if theta t's cancel out this thing here, I wrote as g here, but a sick gradient, right? And so this thing simplifies to J of Theta t minus epsilon g transpose G. This term here is bigger than or equal to zero. So it's saying that with my linear approximation, I expect if I step in the direction of the negative gradient to decrease my loss by, by epsilon times the gradient square root essentially. Alright, we know that all in well from first-order gradients. Second-order methods now include also the second derivative, the optimum optimization. So let me first give the intuition of why you would ever want to do this. Let's say that I had to loss surfaces. So my first launch surface looks like this. It's nice and shallow. The second loss surface looked like this. So it's very steep. In these settings. You will want to take different size. So if you were at initial starting point here, right? If I want to get to the minimum in the first setting, I want to take a large step. If you think about why I want to take a large step is because this thing is shallow, right? It has really built curvature. And so because it has low curvature, I can take a relatively large step, whereas when something is steep, I need to take small steps because I'm articulate large step and point to start. Essentially added this local region of the loss. So here I needed to take small steps. What this tells me and what is the intuition of second-order method is. Therefore, is that if I were to make instead of a linear approximation, a quadratic approximation of my loss function. That quadratic approximation, remember the second derivative tells me the curvature. This quadratic approximation will be wider if it's shallow and narrower if it's steep. And what I can do is I can make this quadratic approximation and then the bottom of this parabola. And that'll be my new parameters. So in the case that we had a really flat loss surface by quadratic approximation, which I'll draw in purple at this point might look like this, right? So if I stuck to the bottom of this parabola, I'm going to make a large step. Whereas if I make it quadratic approximation here, my, my parabola might look like this. And so in this case I will take a very small step because my parabola is very narrow. Intuition of what a second-order method is. When we come back to lecture on Monday, next week, well, I finished talking about the second-order methods. 
Hello. As far as follows, homework number four is due this Friday. We announced last time this homework has a lot of coding and it's also optimization at the end. So please be sure to get an early start on it. I will be sending a midterm announcement later on through and learn and Yasser today, please read it carefully because there are a bunch of details that we have to get right with a class this large. And so first off, because we have, we had a fashion that only takes 420, but we have over 370 students enrolled in this class. We are going to ask some students to take the exam at one of the smaller classrooms and voice call, and we'll do that by last name. So we're going to send out an announcement with those details. In that way, students can be more spaced out of this room as well. Some logistics details. That'll all be in more detail in the email once we finalize those locations. Midterm logistics details that you are allowed for two chips, each piece of a 0.5 by 11 inch paper. And you can fill out both sides with whatever you want. And so you have eight total size. The midterm is going to cover material up to and including this Wednesday's lecture. And so it'll cover any food up to including convolutional neural networks, which we're going to start off with today. We're going to get to today. You can find past exams. I'll put it in there. And so we've uploaded all of the exams that we've got very different. But this practice question, you may bring a calculator to the exam even though the define the exam so that it can be done without a calculator. But you can't bring any other type of computing device to the exam. You can do the exam and pen or pencil because the TAs and I are going to be scanning your exams after after we complete them. And then lastly, if you are in the MS combine program, this announcement that was sent out, we'll have more details on this anyway and how we'll distribute the exam and then walks that asks you to upload it. Any questions on homework or midterm logistics. Alright, and then just a reminder that there's going to be a midterm exam review session this week. On Thursday is going to be 6-9 pm on young CS 50. I don't see us 50. The TAs will have a Zoom room open, but they won't be monitoring the chat and that's a recording of it will be posted. And we recommend that you at least attend to or watch that review session. Boxes as we strongly recommend. Alright, any questions? We will get back into material then, our last lecture, we talked about these different optimization techniques that we could use for neural networks. We talked about the idea of adaptive gradients at those two are combined into an optimizer called Adam, which many of you have already heard of. And it's a really great fit. The first-order, the classic gradient descent, that includes both the momentum and the second moments of the adaptive gradients. I wanted to just start at the beginning of the semester asking if there are any clarifications on any concepts from first-order optimization, last factor. Alright, so there we left off by sector we are. Once you give a high-level overview of what second-order optimization is and also talk about why it really isn't using neural networks. And so be recalled that you are finishing on just differentiating first order and second quarter gradient descent methods. In first-order methods, what we do is we make a linear approximation as some parameter setting theta t that's given by this equation, which is just the first-quarter Taylor expansion of the loss J Theta t. And if we were to do is to cast a gradient descent step That's first-order. Then all we do is be updated data by just stepping in minus epsilon times the gradient direction. And if you plug in this update step to estimate the loss of index. Setting up a theta. Your linear approximation is always going to estimate that your loss will decrease by some amount, epsilon times the gradient transpose. Right? And when you take a small step, this approximation is closer. But when you take a large step by step to this blue point right here, let me redraw this loss function. Just take it a bit more clear. Maybe the loss function actually looks like this. If we take a large gradient steps from theta t, theta t plus one, we are going to have expected that our gradients that are lost would have gone from this value here down by this linear approximation. And so this is my epsilon transpose Gy that I expect my loss to decrease by if I make a linear approximation with the first-quarter method. But if we were actually to step two, theta t plus one or loss will actually be higher than, alright? And that's because the linear approximation no longer holds when you take, it holds more poorly when you take a large step size. Any questions here? Alright, so we were saying, Well, isn't there more information that we could use to take a better step? Let us a second-order methods. So the idea with the smallest, we drew these pink curve last time. And we said, if we're in a relatively shallow, reasonable loss, that we want to take larger steps. Whereas if the loss is really steep, like this example over here, we want to take small steps, alright? And we can formalize this by saying, well, to know how shallow or how steep the loss surfaces, I can compute its curvature, the second derivative. And so that's why these are called second-order methods because they make the second derivative. So if the curvature is low, which means my second derivative is small, then we want to take a very large step in gradient descent. And at the second derivative was large, that means I have high curvature. I would want to take small steps. And so the idea behind the second-order method is to start off at your parameter setting theta. Then you make a quadratic approximation. Where again, because the quadratic approximation includes the second derivative, it estimates the curvature at dislocation. And then after you make this quadratic approximation, the way that you stopped is you just stepped to the bottom of the parabola. So if we were at the speed of t, I will make this quadratic approximation turn red. And then my step would be to say I'm going to start to the bottom of the parabola. And that would be how I update from beta T over here to Theta t plus one. T plus one would be over here. So you can see if the curvature is shallow, then my parabola is going to be wider. And if my parabola is wider, when I stepped to the bottom of the parabola, I'm going to take a bigger step. But if I pluck summation here in purple is a very narrow parabola with my curvature. When I stepped to the bottom of the parabola, I'm going to end up taking a very small. That's the intuition of a second-order method. Yeah, perfect. That's a great question. Which is the question is, is there a hyperparameter in the second-order method? There is. Because the hyperparameter for Vicky descent first-order was epsilon. That tells me how big of a struct to take. But in this case, the step size is dictated by the width of the parabola to the bottom. Alright, so that's the intuition of a second-order method. We have here. A derivation will go through the derivation because it will remind you of some things that we've done before. We're going to talk about why we end up not using these methods too much in deep learning. So in going for the second-order optimization, we take this first-order Taylor expansion, which only includes the first-order derivative, and we now make it a sucked into order Taylor expansion. So now we did the second derivative term and the second derivative. So this is the same as from the prior slide. The second derivative term will include this Hessian matrix. This is this matrix that we define what we're talking about gradients. This is the multivariate analogue of the second derivative, right? So if you just take a scalar quantity with respect to a scalar, possibly the derivative twice In order derivative, the multivariate case, there are many dimensions to take best second-order derivative. And so this question is this generalization of the second-order derivative? We usually write it as this notation where we're doing our nabla twice. I need the second derivative. And you'll recall is just matrix where along the diagonals is derivative of loss with respect to Beta one. But the second derivative, the 22 element, would be d squared j theta. With respect to d, beta two squared, et cetera. And then the off-diagonals. I don't have that much straight, so I'm just going to stop sharing. This would be a dy squared J of theta with respect to d theta one, d theta two. And this term here would be d squared J of theta with respect to d theta two, d theta one, and so on and so forth. So this is the Hessian matrix. Again, the multivariate generalisation of the second derivative. So this is the second order Taylor expansion. And so this equation here is this red parabola, my aquatic, my quadratic approximation of the loss function around where my current parameter setting. So if I want to do a step to the bottom of the parabola, what I have to do is I have to take my parabola, set the derivative equal to zero and then solve for theta. Don't want to take our parabola equation right here. We're going to set the derivative equal to zero and then solve for the Theta that minimizes this quadratic approximation. Any questions? Alright, so for this, we're going to use two gradients that you all know already. The first is if we have gradient with respect to Theta, theta transpose G, that's just going to be equal to g, The gradient. And then we also have this one. If we take the gradient with respect to theta of theta transpose h, This is going to be that H plus H transpose Theta. That's the gradient that we had derived from. Bye. Okay. Great. Wonderful. Yeah, so the question is, in this Taylor, Taylor series expansion, we would have a theta minus theta naught times the gradients. And then for the second order term, we would have a theta minus theta naught squared times the second derivative. So where's the squared here? So it actually exist because we have a theta minus theta naught transpose. And then this is the other one, theta minus Theta. But then you have to do the order of operations correctly because they're matrices and vectors. So these two are my theta minus theta dot squared. Sorry, can you repeat the question? Yes. Thank you. So a student is just clarifying this parentheses here. Each of parentheses, beta minus theta naught is not a function. H is multiplying this vector theta minus theta naught. So this is a matrix vector multiplication not assumption page. That's a great point. I've, I've never I've never received that question before, so thanks for clarifying that. Alright. Okay, so let's go ahead and do this gradient. So we have j theta naught. We're going to differentiate this with respect, to, differentiate this function with respect to Theta. This has no theta, so it goes away. This thing here is, I'm just going to call this g for shorthand. This thing is equal to Theta transpose g minus theta naught transpose G. If I expand out this multiplication. So Theta transpose G, I differentiate with respect to Theta, I get HE out. And so that's the G here. This being doesn't have a Theta, so its derivative is around. Last one here is this gradient. And so here, if we differentiate with respect to theta, then we're going to get a one-half parentheses, H plus H transpose instead of minus beta naught. H and H transpose for the hush gender is a symmetric matrix. So H plus H transpose will give me two H. That is going to cancel this one-half h of Theta minus Theta naught. So when we take this gradient and set it equal to zero, we get this equation. And now to solve for what the Theta is that minimize this equation. You get this beta naught minus the inverse Hessian times. Right? So this is essentially the Hutchins in the first-order, right? First order approximation for gradient descent, we would have data is updated as theta minus epsilon times the gradient. And then the second-order, we replaced that epsilon with this inverse hashing. So theta is theta minus H inverse gradient. Question. The question is, can I explain what happens to the second term of j theta, this term? Just want why would I differentiate this? I get just this is not the question. Oh, this term. So this term just goes to zero because this is J evaluated at a particular thetas are up. But I'm taking the gradient with respect to theta because I want to know where I'm going to write. This forms can also be pretty intuitive, right? Remember, we said that what's going to happen is if by curvatures fall or sorry, if I lost surfaces shallow, right? If it's wide, which means my curvature is small, I'm going to take a larger step. Alright? Curvature being small means the second derivative is small, which means a heterogenous small. But here I'm inverting the hashing, my curvatures wall. When I take one over that, I'm going to get a big number and that's going to lead to a large staff data. Whereas if my curvature is high and h is height, one over that is put to get really small number. And this is exactly what we want. It measures the curvature using the Hessian. It takes a step in each direction based on the questions that I kept getting. Rocks. Just question is, is it always going to be guaranteed that the Hessian is not invertible. So no, so the Hutcheon will be estimated from samples. If we have enough samples, in general, it will be invertible. However, it may not be a well conditioned matrix, which means that it might have small singular values in some directions. Those cases, there are ways to condition e.g. by kind of like an aldehyde to make this invertible. So the DLP assume that they even brighter. Right? So, sorry. So the Hessian will not be a 4D tensor because it's still a scalar with respect to a matrix. Oh yeah, you're right, sorry. It's adult. Yeah, you're right. So rock to this, pointing out that this fashion, if theta is r matrix, will be a 4D tensor. Because at first, the first derivative will be a matrix and the second derivative will be the derivative of a matrix with respect to the matrix which would be affording texture. That's correct. Yeah, because if theta where r matrix ingredients would also be matrices, so there's be a 4D tensor type of 2D matrix which will give you a 2D matrix. Sorry. Yeah, Jake is asking you about this step here, which when I take the gradient of if I were to just differentiate, I'm just going to use, I'm going to use x instead of theta. I do x transpose H x equals H plus H transpose. But then in this case, the Hessian is a symmetric matrix because of the way it's structured. So if I differentiate a scalar with respect to theta one, theta two is equal to if I take that loss function and differentiate with respect to theta two and then theta one. So because it's symmetric than these two simplify to equal to h x. And then this two cancels out this one. Great, Yeah. Other questions. Alright, so this is called Newton's method. And this seems like intuitively, it does a wonderful thing and we don't have to do all this optimization of epsilon, although again, Adam, Adam and Adam and RMS possible by our lives there. So the question that you may have is, why don't we always use Newton's method, right? But think about it for 10 s and I want to see if anyone has ideas for why we don't use Newton's method in practice for neural networks. Wonderful said. Calculating the cash-in and computationally inverting it is unbelievably expensive. That's correct. So let's look at this in three points. The first is memory. So it is computationally expensive to store the hash here when we have large networks. So storing the Hessian is expensive. Let's say that we have a neural network with 1 million parameters. We're going to talk today about seeing answer. We're going to see our CNS sometimes have hundreds of millions of parameters. Let's just say we have. N equals ten to the six. So 1 million parameters in our neural network. Let's say that each number was floating point single precision. So four bytes per number. If you compute how much memory it requires to store the gradients, we need to store 1 million numbers each four bytes, and that comes out to 3.8 mb. If you want to store the hessian, you need to store 1 million by million matrix. And you do the math on how much memory that requires. This is 3.6 tb. So first-off sorting and hashing, even a very, very large number is prohibitively expensive. Second, inverting the hashin is expensive. So inverting the hashin, we know that matrix inversion is order n cubed. I said you can imagine this is going to get slower. Let's quickly, much more quickly I certain number of parameters increases, right? And then an empirical result That's relates to all of these is that to estimate the Hessian or approximate it well, typically requires a very large batch. Right? That makes intuitive sense because we have so many numbers did you get an estimate? So we need a lot of examples to be able to estimate those numbers. Well, for all of these reasons. Nazi second-order method to use. Even though that, even though they're very great idea, All right. Any questions here? Right, great. So this question is actually getting us to the next slides, which are supposedly is, what if someone came along a way for us to estimate h minus one in a very short amount of time to be end up using it. And the answer is yes. So the following slides are not tested. This is really just for your information and I want to point you to ECE 36, B, and C If you want to learn about these things. But because it has CNS, so prohibitively, essentially, there are going to be ways that people have developed to be able to do Newton's method called Quasi Newton methods. And so BFGS. Okay, actually that's less than I would've thought. Since all sides never asked me how many have heard of. So BFGS is named after these researchers who found a recursion to approximate the inverse Hessian. And because it's a recursion and it can be written as a bunch of outer products. We actually know how recursion for the inverse Hessian, so we never have to also inverted. And so this is one method that can be used to still do the order dispatch with an estimate of the Hessian from this recursion. Then there are other techniques like limited LBFGS, which, which limits this recursion. And then there are also other methods that you might hear like conjugate gradient methods that tried to do as so-called free optimization. Essentially trying to approximate second order optimization without ever having to compute the Hessian or storage. So those are all beyond the scope of the class, will never be tested on that. But I just want to point them in case you're interested in looking at this further. And again, ECE to 36, the entity will go over these in more detail. Okay, great. On one says it's 236. See, that goes over these B does not. Okay. Any other questions on second-order methods? Great. The question is, for a second-order method, will it be problematic when there is a saddle point? Problematic in the sense that you may not find the direction. It's a second. Yeah, so true exactly at that subtle point. And there's no noise in your estimates, then you would just stop there. Other questions. Alright. So that's the other optimization. Sorry, no team working off medication. There's something called the fundamental problem of deep learning. And we're going to talk about it more and more as we start to talk about practical convolutional neural networks as well as recurrent neural networks. But essentially this fundamental problem of deep learning is that we have stacked many layers for neural network. And we know that eventually there's a loss function. And let's say there are, say, 20 layers, right? We know that we have to backpropagate through these 20 layers. And we have already seen examples, e.g. in the initialization where. Maybe these weight matrices are relatively large. And after you're back propagating across 20 of these weight matrices. If those weight matrices are large, after 20 repeated multiplications, your gradients could explode. Or if the interest rates are small, they could vanish. Alright? If your gradients explode or banish. That means that for your update at the first weight matrix DLD w1, either at they explode, w1 is one step up in some crazy direction, nobody will occur. And if DLD W1 is zero, then it doesn't learn it all because the W1, alright, so the fundamental problem of deep learning is making sure that when you get gradients they haven't exploited or famished. Here's a really straightforward way that we can avoid exploding gradients and it's called gradient clipping. So here's the idea. We do is every iteration of gradient descent, we take the gradient and we look at the norm. The norm if it exceeds some value, some hyper-parameter, I'm going to call flip. Then what we want to do is we're just going to scale down a gradient so that is no longer bigger than, Alright, so basically if our gradient look like this, but clip and the magnitude of this is say 100 But clip was equal to ten. We were to scale down our gradient so that it only looks like, alright, so when we do gradient clipping, all we do is we check to see if gradient is that this magnitude is larger than clipped. If it is we up to, we update the gradients polar. So we take our gradient which is bigger than clipped, we normalize it to be unit vector length, and then we multiply it by. So this will scale down the gradients and practice. This actually helps quite a bit with exploding gradients. All right? Any questions there? Yes. The question is, what's the difference between clipping and reducing the step size? So the clearest way to see the difference is that these gradients, remember our backpropagated. If you reduce the step size, you're just taking a smaller step for every single parameter. But when we backpropagate it, the gradient, if it was clipped over here, will be smaller than what I've got propagates to the prior step. Alright? So instead of having everything would be large and then try to typically small step where there'll be creating a balanced, maybe these didn't explode, and these did explode. If I tip it every step in nothingness. The question is, when we have a vanishing gradient, doesn't that actually mean since DL DW goes to zero that we reached optimum? Not necessarily. So it'll be a local optimum, but it can be a very poor local optimum. And so we'll see sometimes we have vanishing gradients just because of, you know, like a poor initial position like the waste or too small. The question is, is gradient clipping done when we are updating the weight vectors or when we are calculating DL DW. I believe it's typically done when calculating DL DW view. So if you put this gradient, I believe you would use it for backpropagation. Can you check me on that? Just to be sure? Yeah. The question is, is there something like batch norm is something similar for regularizing the gradient norm? Not that I know of. Something like batch norm would help us to get a deeper. So it has non-explicit but some intrinsic effect on the gradient norm. Sorry about that. Great. Thank you. That's because there's plenty out of the city's planning I wrote greater than or equal to. This thing still has a normal credit. Yeah, the question is, can I go over and exploding gradients and what that looks like? Yes, Let me pull up the fire lecture. Actually. I'm going to use this example from when we did initialization. And there was this case where we had a ten layer neural network and initialize the weights to be too large. And so when We initialize the weights to be too large. We saw that the activations exploded. And this is just an example of an exploding gradients because the activations have gone to, let's call this 120 million in value. When I do my backpropagation. Didn't, never mind, I thought I had this slide, but when you do a backpropagation, you're starting off with the softmax scores. And then to backpropagated to the next weight, you need to multiply it by the activations transposed. So there's an example where it then your gradients are gonna be like 100 million. And Mr. Bass arm, I didn't know anything. You're not going to stop and tell her the whole question of the questions. I'll explain the gradients, right? Is that right? Yes, it'll help to some extent because I'm a range of activations, a reasonable range. So to avoid, like the exploding gradient is used to answer your question. I've said it isn't a panacea. So we're going to learn that for networks like we can only train them up to the order 15 layers, maybe 18 layers. Because after that, you shouldn't have that one. But even if they did have cash, they wouldn't shrink. So we'll have to think of other ways to avoid these exploding and vanishing gradients. Alright? So those are challenges to be aware of. Vanishing gradients is just when gradients go to dances around. And they're going to see this as a really problem when we have recurrent neural networks and we're going to actually architectural solutions to address this. Alright? That is it for optimization. Any last questions on order? Adam, Momentum, Nesterov, momentum. Alright, we're gonna get into convolutional neural networks. Convolutional neural networks are kind to this modern revolution and keep learning and they are one of the workhorses of the cell we're going to talk about. Architecture is motivated from how it works, what the architecture is. Never going to do case studies, ImageNet winners. So look at how we set the various hyperparameters of these compositional numbers. So this lecture will correspond to chapter nine and a deep-learning textbook. Alright? Just like we said in lecture one, this is going to be our motivating example where we're looking at this ImageNet competition. And on the y-axis here, we have the top five error rate. Remember that there are 1,000 image classes instead of C4 time where you have ten and you get to take five guesses. And if anyone of those five and you get it correct. And so that's what top five error rate is. And the competition started in 2010 and it was in 2012, where you can see the error drops by a significant margin. And this architecture here, AlexNet, only convolutional neural network in the competition that year and did everything by almost 10%. So from here on out, all of these architectures, VGG and add CF night you will not rest, are all convolutional neural networks that optimize architectures hyperparameters to eventually get the error down in 2015 to 3.57. And this is actually superhuman performance. So a few grad students at Stanford one weekend came in and they tried to do the tasks themselves with taking a guess at the top five labels. And they were at 5%, right? So by 2015 with this resume, it actually achieved superhuman performance on image classification. Alright. We're going to first talk about the inspiration for the architecture of a CNN is also CNNs have been around since the 1990s. So the first number even going to look at is this one in 1998 called Linux from younger. And in addition to this, I forget if I might have cut it for this year, but there's this architecture from a researcher named Fukushima called the NeoChord neutron. This was in the 1980s. And this has many similarities to CNN's as well. So you can even say CAMs really started in the 1980s that this NeoChord controller, alright, so you should look at this picture and not know what is going on at all. We're going to unpack that entirely. But we're first going to talk about where This architecture came from. And then again, we'll talk about exactly what the architecture is. I came from biological inspiration, particularly some of the seminal experiments, experiments done by Hubel and Wiesel in the 1960s, got them the Nobel Prize. Where it looked at how neurons in the visual or animals responding to stimuli like moving borrowers. And Hubel and Wiesel's work, as well as lubricant, other neuroscientists in the 70s like Tony motions, led to several conclusions about the digital system. And these are the three things. The first is bad. B1. B1 just means primary visual cortex is the first part of their cerebral cortex that processes images from the outside world. E1 has a retinotopic map. I'll explain what that is on the next slide. Is that the one has broadly two clusters of cells. One is called a simple cell. And these simple cells can be approximated by a linear model, followed by thresholding operation. And then the other is that D1 is composed of these complex cells that respond to that, that are invariant in the position of the feature. Alright, so these three principles are going to be less R-CNN architecture. Okay, so first off, you want has a retina topic. Now, what does this mean? Basically, it means that if I'm looking at somewhere, right, things that are close to each other in my field of view are also closely represented together in my tray. So this circle is my field of view and so it's breaking up the space I'm booking or into, into these different regions. So 11 is like the students out there, 90 students to my top-left. Maybe one, maybe three. Here is tomboy and one hears rocks it right here at the bottom. Alright? So Conway and rocks ships are sitting next to each other, each other in space. If you were to open up my brand and record from my visual cortex neurons, you look for the neurons that represent tomboy and rupture. They would also be close to each other. They're actually, this is, I should have said 3.4, because the brain also has this property where things in the right visual field or in the left hemisphere of my brain, largely these in the left visual field or in the right hemisphere of my brain. So if Tom way was three interactions with four, they are representing close to each other. Also in 3.4, goals have been drawn represents any questions there on topic now. Alright, so that's, that's another topic. Yeah. Okay. Then they're really simple cells in the visual cortex. And so when you tried to model the activity of these biological neurons, the researchers would do is that they would model them, but basically vocalize linear filters. Basically just have to focus on linear filters here. These linear filters can be or can be implemented by an operation called convolution, which we're going to talk about. But convolution sum is a linear operator and any linear operator can be written also as an affine transformation. So basically the simple cells are modeled by either convolutions are matrix-vector multiplies, whichever one is easier for you to think of now, followed by some thresholding operation. And a thresholding operations says, if you're above that value, report the value. If your report is there. So this is our greatly. Alright. Then CNN's also the visual cortex has these complex cells that encoded in variance. So P2 features. And the way that the early Shannon's tried to model this is through something called a pooling layer, and we'll talk some more detail about that later on. But this pooling layer is supposed to help to represent some of the data. Just tell them variant arises in or is observed in complex cells. Any questions on any of these three principles? The question is, what was the inspiration for using linear layers for this wholesales? So when the researchers recorded the simple cells, they sent me in their experiments, what they would do is they would e.g. take a bar and show it to a cat and move the, move the bar around. And they would report and listening on these neurons that are moved around where simple cells Keynes is, they want to say, Okay, how does this neuron, the cat visual cortex, respond to this bar moving? So if your bar moving is x and the caps neuron is why you need to find some relationship between x and y. And what they found is that many of them could actually just be described by a linear relationship wx followed by a threshold. So I'll write that as thresh WX. And now it looks a lot like bailey W. Great session bear either x, y or the XYZ coordinates of the butter on it. Also. When we talk about how these neurons are representing the bar, this is deeper to the coordinate velocity refers to all different types of kinematic variables are actually also in today's orientation. Sometimes if the bar is tilted by 25 degrees and it leaves, the neuron responds, but it's tilted by -45 degrees, it doesn't respond. Alright? So given the inspiration of this CNN from system, one might ask, how far did this analogy there? So this is from an ancient neuroscience perspective in 2014 from Daniel Simons, who's now a professor at Stanford and gender Carlo at MIT. Where did these really interesting experiments where they showed monkeys natural images recorded from all different parts of the visual stream. So D1 is at primary visual cortex, is also a V2, a secondary visual cortex of before and there are further down the line. And then after that there's a part of the brain called the inferotemporal cortex, or IT, it should be, but you need to know is that for these monkeys, they recorded from areas like V1, V4 and IT. And so they have now the responses of neurons in these areas as the monkey saw these natural images. Then they trained a convolutional neural network to process these images as well. And they found this really interesting results. Which is that if you look at early layers of the convolutional neural network, they resemble B1. And if you look at later layers of a convolutional neural network, they looked pusher than they look closer to inferotemporal cortex. So maybe there is some analogy between the CNS, the visual system, and that early layers. Cnn put more likely to early layers of the visual processing pipeline in the brain and later a bigger area. But these analogies break down when you think about that a bit further. So just like regular neural control, that perhaps they're going to be a significant limitations to biological analogies. So here are some examples. Our brain, the neurons in our brain. They had feedback before me, current circuits, whereas CNN's we're going to see are another type of feed for neural network at layer one, layer two, layer three. There are just differences about the tasks that we do. So our brain is doing a lot more than just the misclassification rate, but it's so much information about an image that a neural network number of arms. And, and want to see that the pooling layer that we add to the CNS is also not really a great way to incorporate and variance. And so putting these limitations enough to hesitate to me to see an edge, e.g. to study exactly the visual processing pipeline. But nevertheless, the CNN square initially inspired from the visual system. Any questions there? Alright, let's go ahead then and motivate why we want to see an n instead of a fully connected neural network which you all have been selected on homework number 3.4. So the motivation we're going to start with is actually one of number of parameters. So if we consider C4 ten images, we know that these are 32 by 32 by three tensors that have 30, 72 values. Alright? And so if I were to have my input image, right, it's this vector that is 30, 72. And then that's going to be fully connected to a hidden layer. I just look at one neuron in the hidden layer. That neuron is going to have 3,072 connections or waves to Mike inputs. Every single neuron therefore has to have 3,072 weeks. You'll notice though that the CFR ten images are really small. So if we're working with an image that's more, battery, the images bigger, like if it's a 200 by 200 image, right? 200, 200 pixels. And then there's an RGB says 200 times, 200 times three, then every single neuron would have 120,000 parameters. So I soon as you get on the order of 1,000 million parameter neural. And so as your inputs grow larger, this network soon gets out of hand in terms of the number of parameters, because there's so many parameters, we'll also know that network is going to pick from to overfit. Any questions? All right. Just taking a look at the time was baffling. Take our five-minute break. When we come back, we'll talk about the CNN architecture question, but it gives a better approximation. That's ability to challenge you to tell us. Yeah. Like okay, now I need to know. What is that? Maybe it's just a very natural one to the power of the test. Remember exactly. I think that's probably fine to go back to the house movie or some kind of proxy for how long. But the other thing to think about what's exactly happening? Yeah. That's good. Yeah, definitely. Definitely. Okay. I will all right. All right. Everyone. I will get back to one of the parties, Earth younger with you. Alright, got a complication operation is done accommodation before. Okay. That's over half the class. The convolution operation is given by this integral in the continuous form, in the discrete-time form is given by this sum. And you may look at this. It's totally unintuitive the first time. Haven't seen it before. You have no idea what's going on in this operation, but it's actually very straightforward. So the convolution operation is, is this operation versus going to do an example. And then after that, you see that example. You'll likely understand how conversation works. I'll be happy to answer any questions. Because many of us are coupled reaction has a very particular meaning. It is this integral or this summation for discrete-time signals. And colloquially we know that the intuition for combinations, right, is that they take a signal, you flip it, and then interacting, and then you do multiplication and so convolution, colloquially called the drag. It turns out that in neural networks, we don't do the flip-flop, we just do the drag. But for those of you who are mathematically inclined and you'll know that it's not actually called carbocation is called cross-correlation. So the convolution code and click operation we're going to do in your own networks is going to be our convolution operation except we aren't going to flip the signal. So this minus sign, plus sign, we're just going to drag and that's called cross correlation. And this operation is not commutative. E.g. we know a couple of weeks you get the cost correlation is not. So that aside, but it's still going to call this operation Convolution because that's what everyone calls this operation. Even though it's suddenly a bit of a misnomer. All right. Okay, so what is convolution? So let's say that we had two signals. In this case, we're going to consider matrices that we want to come. Let's call this signal or this matrix a, and we'll call it this one here. If I wanted to call these two, what I do is I take my signal B and superimpose it on a, starting at the top left corner. So what I'm going to do is I'm going to take this two-by-two matrix B. I'm going to put it over here. So that overlaps with top two-by-two of my matrix. Let me make the slip speed a and B. What I do in combination is ai pointwise multiply all of the overlaps together. So I do one times w plus two times x plus four times y plus five times z. Alright? I just add those numbers together and that's gonna give me a single number. So I'm going to multiply point-wise and then add. And that's going to give me a single number. And this number will be one times w plus two times x plus four times y plus five times z. Any questions on that first part that I did? Okay, Then for cognition or we then do is be tracked. Okay, so we first drag along a row and then after that alone and columns. So I'm going to take this two-by-two matrix, I'm going to drag it over to the right. And so if I drag it went over to the right. Now it is going to be overlapping. This two-by-two matrix. I'm gonna do the same thing. I'm going to point wise, I'm going to element-wise multiply two by W, three by x, y by y six pi z. And that's gonna give me one number. I'm going to add them all together. So together, this would give me another number, which is two w plus three x plus five, y plus 60. Okay? I've been dragging this two-by-two matrix to the right. Hit the end. I can no longer drag it to the right. So now I'm gonna go back to the left-hand side and bring it one down. So now I'm going to move this matrix and put it over here. Okay? Again, do my element-wise multiply and add handout, give me four w plus five x plus seven, y plus z. And then I'm going to drag it over to the right by one. And I'll put it over here. And if I do my operation again, I'll get a five W plus six, x plus eight, y plus nine. So convolution, which is given by the star operator of a matrix a and matrix B, is going to result in this drag operation, where at the end I'm gonna be left with this two-by-two matrix. And these are the values. Of all the numbers up this two-by-two. Any questions? Alright, that's super important to understand. So this is the first time you're seeing convolution. It is really that simple. The clarified even a standard font combination because we have to talk about the theoretical results, but the operation itself, it's simply taking one signal, dragging it over the other. And every single point will apply all the values together and then just sum them. Questions. Alright, so that's the convolution operation. This is just a slide where we just did the prior example. Okay? So there's something else in Coalition, which is that for this class, we'll only be doing what are so-called valid convolution. So what a valid convolution means is that my input signal, this was our matrix a, and then my filter, that's fine. It should be for me to be able to get a value b and a has to perfectly overlap. So e.g. a. Convolution that would not be valid is if I put B over here in this corner. This case, my filter be overlaps with a only in one place. And for convolutional neural networks, we would not do this convolution. Alright? So this essentially tells us that whenever we do a convolution filter being has to basically have a corresponding value and a, for me to do that multiplication bar. And for this reason, whenever we do a so-called valid convolution, my output is always going to be smaller than my input. And the amounts that it will decrease in size by is given by this equation, w minus w f plus one, where W is the width of a. So this here is W, This here is height h. Here is the filter WF, and this is the height of the filter feature. So whenever I do valid combinations, my output is going to be guaranteed to be this size. Any questions there? Because we will be performing other constellation after this one. I was expecting that question. So the question is, this is a bad thing because the output is always going to be shrinking. And so we cascade many combinations. What's eventually our representation of the network go to a negligible size. And the answer to that is that if you just didn't valid convolutions, yes, it would become smaller and smaller and that's bad. But what we're going to do is we're going to intentionally pattern input with zeros that the size could stay similar. What's called zero-padding. We'll see them in just a few slides. The question is, can we do something like decomposition, making which dimensions? You're saying can put a three-by-three after deconvolution is tonight, but I'm not trying to accomplish operation you're referring to. Okay. Yeah. So I'm not sure what operation that would be. The clearest way would be to make your input bigger by zeros. We'll see that in just a few slides. Any questions here? Okay, Then we're gonna get to the convolutional layer. So I'm going to consider that we have to see for ten image, right? So the Sigfox can image is going to be a height 32 with 32.3. And what we're going to do is in our convolutional layer, we're going to have so-called small filters. So these are my parameters. Filter. So I get to learn the values in this column is no filter, right? This is same thing as what we had before, where our filter had these values, w, x, y, z. These are the values that I will get to learn so that these convolution operations be good teachers that can classify, see far well. Okay, So here's the first thing about this filter. Depth of this filter D is always going to match the depth of whatever we're filtering against. Alright. That's just going to simplify our lives so much. What that means is that if this filter Um, this filter will have a depth of three. And then like, let's say the height was a five-by-five, right? So, so be a five by five by three filter first. We did a convolution in 2D. What does it look like in 3D? Well, because we matched the depth of the input and filter. When I take this filter and I start at the top left-hand side, I only do compensation by dragging it to the right. And then after it's done, drank it down, then dragging to the right and the next row. The next row because its depths are matched, I never drag it and as you just mentioned, okay, so what happens is that because the filter and the input, what I will do for my convolutional layer is I will start to filter in the top-left as we have here. Now. This is going to be a five by five by three filter and my input. And just like for the matrix example, we're going to take all of the pixels, are all the values, multiply them together, and then add them up. And that gives me one number, which is this number over here. Okay? Any questions there? Okay? And then what we do with convolution as we start to drag this filter to the right. So I'm dragging, I'm dragging, maybe I end up at this location. Again, I multiply all of the values here together and then I sum them together to get one number. And so at this location, that would give me this number. After that, I drag it I continued to drag it to the right until it's in dislocation. This is like the second to last location, so that would give me number here. There will be one more drag where I drag it to the very edge right here. And that would give me this number over here. After that, always the right, move it down one row. When I move it down one row, it becomes over here. So my filter has been dragged down one row. I do my pointwise and multiplies and then I add them together. Now give me this value over here. Okay? So if I convolve my 3D tensor image with this filter here with matched up. The result is going to be a matrix. The matrix dimensions will be that equation that we saw on the slide is going to be a width of w minus wi plus one and a height of h minus one plus one. The question is, does the output at the same depth is no. So in this case, the color tunnels where this depth of which there are three of them, right? But then my output is just one matrix. So this matrix has used together information across my three color channels. That's really important to realize. Any questions. You happy having to follow the spine. Awesome. Okay, we can move on. This is the output of my input image with one filter. So in a convolutional neural network, what we do is for every single layer, we haven't many filters. I'm going to call this and add filters. Well, and by the way, I forgot to mention something which is our filter, right? It is, in this case five by five by three, right? So that means that the number of parameters and this one filter is going to be five times five times 370, 5 g. So there's 75 numbers in this filter. And that's because if I know that 70% of the entire copper, so the number of parameters in one of these filters that are five-by-five by three will be 75. It's actually going to be one more. So actually I should do this now because we're going to have this later on. There's also going to be plus a bias. So the output, we can also add a plus B. So formally it's actually 76 g. Alright? So what we can do is if I convolve a group feel through my input image, I get this matrix. Now in one convolutional layer, I'm going to have an F filters, right? So this will be, but through one. This is filtered to all the way down to my filter. That means that if I were to convolve each of these filters with the input, I would be left with any of these. So I will have n outputs. And what we then do it a convolutional layer is we take these matrices of which I have enough of them. Just stack them together to make a new 3D texture. Alright, so now this new 3D tensor is going to have a width which is W minus W f plus one, which is h minus h f cross one. And then the depth of it is going to be the number of filters that were in my prior normal birth weight. Right? So then after that, I'm going to pass them through a ReLu. And then this will now be my activations that we previously called H1, right? The activations after the first layer of my neural network. And this will be then what goes to compute h k. So it's gonna be the input for the next layer. And again, it's going to have a width, W minus W f plus one, height H minus H F plus one, and then a depth equal to the number of filters in my player layer. Okay? Any questions that they didn't get it. I understand correctly the question is, do we ever essentially add some type of other operations on top of the convolution. Something more sophisticated than summing all these values and I'm having, I'm sorry, multiplying, doing this. Yeah, So the only nonlinearity comes when we just take all of these activations and we raise the question, if I predict correctly is, why do we always match this with this D, right? And the question is, can we choose not to do this? Yes, you can choose not to do this, but don't make your line a bit harder when it comes to writing code. So the reason that we matched this D to this T is because then the output of this convolution will always be a matrix. And then I know that the depth of my next layer is just going to be however many filters, dynamics. Let's say that this was never going to see this. And Cnn, let's say that this was step three, but I chose my filter to have a depth of warning. Or maybe a depth of two. If it has a depth of two, then what happens is, if you think about this picture, they aren't nationally depth dimension. So I'm also going to have to drag it forward. And now my output won't be a matrix is going to be also a 3D tensor. And then I'm gonna have to concatenate different 3D textures together. So for the sake of simplicity, we matched the depths so that the output of this operation is always a matrix. You could decide not to do it. It'll just be a bit more, right? Great, Yeah, So now another student is saying, in this case, would it make sense to match the depth because that's our RGB values. Yeah, So that's a great point. There's also an intuitive reason for wind CMS deaths, which is for your classifications, you probably care about the color the pixels to get the color the pixels, and you didn't know RG and B. Whereas if you had this adept to, you would only get RNG in one feature and then GMP and the next one. This generalizes to later layers of the neural networks. And so you can think of each of these matrices as a different feature of the inputs. And all of these features are useful and they can contain complimentary information. So by having the next layer have adept and matches and F, We're saying, use all of my teachers together to try to compute the next useful feature. I'm drawing it the same as the previous study. But instead of having, maybe there's all these connections. Now we've got sparser connection. Now. Listen to this. Wonderful. Yes, it's always raising a point, which is that you 15, couple of weeks before he got a couple of issues with linear operation. Every linear operation I can write as w x plus b, which is a fully connected neural network right here. So what is it that this competition buys us? Or how's it different than just building a fully connected neural networks? We'll talk about that in a few slides on my punched out one, which is that there are sparse connections. I'll explain what that means to. There's also parameter sharing. And then three, Tom, I also mentioned the feature activations are going to be much larger. They get to that in a few slides, so don't worry if you didn't follow what I just said. Other questions. The question is, how do we guarantee that these Different filters are picking up different features because I come from different initializations. So on the one hand, we actually can't guarantee that these filters pick up different features. But those different features or what would be helpful for eventually reducing your cost center for Boston guests are built learned and if they're initialized differently than yeah, they will converge to different parameters. Question is, is there a bias term and bulk? Yeah, so you could just imagine that there's a plus b1 that I have drawn here. And oftentimes you can just like, we'll always have this, but it's never drawn B and S. All right? Perfect. Yes. So this student's question is, what if we wanted to combine filters of different sizes? Like maybe this would be a five by five filter and maybe this would be like a seven by seven filter. We can't do that in a single convolutional layer. Because remember I have to concatenate these matrices together, but then they would have different dimensions. So there are a few things here. The first is intuition is right, that different sized filters will check different features. And therefore, it's probably a good idea to have them be different sizes, maybe in different layers. But then the second thing is Google at the same idea. So in 2014 did something called the inception layer, which we'll talk about the Inception where we'll have you do convolutions of different widths and heights, filters in the same layer. And I combine them all together. So we'll talk about that next lecture. We talked about the inception module. The question is, is different than the number of color channels. And if it's a hyperparameter, you can select it, you can make it 100 filters are 1,000 filters essentially. The question is, are the filters themselves also hyperparameters? Hyperparameters, they are parameters, so the values within the filters are learned via stochastic gradient descent. Then the number of filters that we have is a hyperparameter that we choose. One last question here. The question is how they've seen filters, the one-by-one, three-by-three, five-by-five. Most commonly, most of you have not seen. But it is what is going to be something that we're going to do 13 by 35 by five combinations. So I believe that it is the case that you want medium to low frequency features you watch. These are larger filter. And we're going to see that AlexNet starts off with the 11 by 11 comfortable lesions. But then the empirical result after VGG notice that three-by-three exercise. And so I don't have a better answer for you aside from Cherokee works. Okay. So that is the compositional layer of the CNN. So then what happens when we have a multi layers cnn is that we're going to start off with some image. Let's say orange carrots, MYC4 ten image. So it has a depth of three with a W and a height of h. Alright, and then let's say that I make a convolutional layer with 100 filters, right? So then in my next layer, my depth is going to be 100. And there's gonna be a new w prime and a new H prime. This width would just be equal to my original width minus my folks, or width plus one. H prime would be H minus H F plus one. Alright? If I wanted to put another convolutional filter, let's say I would do a layer that has, I'm going to call this ten filters. So each of these ten filters would have some width and height, but their depth would be 100. Because these filters have to have the same as what's going into that there. Alright? So then that would give you a new 3D tensor with a new width, height. And you can repeat this. However many layers of a convolutional neural network you want. Any questions here? Alright, so that's the basic convolutional layer. Let's get to this question now that Tom white hat, which is why is it that there's maybe a good idea, especially as it relates to the motivations we did before. So let's look at a fully connected layer. And here, instead of drawing, flipped our calculators by 90 degrees. So this horizontal rows is there. One in this row here is layer two. So we know that a fully connected layer, we have a weight matrix w. One right there, That's H1, all these flight activations to a12, all these flight activations. Alright, if you look at what a neural network is, a convolutional neural network is doing. What it's saying is in a convolutional neural network. If I were to look at the activations of a single artificial neuron, a convolutional neural network. This neuron is only computed from the values where the filter overlaps the input. So this neuron only really has connections to these orange values in the input. Alright? So I think that'd be set, this sculpture was five by five by three, right? And that's 75. So this neuron is only computed from 75 pounds. This neuron here is computed from a different $75. And so instead of this neuron being connected to every single value and my input images so I can click it to us, but to a sports amounts of inputs. And so that's what we mean by sparse connectivity. If we're doing a convolution across just three units, then if I look at the value of this neuron and layer two is only informed by a subset of the units. All right? The first going from fully connected to CNN layers be replaced for connectivity with sparse connectivity. Because every neuron value is only determined by a small subset of the important. The other thing to note is if we looked at this issue three. So let's say that here we had a convolution over just three neurons in the prior layer. All right, so let's say that both compositions have waged a, B, and C. The value of this wire is, the value of this wire is B, the value of this wire is C. So another thing that convolutional neural networks do is that they share parameters. Meaning of that. Because convolution is this operation where I drag this filter to the right. This blue filter is the same blue filter irrespective of where it is. If its values are 123456 is gonna be 123456 here. I'm just gonna be 123456 over here, right? So what that means is that whenever I look at this, if I were to look at what the wastewater for H2 for, right? This is a same convolutional filter. Now drag it over by one. And its values are also, let me give this in a different color. This value is a, this value is b, and this value is c. Alright? And then I'll do one more. If I drag it over one more, this value here is a. Specialty care is B. So convolutional layers as forest connectivity. They also share parameters. Okay. Any questions on either of those? Wonderful. Tommy's question is, is it true that I can take any convolutional neural network and rewrite it as a fully connected neural network layer. Yeah, ready as a neural network, the fully connected network with only particular nodes feeding into the next one. The answer is yes. Remember, you want to all get accomplished on them. And you're there for a bunch of have to implement this operation of convolution and do that with, I believe, a quadruple for loop, which is going to be really slow. And so it's gonna be so slow that it will be something that we need to vectorize. But in homework number five, we've vectorized. So the way that we do it is by writing it as a fully connected layer, but it's implementing convolution. Okay? So force connections reduce computational memory. By this, we simply mean you have far fewer parameters. So in this example, each of these wires is a different value that we have to store. Versus in this example we just have three numbers, a, b, and c, and that specifies my entire convolution operation. Then this also says sparse interactions reduce computation time. What that means is that if in later one I have n neurons, and in layer two I had N neurons. Then to compute layer to you from layer one, I would need to do order of m times panel operations. In this case, we may have m neurons in layer one. But remember in layer two, every neuron only has connections based on the size of the convolution. So let's call that K. So here, the number of connections k is equal to three. So the order of the time it takes to compute will be order n times k. Instead of order n time. Alright, so convolutional layers have sparse interactions and they reduced the number of parameters. One thing that you may be concerned is that because of the sparks interactions, convolutional layers may be limited in their computation in the following way. Let me just go through this image. Here. The value of this neuron is based off of things in the top left-hand side of the image. And the value of this neuron is going to be based off of when I finally had to drag this filter all the way to the bottom right. Because they're spatially vocal. This neuron only talks to the bottom right, and this neuron only talks to the bottom, to the top left. You may think this is really bad because there's no neuron that looks like the entire image. If I'm doing a self-driving car, I need to use things from all parts of the images. I need to know if there is a pedestrian crossing the street. And if I'm at a red light, which in the top-left, alright? So there is no neuron here that we see both the bottom right and the top left. So how do convolutional neural networks to deal with this potential limitation? What we do is we're going to stack many convolutional layers together. So now, if I look at one neuron, right? Let's call it this neuron H3.3 here, because my convolution only had three values. It only sees three layers, sorry, three artificial neurons in layer two. But my original images layer one. And so even though my convolution only has three values, if I look at my input x, this neuron is going to see five pixels that my initial input image. And then you can imagine if you start from any of you sit together, if I stack another layer, then a neuron in this layer would have seen inputs are seven. To get over this problem where you only see things vocally and the prior layer, we're going to stack many layers together. You can see them. H3.3 will see all of these input values. Any questions there? Not because they use proper thing to get these spaces on the right. So rocks, this question is, I can rephrase it in a different way, which I think the answer is yes. But the fact that convolutional neural networks work tells us that the force interactions are a way you, with sparse interactions you have in front of ways so much information they can no longer do that task. Then Roxanne is saying, is there a connection dropped out because he dropped out, we rapidly throwaway connections and still do you think I would say that CNN's tell us that the sparse connections are sufficient to do the task. And so for that reason I might take that adverse want drunk, I was being uncertainties. Alright. Let's go ahead and just plug in some numbers to make sure that you understand what's going on for the convolutional neural network. So we're going to consider an input that is 32 by 32 by three, that's c bar tab. And we're going to consider two architectures. The first is a fully-connected network with five hundreds neurons, and the first layer. The second is a convolutional neural nets. Were there for filters that are four by four by three. Alright? Okay, so the first question is, how many output neurons are there? And the convolutional neural network assuming only valid combinations. That's what this whole firearms. When I say an output neuron of a convolutional neural network. I'm calling each of these values here. And output neuron, right? It'll be out the neuron that is sparsely connected to a local patch can be input, right? So whenever we asked you that question, or whenever someone asks you the question, how many output neurons are there in a convolutional neural network? Essentially we're saying how big is the out, put it back convolutional layer, right? So let's go ahead and answer that first question. One, we have an input which is 32 by 32 by three. And then we're going to convolve it with filters that are four by, four by three dots from over here. And then I have four of these. I want to know the number of routes of neurons. I'm going to first see what is the output size of convolving one of these filters with my input. Remember that the depth d is always matched. And so the output would be a matrix where the dimensions of the matrix are going to be w minus w f plus one by h minus h f plus one. So in this case, at 32 minus four plus one is 29. And so the output of one filter is gonna be a 29 by 29 nature subclinical one filter. But then remember, I have four of these filters, right? Because I have four of these filters, output is going to concatenate four of these 29 by 29 matrices. So the number of total output neurons that I will have is going to be 29 times 29 times four. So this is the number of neurons in layer one. Any questions there? It just gets us something that Tom I was talking about. Earlier on. We said that the CNNs will have a lot more output. You can see that planning to be true. So if I had a fully connected layer with 500 neurons, right? Then the dimension of H1 is equal to 500 for my fully-connected layers because I just have 500 neurons. But the number of protons here is 29 times 29 times four, and that number is much bigger than five. So there are many more activations in a convolutional neural network. Right? Any questions there? The question is, if we're starting with four filters. Oh, sorry. Yes, because basically the four-fifths, so each convolution is gonna give me one of these matrices. And then because I have four filters, I'm going to have four of these matrices. So the number of activations are output neurons will be 29 by 29 and then tax works. I have four of them. Right? Other questions. Okay, Let's do the second calculation, which is how many parameters are in each model. So for the fully connected architecture, we have 500 output neurons. Each of these neurons will have a weight to the 3,072 elements here. So the number of weights are going to be 32 by 32 by three. I'm going to do a plus one for my bias. That's the parameter. Every single one of my 500 neurons has this many connections to the input. So this has to multiply 500 to get my total number of parameters. And here we see that this number is 1.5 million params University for 1,000 neurons in person and they already have over 1 million parameters. It might convolutional neural network, the number of parameters or the number of the number of parameters in one filter. So the number of parameters in one filter will be four times four times three. And I'm going to do plus one for the bias times the number of filters that I have. The number of filters I have are for. This equals 196 g. So as far fewer than diploid can't demand even if it wasn't, you know, even if it wasn't for filters, if it was 400, right, it'd be 19,000 parameters for hundreds of water filters. And this number 19,600 is still much less than our loan 0.5 million parameters, right? So you can see here plainly that the CNS or less parameters and boys and girls. In fact, when we talk about an architecture called VGG net, which is a very deep 60 to 19 layers convolutional neural network. It's going to have, I think on the order of it's going to have over 100 million parameters. I believe something like 70 or 80 million of those are coming from fully connected layers and the rest of the population. So those bars, neural networks that we've talked about, most of the parameters come from the fully connected layers. The question is R-CNN is much more parameter efficient in terms of their performance? I believe by that, you're asking me higher accuracy with far fewer parameters and the answer is actually wonderful. Yes, it's always pointing out the following, which is that for the fully connected layer, we only have to have 500 numbers to describe each one. But for the CNN, we need 29 times, 29 times four numbers. So don't have any more activations to store and Ms. Cnn. And doesn't that take up more memory? The answer is yes, best one quantifies CNN. So VGG net, which I'll just bring up again because that's going to be a huge neural. Now, we're going to see that I really do need. Then you just put their compensation costs many GPUs because it's easy to use if we actually don't have the memory to hold all of the activations required for you took their raiders will actually go ahead and compute how much memory is needed to do a forward and backward pass and it's pretty large. Great. Now the truth, is there only one bias per filter tensor, or is it one bys PR depth and it's the former is one bias. For this tensor. This tensor will have flux of B1 plus B2 to devices just one scalar tensor. Let's go back difference. Yeah, The question is the bicep just gets added to this foundation? That's correct. The question is both a larger filter, increase the memory consumption. Considerably. Larger filter will result in activation, because a larger filter will, since the output activation is W minus Ws plus one, right? This number will be smaller if WWF disorder. So actually to help with the memory is starting the activity. The question is, if you wanted to have it out to do the matrix multiplication, do you mean by rewriting this convolution as a matrix? If you do that, there'll be fewer zeros in that matrix representation of it. But you would still, the major membrane concern is for storing the activations. Alright, we're gonna move on for now. So those are just values in the convolutional layer. And this is actually a slide that we've already talked about, but it's just a reminder all accomplishes in this class, there's going to be valid convolution, convolution to CNN. So if you're coming from signal processing, you don't do valid convolution. So usually, usually your inputs are not always over, the filter is not always overlap. So that's why I say this. So people coming from signal processing may not have that be the first thing they encounter. Okay? So there is a student question earlier that said, ya plus size is always decreasing. How do we deal with this? What we'll do is we'll take our input and will pad with zeros. And this will prevent the size of the noise decreasing. So if I have a three-by-three input and I do cat equals one. N equals one means put a one layer of zeros around the entire input. Cat equals two is two layers of Sarah. So here, this new width would be equal to w prime, and that would be the width of my original filter if this is WF plus two times the value of pad, in this case equals one. So now the width went from three-by-three to five-by-five, went 3-5. Okay? So the output of the valid convolution will now be w minus w f plus one plus two times pad. So if hat equals one, which means I just put one ring of zeros around my input. And I said WWF equals to three. Then I'm going to tap at the output width. W prime is gonna be my input width minus w f of my filter, which is three plus one plus two times tag, which is one. This is equals to W. I said you pad and one, and you made your filter size three, your inputs never decrease in size. Any questions there? Yeah, The question is, do you use padding so you can keep multiple filter outputs the same size? Essentially, yes, and to also stop beside this from decreasing. So a student here, I mentioned that we usually do one-by-one and three-by-three or five-by-five convolutions and are able to make the five-by-five convolutions have distinct emphasizes that three-by-three for the three-by-three, pad by one. And for the five-by-five we pipeline. The question is since there are so many zeros at the edges would rescale the activations. We wouldn't be zeros, wouldn't make it smaller. But then if it was helpful to scale them up and hoping the optimization with darker. And then this relates to Goodfellow who says empirically, the amount of optimal padding is somewhere between type equals zero and the value that causes B inserting the African person with padding. Another hyperparameter we haven't or convolutional neural about just called stride. So we've talked about how in convolution we drag things, right? And we've been dragging them. One by one by one. Stride allows us to drag by more than one. So here, if I have my filter, which is this three-by-three red thing here. If I do strive to two, then when I drag it over, I drag it over by two indices, not worn it. Alright, and then it happens everywhere. So after I reached the right end, when I drag it down, I would actually drag it down by two here sponsor. So here, when I drag it down, it would come over here. Alright, so let's try just is a hyperparameter that allows me to do more, to drag it more indices, bunch of Swarm. When we set this tried, this triad has to always be talking about compensation. So if I were to stride the value of three, this would not work because if I were to stride with a value of three, when I get to the right by three, the first one would be fine. But now if I drag this over to the right by three again, it would end up over here. And this is not a valid convolution. So that would have worked out the chest since you guys now is compilations. And then if we just look at this stride equals two example, we can see that if I were to take, if composition, I would end up with an output that is a three-by-three matrix, right? Because I can drag this book by stride 23 times along one row. And then going along the columns, I can only attracted to that three times also. So when you have stride, the output of your combo with snow layer is now going to be given by this matrix here. Basically the width gets divided by the stride and I guess it's bounded by this trap. Any questions here? Right? Yeah, great. The question is, maybe you want to choose a stride equal to the size of your filter so that these work, the outputs one had any overlapping. But is there a use case for that? Yes, there is actually one use case for that will be performing layer, the pooling layer. This is what we talked about before, will model the complex cells that give you some position on variance. So what we do with a pooling layer is we define a filter size. This case is two-by-two. Then we define an operation like maps or average. Alright, so this is a two-by-two max pool. Then what we do is we start at the top left and we looked at two-by-two matrices. And then because the operation is Max, I'm going to take the max of these, which is six. And then usually when you do a max pool or an average pooling, your stride is going to equal your filter width so that you're never overlapping. So when I strived by two, I drag this red two-by-two matrix over by two and it overlaps 3478. That's the screen matrix. The max value here is eight. And then I stride down by two. That will give me this section over here. So this is the output of a pooling layer and typically they're destroying them. Okay? Any questions? If you see average pool, right? And sort of facts is just taking the average of these values. So if you average pool these, these will be one plus two plus five plus 6/4. So the output of a pooling layer has these dimensions. And then because you're pooling layer is essentially reducing this two-by-two matrix until the one number. You can see why this incorporates some position in variance. Because if things shift slightly, the maximum within your filter might still be the same. So we've done that with an example here. Remember the pooling layers introducing conditional on, very excellent. So what I've done here is I'm drawing the output of a max pool over three inputs. So this is layer one on the bottom layer to talk. And every single input here, for every single output, I look at the three input values and I take the max over. All right, so when I did this pooling layer, the purpose of this example is to see that what I could do is I can take layer one and shift everything over by one. So the 0.2 concepts here, here, 1.5 goes here, et cetera. But the max pooling operation, even though the layer one has shifted so that every single value is different. In my output layer, I still have some overlap. 1.5 and 1.5 here are still in the same position. Alright? So this gives some positional and variance or robustness to another, do an excellent job of it. But obviously, any questions on this pooling layer, customers, but his position on brand image is a silicone. It took effect even if it's 6/5 years. So that's what it corresponds to. The lecture. See you all on Wednesday. 
All right, everyone ready to get started? I'm sorry that this screen isn't working. If you have trouble saying, I asked you to move to the center of it. Alright. First reminder, homework number four is due this Friday. I'll put that degrade scope. We sent out a midterm and announcements on Monday and please read it carefully for details on the midterm. And if you have any follow-up questions, please post them on Piazza. We've announced some of the other midterm details, but the new one from the email is to make space. We reserved three other rooms and rice Hall for students also take the exams there, and we're doing that based off of last name. And so you have a last name beginning with KRL them last elliptic examine voice 1234, etc. In these rooms there are about 56 per room, and they look like this. So the C-star tiny bit more spaced out than in this room. But we think that if we flip the students, several of you in these rooms and that'll make more room here. Alright, so please follow those assignments. Any questions on any image from logistics? Alright. The midterm exam review session is tomorrow in young CS 50. And the TAs are going to upload the review problems by 09:00 P.M. tonight in case you wanted to take a look at them ahead of the review session. And again, we encouraged you and strongly recommended you attended the session or watch it later on. Friendly reminder that this Monday is the President's Day holiday. It's a holiday for UCLA and therefore there's no bacteria or office hours on Monday. And then my Wednesday office hours after class on Wednesday when we have the midterm exam, they're going to be canceled because the TAs and I have to scan all of the exams after the mid-term on Wednesday. All right. Okay. Any questions? All right, let's get back into material bank. Alright, so last lecture we introduced the architecture of the columns shelf drill number. We're going to just recap what that architecture is. And then today we're going to essentially into a case study of the really influential convolutional neural networks that have led to this deep learning revolution. So remember, what happens for a convolutional layer is we have some input to a layer. That input is going to be a 3D tensor with some width, height, and depth. And what we do is a convolutional layer is going to comprise some of mounted filters. Filters for each filter is going to have some width and height, and then it's going to have a depth that's matched to the depth of the input. Alright? And when I do the convolution of one of these filters with Mike input, I get out a matrix. So if I had NF filters, I'm going to have an F matrices. And matrices will be the convolution of each of these filters with the input. What I then do an accomplishment layer, if I take these NF matrices and I stack them together to make a new 3D tensor. So let's say that I started off with images from CFR that were 32 by 32 by three. If I were to convolve with 100 of these filters, I would have 100 of these matrices. Can I stack them together? And that becomes my new 3D tensor. That is by activations for the next layer. Alright, and then the next player might convolve with some other number of filters and that would give me a new 3D tensor. That's okay. The question is, can you think of this as a fully connected layer on the channel? The channel double, meaning that, right? Yes, because I think I understand the sentiment of your customers. You said you're always combining all of the datasets for the local patch state you're in. So you can think of it. Great. The question is, where is the plus one from when we did the parameter activation is coming from. So basically every single one of these filters, if this was say, a five by five by three filter. First one here. The number of wastes would be five times five times three, the volume of that tensor. But then we have a plus one. Because each of these filters also includes a bias term that's just added. So that's where that plus one comes from. Other questions. Okay. So that's the basic convolutional layer. We're going to do several case studies. They're going to become more familiar with this. As we look at these examples. I'm sure we talked about how all of the convolutions are so-called valid combinations, which means that we only do a convolution with a filter fully overlaps the input, right? So that would be at these four locations given by red, green, blue, and purple here. This means that if you were to compute the output size based off of input having a width w and height h, and the filter having a width WWF H, F. The output will always have a width w minus wi plus one, and a height h minus hf plus one. So as long as the filter is bigger than one, your output is gonna get smaller and smaller. So we then also introduced last lecture this idea of padding. So sometimes to keep the output the same size as the input, we'll go ahead and pad with zeros. And if you had with hat equals one, this means including one surrounding zeros and foot. I'll put it the complication that becomes these dimensions. Alright? And then we also talked about strive last factor, which means we do the drag of the convolutional filter. Stride equals one, which is the default that you're dragging this filter over by one all the time. But if you wanted to track it over by two, then you would set a stride equal to two. Now you have tried that will reduce the output size. All right. Any questions on padding or stride? Yes, that's correct. So the question is, when you tried by two, you're moving over by two. When you go to the right. But then when you go down, you also go down by two. So when you strike by two, you strike by two also in the vertical direction. Question is, is there ever a case where the stride size is bigger than the filter size? I've never seen that. Because then you wouldn't be missing entire sections of your input. Question is, how do you know whether the stride is valid? So basically, I realized I didn't have this slide here. Let me just from our last lecture. And this is the output width and height for a value of stride. And to make sure the stride is valid, this number should evaluate to an integer, not a decimal. Other questions. Alright? And then we talked about the pooling layer, which applies in operations such as if you have a max pooling layer, it'll look for the maximum value in your pulling size, in this case two-by-two. So something a two-by-two matrices. And it just extract some Epsilon. So here the maximum is fixed. Here, the maximum is eight, et cetera. So that's a point here. And this is the layer that is designed to try to introduce some positional invariant into your CNN. Any questions on the pooling layer? Yeah. Great. The question is, is there a reason the operation we do is max rather than me? So you, you can also do the mean. So that's usually called the average pooling layer. And we're going to see some of the neural numbers that we talked about today do an average pooling better than a Mac scoring? I believe that's pointing is usually chosen for empirical results, which that's a better performance. But you can do average pooling as well. Great. Yeah. The question is, to clarify, pulling is done channel wise. And that's correct. So when the input is a 3D tensor, so if you imagine that this input was a 3D tensor, you would do your pulling on every single matrix within that cancer. And then the outputs will be stacked. So you would do pulling on each individual matrix and then stack them to return. Also a 3D tensor. Right? Gentle thinks there's a spatial down sample and not the channel gram sample. That's correct. Their cousins here. Alright, let us spend debt into Linux. This will be the first. This is the convolutional neural network from me on LinkedIn and colleagues in 1998. It's the simplest one. And then after that we're going to go into. Alright, so this is the architecture of the input is going to be. So they use a dataset called or they used to. They wanted to classify digits. And instead of having these be RGB images, they were grayscale images. So these are just 32 by 32 matrices or not. Alright? So what they do is in the first layer, what they have is they have six, five-by-five convolutional filters. Okay? So let's go ahead and write this out. And these filters are applied at stride equals one and Patty equals zero. So the first question that we'll ask is, what is the size of the feature maps? That means the output of the convolution layer C1. C1 contains these 65 by foreign competition of filters. So what I'm asking you this essentially, when I apply six, five-by-five convolutions not alters. What's the size of the output? So for this, we're just going to apply the formulas that we talked about before. So when I do the convolution, the width and the height are going to change by w minus w plus one. So in this case, my initial image head was headed with it 32 and a height of 32. The thoughts or width is five, and then I add one. So the output of the convolutions are going to be 28 by 28 in width and height. And I can someone tell me what the doctors did. You just set it up. Perfect. Yeah. So I have six of these. So in total, the size of the feature maps are 28 by 28 by six. So that's what's drawn here. These are the six feature maps, each of them 28 by 28. How many parameters are there in one layer? So here we have six five-by-five convolutional filters. And so each five-by-five filter, It's going to have five times five equals 25 parameters, right? And then remember there's that plus one for the bias. So we're going to do it plus one for the bias. So this is the number of parameters in just one folder. And I have six of them. So in the first one layer, I have a total of 156 parameters. He does anyone want me to do that calculation again? Alright, great. We asked them signs up. Okay. Okay. Yeah, yes. The tongue way of saying if he asked for feature maps, the feature maps refer to just want to be. So let me just say size of, let me change these off the size of output, just to be clear, of C1. And then this will also be size of output. Size of output. Right? So the next after that, we're going to apply a pooling layer where the pool, the pools are two-by-two and they're applied at a stride of two. So we know that if you do this two-by-two pooling layer, where the filters are two-by-two hundred provided a stride of two. And I don't have the equation here, hasn't in the prior lecture. The output of the pooling layer will be w minus four, which is two, divided by the stride, which is two plus one. So we'll have a Tony eight minus two, which is 26/2, that's 13 plus one, that's 14. So the output of this pooling layer is going to be 14 by 14. And then just like Daniel is asking for the fight on each channel in the deaf separately. So if I had applied, I applied on this first matrix that gives me this. Then I play on the second matrix that gives me this one. The third matrix that gives me just wanted to say that this will be 14 by 14 bytes, six. Alright, and then how many parameters are there in the pulling? Someone who just have the answer? Yeah. So remember for the pooling layer, all we're doing is we're looking at some values and then taking their maximum or take their average or some other operation. There are no trainable parameters there. Alright? Saying for the point operation, is there a ceiling or floor? What if it's not an integer? Oh, you mean this equation? So again, just like for the convolutions, how the streets had to be valid. The stride will have to be valid for the full officer. So you need to choose a stride such that w minus w p over stride is. Before we do cited the output of s3, I want us to do a different question. And this one I'm going to ask you to think about. Number of parameters in S3 or third, comparable or display or C3, which is a convolutional layer. It contains 16 five-by-five convolutional filters. Okay? This is what width and the height of this bump there. Alright, take 15 s to think about what expression would give you the number of parameters in later S3. And then I'll ask someone to give me the answer. All right. Can someone raise their hand and tell me and you don't have to tell me that exact number to say like five times, five times, whatever. How many parameters are in this compositional layers. Okay, So the initial answer is five times, five plus one times 16. Who agrees with this answer? And if you don't agree, what would you change about it? Perfect desk. So I asked you guys to think about this one because I noticed that it's just something that's easy to forget. We will usually always write the filters in terms of their width and the height. And we're going to leave off the depth. The depth is always assumed to be matched to the prior later. So because the prior layer was the output of this pooling operation and there are six that adaptive six. Remember them two convolutional layers will be five by five by six. So the number of parameters in S3 is five times five times six. Because this is the depth input to this layer. The question is, isn't it five instead of six nodes? Six, because the pooling layer, I'll put his 14 by 14 bytes six, so it's matching. So that's the number of parameters and C3. If we want the size of the output of s3, then what we do is we do the width minus the filter with the plus one. So in this case we'll have, let me do this calculation here. The input is 14, the width is five. Now we do a plus one, so this is equal to ten. We have 16 of these doctors to be. The size of the output of s3 is ten by ten by 16. Any questions on any of this population? The question is, do we assume the input is base coat for this Linux, the inputs word grayscale. So the first one is 32 by 32. The question is, why wasn't there a gap? And the first one layer is because the input image was grayscale. So this is like, this is just a 32 by 32 matrix, ready to register 32 by 32 by one 3D tensor. And so the depth is equal to one. I'm sorry, what's the question? Does the output of which layer? I'll put up c3? Yes. So the output of s3 will have a depth of 16 because there are 16 of these structures. Sorry. The picture to have in mind again is when we do the complement cell layer, the filter Jeff has always matched to the input. So the result of the convolution is just a matrix. And then if I have 16 of these filters, I'm guessing, I'm going to concatenate these two matrices together. Are you referring to these five by five filters? The depth of these filters is six. So we could have written these are five-by-five by six convolutional filters. Because remember in a convolutional layer, the depth of the filter always matches the depth of the input. So that's kinda the checking using convention. We just write the width and the height, but you always have to remember that there's got to be smashed to the fire. Alright. Any other questions? No. So let me just explain one more time what's going on here since I, to make sure everyone's on board. So the layer C3 is a convolutional layer where the input is 14 by 14 by six. When we say that we have a five-by-five convolutional filter. The convolutional filter will always have a depth that's equal to the Delta B input. The convolutional filter always has six investigations to be inputted. Six, that's quite a number of parameters, will be five times five times six plus my bias. And so one of the filters convolved with this input is gonna get the a matrix that is ten by ten. But now I have 16 of these filters. And so that's why the output of the sphere is ten by ten by every day. Write down the dimension, becomes very clear that one of the great, yes, it's almost right down the dimension itself. We can put it everywhere because CNN tells you what the filter data should be, right? And we're gonna do this many more times today. Don't worry, use the CT scan. Are there questions? All right. So that is some examples of sizing on this convolutional neural networks. We're going to just show a few notations and then we're gonna get into studies out. Alright? So usually in a convolutional neural network, they will be comprised of several convolutional layers are stacked together with a ReLu in-between. And we're going to use this notation called Plate notation, where if I have a convolutional layer, an array blue, and then I drop box around it and I write a little n in the corner. I'm going to repeat that concrete root n times. So typical architecture for a convolutional neural network is that we're going to have the peaks of a Combray loop com, for a loop. And you usually, we're going to see this N is going to equal two or three, usually will staff, you know, two or three concrete. Concrete is concrete lose. And then after that is going to be followed by a max. Cool. Alright. And then we're going to have a bunch of ANDs, three called Rayleigh's, followed by a max pool stat that's displayed to n. After this, we're going to use k fully connected layers. And this is just something that we'll commonly see, although we'll see later on in architecture called Google that removes fees. And then finally, we're going to have our softmax output. Alright, so this is a typical, fairly conventional CNN architecture. All right. So with that, we're going to go ahead and give each state buddies, right? Yeah. So there's only one maximum. Yes. So what this is saying is that for every, let's say n is equal to three for every three called ReLu calibrated comp rabies. Sir. For a comprehensive cooperative new comp, remove it. Then we'll go through one max score. Alright? And then that architecture of comp ReLu, ReLu, cooperating back school is repeated n times rate. The question is, do we generally will, do we generally have batch norm? Yes, batch norm is something that you will watch to insert here. It's not going to be present in the neural networks that we discussed today because batch norm was made after these ImageNet competition. All right, okay, so we're gonna start off then with this case. Sorry. Okay. Now, before we aren't gonna do a bit more until next. So it's the one that we were talking about. There are just a few calculations I also wanted to do here. The first is how many connections are there? The first convolutional layer. So if you want to put Lynette paper, they're going to give they want to say something in that paper like that, there are 122,304 connections. And the first way I'm walking over that number comes from. So this also is going to give us some idea of nomenclature. So the first thing that you might wonder is what is meant by connection. So these 28 by 28 by six, I'll put one layer means that I have 28 by 28 by six artificial neurons. And I've drawn in red, one of those neurons is, alright. We know that this neuron is the output. The input convolved with a five-by-five filter, where this neuron is the value when that five-by-five filter is applied at the top-left. Alright? So to compute the value of this neuron, right? We know that we multiply the input with the filter. And we. Sum up all the values. And there are therefore 25 connections for the five-by-five filter from the input to this one neuron. There's also the bias. So there's gonna be the plus one for the bison, but we'll call that a connection, also positive connection. I want to know how many connections there are in the first layer. The first thing I'm going to say is how many connections this every single neuron in the output of the first layer had. So every neuron. Tomboy, isn't the bicep applied per filter? The answer is yes. So John McCain still causes the connection. So basically there's a bias b. Let me draw this in red, also a bias p for this one filter. And even though this bias b and this filter is five-by-five filter, or the same filter that I then used to calculate the value of this neuron, right? I shipped to filter over. Even though the bias b and this filter weights are exactly the same, the call each other connections. Alright? So every neuron has five times five plus one connections. Again, those corresponds to the five-by-five wasting my full term plus the one part that I want to know how many connections that are then all I have to do is then say how many neurons are there, and then multiply it by the number of connections per neuron. So let's write that out. The number of neurons I have. The output of my first layer is going to be 28 times, 28 times six, because that's the output of my first comp layer. And therefore, the number of connections is going to be the product of these two, the number of neurons, which is 28 times 28 times six, multiplied by the number of connections per neuron. And that's five times five plus one. And this gives you that number that you'll see in their paper, 122,304 connections in that first layer. Any questions there? Yeah. The question is can I explain why we're concerned? But the number of connections, it can give you a sense of the volume of calculations that you're going to be doing. But primarily in exactly like this. I think it's more for teaching purposes to them, you're understanding exactly responding on compositional errors. Like I said, this will help you off to get a sense of the hardware required because it tells you the number of operations that you need to do for this application. All right, so with that we'll go over the entire texture. So I'm gonna be using this notation. Where for every single layer, when I say layer here, I mean I just use a different version layer. Usually layer, layer is going to refer to either, either a conversation on transformation. So even though there are seven or eight steps here, this neural network only has four layers because that only counts the convolutions or fully connected networks or linear transformation. So C1 here is one layer, C3 is another layer, as tutors mark Kennedy layer because a point it's not canister layer. There's another pulling that happens here that doesn't cancel. Layer C5 is another convolution, that catalyst layer, that's the third layer and then F6 is my fully connected and that calf for here. So therefore total layers. But then every single computation, we'll give it a number. For each computation, I'm going to write the app, the size of the output. So the output of the first column player is gonna be 28 by 28 by six. And then I'll give a description of what's in that layer. So this will have six convolutional filters, each hip and five-by-five apply to strike one. So then after that we have the pool that we talked about, two-by-two pool with strike two. And then the lunette didn't use a max pool. They actually did a pool that how to train your book coefficient, but this is not used today, so we won't go into the details of this. And we've already calculated that the output of this pole is 14 by 14 by six. Then after that, we have another convolutional layer, 16 convolutional filters but with five and high-five. And then as per the questions that we just did on the prior slide, remember that these have a depth of six. Because this depth of six is always going to be matched to the depth of the input of the prior layer. So always remember that when computing, now for our parameters, there's layers. Okay? I said Tom was saying John McCain and others call something a layer if it attaches trainable parameters. And so these comps, layers that are trainable parameters. All right, So tomorrow is asking you about the details of this pooling layer. Told me I'm going to stay just take a look at the paper for that one. So it's not used today, so we won't spend more time on it. But he has a specific point there here that's not no longer be used. After this convolution. We go into a pooling layer with two-by-two filters applied at stride two, but a couple of width and the height by two. Alright? And then after that, there's one more convolutional layer. There are 125 by five filters. Five by five filters are going to have a depth of 16 because the input from the past point where has it up to 16? Because the width of the filter is equal to the width and height of the filter or equal to the width and height of the image, then the output is just a scalar. And because I have 120 of these convolutional filters, that just gives me 120 numbers. Right? After that I have a fully connected layer, that's my linear layer. And then after that, they do a comparison to them. They have a loss function which computes in a square error. So this is the architecture. You wrote it in terms of something like the plate notation. We would say that there's a column for column four, that's this times two, I'll apply a comp, sci and MT output. And that is just a concise representation of these $0.70. Okay. Any questions on this architecture? Great. Question is, can I explain why? For number five, the size is just one-twenty. So remember the output of the convolutional filter, sorry, a vicinal layer will be stacking the outputs of each filter. And for each filter, the output width and height will be w minus w plus one. So in this case the input width was five. The output with the thoughts are with this five. And then we have a plus one. So the output is just one-by-one. The way to picture this is that in this convolution, because the filter is the exact same size as the input. There's no drag that. You just put the filter on the input and you can drive it at all. It's only one vowel combination. And so the output is just going to be a scalar. And then I have 120 of those. So those 120 scalars gets stuck together and be after the fat layer. It's just 120 numbers. Justify understood. The question is just summarizing the architecture, which is saying it's architecture. We have cough, columns followed by pools. Essentially reducing the size of the features until we have a fully connected layer at the output. That goes to my ten classifications. That's correct. Answer your question or was it? The question is, what are Gaussian connections and why are we using MSE? I'm not gonna go into those details because they're not used today. Instead of the best practices if you have ten outputs, right, we would use a softmax classifier with ten, with ten classes. And that's what's going to be done for all the image guy, CNNs. But back in 1998, that was the prevailing way to classify these statistics. They actually had a template for each digit that they computed. The mean square error. For the shoes are not used to date. What if the reason they're not as well as performing territorial empirically, they're worse. Alright, so let's move on then to the ImageNet classifier. So now these ones that we're going to start talking about, beginning with outstanding 2012, carrying practices that are very commonly used today. So we're going to start off with looking at our tech, which is that first CNN that really reduced the top 5% error classification on the initiative dataset. Right? So before we talk about the architecture of AlexNet, I'm going to talk about some of the data augmentation and organizations have been used. And these are the things that should be familiar to all of them because we've talked about them in prior lectures. So first off, AlexNet is round and this next initiative we know is that dataset that has natural images, that volatile wasn't one of 1,000 classes. Alright? Images are usually relatively large. So what happens is because the Internet has variable sized images. Our first crops them to all be the same size. So what they do is they'll take an image and they will either resize it or if down-sample it so that the shorter side is 256 pixels. So the image may not be a square. Maybe it looks like this after cropping or down scaling. The shorter side is 256 pixels. The longer side to something larger than that. What they do is they crop out the central to the six by six pixels. So this operations says take the center square of this image. This will be a 256.2 56 width. And then the actual inputs of the CNN are then going to be this augmentation off of the image. So for this district, what they do is then they cropped out 224 by 200, 204-20-4204 crops of this image as ways to augment their datasets. So they might take this to 24 by 224 crop here. And then maybe on the next crop, it's going to end up being this to 24, 24 crop. Then those costs comprise different crops that are all now inputs to the AlexNet architecture. So in their paper, they say the inputs are 224 by 224 by three. We're going to see that if you apply their architecture to image at this size, it doesn't work out, which means that they have a bug somewhere. So we're gonna just assume that the inputs, perhaps it popped up to 27 by 227 by three. Alright, that's just a minor bugs somewhere in how they wrote the paper. Then we're gonna subtract the mean image over the training set, just like we do for our kind of nonlinearity. Alexnet is the architect enough to popularize the paper. What they did is they trained various of these convolutional neural networks where they use the ReLu after the convolutions and compared it to applying tan h units after the convolution and presented as over here. So what they do on the x-axis is they show the number of training epochs needed to get some training error. Lower the better. And the dotted line is tan h and the solid line is rabid. So they saw that change approximately six times faster than tan h. And therefore they use ReLu for their architectures. And so when we write out the architecture, remember that every fully-connected layer and convolutional layers followed by a rabid. Also train on multiple GPUs. So if you look at this image from their paper, this is the actual image. I didn't crop it to remove the top part of it. What happened is that back in 2012, they had GPUs, but just regular bytes of memory. And that wasn't enough memory to hold all the images that they needed to train on. So they literally split their architecture into two paths. So their first layer would have 96 convolutional filters. 48 of this would be put on one GPU and 48 on the other. Alright? So this was hardware constraints. But then say to do this step has a few other details. They use something called local response normalization. We're not going to talk about this because the next two architectures from now, VGG net, they determined that local response normalization doesn't help. If you're interested in learning more about this. Feel free to drop by my office hours is basically an intuition that if a given layer, one neuron is really loud, it's just silence for the other ones. That's something from biology. But again, made her paper so that this doesn't help. And then they're pooling layers are done with some overlapping. So usually pooling layers are done with the stride equal to the width of the pooling layer. But in this case for Alex that they have some overlapping pair dataset augmentation. So in addition to the cloud, So they did they also, after extracting out the 22224 pixels, they took their horizontal reflections. And therefore they had in total ten images from one, because it took five patches over to 245224 cluster reflections. With these ten images and put into AlexNet you again at 10:00 batch distributions. What they would do is they would output the 10th softmax probabilities together. And you can think of this as a sampling across and put examples. And this reduces the error rate by 1.5 per cent. Also did a few types of color augmentations to the images. And this also had the effect of reducing the top one error rate. And then finally, they didn't drop out with p equals 0.5. And they found that this substantially reduce overfitting, but lead to ten times over twice as long. Questions on any of these augmentations has accepted. The 1% is an absolute error rate. So I believe that the error rate, the report is either 16.4, 0.2, 0.4% error rate. So this 1% means like if they were to do this, pull their occupation. If they were not to do it, it would have gone up to 17.4%. Yes. So let me just repeat what Tony was saying, which is clarifying the crop. So basically when you get a test image, when after the crop down to the residency, but it's expected that they would take five to 24 by 224 prompts, as well as their horizontal reflections. So this gives ten subset images. Those images would go to AlexNet reading ten softmax probabilities are ten softmax distribution, then it would average those together. Any other questions? Question is, is the augmentation done at train time or just for testing? It's also going to turn the question is, is there a reason they chose 24 to 24 by 224 rather than say like 250-65-2506. I don't know the reason they chose that, but I imagine it has something to do with making sure you grab the initiative, classify it like if you make that number too small, but you may just be taking out like a small patch of say, attack or something. And then if it's too large, then you have more computational intensive to 24 of them are in the sweet spot. Alright? Okay. You more things for AlexNet. So the optimization with SGD, stochastic gradient descent with momentum and weight decay. Remember weight decay just means L2 regularization. Recall when we talked about L2 regularization, you could probably see it as decaying the waste every STD stuff. So weight decay is just L2 regularization. Batch sizes were 128 images and their momentum factor was 0.9. They did manual and kneeling of the learning rate. So what he did is, um, they started the learning rate of 0.01 and they would monitor the loss over, across epochs. And when the loss fat code, what they would do is they would manually stop the training and then decrease the learning rate so that the learning rate is now smaller. And then after that, you would have a decrease in the loss and ended up going. And then they would again when it plateaus, comment interests burning rate, allowing the loss curve to go down further. So oftentimes you'll see boss curve that looks like this. And when, you know, it takes another drop that just is and indication that at that point the learning rate epsilon was decreased either manually or by schedule. And then the L2 regularization with any questions there? All right. I'm trending time to train alex that took roughly five to six days on the GPUs that they had available to them happened time. We talked about how it's good to ensemble of networks. So even though these numbers take a long time to train for this competition, they trained up to sudden AlexNet and they ensemble. There is both together. So if you just have one AlexNet, it achieves an 18.2% accuracy. And then when they had the final ensemble with seven CNN's, take it off the accuracy down to narrow it down to 15.4%. This slide here is just a reminder of what the image looks like. So these are what the images look like. And again, they're in one of 1,000 labels. And whenever we report error rates, we're going to be reporting a top five error array. Usually. And whenever we say the top five error rate is 15.4%. That means that it only means that it aired on 15.4% of the images and error occurs in the following way. For an image, AlexNet got to make five guesses at what the image was. And if one of those five is correct, then he caught it right? None of them are correct and they got it wrong, right? So I believe these examples, AlexNet only got these two wrong. And again, this one Dalmatian behind some cherries. And so I think that arts, that part of it a reasonable job here in terms of producing the foundation and shared. Alright. Any questions on any other, I guess, preprocessing detail from our time? Great. Question is what it means to ensemble multiple at the same type of neural network? Are you asking this from the perspective of these neural networks actually producing independent great, yeah, The company, is it different initializations or is it something like bagging? So I believe although RCTs check me that the service CNN just started from seven different initializations. And even with just different initializations they converge to I'm somewhat independent predictions. Okay, I just check the time aside to 50. So let's take a five-minute break and when we come back, we'll go into the details of the architecture. Like about the five different patches. And if your function is touches the same, right? So this works like footballs, interesting cases. So in training as you want to turn on those data, augmented patches. So that effectively gives you more samples, although they're somewhat correlated, we get some more sample training, which is a gift doing any testing. You're also doing it because even though they're highly correlated, they'll produce different softmax probabilities. And so maybe on one property getting more of it's scary and not the foundation for exams. So April that has an effect of reducing error. So in that case, is it unique for each image? You got? Like an image after Prop to be like to stick to J. You got patches, five plus five. You got some pictures. So you've gone from ten times, is that right? Yeah. There's actually an x here. And like for the taxi for each test intimate you about patches of it. I make the final prediction. Exactly. Yeah. Okay. I see that reduces heart rate by 1.5%. Considerable. Yeah. No, it's me again, my colleague S6 filters for this matches. So 16 is a hyperparameter that patient. So they decided you want 16th notes. And then they also showed that the size five by 5. " at any arbitrary tensor for the filters, that will be a hyperparameter that you need to really try to image data memory. Sorry, not all, but a bathroom. Oh, just a bachelor. Yeah, that's fine. Right. The picture to have in mind despite marked here over here and you're looking great spacebar to myself clear here, like standing around a similar law school, but then six bombers and he's definitely right. Every system, what we should think of that as a texture. So I have in mind, this is the most accurate drawing of it where the filters will always have adapt best match to the oh, yeah, yeah. That's all right. Thank you. Right. All right, everyone. We will get back to it. Any questions on Alex? Jeffrey processing? Alright, well, go into the architecture of Alice is going to be an eight layer neural network. So there are going to be eight layers with trainable parameters being in publications or fully-connected layer. We're going to stay the input as to 27, 27 by three. And the first layer, there are 96, is a convolutional layer with 96 filters. Each of them 11 width by 11 pies applied at stride four. Okay, So then what is the output size? We're just gonna do this. But it'll be an application of that formula where it's going to be w minus Ws. There's gonna be a plus two times pad, but pad here is equal to zero divided by stride plus one. So in this case, the input W is to 27, the filter width is 11. The stride is equal to four, and then we add one. So the output of this first layer is going to have ensures that our 55, 55. And then, because there are 96 of these filters than the depth at the end of the first, at the output of this convolution will be 96. So the first conv layer output is 55 by five by nine. Any questions? Question is, is there a reason the filter sizes are odd numbers? I think it is likely. Well, later on, we're going to see it's tried to is equal to one, right? If we want the width of the output to match the width of the input, going to need this minus Ws plus two plus one to equal my original width. Because two times pad is an even number and then this plus one makes an odd number. Then width of the filter, we've got an odd number. Alright? That's the size of the output features at the first layer. Next question is how many trainable parameters are there in the first convolutional layer. Services like eliminate question, I'm gonna give you 15 s to think about it, hoping to talk to you. It doesn't. I'll ask someone to shut him and asked her so how many trainable parameters are in the first column? All right, someone give me an answer for this. 11. 11 plus one. Perfect, That's right. So the students have the correct answer. I'm just gonna write it out. So you took the filters, has 11 by 11 by three. Remember because we input image as RGB channels, so step two is three. So the number of parameters in one filter is 11 by 11 by three plus one. So the total number of parameters will be 96 times. So the number of parameters and the first convolutional layer is 96 times 11 times 11 times three plus one. And this is equal to 34,944 parameters. All right, So that'll be written out here. Remember, this is gonna be the size of the output at the first convolutional layer, 55 by 55 by 96. And then this layer will have 96 filters. Each of them 11 by 11 by three apply to strive for. I'm going to skip this. This is just more application of the sizing formulas to just get to the architecture. There's one question here which is company trainable parameters are in the first poem layer. And just like in Baghdad, this is a trick question, right? The answer is zero. A pooling layers contain zero parameters. So this is the first three computations in AlexNet. The conflict are followed by a pool. They're pooling filters were three-by-three applied at stride two. And that will lead to an output that is 27, 27 by the same deficit and put 96, then they apply this normalization of error. Again, you're not responsible to know what it is because no paper uses it. Since AlexNet. I had forgotten I bought some of these calculations on the slides because, because they're really mechanical. So let me just move on to the whole architecture of AlexNet. So you can see arch that will follow this comp. Remember there's a ReLu after every single company convolution followed by a pool. Then there will be another convolutional filter. These ones were 256 filters of size five by five by the same tap. This layer 96 replenish dry 1.2 because they are five-by-five and the cat is equal to two. Remember that the output width is going to be the input width minus the filter size five plus two times pad. In this case, pad is two plus one. And so you can see that this cancels out to give Wn. And so the output of this convolutional layer is also going to have the same width and height as the prior norm layer 27 by 27. Then they do another pooling layer that takes the width and the height down to 13. And then after this they do three convolutional layers stacked together. Each of them with three-by-three filters, quite a pad one. A three-by-three filters applied a tad one are also going to have Wn equals w out, since it will be minus three plus two plus one. So three-by-three iPad one keeps this ties to say, five-by-five, catchy, saying, alright, after the three layers. So this is three layers here. They have a pooling filter. Again, take you down the width and the height. And then after that they have three fully connected layers. And finally, their softmax layer, which gives you the softmax probabilities over the 1,000 classes. Okay? So that is AlexNet. Any questions about AlexNet? Today's question is, is the reason that we use maybe she's an odd number for the filter size to match the input and emphasizes, yes. Any other questions here? Besides? Just suppose, yes, this student is asking, if we have an even padding. Let me just write out the formula again. This is with stride equals one, wn minus Ws plus two times pad one. Right? Now, the padding has always given meaning of Paris equals one. We put zeros on both the left and the right. That's why there's this factor of two here. If you did uneven padding, you haven't even size filter, but that would be not to take care of him. Kennedy. Other questions here. Can you have decimated? Perfect, Yeah, so rocks the same. Another reason for odd filters is an image processing. You can think of a point that you're interested in faltering. If you have an odd filter, essentially, the center of that filter is at that point, and you have the same number of filter traps, both going left to right. And so that's another reason for an optimal tree structure. Other questions. The question is, how did they come up with these hyperparameters like 384 3d4 to 56 filters. They didn't do the, they would have done some hyperparameters me. But this actually leads to the next ImageNet winner. Because in 2013, the ImageNet winner was called ZF net. And we're going to spend very little time on that because all Z F net is, is AlexNet with different hyperparameters. So basically these authors as bi-layer and they came up with a technique that they pulled the cockpit. So try to identify the features that were being burned by AlexNet at e.g. in their paper, they say AlexNet, which uses at the first layer and 11 by 11 filter. They observe and Caribbean island and filters tended to extract Phi and low frequency features of the image, but not middle frequencies. So they made some adjustments. The first is that they made the first 11 by 11 filters, seven by seven. And we saw better frequency coverage there. And then they also found that it was better to apply smaller filters, that smaller stripes and to increase the number of filters. So you were putting out the number 384 and I believe they increase this number to 512. Sorry, I I actually took the wrong slide, so I'm missing a few slides here, but I'm missing a slide. So in the third, fourth. And actually let me just draw them to fire one. For these layers here that use three to 43 to four to six and Z F net used. They use 51210, 24.5, 12 filters. Alright? And the effective, this hyperparameter optimization was huge because the F net dropped the error rate from 16.4% down to 11.7%. And if you do the math, that's approximately at 30% reduction error rate, nearly by just changing the hyperparameters of the AlexNet. Alright, so much more to be gained simply by optimizing hyperparameters. Okay, Any questions there? What are the significant difference between between VNET and AlexNet? Alexnet is outfit has a trainable there, so it has eight convolutions and pulls. Whereas That's only had four, sorry, sorry, not companies had 8 m, which means eight convolutions or fully-connected layers. Whereas only four combinations and Boy, heck of air is deeper than that. And then AlexNet also used the ReLu activation, whereas Lynette uses sigmoid. Then the output of Lynette is this comparison to a template mean-square error, whereas for our extent is the softmax loss. Alright? So that's Alex and z F net. Beginning then in 2014, there were two really notable architectures there, VGG net and you go back. And these were the two architectures that achieve the lowest error. On ImageNet. You can see people that just squeaked out the windpipe by 0.6 percentage of an error. But the main significant thing is that instead of a layers, which is what our tendons you have network, they went to 19.22 layers. So let's talk about how they did that and some of the experiments they did. So BTT net is an architecture that came out from this research lab at Oxford called the visual geometries Britain. The basic summary of the architecture is in this quote, which is our main contribution, is a thorough evaluation of networks of increasing depth. So going from eight layers to 11 to 16 to 19, you've seen an architecture with very small three-by-three convolutions. And they show significant improvement over require our configurations. Prior state of the art being our tendency FOR, alright. So there was to focus on this small three-by-three convolution more extended down. Alright. This is the summary of architecture. And then let's talk about some of the design made. The architecture is going to have I'm calm ReLu, ReLu, followed by a pool. And then they repeat this three times. So in total there are six layers, six columns, convolutional layers within this first segment. Then they stack three convolutions, followed by a pool, and they do this twice. And then they finally have three fully connected layers that then go to this softmax. Every single convolutional filter will be three by three. Stride one and add one. So the width of the input and the output of a convolutional filter will always be the same in this network. Then all pool filters are the two-by-two max pool is applied at a shutter. Every point butter is going to reduce the width and the height by a factor of two. With this architecture, they took the ImageNet error rate from 11.7% down to 7.3%. And that's approximately a 40% reduction in error. So this is fairly significant. Alright, so the first thing we should think about is what are the implications of using three-by-three filter? Rather than what AlexNet and z ethnic did, which was at the first layer. Having an ALEKS test case, an 11 by 11 filter, or in CF next case or seven by seven filter. So do you have had a 7-by-7 input filter? Whereas VGG net uses a three by three filter. Can someone tell me that they can potentially of using the smaller filter but with larger one. Right? At the same, you may lose. You said larger frequency information, but I'm just going to make it more general. Three-by-three and 7-by-7 are likely to pick up on different frequency features. But you mean yes, that's true. Any other cons? It can take more time to train. Uh, we'll revisit that one. Other other ethical concerns. I have it. I think it's I speak receptive fields. What I mean by receptive fields is the last lecture in this image that we had where, where we talked about how neural networks have sparse connectivity. So if you have a three-by-three filter, right? If we just look at the width dimension, then neuron is only going to see a three-by-three patch in the image of the preceding layer. So the colonies and it required some more mature spore final. Because it's seamless and perfect. Yes. Yeah. So let me just explain what the student answered, which is correct. Remember, we gave us motivation that a reason why small filters maybe bad, is if I'm a self-driving algorithm to try to do a self-driving car, I needed to make use of information from different parts of the images together, right? So if I only see a three-by-three patch, I'm only seeing activity or I'm only seeing parts of the images that are very close to each other. And I had just one layer. I will have no neuron that uses information from the top left of the image in the bottom right. Alright. In GF net, they have a seven by seven filter at the start. So because it's seven by seven, we will draw this as number of books like this. We call the receptive field of this neuron seven because it sees seven pixels or seven features from the preceding layer. Whereas here, VGG net would have a receptive field of just three. So the way that VGG net address this, this, and you would get a sense of this from the architecture where they stacked three comps layers together. If you suck, convolutional layers together, if I were to draw it like this. These convolutional layers together. So now I have two stuck together. This neuron up top has for just two layers receptive field. That's effectively far, right, because it's through two layers. It can see five pixels of an original input. Now if I were to add one more layer than the effective receptive field of this neuron would become center. So one potential con, of using smart filters is that you don't get to see all of the pixels of your input, right? But you get around this by stacking many convolutional layer together where every single stack is going to increase their receptive fields by plus two. Okay? So that's why did you do next? Well, get around this by second many compositions together. Any questions there? Three by three filters. With a three-by-three filters and NOPAT equals one the output with always Matzke and quickly. Did I answer your question? The question is, do we want to be smaller? In deeper layers that we're going to use the pooling layers to reduce the size. That leads me to the second question. So genitals question is, is it the case that having a stack of three of these three-by-three filters, is it better than having just 17 by seven filter, either in terms of expressivity or in terms of number of parameters, right? So let's go ahead and firstly, the parameter question, which is, which has more parameters, is it? Yeah, I just want to point out that there is something simpler. So you have an extent where he explains that while they're in that if you have a deeper there so they can eat glass. Why they're not independent. So the model generalizes. Okay, yeah, so tomboy is referring to a Professor, Sorry, I can agree. Who published the paper four months ago explaining why the deeper layers are better. And I'm just going to I'm not sure I follow it exactly what you said. Write it down for the acinus look up. Alright. Okay, so VGG night, which layer has more parameters? 17 by seven filter into the F net, or three of these stacked three-by-three filters and VGG neck. Alright, so let's go ahead and do this calculation. We have Z F net, and it uses 7-by-7 filters. We're going to assume that the input depth is going to be some variable CAN. And then the number of filters, which will be the output of this layer. I'm going to call C. Alright? And then I'm going to, I'm going to drop the biases moving forward. So we're always going to count the weight parameters. So if I count the number of parameters here, the, each filter would have seven times, seven times the input death number of parameters. And the number of filters I have is c out, alright? So the total number of parameters and one CFF layer that has a seven by seven coalition is viscosity of repair. For VGG net. We're going to stack three of these three by three filters. Alright? So for a three by three filter, the number of parameters will be three times, three times the depth of the input. And then I'm going to have c out of them. And that's the number of parameters in just one of these three-by-three filter convolutional layers. I know that three of them. So I'm going to take this number and multiply this by three. Okay? So just to simplify it, if I just call CNN Ci equal to some constant, big C, G, F net would have 49 c squared parameters, and VGG net would have 27 times c squared parameters. So the first thing that you can see is that high stacking these three-by-three filters, we actually have a reduction in the number of parameters compared to Z F net buyer, the function will be a factor. That might be one trout, which is number of parameters being less. There's also a capacity argument. And this is going to be a bit more intuition and maybe where the fruit is in experiments. So why might need three-by-three filters turned into a good thing. So I want you to remember that after every single comp layer is a rabid. So there's a ReLu over here. There is a ReLu over here. Then if we add one more layer down here, there would be rabies over here also there. Where it says Ye Fnet is just going to be one convolution followed by a ReLu. And VGG. We're going to be stacking three layers. We're going to have three values. And this means that in terms of the computation, there are more non-linear operations. Practically empirically, what we see is that stacking these and adding more nonlinearity leads to better performance. Because in the gradient descent, the stacking of these phenomena charities allows there to be more expressive features capture than just having one nonlinearity. So why might it be a good thing? Jackie, these, we introduced nonlinearity and then empirically, this leads to better performance. The question is, can I go over what the three stack three-by-three filters looks like. In terms of the implicit, the outputs. I will show us, find that back in just five. So if it's still on there, please raise your hand and ask it again. I'm sorry. Can you repeat the question? Oh, yeah. The question is, is the increase in receptive fields for these three-by-three filters, always two. The answer is yes. So if you were to just draw out what the next layer would look like, you would see that it also has said that the effective receptive field of this neuron is now seven by seven. The question is, what if you have five-by-five filter? We would have to draw that out because I can't, That's something that seems simple, but I can't do it in my head. So I guess it would be so the first would be five-by-five, and then the leftmost would see a self, a five-by-five. So I believe interests by four. So the first way it would have receptive field of five. The next one it would have a receptive field for nine. I believe that's the answer. I see that it can rupture. Okay. So Russia saying that the receptor field will increase by W S minus one effective receptive field. Yeah, I'm pretty sure that's correct. The question is, we mentioned that different filters may pick out different frequencies, their features. Can I explain that a bit more? So I don't know if there is an intuition to over whether you have a larger filter should get the high and the medium or the low frequencies. But at least the optimization that ZF that did where they visualize the features with this decomp, identified that 11 by 11 large filters. We're good at picking up high and low frequencies, the missing medium frequencies. Because these filters are optimized to reduce the loss. Difficult for me to think of an intuition for why that would be. But that's all I'm going to leave that as a terrible results. Other questions? Alright. So why might stacking three-by-three filters turned into a good thing? This is because there's more nonlinearity. There's more nonlinearity. E in this case, three re lose VGG net versus one ReLu four. Yes, ma'am. All right. There's one more potential. The question is, why is born nonlinearity a good thing for the model? So this is where we then run some new variable results. So at it, having more layers and more nonlinearities up to a point to be getting better performance. You can think of maybe an intuition being born on linearity allows you to learn more complex features that maybe number for classifying. All right, there's one more con, of using small filters and more layers. And this is just going to be memory. So if we look at the memory of Zf match and VGG net used to store the values of the activations in the past which we get cash or cash. Remember, the F net, we have 17 by seven filter. And just apply that hat equals one. And so on. W out, the width of the output would be wn minus the width of the filter, seven plus two times pad, which is two plus one. And so the output would be wn minus four. So the output of 17 by seven filter is, it's going to be a width of wn minus four and then a height of h and minus four. And of course, however many ports are. What we have, we have that many of these matrices. For VGG net. We're going to have three of these three x three filters applied at cat equals one. Remember that 1.3, the input into the output width are the same. So at the output of my first three-by-three convolution is going to be a size wn, Hn, that I'm going to pass back through another combination. And that output is going to be WN h, n. And I'm gonna go through one more three-by-three convolution for which the number of neurons, all right, In the output will be Wn by agent. You can see therefore that since I have to calculate all these activations for VGG net versus just one output activation for Z F net. That essentially comparing these two layers, I need to store more than three x amount of activations compared to the F9. So this will be more memory intensive to hold the output of all the faders. Questions there. Alright, so this is a VGG architecture and I hope it gets towards question from before where we show exactly what's going on. Vgg net is going to process 224 by 224 by three images from the ImageNet. Do a similar company to AlexNet. Then what begins is first two columns. Remember there's a rabid following each column, where we have 64 filters. And remember, every single filter is three-by-three. So that's why I haven't been the filter sizes. So in this first convolutional layer, there are going to be 64 filters, each. Three by three by the depth of the input, which is three. These are applied at PAT equals one. Then if I look at this layer, the second column, there's gonna be 64 filters. And now each of these are going to be three by three by 64 applied at pad equals to coordinate. Alright? Because the width and the height are matched, the first two-dimensions remains to 24. And then the depth is just going to be the number of filters in that way, or 64. Alright. The pool is this two-by-two pool of plaintiffs trying to and that reduces the width and the height by a factor of two. Then following that pool, we do our three-by-three convolutions again, there are two of these data, but another pool that reduces the width and the height by two and then restart the three com filters to all three columns, 43. And then finally, the boy connective errors. Any questions on this architecture? Instead of each question is, is there a ReLu following each column? Yes. So every single one of these has a rail in-between. Other questions. Question is, how do we go from full to FC? Great question. Who is a seven by seven by 512 texture. And away we go from four to FC is just like how we go from in the homework. We go from a 32 by 32 by three image to a layer with say, 100 artificial neurons. The way that we do that is we reshaped 32 by 32 by three into 3,072 vector. And the fully connected layer would therefore be 100 by 30 72. Here this fully connected layer is going to be 4096 by the product of seven times seven times 512. So we received this thing into a vector and it'll be perfect. Yeah, So the student noticed that every single time we do a pool that reduces our width and a height by a factor of two. The next column player, we increase the depth by a factor of two. Why did we do this? The reason we do this is so that the number of operations, the number of multiplications in every single layer, It's the same. Let's go ahead and write this down. So if I look at the number of multiplications, this is the same as the number of connections that calculation that we did for the minute. If we look at the number of, ofs in this layer, the number of operations will be the number of operations per neuron times the number of neurons. Let's do the first part. How many operations are there per neuron? So for a given neuron, the neuron is doing a three-by-three convolution, right? Match to the depth of the input. So every neuron. The output of a three-by-three convolution and adapt is a depth supplier input which is 64. So every single neuron is doing a filter that has three times, three times 64 multiplications. Then we multiply this by the number of neurons in that layer, and that's the total number of operations. So the number of neurons in this layer is one-twelfth times one-twelfth times 128. So this number of apps will be one-twelfth times one-twelfth times 128. And again, I'm dropping the biases here just for convenience. If we then look at a different layer. So if we look at this layer here and we count the number of hops, the number of apps will be the number of operations per neuron. So the neuron does a three-by-three convolution. And here the depth of the convolution will be 128. This is three times, three times 1.28. And then this multiplies the number of neurons, which is 50656566 times 56 times 256, right? And then the purple, the purple and the red numbers are equal to each other because in the red, I had one-twelfth, One-twelfth, they got divided by two. That's 256 is. But then once 28 got multiplied by two to the 6.64, we got multiplied by two to 120 years. So these two numbers are the same. And that's why whenever they decrease the width and height by factor of two, increase the depth by a factor of two. And that leads to the same number of operations for every single. Question is, why would we want the same number of operations per layer? I would say. So that there is no bottleneck layer, e.g. that is either taking up more time or doing something that's much more computationally complex on the other layers. Alright. So that's the VGG architecture. We mentioned that there's this problem of memory. And for VGG net, we have to store many activations which are the output of all these three-by-three convolutions. So in this next slide here, what I've done is I've written out the memory needed to do this one for cost of VGG. So the memory needed to store the activations is just going to be the size of the activation. So that's gonna be the width times the height times the guy, right? So what we do here, first we calculate the number of activations and every single layer. And then every activation for every value is going to be single float precision. So it's going to be four bytes. Reactivation is four bytes. If you add up all of these numbers for just one, for passive VGG net, you're going to have to compute 24 million numbers. That's just the sum of all of these. Then if the other 24 million numbers in each event is four bytes, then just to compute a fourth pass requires 96 mb of memory. Alright, and I remembered you have to catch these valleys because they have to be used the backward pass as well. Backward pass, you're going to have to continue all those gradients. And so you're also going to need about 100 mb of memory just to store your backward pass. Any questions there. All right, and then let's do one more thing which is to calculate the number of parameters in every single layer. So we're doing here the same calculation we've done already, except I be dropping the biases. So let me just write that here. I'm ignoring the biases so that it's clear to anyone reviewing the next day they're on. I want to know the number of parameters in this convolutional filter is going to be the size of the filter, which is three times three times the depth of the prior layer, and then times the number of clusters. I can write. The point of writing all of this out is for you to notice the trend, which is VGG net, has over 100 million parameters, right? If you sum up all of these numbers, the total number of parameters and VGG net is 138 million parameters. But if you look at where that 138 million just concentrated in these fully connected layers, right? You sum up just the number of parameters and the fully connected layers. The Fc layers. They comprise 122 million of these 138 million parameters. So really the thing that is making this VGG net very large, It's just the last three fully connected layers, not the convolutional layers. And that's entirely consistent with what we talked before in the past regarding these convolutional layers some way after the CNS also have a baby's? Yes. So there's a rabid after this one? A rabid after this one. There's no rabid after this one because this is the soft max score. So this goes directly to your song. 1,000 scores. The question is, what is enough? What is the way for us to reduce the number of fully connected layers at the end. The asteroid will be the next architecture. We've talked about people that will remove all of these estimators and we'll see that it's much more likely than VGG. So let me summarize. For VGG net. It has these calculations. A lot of memory is required just to compute the forward and the backward pass 100 mb for one-four past. There are 138 million parameters, but then a lot of the number of parameters are in the fully connected layer where there were 122 million people networks we'll talk about next as well as at school. That would definitely remove the fully connected layers. And that will be two numbers moving forward that are far fewer parameters. And VEGF, VEGF is one half of March or CNNs are a few other experiments. So using this idea to step into three by three filters, they tried to do an extended to say is deeper better. So this is a table from their paper. And the identities are given by these later bachelor's ABCDE. That just corresponds to the number of layers because I did a bunch of other experiments. So, so all you need to know is for this table, as you go down it, you'd have more layers. And the networks that do the best that achieve the lowest error are the 16.19 layer neural networks. Alright, so deeper appears to be better. Although we're soon gonna find out that you can't just arbitrarily Nicky, just deepest. Here are some other details about VGG net. So the image input is 224 by 24, just like AlexNet from a crop, they do the bubble means traction. I had mentioned that Alexander has this local response normalization layer of VGG, did the experiments to find that it didn't increase the performance and so therefore they removed it. Vgg net use stochastic gradient descent with batch sizes of 256. This is double that of AlexNet. And they also just use momentum. They had an L2 regularization penalty. They had dropped out for. The two-point can have two layers, the ones before you get to the softmax, they adjusted the learning rate the same way as an artifact. So they also yield that learning rate whenever the training loss factor. Alright, when VGG net was treated, Xavier initialization that we talked about was thought, yep, now, we know that the visualization is particularly difficult when the number of layers gets more. So they actually did something creative here to try to get their network to converge. Which as I said, they wanted to train a 16 layer VGG network. They would train a shallower one, let's call it 11 layers. And then after the 2011 layer of VGG, not that was used the parameters of the first investment layers as the initialization or attending a 16 their neuron. Remember, that's how they got around this initialization problem. Later on they found that solve your initialization was sufficient to initialize your network to get good performance. They performed. Data augmentation is including horizontal flipping, random props and RGB shifting that AlexNet VGG that took two to three weeks to train it off for GPU machine. So that's a considerable amount of time. And by the time of their submission to have finished seven of these factor Xa. So the output that got them there, I error rate as ensemble of a sudden. We're all set. Question is, was this two to three weeks per net or for all seven months? I believe it was pertinent. But you could have had them on parallel computers. Alright. Any other questions here? Other question is what would be the chain time for a high-end GPU today? I'm not sure the answer is anyone to speed up much faster? How much data do that? I agree with that answer. Correct? Yeah. These are the key cues, again, from Twitter from 2014 and so it's been almost a decade. Question is what stopped them from using 16 GPUs? I don't know, but I have some insight, which is that these are academic labs and we only have so much. Any other questions. Right? The question is, what is this 1,000 in the last later, this is the number of classes and image tag. Image tag has 1,000 process doc, but I can see part-time. Alright, so that's the VGG net, but not absolutely win the ImageNet competition in 2014. And at least at the start or at the end of lecture here I want to introduce some take-home points of people met and show you the basic inception module, which is this idea that they introduce to take advantage of the fact that different convolution filters, they pick out different features. Alright, so here are the main take-home points that buccal net was able to get to 22 layers. They introduced this inception module, but I'll show you in just a few slides. They get rid of the fully connected layers. And remember, the fully connected layers are where most of the parameters are. So Google net has only 5 million parameters. That's 12 x is less than AlexNet and 27 less than VGG net and Google net energy. In 2014. Inception module that they introduces a good idea. Part of the reason they were motivated to remove these fully connected layers is that they wanted to keep the computational budget. They wrote in the paper that they wanted to do this so that it's not just an academic exercise and they need some really fancy and GPU to be able to train these neural networks. And so I believe that the authors of Google net actually trained this on a CPU. And they won ImageNet in 2014 for the top five error rate of 6.7%. So this inception module really has two goals. First, we'll talk about is to let the network take out the most important features. Rather than us saying everything is gonna be a three-by-three convolution will see that they allow other types of publications or other sides of the composition. And then the second point is to reduce computational expense. So we're not gonna be able to get to the second point today in terms of how they reduce computational expense. But we're going to see that their architecture and incorporates some really clever, a really fun idea to make the computation a lot. Alright? So people that again was motivated by the spinning of saying going deeper is better. But like we saw with VGG net, as you go very deep and you have this boy connected layers, we have more parameters, more computationally expensive. So how do we address this? And this is through the inception module. So this is what the naive inception module, but this is my E because rebuttal to make several modifications to this, to make it computationally efficient and to not explode memory costs. But the basic idea is the following. They call this network and network. This whole block here is One inception module. And the idea is the following. You have the previous layer, this is your first layer, then it's actually the input image, right? And what you do is, you do one-by-one convolutions, three-by-three and five-by-five convolutions as well as max pooling in parallel. Right? And then because he's convolutions are of different sizes, are extracting different features from the data. So you can think of these different operations as all extracting different features of the data. And then what they do then is they concatenate all those features together. Again, this is the naive version because it's that I did. It just concatenate everything. We'll see. But they concatenate these pieces together at a high level. And then that becomes the input to the next layer. So now the next layer will have the features from the compositions of these different filters as well as the matrics pulling. The next leader can choose how to weight those features better. So there's inception module. I give the flexibility of saying, I'm not sure if three-by-three compositions are the best. Maybe five by five bar, we're just going to do them altogether. I mean, the next layer. The next layer. When we come back to lecture next week, because remember Monday is a holiday, Wednesday to determine the Monday after that, we'll get into the details of how this inception module works. 
I hope you're all staying as dry as possible. That's urinalysis important again, the first is that homework number five is due this Friday, March 3rd, up with the base scope. It is our last homework assignment. And I hope you all will the sheet that over the course of the CIFAR ten dataset, we've gone 35-40% accuracy, which was an assignment to with just a softmax classifier to oversimplify percent accuracy with CNN. So almost doubling the performance by using neural networks. On Friday of last week, we uploaded the project and its accompanying data sets to learn. The project is going to be due Monday of finals week. So that's gonna be March 20, 2023. And on the project, you're going to be allowed to use whatever code you desire to use. So that includes applied towards TensorFlow and Keras. You can use any deep-learning library you want. You all haven't had exposure to deep learning libraries. So what we're going to do is on Wednesday. The next lecture. Having two of my PhD students cover how Keras and fights work, work. And you'll see how these deep learning libraries really make implementation of neural networks straightforward, such that you really don't have to understand how neural networks work to be able to train them. Of course, because all coded everything from scratch. You'll know how everything is working beneath the hood. But yeah, we'll have these two PhD students on Wednesday cover the particular syntax and how to get started on the project using Python. With that lecture, we're also going to provide some starter code to implement either CNNs or RNNs with Pi, torch and Keras, alright? And then in the project guidelines, if you haven't read them yet, you also give you the option to do a custom projects. You give guidelines for what that customer project should cover. And I just want to highlight. So you all know that if you want to do a custom project, you need to send that request to me. Nobody can describe it. Are there any questions on the project or the guidelines that we uploaded? Right. And then lastly, the TAs are working on grading the midterm exams. And our goal is to release midterm grades back to you by the end of lecture on Wednesday. Any course, logistics questions. Question is for the project, are there any architecture? So you have to please take a look at the project guidelines. Basically, for the project will require you to connect something that we took her in class, that is Post CNN. So the most natural one will be the one that we, We'll begin today, which is RNNs. But you're also welcome to implement things that we may not have covered in class. So in class we'll also cover variational autoencoders, generative adversarial networks. We may also cover transformers. We don't have that material for today, I did, but I'm working on a new lecture that hopefully will make it in time for, for this iteration of the class. So you don't have to expend something that's after CNN's in the project. Other questions, great. Rock asks, Can we use late days on the projects? Are not allowed to use on the projects. So the projects all have to be submitted by Monday, March 20, 2023. Any other questions? Alright, we are going to get back to material. So our last lecture was quite some time ago. So just to recap, what we were doing is we are beginning or we were welcomed into the survey of neural network architectures. We talked about the AlexNet, the first CN end up on the ImageNet competition. We've talked about ZF nets, which is basically AlexNet foot with hyperparameters optimize and not one in 2013. In 2014, there were two architectures that were remarkable. One is VGG net, which we talked about at the end of our last lecture. And that was this neural network architecture that went deeper to 19 buyers. And remember, the key features that they did was they made every convolution, a three-by-three convolution, and started to stack them. So there would be for ductile three consecutive convolutional layers. Alright. And then last lecture, we were just about to delve into Google neck. And so we just started the motivation of the woman, which was a few things. I wanted to first propose a new architecture. That relies on this thing called the inception module. And the basic idea behind the inception module is what we choose hyperparameters for a CNN, you can choose the filter within SOC as a filter with a title. But which one is best? We're not sure. Why don't we do a bunch of these different ones and then let the network pick out where the most important features. Alright, so that's number one. The second motivation is that they wanted to reduce the computational expenses of these neural networks so that they're not merely an academic exercise so that people can train them if they don't have huge computational servers. So Google has only 5 million parameters. And so that's far less than AlexNet VGG net. We're also going to discuss one key innovation that they make any architecture that keeps them computational expense dance. So this architecture won ImageNet in 2014, and it did so without training on a GPU. Alright, this is in contrast to VGG net, which had multiple GPU servers and it still took three weeks. But pupil that is able to train without a GPU. Any questions on motivation? Alright, so let's see how Google net does this. And this is a basic building block if you go out with just the inception module. So in this image here, this thing here is the input from the prior layer or the actual charge. Sorry, this is the input from a prior layer. And then what you see is that this input gets sent to convolutions of different sizes. So Google net will do one-by-one convolutions, three-by-three convolutions by by comma, five-by-five convolutions and max pooling. And you can think of each of these different parallel blocks as extracting different features of the data. Then what people will do is it will concatenate all of these features together. And I'll send that to the next layer. The next layer just to see the output of the one-by-one, three-by-three or five-by-five convolutions as well as the max point. And then it gets to determine which of those because it wants to use to be able to maximize, minimize the loss. Alright? Any questions there on this basic idea? The question is, we don't do pulling after convolution here. At least in this illustration. We just do pulling on the previous layers and quick to maybe try to extract and variance position invariant. Student asked, how is this less computationally complex than same VGG that because it sounds like we're now, instead of doing just three-by-three convolutions, doing a bunch more convolutions and concatenating them together. So we'll get to that in just one button. The question is, what is a one-by-one convolution? So a one-by-one combo regime will be a filter that has a width of one and a height of one. But remember that gap is matched to the input. So let's say the player input had a depth of 256. So it'd be basically a one-by-one by 256 filter, as you can think of it as combining across the features of cost. But the third dimension up here and protect, sir? Sorry, can you repeat the question? Yes. Perfect gases didn't ask. Here we're using combinations of different sizes were built. These produce different size feature maps. And then if they do have you concatenate them so well, adapt right now. So let's say that our total net receives an input from the past layer, that's 28 by 28 by 256. Alright? It does these convolutions. So the first question is, what's the size of the output of a one of the, when I say 128, I'm going to have 128 filters here. Each of them are one-by-one by the prior layer. So these are one by one by 256. These are 192 filters that are three by three by 256, etc. The output size of the 128 one-by-one convolutions. This is straight forward. We know that w l equals Wn minus Ws plus one plus two times pad. Alright? We're not going to pad here, right? So if it's no padding is just gonna be WN exercises one-by-one w at this one and we have a plus one, right? So the output size will also be 128 by 128. Sorry, I misspoke there. The output size of these one-by-one convolutions will have the same width and height. So 28 by 28, because there are 128 filters open, 28 by 28 by 120 years. Okay? So the output of these 128 one-by-one convolutions, it's gonna be 28 by 21, 22. Any questions there? Okay, so to be able to concatenate these tensors together, the size of these convolutions also have to be 28 by 28 and width and height. Otherwise I can stack them together. So the next question is, what pairing do we need to keep the output size consistent for the 192 filters that are three-by-three. Alright? And we know that if we set hat equals one, right? We said add equals one. Then at the output here will have the width and the height. So 28 by 28 by one. So basically what we do in Go on that is that for these convolutions was set equal one for the three by threes, and then paddy equals two for the five-by-five. And that guarantees that all of them have a width and the height of 28 by 28. And therefore, we can go ahead and concatenate them all together. There's also one more thing which is a three by three fourths of a three-by-three pool is not that the pole shifts is overlapping and the overlap is chosen so that the feature at the output of a three-by-three pause until 28. 28. Then the depth of the three-by-three pool will be 256 because it's three-by-three pool will be applied to every single every single matrix in this 3D text or every 28 by 28 matrix. Any questions on the sizing of any of these? Great, yeah, so Jenna's question is when we concatenate with the candidate or the third dimension, That's exactly right. So we just take all of these features or which we have 128 from the one-by-one convolutions, one entity from the three-by-three of sector and we just stack them. And so one really involved with the textbook. So this will be 28 by 28 by 672. Any questions there? Does anyone see something concerning there? Perfect. Yeah. So the student says, we just keep doing this. It's going to grow and grow. You'll notice that the output of this concatenation is never going, that the depth is never going to be smaller than the input. Because this three-by-three pool will always have a depth of that matches the input. And so if an addition to this, I'm concatenating other features. This number after the concatenation is always going to be bigger than 256. Or the emphasis mentioned. The question is, why are the dimensions of the three-by-three for the width and the height, the same as the input. Yeah, so this is traded at such a number which I can get off the top of my head such that the app is 25, 28. So it's not strike it at three, but it's treated as either one or two. Alright? So you're going to address this problem in just a few slides. But before that, I wanna do one more. I want to do one quick calculation because it's going to relate to how we fix this problem and how the author reduce the computational expense of people max. So the question is, how many operations are there in this inception layer? This will be how many operations are there in these combinations? We've done this calculation many times, and so I'm gonna do this quite quickly. But stop me if you have a question. So let's look at the one-by-one convolution. So for the one-by-one convolution, we're going to ignore biases. Number of operations for just the one-by-one convolutions is the number of output neurons I have, which is 28 by 28 by 12828 times 28 times 128. This is the number of outputs, output neurons. And then this is times the number of operations it takes to compute the value of one output neuron. So one output neuron is the result of a one-by-one compensation. And this one by one convolution has a depth of 256. And so the number of operations here will be one by, one by 256, the filter size. Any questions on that calculation? All right, so then let's do another one. For the three-by-three Kong. Number of operations will be the number of output neurons, 28 times, 28 times 192 times the number of operations to calculate one output neurons activation, that'll be three times, three times 256, the size of each of these three-by-three filters. So I go ahead and calculate that number for the comp layers, the one-by-one to see if I can multiply by five comparable things. Then the total number of operations in this particular inception module, we've drawn out, it's going to be 864. No, Ian, any questions there? Okay, so here comes one of the new innovations of glucose. So remember we have two goals. One goal is that we want to make sure that when we concatenate the outputs of these, that this number is to just continually drilling. Alright? And then the other goal is to reduce computational expense. So what we're going to do is we're going to insert additional complications. So what we're going to do is we're going to have a new block. I'm going to call this a block with 64 filters, each of them doing one-by-one convolutions. And if I put this block somewhere in this computing chain, right? Then what we're going to do is we're going to effectively decrease the number of computations. And it will also be a way for us to decrease the number of features if we so desired. So I'm gonna give you two options. Just looking at the three-by-three convolutions, I'm going to tell you that we have the option of placing these 64 one-by-one convolutions after the three by three ones. Or I could put this filter in purple right before the three by three ones. I might have misspoke oranges. I put the 64 one-by-one convolutions after the three-by-three filters. And then purple is I put it before. Okay? So let's just worry right now about the question of which one is better from a computational expense perspective. So I want you to take 30 s to a minute to think about. Is orange or purple better in terms of reducing the number of operations in this backward. And feel free to talk to your neighbor. Alright, excellent, good discussion. This is not an easy question to answer in 1 min, so don't worry if you're right or wrong, but I'm curious who thinks it's better to put these one-by-one convolutions before the three-by-three? How about after? Okay. I would say more answered that. It's better to put it before and that is correct in this case. So let's see why this is. So what we wanna do is we want to calculate the number of operations. In the case that we put these 64 one-by-one convolutions before versus after and see which number is smaller. So we put it before. Then. The output of the 64 one-by-one convolutions will be 28 by 28 by 64 because we only have 64 filters. And then it'll be times the number of operations that it takes to calculate one output. And that's the size of this filter, which is one-by-one by the depth of the input, which is typically six times one by one by 256. And add to that, we then add the number of operations for this 192, three-by-three convolutions. And so that will be the output of the output size of this filter, which would be 28 by 28, by 192 times number of operations per activation, which will be three by three times the filter times the input depth, which is 64. So it's gonna be three times, three times 64. Any questions? Alright, let's do the after one step we put it in after the number of operations is going to be the output size, 28 by 28, we do the three-by-three convolutions first, so the output is 192. And then each of these is three times, three times 256 operations per neuron. And then we have the one-by-one convolutions. So the output will be 28 by 28 by the depths of those one by one convolution is 64 times one times, one times the depth of the entire layer, which will be when added to it, because that's the output of this three-by-three convolutions. Want me to review or go over either. Alright, cool. So if you look at these two numbers, Let's look at this 128 by 28 by one. I did save that here, and that's also here. We have a three by three by 64, gets a three by three by 256. And so we can see that if I just compare these two numbers, this one is smaller by a factor of four. Then if we look at these two numbers, 28 by 28 by 64 times one by one by 256 here, and one by one by 192 here. This number is bigger than one, I need to, but it's not for x bigger. Alright? So some of these numbers is smaller than the sum of these numbers. And therefore putting these 64 one-by-one convolutions before the three by threes results in fewer number of operations and then less compensation. So Daniel's question is, in this case the output filter max are still 28 by 28 by one by six sounding too. And remember the, the key problem here is that the three-by-three pool, it's always going to have an unmatched. In addition to putting these 64 one-by-one convolutions in front of the three-by-three or five-by-five. I'll put us the one-by-one convolutions to after the polls reduce the staff from typically 60, 64. Possible, in this case, the concatenation, it will still be larger than the conflict, but now it is possible to choose the number of doctors so that the concatenation is smaller than in this case if we were to calculate all of these. And I saw right before lecture that I have a typo here. This is 28 by 21, 28. If you were to do these operations. This case, again, the concatenated outputs on larger depth. But I could have reduced the number of filters to make that number smaller if I wanted. All right, I want to pause and ask if there are any questions. Great. The question is, where do these numbers but 192.96. Those are arbitrary hyperparameters that we get this. So we can make this smaller. Alright, so we have said that putting these 64 one-by-one convolutions before the three-by-three or five-by-five should decrease the number of operations. If you go ahead and do this calculation, which I've read snappier, then the number of operations in this inception module is now 271 million. And that's about a four times reduction from before we had the one-by-one convolutions where it was 870 million or something like that. So these one-by-one convolutions reduce the amount of computation by a factor of three to four. Alright, question for you all. Isn't there a concern that we are going to lose the information through these one-by-one convolutions. Basically, I see a bunch of students thought and gas jet is thought to be a difficult question. We definitely lose information because by doing these one-by-one convolutions, we're taking information that was across 256, 28 by 28 matrices that we're compressing it down to 64, right? So there is a very real concern that whenever we do these one-by-one convolutions, we won't be losing information. That may be now this three-by-three convolutions couldn't pick out because that information is gone after these one-by-one convolutions. So that is a concern. But interiorly. And again, this is one of those things where I say the empirical results to make the argument. The performance of these networks is still, it doesn't degrade. And it doesn't degrade. Meaning that this tensor, which was 28, 28 by 256, can really be dimensionality reduced. It can be squeezed down into 28 by 28 by 64 tensor and still contain the relevant information that will be necessary for performing classification. Alright? So, yes, information is lost when I put in these one-by-one convolutions, but empirically still do well with the additional benefit that would reduce the amount of computation. Yes. So the question is, why don't we just do three-by-three convolutions and have 64 and then e.g. right. Yeah. So there is one additional benefit of having this one by one convolution here, which is that we get an additional layer of nonlinearity will be given additional layer in a neural network. So this actually counts as a two layer inception module. And because there's another linear followed by ReLu transformation that also empirically helps to increase performance. The question is, how do we change the size of the feature map? Because we always have a one-by-one convolution. So in the Google net architecture, what they'll do is they'll have these 28 by 28 by hover feature depth layers at the output of the instruction module. And then they'll reduce the width and the height by using that spores. The spores will decimate by a factor of two every single time they happened. Is it weird to think of the inception module as a mini auto encoder. In each layer. I see where you're getting with that Daniel, that. So I wouldn't say it's on an autoencoder because it's not trying to reproduce the layer input, but it is a dimensionality reduction, which we also know is that picture of an autoencoder. Intuition of reducing information, squeezing it into the 64 is correct. Alright, so that is the inception module. And this is a figure from the Google Map paper showing through the entire architecture where you can see I've had to shrink it to fit it in this slide. But basically each of these things where you see like these rows of convolutions and the max pool. These are all your different inception layers. And many of them are something that are very deep neural network. Each instruction layer. This is the overall architecture in terms of the order of operations. So the column max pool, comp max pool, and then after that they have stacks up in sections, followed by next calls. Every single one of these inception layers. To write the depth of two comes from having these additional one-by-one convolutions. And so when you sum up then all of these inception layers and convolutional layers, you get that this is overall either 16 or in 19 bear neural network. Let me go to my fireflies, I tell you the right number. Oh sorry, 22 layer neural network for people. Then there's one more feature about That's interesting, which is, if you'll recall, when we talked about VGG at 100 million of VGG nets parameters, we're in the fully connected layers. At the output. Gets rid of those fully-connected layers entirely. So it has columns and max pools. And then you'll see at the end, all they have is they have a linear layer. This is necessary to get from your features to your 1000s softmax scores for each of the 1,000 classes and ImageNet. So that's just unnecessary linear layers to transform it to softmax scores. But aside from that, there are no other additional fully connected neural network layers here, but that's also probably kept the number of significant B since there's no FCS kick. Yeah, great. So just saw that there are three softmaxes here. Yeah. Yeah. That's a really astute observation. We're going to talk about that on the next slide. Yeah. Any other questions on the architecture here? Yeah. The question is in the max pool is bear, you mean within the interception module? Is there padding? Almost surely, yes. So I don't I don't know the stride and the cutting off the top of my head. So I'm going to ask rocks at their tongue what it says, calculate or look up in the paper what the straight into the setting of the players are. So if the infection Any other questions? Yeah. Yeah. The question is, what's the difference between average pooling and max pooling on in terms of like what might give better performance. I don't have a good answer for you. I'm pretty sure that in this case they probably just tested both in one byte for better performance. And the question is, why is Trump got only applied to one layer? As opposed to all layers usually applied on fully connected layers. And so that's why it's only applied on this last linear layer at the antipsychotics like a fully connected thing without the, without the activation. Other questions? We're going to answer Jake's question next switches. What are these additional softmax classifiers here? So we're going to discuss a problem with Google Maps that lead to them having auxiliary softmax classifiers. And that will ultimately be solved by our next and last Architecture top advisor. So this is the architecture of BubbleNet where I've reduced the inception of layers to these yellow blocks. And this two x means that there are two of the assumption layers, vaccines, there are five. Alright? Then I just also want to point out again that I'll extend them. Vgg net had many more estimators here, which is why they have so many more parameters and go that only has linear layer to the soft-max. That's why it has so many fewer parameters in this architecture. This architecture, this architecture has 22 layers. And what that means is that at the output, I'm going to calculate a loss. I'm going to call this loss L two for now. And how to update the parameters of my convolutional filters and session layers which include convolutions. I need to backpropagate through 22 layers. Right? I mean, we know that there's this fundamental problem of deep learning. Where would I do repeated iterations of my chain rule across 22 layers, grab my gradients, might vanish or explode. And therefore the gradients when I get my stayed at the activations in layer of wine were called H1. If I were to compute d, L2, the loss DH one, or this one could have exploded or vanished, right? And therefore, my weight updates and these earlier layers might not be good updates. And if the early layers aren't identifying the features, I'm never going to get good classification. Alright? So in some, because there are so many layers and backpropagated gradients. When I back propagate all the way to the start, can be very important gradients. And therefore, there will be no learning that happens in this architecture. Google Maps solve this by adding two auxiliary classifiers. So what they do, what they did is they took the layer 11 activity. Let's just pull that activity H 11. They took the layer 17 activity, we'll call that age 17. And then took the HL7 activity. And they put that through a couple of fully connected layers and then to a softmax. So this would give a loss L one. This loss, remember we called L2, I'm going to call this loss L. What this means is that when I back propagate, I don't just have a backpropagation through L2, right? But I'm now also going to backpropagate from L1. And when I do that, I'm gonna have to calculate a gradient at this point. We'll call this gradient D01. D. Alright? But now the gradient from L1 only has to pop back, propagate through 11 more layers. Alright, so what's 22? And therefore, the gradients from this pathway will lead to reasonable signals or reasonable gradients, sorry, to update the weights of these convolutional filters. So that by layer h 11, these features are good enough to perform your ImageNet task, right? Same thing for a 17. So you can think of this as reducing the effective depth of the number because it's saying by later HE, but then you have to have good enough features to do my task. Also by their age 17, you have to have good enough to just do my task. And then of course, throughout the entire network should be able to solve any questions on that. The question is, doesn't this add a lot of parameters? It does especially because there are these Fc next. However, when they then do inference, so when you want to actually make a classification, you don't need these auxiliary classifier. So this is only in the training process. But then our last, our final architecture or the final architecture they submitted for the competition is just the sector path. Great question is, how are the losses for L1 and L2 and L3 calculated? All of these are cross entropy loss. So these will be the same cross-entropy loss for by 1,000 classes of ImageNet. Just Try to decode it from just trying to classify from h 11 versus age 17 versus the entire network through losses. Are you aware that the question is, do we put equal weights to all the walk or do we wait L2 more than L1 and L3, I believe in the paper. They waited them equally with 0.3? Yes. Yes. That fingers from seven. Decent enough. It's a tomboy. Question is, if you have a good loss over there, 17, why do I need the additional five raters? So ignore this point for now. Because it says that it had a minor effect. But it could be that your loss after these five layers is less than the loss from the upper layer 17 activity. So that is the congregations in these layers can still be extracting out better features to make L2 and L3. You are using. Explicit function. Says In general, L2 should always be less than L3 because L2 gets additional parameters to reduce the loss. And that intuition is correct, although we're going to see that practically it doesn't always happen later on. So I can see that two events ingredients. But I would say, that's a great question. So the question is, this student says, I can see how this would tackle vanishing gradients, but has tackled exploding gradients. That's a really great point because if I backpropagated here, I say D L2, d h 11 by say also backpropagated through this path to get a deal three, d h 11, right? The total gradient at age 11 will be the sum of these three paths. If we have exploding gradients, then this thing is going to dominate, right? I'm just going to be a really big gradients that won't help learning. So really this type of approach helps with the vanishing gradients because in the case that they banished, this goes to zero, but we thought had good gradient signal from L1 and L3. So then the question becomes, well then how do we deal with exploding gradients? So dealing with exploding, I'm usually not as bad because we can use the gradient to things that we talked about in the prior lecture to always make sure that our critics have no norm bigger than whatever we said. That hyperparameter to. Your question is, could you actually use this as an ensemble to increase performance? You could. But then remember the ensembling works best when the predictions are independent. And in this case, because the predictions are coming from literally like the same pattern of network, there'll be more correlated. So ensembling is less likely to be beneficial. And given that they didn't ensemble trees, I'm not pretty sure it didn't help much. If at all. Other cups, keys. Yeah. Yeah. That's a good point. Yeah. So the student is saying that it's trying to reduce the computational expense by not using fully-connected layers, but they do have them in these auxiliary networks and so on. I want this increased training time and the answer is yes. Well, I've included them because I'm presuming that without these additional effects innovators, they didn't need this for computational capacity to do better. Their final numbers, they don't report these layers, but you're right that they are there in China and they will increase competition with other questions. Alright, so that is pupil that later work showed that these auxiliary classifiers minor effect and you only needed one of them. I'm presuming that means that initialization, alright? And then like I mentioned, these auxiliary classifiers are discarded and France, so when they actually did ImageNet competition, they only use the central path. Other details that was trained with stochastic gradient descent with momentum. The momentum hyperparameter was there a 0.9? They decrease the learning rate by annealing it by 4% every eight passes through the training data. They used 144 prompts per image, as well as other data augmentation operations. And then when they actually Competed. They trained setting to go next and averaged the results of this happening, bottlenecks or ensemble the results of the seven Google Maps. They did not use GPUs to train this network. And I have four or less do for fewer parameters as we know, then Alex and epigenetic because they don't have this fully connected layers and they want Initiative in 2014. Right there. That's my last slide on B woman. So any questions about people in general before we go to our last CNN architecture? Right? Is that a great? Yeah, The question is, this bottleneck outperformed VGG net with far fewer parameters? And is that a theme of trying to find architectures that are more parameter efficient and get higher performance. There are definitely groups working in that area. It turns out that after the next architecture we're talking about guys, that's there. We're than people who are combining the idea of resonance with Inception and those were running ImageNet competition is for years, but really the more modern day. The key architectures, even in computer vision, you use transformers and those are, those are pretty parameter unhappy. Other questions? Yeah, Tom, I ask these people inspired by minutes. I believe I believe that's why they capitalize this out. For that, not looking at Jordan says just appointment. Alright. Other last questions on diplomat. Alright, let's get into resonant then. So restaurant will be the last major architectural innovation that when you talk about this being is really important. Even when you look at architectures like GPT, right? They will make use of residual connections because this really helps you to be able to take neural networks that are on the order of 20 and go to hundreds of letters. Resonate. Won ImageNet by getting 3.5 to 7% error rate. Remember that human level performance is like a 5% error rate. So this performance in 2015 was super cute. Alright, so here's the motivation for resume. So far, was that going 2012-2014? As long as you look when you look at the ImageNet winners, AlexNet was eight layers, VGG net was 16 or 19 layers, that was 22 layers. So why not just keep adding layers? I should be able to get better and better performance. Alright. So this is a figure from climbing host paper on the resident. He's the one who first author on the rest of that paper. And what he did is he just took a convolutional neural network, one that had 20 liters and one that had 56 layers. Each paint them on condition x. And these are the training errors and the errors that he observed. Alright. So let's just start off by looking at this industry. Very oxygen. So why does this book odd to us? Yes. Yeah. So Daniel says a 56 layer neural network has many more parameters than the 20 layer neural network. So at least forget about test error for now, but look at the training error, right? If you have a higher capacity network, if you have more parameters, different training error should be less than your testing error. That is, you should be able to overfit the data better when you have more knobs to turn your 56 layer neural network. So for training error, I would have expected to see this red line lower than the yellow line. So what we can tell from this is that the 56 layer neural number isn't even capable of overfitting to the training set. In fact, because it does worse than the 20 layer neural network and training error, right? This is actually a hint to us that relative to the, relative to the 20 layer neural network is actually underfitting. Alright? It's doing worse on the training data. And then neural network with far fewer parameters. Any questions there? Alright, so this result is not intuitive. It's already non-intuitive that I did not work with. More parameters would achieve worse training error and then network with fewer parameters. Let's try to extend that a bit more. This result of this bone intuitive and asked me why for a more mathematical argument for why a 56 layer neural network should always be at least able to mask the performance of the 20 layer neural network perfectly down to 20 layer network inside of it. So let me write that down in words. Let's say we had a 56 layer neural network. One way that I can have the 56 layer neural networks match the performance, but 21 layer neural network is that I could copy the parameters of a 20 layer neural network. And then we know that each of these neural networks can be implemented some pretty complex functions, but they can also implement the identity. So after I copy the grounds of the 20 layer network, I can set the remaining 36 layers to implement the identity. And in this fashion, I could've already, just by hand design, designed to 56 layer neural network that's better than the optimized one using the results of this 20 layer neural network linear systems. So tomboy says this is not a linear system. So how could you concatenate blocks and expect it to do that? So I'm going to do there. So the first 20 m don't have to be linear, but the remaining 30, 60 happens infinite money. So you're right, and I guess surviving that are non-linear. Some implementing the identity isn't as simple as setting like a W2 equal i. We may have to tune the weights in a certain way to make it equal to identity. But we would also know for sure that a neural network layer could implement identity if the parameters were separate directory. So that's what we're saying here. The neural network has the capacity to implement the simplest transformation identity. And if it can do that, then the 56 layer neural network has to be at least better than a 20-year corona. Yeah. Oh, great. I'm assuming it's asking we're making an inference based on two points to 20.56. Why not show more numbers in between? Yeah. It would have been nicer if they're stuck waiting for more than just two points. But this is a general trend if you were to increase the D6. So they did, do some experiments later, like 100 something, it would be even worse. Yeah. Great. The question is if we made like 22, 22 with an intuitive result, be still non-intuitive. Yes, I don t know which layer will have the minimum training error. Like, Yeah, you're ready to be 22-year, could be 27 and can be 33. All we know is that statistics is first in twice. Yeah. I'm sorry, can you repeat the question? Oh, yeah. Yeah. So the student is kind of extending out this idea by saying, what did we say in 202-120-2203 events, we see where the inflection point of this training error is. We can definitely do that. We would see is that yeah, there will be some inflection point. But the key thing here is that once you've got too many letters, it's relatively undercutting the content stuff. I'm always asking what do I mean by identity for these 36 layers? What I mean is that there's gonna be some neural network with 36 layers. And if its input is, you know, the 20th layer activity, the output would be the 20th activity Also after these three distinct layers. Without a miscarriage. Or just looking at the time, Let's go ahead and take a five-minute break. When we come back, we'll then talk about how we use this to design the next architecture of the resume. Existing research projects we support right now is like little slow. But what do we write about things that have to do the work for the exact makes sense. Like I write about my results, what I've done. Hi. All right. In fact, like I can't quite get time. 13 or I forget the exact answer. My guess is that it pops. My guess is because this topic Good question about your question about dropout. I was just wondering like, why is the accomplishment layers usually? I don't have a good answer beyond like, it's probably an empirical. So if you think about like dropped me as a parameter, tcp, like you're, you're setting, setting some values to zero. And I couldn't because of books already doesn't have that many parameters. And you can think of that as like really dramatically changing the output. I actually don't know 100% for sure. On convolutional layers doesn't help. So feel free to research guides. I'll do my desk if it That's not regular. But yeah, others might have. As you can maybe get five to the alpha, two humps, alerts, set of output neurons. But I'll look into it. Rate or the top five error rate. What's the path? Thanks. Alright everyone to look at bacteria. So rocks, you got the answer for the next school. Mismatch score is applied at a stride of one. And so that'll give us the same output size. Alright, coming back to this idea of going deeper, Are there any other questions on what we've said so far? Alright. So this idea that as long as the neural network could have implemented the identity, identity is transferred, identity transformation easily, excuse me. And colleagues came up with this architecture called the resume. Alright, so this is a quote from their paper. This degradation problem, meaning that as you add more layers, performance gets worse. Suggests that solvers might have difficulties in approximating identity mappings by multiple, non-linear there, right? That's saying, if it could, it could have implemented these 36 liters of the identity. Recent done. So at least it seems like neural networks don't learn identity transforms, right? So then what they do is they come up with this residual learning reformulation, alright, that I'm just a bit. But the residual learning reformulation makes it very easy for neural networks to learn identity mappings. Let me actually just write it out first. So let's write out the goal. The goal is to change the neural network architecture so that it is easy to learn the identity mapping. Alright, so if you imagine that for a standard neural network, you had an operation like the activations that layer I plus one are equal to ReLu times WH. The prior layer plus B. The net changes. This architecture is not exactly how it's implemented, but just to give you the idea, it's gonna be h i plus one equals each eye ray. We'll call this W tilde, the HI plus b Tilda, right? You can see that in the resume, it's a lot easier to learn the identity mapping because as long as these are close to zero, so w tilde and b tilde are small. Then h plus one is approximately HI. Whereas for this thing to learn an identity mapping, right? One way would be for w to approach a diagonal matrix that's close to the identity. Not exactly right because there's a bias and rabies. But in this case, learning identity is really easy if you just said w and b close to zero are any questions there. Alright, so that's the key idea which you can think of. This is changing the architecture so that it is easier for the neural network to learn the identity mapping. If the identity mappings happen to be optimal, then they're really easy to learn because all you have to do is drive the weights and the biases to zero. And they will implement the transformations. Any questions there, perfect discussing gets asked every year and I'm glad you caught it. So the student asked ReLu is non-negative. Therefore, doesn't h i plus one always become bigger than HI? And this is where I say, I not only have the exact, the exact resonant architected, this is just for intuition. In the exact resident architecture. They think it'll be easier if I just show it on the next slide. After the convolution, ReLu, then do another convolution so that this thing is non-negative. Then after that, you do your identity mapping, and then you have your ReLu, applied after this. You can think of that as if I were to write this out, there would be like maybe a W2 over here. This would be like a reactivation being like an a that goes into a ReLu and there'll be like a W2 Tilda here. That's how it's normally done, but I'm going to just erase that so it's not confusing to students. The question is, why are we finding as opposed to just dropping bears? You mean like I going from 56 layers back down to 20 layers. Yeah. So the idea is we know that with deeper neural networks, with more layers, performance should be increasing performance and keep getting better if we were able to train them. So because we have that prior, that deeper should be better, we see that keepers worse. But this is not because of a theoretical reason. Theoretically, this should always do better than 20. This is for a training reason. Then the solution is, but tried to pitch decks for any reason by making it easier to learn the identity. Answered the question. It's all grounded or optimize the network, decide how many years it needs to. Oh, I see. Tom ways saying, you can also think of this as, because it's so easy to learn the identity. Maybe you're in the case where for downstream layers the biases are zero. If they were, let's say the weights and biases of you must have a 1,000 layer number. And the last 100 layers have different biases or equal to zero. Now I tell you that our 900 layer neural network is his best. That is correct him like an intuitive sense, but I rarely does it actually play out in practice like that. Other questions, yeah. The question is our w tilde and V tilde going to zero, easier to learn because of the way we initialize them. So initialization will play a role, but it's not the only like e.g. there's usually weight regularization, right? That will cause weight decay on these. And it's in general, easier to just push these two twist. Is there ventilate some exact value like w equals identity? The question is, why not put the ReLu before? I believe it is this problem of wanting to avoid the activations just getting larger and larger with each successive layer. Alright, so let's see, I have a quote from this paper. All right, so you're probably thinking identity mappings are far from optimal and over a police officer, the court says, it is unlikely that identity mappings are optimal. Formulation may help to preconditioned the problem. At the optimal function is closer to an identity mapping and a zero mapping. It should be easier for the solver to find perturbations with reference to the identity, to learn the function as V1, alright, that's a lot of words. Learning the function has to be one means, learning to W and E to implement whatever function you want. What he's saying is an optimizer is easy to make w and b go to zero, right? And it's probably the case that it's more optimal to have an identity rather than a napping sits around. And therefore, you can think of this as an architecture that includes, or the prices would not work, find these mappings that are likely going to be better than a zero that is optimal. But this is something that helps to bias the architecture to at least have nappies that are closer to identity. And then based off of this prior intuition from the slide, hopefully lead to a better performing, right? Yeah, Tom, I said isn't as their own that people always bad. Exactly. You don't want your activation. So you got Is there a, alright? So with that, we take our normal architecture, which is taking some input or higher their activity you would normally have comparable on understanding the resident architecture, we do Combray, the comp review. And then you can look at the output here. If we call this f of x, this is x plus f of x. If we call this thing h of x, you can think of what the residual layers are doing, are learning. What is blood residue at the Baxter add to X to make the representation optimal. So if h of x, what's optimal, then f of x should equal h of x minus tax. And I believe I have that on the next slide. So h of x happens to be the optimal transformation here. The residual layer with just learn h of x minus x i e what to add to x to get that representation. Right? There's one more thing. So sometimes when I have a plus sign, right, the dimensions of x and f of x has to match to be able to add them together. And sometimes they don't. So in those cases, sometimes the linear layer or one-by-one convolutions that some padding to make it match here. All right, so we're working the assignment. You want to add anything up because its gradient and with that, I'll do on that whether Biden is helping or gradient, that's great. I'm going to say if you didn't hear you on for taking my question, don't worry. We're going to have several slides later on, so I'm going to preserve that until the later slides. When we asked why does that actually work? Alright, so. These are quotes from the paper that said our reaffirming what we've already started. Alright, so network architecture, the rest of the design goals of VGG net every single convolution or three-by-three filters. And remember, VGG had this thing where if the width and the height divided by two, then the number of filters would be doubled to the computational complexity is the same everywhere. Then the output has average pooling followed by 0 linear layer to get 1,000 softmax scores when it goes to a softmax. Then all of the layers that have this three-by-three convolutions become residual layers, which means that they add this identity connection over here, right? They perform dataset augmentation. They also use the Catholic gradient descent with momentum of 0.9. They have weight regularization on their parameters. And then whenever the way they yield the learning rate is whenever the learning rate plateaued and they decrease the learning rate by an order of magnitude. Any questions on any of those details? Always increasing. To me. The problem where the question is, whenever we decrease the width and the height doubled the number of filters, do we ever run into the problem where the high becomes so small? So we decide when the width and the height our app, because those are done through pooling layers. But these columns will always preserve the output width and height. So actually we can fill that all entirely if you wanted to hundred layers and we have to have it six times, I'll put the pool right approximately every 100/6 layers. Great. The question is, why did these networks not use adam or rabbit or more advanced stochastic gradient descent optimizers. Anyone have an answer for that? Wants you to say burning meant to do so as I believe Adam was 2014. So as I read, Adam did exist, but it actually took me on the date of atoms, but adding graduates 2011. So I think that definitely is empirical results. Empirical results. Yeah, that's true. It comes back to this slide that we had in the gradient descent lecture, where we start that momentum. This, this is the reason tends to bind this really shallow local optima. Where if I plot the loss and the weights, I can wiggle my waist quite a bit and still have the same loss. And so these local minima are believed to generalize better because they are robust to perturbations in weights. Whereas, because Adam can really quickly ticked on your learning rate if you take a large step, Adam and RMS prop and added grad can find more of these steep local optima and their steep, then you may not generalize as well. Because if, if, if a bit of points in your system, all of a sudden you're at a high loss location. So that's the general reason why it's believed that all of this works for why SGD plus momentum leads to better empirical results because it's finding these really flat. Question is why do we anneal the learning rate plateaus? Rather than doing a smooth drop-off and learning rate. We could do that also affected by decreased by 4% every 80 bucks. And so both of them are possible. Other questions, okay? So this is the ResNet architecture and may have to zoom in even more than you go in there. But you can see they have these three by three columns. And then you can see the skip connections that come for the input. And it goes all the way through these many convolutional layers. And then you end with a linear layer that takes you to 1,000 softmax scores. Alright? So I believe that this iteration might have been the 34 layer for us net. And when they went ahead and tried to train this architecture, what they noticed was the following. If you just take the left-hand side is just vanilla CNNs. We saw this before for 20.56 layers. Here they're comparing 18.34 layers. For vanilla CNN if you increase the number of layers. So the bold line is validation accuracy, validation error, and the non volt blindness is training error, I believe. But neither case with 34 layer error is worse than the error. That's the same problem we saw before. But now if you make these ResNet, CNN's. Then when you increase from 18 layers to 34 liters, now have better performance. So what should it have intuitively happened that is going deeper, there's a better performance. Actually funnily happens when we change the architecture to the resident architecture. This doesn't go on forever. So in this paper, and some students are asking you for video number six here. Yellow is 20, this blue is 32, the screen is 44, and the spread is 56. So they did have more of these plotted. So this was for vanilla CNNs. And we see that actually 20 was the best. And when we go to the rest net architecture, you can see that as you increase the number of layers, the error does get better. Alright? The black one is a 110 layer ResNet. They also said, how far can we take this? And so they trained a resume, but 1202 layers. And this, this did do worse than the 110 way or rise parents. So it's not that you get this for free and you can always make these arbitrarily large somewhere, 110-1202. Section point questions here. All right, so this is the empirical result that says raise nets work. Now the question becomes, why do rest? And that's after we work. So we have motivated it from this idea that identity nappies might be optimal for neural networks and so on. Make it easier to learn an identity mapping, then I should get better performance. And this is actually not the case. So what you should do is you should look at this neural network. Let's look at a 34 layer one year. And this is what Tom was pushing wasn't earlier. What do you notice about this network when you do backpropagation? Yeah. I can't say group as gracefully as Daniel, but he said, the goodies go look. Basically at every single one of these nodes. There's a plus sign. What do we know happens at a plus sign for backpropagation? A plus sign just passes through the gradients, right? So if I have my lost here, and this is like a D L, D h, Let's call it the 34th layer. I backpropagate through a plus sign and my gradient sustained change. So basically what I have here is a gradient highway. Suspect I won't encounter vanishing gradients anymore because through this plus sign, my gradients are always going to get all the way back to the very start. Alright, good. Alright. So basically the ResNet. So all this fundamental problem of deep learning, because the residual skip connections or a gradient phi way that backpropagate accurate gradients from the end of your network all the way to the startup. Haven't proven to you that that's the answer, but that is the answer question. Jenna's question, is there a few times skip connections. Are they different from somebody? They probably are. I don't know off the top of my head. So TAs to do but there are questions. The question is, are the gradients pass through essentially similar or exactly similar? It depends on if these skip connections have any weights on them. So they do have weights them, there'll be, there'll be a multiplication by those grapes. But the ones that are just purely the identity the gradient passes through exactly. Question is, will there be any chance of exploding gradients? The answer is yes. So we would still want, if you see that the gradients, the questions, You spoke a little bit but they did, it would be similar. Then he get the same thing that I wanted to give you something. So you're saying Can we just tap out many auxiliary next year and I would help with starting as well. You wouldn't have a similar effect. It could, but you would probably have to put in several auxiliary members. Remember that those auxiliary verbs also add more parameters versus a very clean way to get the gradient from the foot. So let me now show you at least one study. This is believed everywhere, everywhere believes, everyone believes that work because you get the gradient highway. One study. And this was by this person Larson, who in 2017, when people are still debating this thought of a really elegant experiments, I think the reason that this work is because they give you this bridging highway that reduces the effective. Alright. And so what he did is he thought of this idea called scrap. This is just an experiment to test out the effective depth or the identity transformation that matters where this is an eight layer neural network. But then he also gives a four layer path, a to air path and a one layer. So even though this is an eight layer neural network, it's effective depth is because there is a one layer path, right? Just effectively shallow, even though there are eight layers. That's kinda like Resonate where even though there are 34 layers here, if all of these projects and there'll be effectively one pass-through of ingredients all the way from the output to the input. And what he showed is if you just take this fractal net architecture and you run the same exact experiments that resume, which is that you make a neural network with 510, 2040. So on the left is the vanilla CNNs. For the vanilla CNNs, we see the same problem that resonance saw, which is red is five and bluest 4D and green is 20. So when I went 20-40 layers, my training loss worse. That's the problem that we've seen. Fractal net going 5-10 to 20 to 40, we see that the training loss always gets better. As long as this fractal that you would add more layers. And so this 40 layer franklin left. Again, there's a path that has 40 convolutions, but there's also a path that has one copy of. This is a proof that this is an illustration. Having a pathway that reduces the effective depth of your network leads to solving this problem where where when you make the neural networks deeper, the training loss gets worse. Just having defective, shortened falls, this any questions better? Alright, so that is the last of the CNN architectures that we'll talk about in this class. There are architectures I came in 2016, 2017, 2018 for the ImageNet competition. But there were basically iterations on these same ideas. Here's an example. In 2017, this architecture inception V4 did very well. And you can see that what inception V4 is, is that it's an inception module with a residual connection, right? So after the resume, there were some architectural innovations, but they were iterations on similar ideas that we've talked about. Alright, I've got some summary plots here that show you the relationship of algorithms to another. So in this particular plot, x-axis is the number of operations. Y-axis is performance, or higher is better. And then the size of this circle isn't number of parameters. So you can see VGG nets are sitting here a large circle, so many parameters. Whereas inception before even the resonance is kept far fewer parameters and get better performance in pure operation's survey of convolutional neural networks. One is that architecture can play a key role in the performance of the network, especially for the resume, the architecture is important to get that gradient find ways that you can train. There is evidenced in general, having deeper and wider neural networks result in better performance for CNNs going deeper. So a point by this, I mean, we saw that you could make but at 22 layers, but after that, performance difficulties, no numbers but then the resonate, confess that problem, then it appears for the rest. Next, one important thing that plays a role is the effective depth of the network. That is, to avoid the problem of the fundamental problem of deep learning, which is exploding and vanishing gradients. You want to reduce the effective depth of the network so that they're ingredient pass from the output to the input layers and they get a bit until there's clear. Any questions there or across our entire survey on CNN. Today's date, it might not be something that's say by this thing. Right direction. It's asking that we've been talking about in the context of computer vision. You could be in different applications. Wireless signals, where there are a few questions. One is convolution still the best operation to use? And if so, how do I decide those architectures? I don't have a good answer for that and I don't think, I think there will be specific for each and every dataset. So e.g. the project, if you're doing the default class project is on data. And if you look at some of the papers that we cited, one thing that we want to is often seen as useful feature data is to look at the power in different frequency bands. So at the very end of art, project guidelines cited paper called EEG net. And they do convolutional neural networks. But one convolution is temporal, right? And we know that a temporal convolution basically either a low pass band pass or high pass filter. So it's extracting out different frequency information from the signal. So basically I'll say is domain specific depends on the exact characteristics of other questions. All right? Yeah. The question is, if you have more skip connections does help in performance. Almost certainly someone has done this experiment. I just don't know off the top my head. Okay. Okay, great. Yeah. So there are many iterations beyond resume. And I'll leave that up to a literature search and research. Other questions here. Alright, so with that, we are going to get into the next topic, which is recurrent neural networks. So basically I think in the 20 min that we have, we'll just be able to motivate why we would want to look at the current neural networks. On Wednesday, we'll have a deep-learning library sector from the OH, minus and then, and then we'll be finished. Recurrent neural network architecture is a week from now on one day, right? Okay, so why did they turn real numbers? So we have talked a lot about fully connected neural networks and convolutional neural networks. But one thing that you'll notice is that they don't have recurrent connectivity. What do I mean by returning connectivity? Essentially, I just mean feedback connection. So if I have a fully connected neural network like this, all the connections just go from layer to layer I minus one. And we never have connections from a downstream layer to layer in the neuron in the previous layer. So when I say recurrent connections, I mean feedback connections like these orange ones. And then I wrote the sentence, parents have no dynamics. You probably don't understand what that means that I will talk about them in just a few slides. We'll talk about the impact of what these are connections. These feedback connections that then leads to the network. What they, what they, what they mean for the network. Recurrent neural networks through language modeling. I know that most of you know today that the best language models include transformers which are not recurrence. So in a sense, motivation is limited in that while it does motivate recurrent neural networks, there are before obeys to do even better, I think was falling. But still, it'll get across the idea that returns could be important in particular. So let's look at the following problem. Let's say that we're training a neural network language setting to do character prediction. So I'm kinda character prediction. All right, What we're gonna do is we're going to represent this with a 2060 vector. So this is going to be a 26 D vector. And the letter t will just be a one-hot vector where. The number in the alphabet were key occurs, we have a one and then everywhere else. It's, that's my input representation of characters. One-hot vectors of length 26. Alright? And our goal is to do text generation. So what we wanna do is we want to pass this letter t into a convolutional neural network. And I want it to be able to make a reasonable prediction as what the next letter, alright, so we know that in the alphabet, right? You have a t. Like we next letters are like e.g. bowel. So like maybe a has a probability of 0.15. H occurs pretty frequently. It's like THE, so maybe Hs like with probability 0.3, then maybe, you know, Z has probability 0.01 because there are very few instances in English, where's the fall is to do? Okay, So that's our setting. We take the inputs will be one character. There's one-hot representation of t. We want to output a distribution over what are the likely next characters. So this is gonna be a softmax distribution over the 26 letters in the alphabet. Then I want to hear, then I'll put the character that has the highest probability. Maybe it's h in this case, or maybe I don't even want to have the highest probability, I just want to from this distribution. So here's a question for you. Consider that we've typed two strings so far. One of them is th, and the other is th. Th. Alright? My question is, if you were to be in the setting where the prediction is done by CNN, will the next letter predicted, which in the first case you apply the a vowel, right? Like e. Or a. Prediction in the first case, differ at all from the prediction in the second case. So in the second case, you know, maybe it should be a key that follows to start the work. So just think about that for 20 s, feel free to talk to your neighbor and then I'll ask someone to give an answer. I guess I'm wondering as I can and tell me the answer would be the same. And why is that? Right? Exactly. In both cases, the CNN excuse me, exact same input. We know if it's the exact same in a CNN is always going to output the same softmax distribution over those 26 characters. Alright, so it would be very likely that for this particular problem setting, the CNN would ever put a vowel after THE, which is not what we want. Okay. Great. The question is, could we not just go ahead and concatenate instead of just giving you one letter H, give it like several letters in the past e.g. I'm sorry. Yeah. Exactly. So the question is like, why can't I just give it, you know, instead of just the last letter, the last ten or 20 letters. And the answer is you can do that. And that will be what transformers do to solve this problem. So I do have a slide where we say that one of the ways that we could take into account the fact that the outputs after th, th, th, th should be different is that instead of just giving the most recent factor, would you give it a history of characters? And this is one way to solve the problem. This is how transformers do it there. And that's all well and good. But then this also leads us to another way to do it, which is through been recurrent neural networks. And so this gets us to this idea of states. Alright? So a CNN being feedforward and having a deterministic computations is always going to produce the same output given the same irrespective of what happened in the past, right? So because the input here is h and that's all the CN MCs. It has no representation that preceding the H could have been very different things in these two scenarios, right? In some cases is a totally fine thing, e.g. for classifying images. I just want to know if an image is a dog or a cat that doesn't have any temporal structure. But in anything of that. Micro structure by classifying videos are generated. What happened in the past matters for your prediction. So at this point we need a new construction. And the way that recurrent neural networks do that is through this variable called state. Alright, and so we're going to expand on this in just a bit. But you can think of State is doing, is capturing and a single variable. What has happened in the past? Let's say that I wanted to infer some output z of t from inputs x one to x of t. Again, the first way to do it would be to just pass an entire history of conflict. And again, you can do this. One potential quantities is that they're going to many more parameters because you need different parameters to essentially combine all of your inputs in the past. Alright? And then there's also this question of which inputs are important. And so if you get to a transformer architecture and that's where this thing called the attention comes in. And again, it's something specific to those transformers. So that's one way to do it. The other way to do it is to have recurrence. And so in a recurrent neural network, we introduced this variable called state. And the state is going to be influenced by past input. So this is input values as well as the current. Alright? And the way that we make the state to see is that we do something called a Markov assumption. And the Markov assumption essentially state the following. Everything I needed to know about my inputs up to time t is some function of everything I need to know about my inputs up until time t minus one and the current. Alright, so let me write this out. This S of t minus one, you just think of it as the following. This is a state variable that contains all of the relevant information about the inputs up to time. T minus one, the subscript care about, so it's about X1, all the way up to x t minus one to perform a task. Right? So to do my task, right, maybe it's rambling videos or text generation. I would have to keep a history of my inputs because now temporal structure matters. I'm going to summarize exponent all the way up to x t minus one into this one state variable S of t minus four, okay? And that's good. You know, X1 to X2 minus X1 contain a lot of information, but all, all of it is what's relevant to the task goes into f t minus one. And then I have this relationship that tells me how I can take the info about x one to x t minus one, represented through S t minus one and my current input x t. To now develop this state variable, which contains all the info, the relevant info about x1 to x t. So as long as I keep doing this recursion, basically remembers what I need to know from my historical inputs. And that gives me information to be able to then solve my task. So this is a nice formulation because now I don't have to remember all of my past inputs X1 to X2 and pass them in this network. But really all of my tasks and fluids are you just going to be summarized by this t variable, t minus one. Alright? So this is a compact representation of the information I need to know about my historical inputs. And then with this, this it will be dependent state of a recurrent neural network. With that I could do tasks. All right, Any questions there? Alright, so this is called the recurrent neural network when you incorporate the state variable. So it differs from a feedforward neural network because to be able to understand how t minus one t, we necessarily need to introduce the back connections. Let me give you a really simple example in linear case. So let's say that my relationship with that S of t plus one equals eight times S of T, right? Let's say that S of t with a 3D vector. So S of t was some three artificial neurons. Okay? So if I had three artificial neurons, so this would be S1, S2, and let me do a superscript one, superscript two, and f superscript three. And let's say that I had that a equals 123-45-6789, right? When I had this relationship, S of t plus one equals a sub t, What I'm saying is that if I had my S of t minus one over here, and this bent equal. S of T, that the value of my first neuron will be one times the value of my first neuron at the current time step plus two times the value of my second neuron, fire timestep, plus three times the value of my third neuron to fire to construct. And so we would draw this as these three values being a feedback connection with value one. The second neuron intuition, the first neuron with the value two, and then this connection with the value three. And you can see that if you were to draw this out, this neural network now has a bunch of leaves. So it's not paid for, but through these loops, it is able to store information about protest. Okay. Any questions back? Tom way to saying, are the values of these weights fixed? They don't change across time. And the vanilla recurrent neural networks and also LSTM architectures that we're talking about the arm. They won't change across time though, of course, change during the training process when you learn them. But they are static inference. Alright? So in biology, I'm just going to, just for the sake of time, I just want to get to a few examples. So I'm going to show you a few examples. And this comes from a nice blog that Andre cartography, but in 2015 called the unreasonable effectiveness of RNNs. And what he's doing is that same character prediction task that we talked about. Where in this case, just for simplicity, we will have four letters that we could output. And so instead of 2016 vectors afforded vector and the letters h, e, l. And the basic way to think of a return on that bridge is we'll start off by inputting our letter. So this is the letter H because this is a one-hot and the first element is one. And what that is going to do is that's going to go into my recurrent neural network. And this hidden layer is that state. So this hidden layer is that variable. And so this would be S at time one, S at the time to time three, time four. The inputs x at time one, exit time to X at time three. Time four. Actually, I think this should be 012.3. So what happens is that the input goes into the neural network and it modifies the state. And then it gives us an output distribution over the next characters. And so maybe after we put it in an h, we get an E. We then take that E and we put that into my Recurrent Neural Network again. But because my Recurrent Neural Network, my state is also updated to S1. State variable is not different than the past time step. And based off of this, I'll put an L, right? If I put in the L Now, it will output for this given states and other L. Notice here that hello, right? I'm going to put an L again. So the inputs here for X3 and X4 exactly the same. They're both but letter L. But because now the state of the recurrent neural network is different, the output can be dropped frames. So after the second L is the structure in the English language, it should output. Alright? So again, the way that the output of this L is another L, where L is 0. The difference is because the state of the recurrent neural network, it's also been updating all of these inputs, right? So if Andre called Papi, went ahead and he really simple vanilla recurrent neural network. I think he implemented this one in about 100 lines of code. And he started to train it to try to learn tax. And so here's Shakespeare. He did this exact task with just a simple RNN. And if you do 100 iterations where you're just trying to predict the next character. You get out your brush and as you change it, more and more, going all the way down to 2000 iterations, you can see up to thousand iterations as far as to do reasonable, even if you read this, it doesn't. Semantic meaning like, why do that day, right? But it does better to do quotations correctly. It does learn to capitalize proper pronouns. For even longer. It can start to output text that looks like Shakespeare. And so it is combining characters from different plays, right? But that's because it's been trained on all of our experiments trying to imitate what Shakespeare looks like. And again, some of this text is not really to intelligible, but sometimes when you read it, even like in the rhythm of it, you can kind of iambic pentameter. Alright, so that's Shakespeare. I'm highlighted here these lines because I think that these might be like more iambic pentameter likely to show the reigning of the raven and awards. You can turn it on Wikipedia texts and go learn how to do Wikipedia and it'll even do the citations correctly. Here is them training on law, tax law as groups for an algebraic geometry textbooks. And it generates texts such that if you were to just do a bit of debugging to fix the errors which are minimal. It actually generates fake algebraic geometry. And I really like this example because you know how in math textbooks sometimes though, save something as true and then they'll say like the proof is trivial or the proof is omitted because it's an extra assistance. They were never simple to me. If you zoom in on this, this neural network learns to also. We'll stop there and we'll come back on Monday to finish RNN. And it can lead to deep learning. 
All right. All right everyone, We're gonna get started for today. Can people hear me? Alright? Alright, it was a rainy treasures walk away over. One more. I am Jonathan cow. I am your instructor for this course and on behalf of our team of TAs tomboy, can shift to lie, shouldn't be, and remain who will introduce themselves later. We are really excited to be your instructors for this course, which is C1 47, 47 neural networks and deep learning. All right. Today we are going to be basically doing an intro into deep learning. We'll go over the syllabus and you have time. We'll begin lecture material. Alright? So high level of things before we begin. The first is that this is actually one of my favorite classes to teach. Feels really rewarding because as a result of this process sees so many students who come back to me and say that they found deep learning does for further research are useful as they went on to industry. So deep learning is a very relevant topic for many different applications areas today. And we hope that this class will be useful for most, if not all of you. This class will hopefully be fun in terms of, I know that for many of you this is required classes and elected at the same time as we'll talk in the syllabus, there will be quite a bit of work for this class. We'll discuss that with the syllabus, but please take that into account as you determine whether this process for it or not. And then if you've ever taken a class with me, you know that I like to keep things interactive. Lecture hall this bid. So please feel free to ask questions. I will be very happy to entertain your questions. And usually if you have a question, you're not the only one with that question and you ask them that will give me an opportunity to explain something once again. Alright. Any questions before we got one quick thing? So I think I already saw there are many students who are joining via Zoom. So if you think about the lecture, first off, this class will be reported by doing tasks. And then brewing costs will upload those bacteria to grow and learn. Use it the under a tab called UCLA media reserves. We'll send out an announcement with all of this information. Because this class has also taught for the MS Online students. We're going to have a live Zoom Room, which we'll share the contents of my iPad screen and also record the audio. One of our TAs can is going to be monitoring the chat. And so while we won't take any verbal questions from the Zoom room, will be verbal questions for those who are in-person in the classroom. If you have a question on lecture material and just put it in the chat and camera or one of the other TAs will get through any questions there. Alright, well get started. So if you've been following the news lately, you'll know that there's this new chat bot activity who here has interacted with just one half the class. Shattered beauty is created by OpenAI and it's a very powerful language model that can do some pretty remarkable things. And so last night I asked chat TBT. I would be best for me to motivate this class. So I said motivated course to find the corresponding enforce sentences. And this is what she did. She says, deep learning is a revolutionary field of AI that is driving many of the recent advances in machine learning. It has the potential to transform a wide range of industries and has already begun to do so in areas such as images, speech recognition, natural language processing, and machine translation. Uh, deep learning course will provide you with the skills and knowledge you need to understand and apply these powerful techniques to the real-world problems. Whether you are a beginner or experienced practitioner, a deep-learning course can help you take your careers conducts bubble. The rapidly growing field of AI. That's a pretty good introduction, might even be better than this and that I could write on this. And it actually comes pretty close to the intro I usually give for this class. But just to motivate the applications of deep learning and get you excited about how it can be applicable to the things that you're interested in. There's one thing here is that whether you're a beginner or an experienced practitioner. So we just decided to be an introduction to deep learning course. If you're an experienced practitioner student in this class, this class will be easy for you, so this should be fairly for beginners. Alright. I usually motivate this class is going to talk about its impact. And one way to talk about the impact of deep learning and neural networks is to point out that you have publicly today interacted with at least one, might be several neural networks. So starting off with, if you've done a Google search, we know that you're going to use it the page rank algorithm to show you the search results. However, Google also augments the page rank algorithm with other deep learning-based techniques, including rank brain as well as neural matching. Neural matching, e.g. isn't NLP based technique. And one example of how it is this sport is it tries to infer the meaning of what you're saying. A commonly given the sample is if you are searching for a boot on Google and you are, the less you're likely looking for footwear. But if you're in the UK, you're probably referring to the trunk of the farm. All right, So Google uses deep learning to augment their surface. And if you searched on Google today, if it's rapid with a neural network, you might also help me know that I do go, there are reports that there is a code red over there in response to chat TTT as to whether and how that threatens Google search engine. The search engine. You start to knock people if you do not get to today. And you've been sharing videos, suggested videos, those recommendations come from deep-learning based algorithms. In part. If you've been on Facebook today, facebook uses a variety of deep neural networks to cases that comes up. One is something called deep text. So here is a video of Facebook Messenger. Now, I don't use Facebook, so I'm just trusting that this is actually a Facebook Messenger. But if you say something like, I need a ride, then they'll give you a link that says request a ride. Right? But it's not just looking for the word ride and showing you that week whenever you save, right? Because there are many sentences. Include the word ride, but don't need you to request a ride like this one here that says I don't either I or this one below that says, I like to write it down. Alright, so within this speech processing and NLP road, neural networks are really useful for trying to understand sentiment or even the meaning of what they're trying to compare. In addition to this, better uses something called deep base. And so here are two pictures of Emilia Clarke, but one, she's dressed up as dinars target area. And even though the therapists are very different, face, which is metaphase algorithm is able to verify that these are facts, the same person and even more from the images alone, they can deduce many things, including the emotion conveyed and can even guess the age share. So share is predicted to be 31. And I think they say that they're using accurate plus or minus five years. In fact, Image Recognition, Computer Vision was one of the key fields that really helped to spur a revolution in deep learning that we've been experiencing for the past decade. So this is a video of a professor at Stanford talking about the impact of Computer Vision and massive data from ImageNet and the modern CPUs and GPUs to train such a humongous model. The convolution massive data from ImageNet and the modern CPUs and GPUs in such a humongous model. Let me just see if I can find it probably is. I'll just try to hold the mike really first. Raise your hand. And the faculty had trouble getting massive data from ImageNet and the modern CPUs and GPUs to train such a humongous model, convolutional neural network blossomed in a way that no one expected. It became the winning architectures to generate exciting new results in object recognition. This is a computer telling us this picture contains a cat eye where the cat is. Of course you are more things that cats. So here's a computer algorithm telling us the picture contains a boy and a teddy bear, a dog, a person in a small cavity in the background. Or picture of very busy things like a man, the skateboard, railings and lampposts and so on. Sometimes when the computer is not so confident about what is C's. We have taught it to be smart enough to give us a safe answer instead of committing too much, just like we would do. But other times, our computer algorithm is remarkable at telling us what exactly the objects are like the make model year of the car. All right, any questions on this example of the make model year, the car is pretty remarkable because we'll notice a few things. First, all these cards are different sizes, right? So let me backtrack. If I asked you, how would you identify that a car? You might say things like car is relatively large compared to other things in the scene. Or you might say, a car has four wheels, right? But then in this image, the cars can be at different depths and so you don't want them the same scene. There are different sizes. And then this for this vehicle, only two are visible. And actually this image is cut off. And so this top vehicle, only one is visible. You and I all know that that's a car. But if you're just trying to hand craft an explanation for the features that tell us what a car is. Might be a bit harder to put that into words because these images are all cars. Have four questions here. Alright. So beyond those examples, if you've used recently are today, to try to determine what's the page. You'll know that York has a section called popular dishes where they show you things that are popular to order at the restaurant. That includes deep learning, which identifies the images. And what I didn't say, we're going to write that the image is coming from the different reviewers are of the same object. Will also use deep learning to parse the reviews of the users who uploaded those images to determine their sentiments and evaluate, was this a positive review or was this a negative review? And use that to figure out what the popular dishes. If you've interacted with Alexa or Siri or Google, you have interacted with the neural network because the speech processing algorithms on these all rely on neural networks. If you use Twitter, than the Twitter spine line would have determined to show you is based on deep learning. Lastly, if you have lost something, a lot of these recommendation systems today also use neural networks. And there are algorithms that you might be most likely, very likely you have interacted with the neural network today. And that's just the ubiquity of deep learning. Just in our everyday lives. Beyond that, deep learning is able to achieve things that are remarkable to us. So this is based off of something called GPT-3. It's also published by OpenAI and shares many similarities. Similarities are overlaps with that chapter. In this video, what we're going to see is that if you use a neural network to get a description of something degenerate, and then the neural network would generate the code to create that. He said here, some auto-generate a button that looks like a watermelon. And here is the code that generates that for them. Somebody who you might find this even more impressive injections, since it actually generates combined with Voltaire. There's syntax error. Then after correcting it generates exactly what the fastball. Right? Beyond this, you can also generate things that aren't as naturalistic. So here, this is a tool called Adobe, who here has interacted with Dolby on it. I say that somebody is like 30% of the class. You can give a text prompt and it'll generate images according to that prompt. And so this is an illustration of a baby daikon radish in a tutu walking a dog. I'll give you a few seconds to think about what that might look like. Alright, and this is what the AI generated image makes. An armchair and the shape of an avocado Looks like this. And then a storefront that has the word open, hey, I've, it's done. It looks like this. Yesterday went on dali also. And here is a UCLA professor teaching deep-learning in a large lecture hall. The large vector. Hello, What's more grandiose because this, but just a bit. Here's Joe grew and doing cartwheels at the Rose Bowl. Alright. So I also mentioned at the very start that deep learning has this really diverse applications. And many students find it useful in their research areas or even in big industries that they go into. So here's a video of Jack gene. This is from when Google was releasing TensorFlow. And he talks about a very interesting application of neural networks. It system. We built this called TensorFlow and we use it for everything that we do for this area. And so the system we've built this called TensorFlow, and we use it internally for everything that we do for these emphasis area. And last year we decided we would open source it because we wanted people to have the ability to take this software, download it for free, and use it for their learning problems. It's been really great to see different things that people have used it form. So here's one example. There was a Japanese cucumber farmer. And it turns out when you harvest your cucumbers, you have to sort them into all kinds of different categories for sale. Small ones, medium ones, large ones, particularly one's not particularly ones straight one's curved on. This is pretty complicated and pretty time-consuming and harvest time. So the farmer was able to take a camera and using a computer vision model that he trained with TensorFlow, actually have the division model determine what category of cucumber was looking at and then rigged it up to some conveyor belts and some little switches that would push the cucumber into the right box. And so this eliminated many days of labor that the farmer and his wife would have to do it at harvest time. Just one tiny example of something you can do now that would be hard to four. Alright, so I hope you hear there's just been talking about a Japanese cucumber farmers used TensorFlow and in turn disability to straightforwardly implement neural networks. To be able to easily classified cucumbers into different categories. And that's saved a lot of their time at work. When I first heard this, I just thought Japanese cucumber farmers are super hardcore. I teach this stuff and I would've never thought if I was to tell the farmer to do that and also their cucumber farmers of Kennedy go deep learning. So I looked into this a bit further. And they kick up and farmers there sudden as a software engineer. And he was able to build a system that uses a neural network to get a visual classification of the cucumber. And here's that system network. Questions there. All right. Continuing on, its applications are diverse. Learning has a lot of important applications in a variety of different fields. I've been on several PhD committees after having taught this course for students are using deep learning, e.g. to try to classify the fMRI scans and PET scans correspond to certain diagnoses. Here's an example. Here's a recent example from DeepMind, which is one of the challenges in biochemistry and biology, is to understand what the structure of a protein will be based on the sequence of amino acids. And so DeepMind created an AI based off of deep learning that is able to take the sequence of amino acids that comprise a protein and very accurately predict its structure. And this has been a challenge for many decades that DeepMind has a big lead towards solving as a result of deep learning. And this one is called alpha. Beyond that, it's used in other areas and other example I like to give is a cancer moonshot. There is this thing called candle, where it tries to integrate disparate sources of information about the chemistry until drug, how a patient might respond to it, and the types of treatments that the patients are going to try to get the success of that cancer drug in helping the patient. Another area where it's helpful is in developing new technologies. And so this is a video that talks about how there are many challenges and self-driving vehicles. Same time, neural networks are at the foundation of self-driving vehicles in terms of being able to take what's been seen by the camera, parcel of what's going on in the road and then make decisions based off the bat. This is a challenging problem. So let me play this video. And then once you have that problem-solve, the vehicle has to be able to deal with construction. So here's the cones on the left are forcing it to drive to the right. So not just construction in isolation, of course, it has to deal with other people moving through that construction zones as well. And of course, if anyone's breaking the rules, so the police are there and the car has to understand that that flashing light on the top of the car means that it's not just a car, it's actually a police officer. Similarly to the orange box on the side here. It's a school bus. And we have to treat that differently as well. When we're on the road, other people have expectations. So when a cyclist puts up their arm, it means they're expecting the car, yield to them and make room for them to make a lane change. When a police officer student the roads are vehicles should understand that this means stop when they signal to go and we should continue. Alright, so examples are just things that you wouldn't understand all the red, right? But there are so many of these unique roles and edge cases that self-driving vehicles have to be able to take into account. And people are solving this with neural networks. The questions. What did you do on? An example that I really like to use is the recent progress made in games. I like this because when I was a senior and undergrad over a decade ago, machine-learning luminaries at the time, dr. the game of Go wouldn't be solved in the near future. Of course, we know that there was deep bluish, I say I, that Garry Kasparov in 1997 but goes and much more complicated game. And this is demonstrates harvest, the CEO of mind explaining why that is games or kind of microcosm of the outside world. That's why games were invented, That's why humans find it fun to play. There's a rich history of compete attacking board games. Started with games like backgammon, drafts. And then finally there was deep blue 97, the beak Caspar for chats, watershed moment for game. Since then, the really big remaining Holy Grail, if you like, has been done. Chess number of possible moves to about 20 for the app. And go it's about 200. Another way of doing the complexity of Go is the number of possible configurations of the boss is more than the number of atoms in the universe. But if you ask a great Go player why they fade a particular move? Sometimes they'll just tell you it felt like. So you can, one way to think of it is that Go is a much more intuitive game, whereas chest is much more logic-based, right? Yeah, So because there are so many possible configurations. And further that when you asked a really good Go player why they played a move, it comes down to, it becomes really challenging to think how do I train an AI that can beat the best human Go player in the world? Because what this intuition even mean, how do I create an AI? Just have this intuition. Even with these challenges in 2016. And this is actually right when I was interviewing for my job here at UCLA. During those interviews, there was this game going on where AlphaGo and AI traded by DeepMind was playing all who is widely regarded as one of the best skilled players in the world. Before the day reset all before the game, AlphaGo had already exceeded the European champion. But Lisa Dole was at a level of play much higher than the European champion and he was very confident that he could win. The first game was a massive surprise. When AlphaGo won. Alphago won came to an end. Alphago won game three, at least for me, there was this feeling of excitement when AlphaGo won. But then it slowly changed a bit into a tiny bit of despair, as in humans ever going to be the AI. But recent Dole was able to teach game for. Then ultimately AlphaGo won the fight in sets or 21. And this was something again where she learned luminaries had said earlier that the statement of the salt for a long time and get deep learning was able to solve it. One way that these AIs are trained to play Go was to look at the human expert data and in essence try to determine some of that intuition. So from these examples as to what's the best group of plaintiffs, third position. But here's a video from David Silver who led this DeFi projects at AlphaGo pocketed be talking about how they were even able to later on upbeat the algorithm so that they can make even better go fire that requires no human data at all. Alphago Zero is the strongest Go program in the world, outperformed all previous versions of AlphaGo, specifically defeated the version of AlphaGo, that one against the world champion at least at all. And it beat that version of AlphaGo by 100 games deserve to all previous versions of AlphaGo started by training from human data. And they were told, Well in this position, the human expert play this particular movement in this other position, the human expert played here AlphaGo Zero doesn't use any human data whatsoever. Instead, what it has to do is learn for itself completely from self play. So the reason that playing again itself enables it to do so much better than using strong human data, is that first of all, AlphaGo always has an opponent have just the right level. It starts off extremely naive. It starts off with completely random plane. Sorry, that was my bad. I actually don't be attached the video, but the video wasn't much longer. It starts off with continuing, I can phrase, and then they do something where they trained the eyes and unmatched at the same levels. And just by playing other versions of the AI, it's called self play. They are able to become gonna go. This new algorithm which they call AlphaGo. Alphago recently hold the world champion. 100 days is around. The AI has improved a lot even just by, by algorithms. Any questions bear in mind is working on several other names instead, here is the video of their AI office car to play the game of Starcraft. Here's another application that is hopefully showing how diverse application to my own area of research. So I worked on something called brain machine interfaces. And our goal is to help people who are paralyzed. So this is a picture of Christopher Reeve, who many of you know was the Superman and the actor who played Superman and the original trilogy. And Christopher Reed suffer horses. Horseback riding accident, paralyzed from the neck down. For the rest of this month. For the rest of his life, he required a ventilator support to help with breathing. And we use deep learning to build brain machine interfaces. This is to help people who are paralyzed due to spinal cord injury or disease like ALS communicate with the world once again and move once again. So the basic idea of how this works is for someone who's paralyzed, the natural pathways that take information from a region of their brain called the motor cortex. This part of the brain and goes down the spinal cord to the alarm, these natural pathways are broken. But even though these pathways are broken, intense but thought of I want to move my arm still persist in the motor cortex. What we can do is we can read out that activity by going to the brain directly. In the first example, I'll show you what we did is we did neurosurgery to record from neurons in the motor cortex. And the neurons in the motor cortex communicate via signals called action potentials. And so I'm showing you 96 electrodes here, where every single electrode is getting really close to a neuron and bust into its voltage. And these spikes here are the voltage signals neurons in MIT. And that's the fundamental currency of information. And the brand is how your neurons talk to other neurons and how neurons talk with the rest of your body. So what we do is we read out these electrical neural activities from these electrodes, and then we pass it through what we call a decoder algorithm. Basically, we translate that electrical activity into the intent of what the person actually wanted to do. And what you use that to control across thesis by the robotic arm or a computer. Alright? And this decoder, the best ones out there today are based off of deep learning. Deep learning we'll learn the patterns between the neural activity in the brain and how to best decode that, the movements of some device. So here's an example of how this works. In this video, you'll see a 52-year-old woman ALS. She's paralyzed and she's controlling a keyboard that's already completed computer cursor just by thinking about it. To answer this question, how did you encourage her sons to practice music? I think you've got it. Target to target. And then when she gets over the letter that she wants to select. That select the letters if I might be part of a football at Cambridge University. Another video of the participants where instead of using this custom keyboard that we created, she's now controlling the computer cursor on Android tablet. I see the control isn't perfect. Sometimes you struggled to get to the exact location. And there is definitely more improvement to be made based off the algorithm. So many advances we tilt the performance of these incidents. He's able to use this to surf the web. I want you to write emails. The question is, how do you collect the same data progress? Yeah, that's a really good question because these are trying to be a supervised learning, which means that we need to know the movements she intends to break and collecting their own data corresponds back. So actually what we usually do is we will move a computer cursor on the screen for her. And it will say, imagine moving your arm to follow this computer cursor. So when those perfect goes to the right. So imagine wearing her arm to the right. That's how we get the target signal to decode. That's a great question. Yeah, The question is, Venice seems like you're paralyzed and you want to use a system like this, you have to go through some pretty intense training. So actually for these algorithms, they were able to be trained with just 10 min of data and probably could be more fully. Deep learning has done better algorithms. But if you put in once the firefight, but those will be data that have been collected through normal abuse over the course of months. There's also the other thing that is pretty glaring about the system which is either requires neurosurgery to get these signals. And so, excuse me, try next video which is at UCLA, what my lab is working on is trying to make a non-invasive. And so this is actually just from a month ago showing a system where instead of doing neurosurgery and report neural signals non-invasively through this one. But this is one of our graduate students wearing a cap. And now he's able to control this. And this cursor is entirely driven by burning. There are nine neural networks working together in the air. Have you include supervised learning and involving buses, but because of where it equal to that, some performance is not as good as one way doesn't require neurosurgery. And this student is able to just practice on both of those into a high level. But I haven't talked for my office hours. In neurosurgery. What happens is that the participant is first given fMRI MRI. The MRI that asked to imagine you live in. There are, or there can be areas that we did where we want neural signals and then MRI when they're asked to imagine moving your arm. Areas of the brain that become activated will become known to the nurses and the neurosystem. Although this board then will spread through the non-invasive system. The question is, do they use, are we using the same or lupus as what? The invasive subject? Because easy. The answer is. So actually, in the video that I showed you that 52 year old woman with ALS, I said that she's imagining moving her arm, but she actually isn't. She is to move the cursor up and down. She's imagining her thumb going up and down. The cursor left, right? She's imagining her pointer finger going left and right. In this video, I'm, the graduate student is doing right-hand group is for right that kind of movements for both hands and feet for gas. And so part of the reason why that is is because when we record non-invasive signals, they are, they are all such, such for spatial temporal resolution that they can't resolve various movements in the fingers. And so we have to use whole body movements. The question is, if you train this on one subject, is it generally enough to apply it to another subject? In the intercritical case? No. And that's because it depends highly on which neurons we end up recording. Even for the same subject. If you do two surgeries, the signals from the first surgery will be very different. Music comes on the second side, so they have to be retrained. Whereas for these non-invasive methods, because the signals are so poor spatiotemporal be resolved. They are correlated across subjects, and so these do generalize across subjects to a better extent. Take one more question and then we'll continue on. The question is for action potentials, are they in binary or is it important to maintain the analog signal? But also talk about this when we talk about neural network architectures. But the output of biological neurons is binary. Either a spike, what happened? There actually isn't any information can be in the voltage, can be analog signal. All that matters, but that's just my calendar, not the brain actually uses digital communication. That's one of the reasons why we can have pretty good control of our fish, even though the brain has to send electrical signals all the way down our body, that's a pretty long transmission line. And there's a lot of them aren't all the way. But the brain has developed these digital robust mechanisms to convey that information. Because there are so many questions about this. Let me just plug that. We aren't going to cover those in these cost is the cost of the machine learning class. But I do teach about this next quarter in a class 14038 for undergrads at 02:43 for graduate students where we talked about how the brain generate signals and how we can use that for brain machine interfaces. All right, so moving on, whole bunch shown you that deep learning is really becoming a part of everyday life. And you've interacted with the neural network today that has resulted in key breakthroughs in many areas and in different fields. Many people are looking to deep learning to try to boost performance. One thing I learned earlier this year is that if you've seen those little cocoa robots that deliver food out on the street. The founders of the company where students in this class, six years ago in this class, was something that inspires them to do. So deep learning may be useful to research in your area or future. Any questions here on any of the overview that we've given. Alright, so I'm gonna give a very brief background on bidding that even though my expertise is in the brain and then brainwashing interfaces. In this class, you're primarily going to be studying neural networks in the context of computer vision. And that's because like we saw in the earlier video with Professor, That's really where neural networks, neural network research dates back to 1943. Mcculloch and Pitts in 1958 with Rosenblatt, who here has heard of the perceptron before. You, I couldn't have to try and learn your first neural networks class. That's really the first artificial neuron where there is a node that will sum all of its inputs. But so does the affine transformation. Then after doctrine applies, a thresholding operation so that the output of this artificial neuron is going to be zero or one. That was inspired from biology. But the answer from the prior question, distinctive aspects, biological neurons communicate in an all or nothing binary way. So really in the 1940s and 1950s, researchers to start to think, okay, can we design computing elements that have similar properties to biological neurons? What happens when we looked them up? Alright, so Rosenblatt perceptron, if you just have one neuron, is something that's linear. And to make it nonlinear, you need a stack, several layers of perceptrons, giving rise to something called the multilayer perceptron. That gives you a non-linear architecture because now you have an affine transformation is followed by non-linear thresholding operations, followed by a more alkaline than nominator thresholding operations, etc. The challenge was that people didn't go how to train these neural networks. And what happened is that in 1986, Rumelhart Hinton and others published an algorithm called back propagation. And we're going to talk a lot about that proposition in this class. But that was really the key thing that allows you to train these artificial neural networks. Such that by 1989, Yann LeCun, who is one of the fathers of deep learning, was able to train neural networks to do classification of digits. So here's a video of this from 1989. I think before many of you were born. The goal of the algorithm is to look at the digits and write out what we are so accustomed, classified correctly and then put it on the screen. And it can do this if our computer printed or handwritten 45 different styles. You may be thinking, this is 1989. Why is it that deep learning didn't really take off until 2012? This video, they're saying the researchers, which is great, they deserve recognition. In addition to this architecture in 1989, the multilayer perceptron, which we'll call him this class, fully connected neural network can also develop something in 1998 called the index, which is an architecture that is called the convolutional neural networks. And convolutional neural networks, which again, we're going to talk about a lot in this class, are the modern architecture that gives really high performance on image classification and computer vision problems. So we are kind of in a renaissance since 2012, revival of deep learning and its utility to everywhere today. But you might ask, why is it the case that even though these came in 1990, 8.19, 89, why do you only ticked off in 2012? And a lot of that is due to simply the computing power and the amount of data. We're not big enough in the 1980s to really see the power of neural networks. And one thing that really helped to advance this was that beginning in 2010, there was a new dataset called ImageNet. It came out. These numbers that I published or that I had up here are from many Jessica. So almost surely these numbers are even higher. But basically what ImageNet, It's just a collection of many images, tens of millions of images. These are examples of the images. And the images are guaranteed to be one of 1,000 barrels. And so those variables include motor scooter, container ship, leopard, might, et cetera. And this dataset was more in an unprecedented way at the time. And really the goal of this dataset was to try to help people develop better computer vision algorithms for classification, where it gives you an H. And basically they had a competition where we said, for the side, take an image and I'm gonna give you five guesses for what the image is. And if you are one of the slides and adjust them, you got it correct. So if you look at container ships, this is an image container ship, this is true label. And the five guesses that the algorithm takes this container ship and the length of the sides of these bars is basically the competence of the neural network that it is a container ships. So it's very competent to container ship. But then after that, even the next guesses are reasonable. Lifeboats fire both platforms. Even if you look at this example of a leopard, this is a leopard has that with very high confidence. But then the next guesses are like jaguar and she does know wrapper, etcetera, right? And even when it doesn't get it right. So if you look at row two, column three is an image of a cherry, even though there's a domination behind it. And you may ask, why is that? Well, these datasets for labeled by humans, whichever treatment saw this, determined that this is a picture of a chariot. And so this is one where the neural network, it's wrong to judge. The top five guesses are domination, rape, other berry, I'm not sure what the fourth guest is and the fittest is correct. So if you look at that image, you might actually think the neural network did the right thing because the domination is a pretty prominent. Did you end up with conducts most common? Any questions on ImageNet? Competition? And basically, every single year, they were going to have a competition where you can submit your algorithm to the competition and they would measure your performance. The performance will be measured in terms of error rate. So the lower you are, the better. Zero per cent music you've done everything right? And in both 2,010.20, 11, you can see that no algorithm dark better than a 25% error rate. By the way, there are some graduate students at Stanford who wanted to determine what is this human performance on this. So they sat down on a weekend and they did just finish that task themselves. In human level performance is about 5%. So even we make errors when we look at these images. Even in 2012, which is the third column, the performance of these architectures are all still worse than 25% error rate except for one. And this is called AlexNet. Alexnet was not just below 25% error rate. It was far below is x 16/s. So this is actually quite a considerable leap. In AlexNet was a convolutional neural network. And by doing so much better than all of the other algorithms on this dataset. It really kicked off this deep learning revolution we're in subsequent years. You can see the error rate significantly decreases, where all of these architectures or the winning ones are augmentations to the convolutional neural network architecture. We're going to talk in great detail when we get this CNN's convolutional neural networks about some of those architectural innovations that really decrease. Any questions. A key question, how do you know the initial database stuff? But yeah, the question is, how do you know in the initiate a space that is labeled correctly? You don't care. So the initiative base does have to interrupt raisins or at least enable maples that humans with the students. And so all of these were labeled by humans. We're actually going to learn about regularization later on when we talk about image classification called maple, excuse me, We're actually, if you build into your train the assumption that the labels aren't perfect, you actually do even better. Other questions, yes. The question is from Zoom, which is this figure, top five error rate or top one error rate. The answer here is the top five error rate. So for everyone who doesn't know what those terms mean, talking about the error rate is what I described. What you tend to get five guesses at what the correction inches. And if you get one of them right, That is capitalists, right? There is something else called type one error rate. And top one error rate means you only get one guess. And if this radius right, if this one is wrong, one error rate is obviously more difficult. Question. You can ask the TAs to fact check me on this being taught by our rate, unlike 99% sure, but CSPs. All right, Other questions. Alright, so part of what is fueled this deep learning revolution of the past decade is that today we have much larger datasets. And we also have the computational power to build much larger neural networks that are capable of processing this data. There's this quote from Goodfellow textbook deep learning, which will be our textbook for the course. Fortunately, the amount of skill required to get good performance, but deep learning algorithm reduces the amount of training data increases. This is a way of saying if you're in a setting where you have a lot of data, you may not need a lot of skill to build something fit that data. In essence, trained with a pretty standard neural network could get you most of the way there. Now of course, this will depend on the context ribbon, like we saw in the earlier video from Professor David Silver and defined, there are ways in which algorithmic advances, like for alpha goes thereof, can lead to very, very impressive improvements in performance. Not just the step sizes. These are some parts that just show examples of how data sets have increased in size over time. And furthermore, that our networks have also been able to increase the size over time as a result of increasing power of computing and better and better. Alright? Any questions on any of that introduction? I always give a break in the middle of lecture because I think that this is way too long to fit about this. I will take a five-minute break and I'll come back after that. I have not actually enrolled in this course by nice no deep learning and neural networks for my research. Yeah, So I was wondering if I could add it. If you can if you wanted, you could just shoot me an e-mail. I could get. My concern with taking me is because I'm supposed to do my oral examination with somebody else. Yeah. I'm ready to take another class with me or do I thank you very much. I'll get my friends next to me too. I like it. A little bit. Lighter material from fired here just to make sure that we're still determining. For the class, right? Yeah, right. A lot of linear algebra, statistics. Then ideally all say pharmacy. So we don't get to be more challenging. Hi. I don't know exactly what your you taking. Okay. I just wasn't your question or you just not good material for a moment. All right, everyone. So one logistical and asked me about PTAs. If you are early enrolled in this class, you'll notice that we changed the classroom from Mom Learning Center to 39. And that's because you still able to enroll more students than the Cochrane capacity, even though not all students come to the classroom. But because of that, we do have more capacity to involve students. And so if you need a PTE for the class, just shoot me an email with any other questions? Oh, they said if someone said there's an opening for the class, which is pretty impressive because the class is full before I started. Okay. There are one to two consecutive signs. Any other questions? Alright. Yeah, that's a first for me. Alright. We're gonna go through the syllabus now. And we'll be happy to take any questions on the syllabus and then after that, we'll get right into course material. So here's a rough schedule for the class. These will be the content of the lectures. We're going to be following the deep learning book, which you can find online at deep learning book dot for. The class does not have a final exam. It has a midterm exam, which will be in week seven, and it's scheduled the same week as the Veteran's Day holiday. So you had a bit of extra time to study for it. And if you are in MS Online students, we know that your exams are on the weekends, so we will organize your remote exam on Saturday, February 20th. Alright, but for the rest of the class, exam will be in-person in class during the classroom. Any questions there? All right, class logistics. This class will be done entirely in Python and we will provide resources to you for setting up Python three. And then with every single assignment and we will give you a requirements that gives you all the packages that need to be installed. Just fine. Alright. If you don't have fire Python experience, e.g. you just have MATLAB experience. Please budget in that. While there is a transition from Matlab or Python that isn't as difficult as other transitions is still takes significant amount of time. So please plug that in as you, as you determine whether you're going to take the course or not. Alright, we're going to post a set of formal notes on ruined learn. Basically these are going to contain the content for the course. In usually tourists form. The lecture notes I am projecting up here and will be annotating or more informal and ask that you don't publicly post them because I do a lot of images that I haven't resolved to copyright. So we will distribute them on every annotation that is done in class. Coast to the nearness com. Okay? Alright, on discussing sections. Within this class, we have many discussing sections. But in our experience, many of this discussion sections are very poorly attended. And so what we do is we consolidate our discussion sections and instead of holding more office hours. So what's going to happen is after class today, I'm going to send out an announcement on rumor. And that will include details on the consolidation of discussion sections. Basically, if you're someone who values by discussing sections, we want to hold those for you. So there's gonna be a poll where if you are going to attend live discussions, we're going to ask you which discussions can you intend? And then based off of that, we are going to hold on the subset of the discussion sections live taught by our TAs. And the remaining time that T has been normally be teaching discussions for our kids as extra office hours. The TAs will record one of the live discussion sections and upload it to grow and learn. So if you're not able to make it, you can still see it. In addition to that for when we have homework assignments, the TAs will also create a separate coding discussion video that essentially is going to be instructive and helpful to complete the homework and go over syntax and other Python coding things to help to complete the assignments. Any questions on discussion sections are two, rehabbing schedule the office hours then basically the TAs and I tomorrow are going to determine which discussion sections will be held. And then after that, we're going to allocate our office hours, which will be on across the entire week. Alright, for reading in this class, there will be three components intergrated on. The first is homework. So there are five homework assignments. They are primarily coding assignment. So though early on in for homework when Homer tune, they're more handwritten assignments, solving questions. These assignments are going to be due based off of the due date that we put on the assignment as well as on the syllabus. And they are due by being uploaded to Gradescope by 11:59 P.M. on the day that day or two, we use Gradescope so that they can develop a central rubric. It helps to make breathing more consistent and fair and also gives feedback to the students. That's where they got things wrong. We know that life happens and sometimes there'll be unexpected or unforeseen circumstances. So we're giving three freeway days to every student should only be used in extenuating circumstances. And you don't have to notify us if there is no way that you can just submit the assignment grades up to three times 32323 days within the class. There is a caveat, which is that on any given assignment, you can use that to lay days. And so this is so that we don't fall behind. Then any assignments submitted more than two days late receives the great. Any questions on homework or homework policy. Alright, we'll have a midterm in class that's going to be worth 30%. And then in the final weeks of class, maybe in week seven or week eight, we will also release the final project of the class. And the details on that final project will be released at that time. We'll give a project assignment that people in the class can choose to do. But if you are someone who has a side project or research that involves deep learning, we do have a way for you to apply to cap that as your final project. And in most cases we will accept that project as your final project. Any questions here? I'm sorry, repeat the question. The question is, is the midterm exam written or is it coding? So the exam will be taken in-person and you will fill up the exam by writing on it. But we may ask you to write to it on the exam. Are there questions? The question is, is the final project in certain bicycle person writers? So we allow groups of up to four people, I believe. But it can also be done individually. Any other questions? The question is, will the expectations for the group project is different based off the weather is like an individual or a team of four? Yes. Usually if it's an individual, will relax some of the constraints on the performance. Meaning calculation uses less power and less compute power to optimize. Other questions. Alright? Grading in this class is on absolute scale. If you're like me when I was an undergrad. This a bit because what if the professor gives an exam where this exam for the average just like 25 and everyone gets a graph, that's not going to happen in this class. We design the exam so that the average is at least 80. And we reserve the right if we do get a hard exam, relaxed these absolute scales. The reason that we make an absolute rating rather than a curve is so that we're not competing with each other. And if everyone in the class learn the material well, I'm more than happy to give everyone who is constant. And the absolute scale allows me to do better. All right. Any questions on the absolute scale or grading? And I want to say this upfront. When it comes to grading, I will grade you according to the syllabus and according to this rubric. And I will not change your final grade unless I made a calculation error and that's UCLA academic senate policy. So if you send me a request to grade you outside of the syllabus or this rubric, I will not respond to that request. Alright. In addition to these grains skills, we also will reward bonuses to the final grade. One is a bonus of 0.5% on your final grade. Based off of the scale for filling out the cost evaluation feedback which allows us to improve the class for future iterations. And the other is a Piasa bonus class. This large. We find that Piazza is really helpful for getting students questions answered quickly. And we also find that students answered your question or students learn better when they are also answering questions. So to encourage this, we give a bonus of m plus 1.5% to your final grade based off the posting and participating on Piazza. So on Piazza, you're allowed to post anonymously to other students, but your answers will never be anonymous to the instructors and TAs. And that's because that's how we know how much you posted and how we can assign this piazza bunnies. Also please keep that in mind as you post. The reason that we encourage this is also to help your learning. At the same time, we're not trying to offload. The TAs will still be continuously monitoring Piazza. So we know that there are some questions out there where it's a very difficult question and other students may not have the answers. So the T is in general will be uploading Steven question student answers that are correct. And if there's a question where the students don't have an answer to the TAs will go ahead and answer that question on Piazza. Okay. Any questions on that? All right. This should be the primary means of asking and getting questions naturally, the class I can mention be like Piazza to be student-driven. And the TAs will answer any questions as students are unable to answer. If you have any course-related questions that are not appropriate for y'all. So then please e-mail our entire teaching staff. The reason I ask this is because some TAs go above and beyond and answering questions. But then when students find that out, bacteria gets bombarded with all the questions and it becomes a lot of work products here. So if you know all of us, then we're able to divvy up the work amongst the TAs. All right. And then if you have matters that are more sensitive than that, please feel free to always contact me directly. Alright. Further notes on the crops like I mentioned, we have five plants homework assignments that contain both written and coding. Coding in the homework will walk you through Jupyter Notebooks. And usually it's going to be clear whether you have done things correctly or not. So usually when you submit the homework, you're going to get full marks on it or not. We will release the solutions for the written. I don't know why it says homework exams, homework questions. But we will not release the solution code for homeworks. And that's because we use these assignments every single year. Homework is submitted via Gradescope. We talked about that already. And then we also talked about how we gave three questions. Alright? Some of you have a question on how C1 and C2 47 differ. So this is where I have a C at UCLA in terms of how they separate undergrad and graduate classes. I feel and you should be able to take his past respect to the fifth if you're an undergrad or graduate class. So C1, 47. 47 just allows undergrad and graduate students take it together. Undergrad or frequently concerns if they're going to be graded on the same scale as graduate students where I went for undergrad. They actually always just said No, you're going to be located on the same scale. And actually oftentimes undergrad and graduate students. After being on this side of the exams. I know that to be actually the case. Sometimes the undergrad average is higher than the graduate student average, and many times it just statistically indistinguishable. Ucla doesn't like that. And so they require there to be differences. So I'm happy then to bridge T1 47, 47 students on different scales. So graduate students and undergrads are not adversely impacting each other. And then there will be one question on homework number three, that will be optional for C14, E7 students could not see 247 students, the classes or otherwise. Any other, any questions here? Alright. In general, I'm going to post the slides that I'm projecting up here to grow and learn prior to lecture. So if you're someone who likes to take notes on the slides will be able to download them and follow along. Okay. The question is, can I clarify what I mean by different scales? Yes. So this is the scale that is used by default. And if e.g. the undergrads have a median score that is like say ADH, but the graduates of these have a median score that is say like 92. I will, I be relaxed with undergrads scale, but not the gratitude scale. So that mantissa can be relaxed. So that'll be based on just the performance of the students with C1 through C7. At the performance, we can see 24x7. Other questions. Alright, so the non annotated versions of the slides will be posted to the ruler and try to lecture. Um, we'll list the readings in the textbook at the start of each lecture topic. And this class will have a toll TAs. There's one note about the TAs, which is that there's one TA which is taboo and the TAs will introduce themselves very shortly. He was a TA for the MS Online version of the class. And so his office hours are open to all students, but he will prioritize questions for MS online office hours by policy. Then there's also the TA who will run this and check and be sure that there are addressing any questions here. Alright. This class is going to be very practical and theoretical. So we're gonna be focusing on them implementations of neural networks. And I'm algorithms to train them. We will not cover the theory of deep learning, which other classes cover. And in general, this will not be a theoretical class. We will be rigorous whenever we use math, of course. But this won't be true space. Really, this class is more focused on equipping you with the knowledge about practical neural networks and how to implement them that may be helpful in actually using neural networks for research or industry, industry applications. Of course not a statement on the importance of theory. Theory is super important. It's just not what we're going to be focusing on in this class. And actually theory and deep learning is quite difficult. So I'll just give you a preview that several of the questions you're going to ask me in this class, I'm going to have to answer with saying this isn't terrible results because we may not go a theoretical reason for why something works. But we do know that it works. Questions that just like for the last question is, am I saying this thing about theory? Because we don't have the time and not the actual thing in terms of I'm sorry, what do you make is if you say I understand that you don't have to go and you just don't have time? Or is it like, Oh, I did this. I don't know why it works, but it does. Yeah. So students asking me, is it the fact that we're not covering theory because we're a time crunch? Or is it sometimes the reason where really like that theory isn't developed and we dumped out and it's a mixture of both in this class, be documented even at the theory was known, we're still focusing on the practical things because of time crunch. But in several cases, we're still looking for theoretical answers for empirical results. Other questions, alright, fine on academic integrity. To let you all know, can I take academic integrity very seriously? And if you're tangent, violation of academic integrity as defined by trooper and principles that I do follow through on reporting new to the dean of students and any disciplinary hearings set packing as a result of that. So please just note, I take academic integrity very seriously. Any questions? Alright. Other customers, what is the academic integrity policy with regards to pretend homework? You're welcome to work together on homeworks. Just be sure to submit your own carrier. Yeah, In general, you are welcome to collaborate in this class. Alright? So for prerequisites, this class requires a solid understanding of probability as something like ECE one-to-one, or equivalent. Linear algebra. We also, this is a prerequisite, prior exposure to machine learning. And I want to say up front that several students have taken this class without this prerequisite and they've been able to manage. You're welcome to take this class and that I don't enforce the prerequisites. And we will do an overview of machine learning to refresh things and be sure we're all on the same page before going on to neural networks, um, but the class would be more difficult if you're not coming in with any prior machine learning. These are examples of topics I assume, you know. And you're going to upload basically the formal notes for the entire class on Boomer. And so you're unsure if you meet the prerequisites, please go ahead and take a look at those formal notes and then see if the material is has definite that they feel comfortable. And again, I don't enforce the prerequisites. And so you may not have any other requisites you want me to tell the class, but the class will be very difficult. And so that's, I put that on the student's questions. Alright, I've mentioned this already. If you are familiar with Python is time-consuming to transition to Python. And I want to say up front also that this class is a lot of work. And so I understand that it's an elective, but on my end, I feel to really understand this material, you need to spend the time really stroking through actually implementing it. And that takes time. So we give fairly demanding homework assignments. And if this is a quarter where you already have a heavy workload, I wanted to let you know that upfront so that we can take that into account as you determine what classes are, right? So this talk will be a lot of work. Any questions? Alright, about me? This is my sixth time to do this class at UCLA. Like I mentioned, it's one of my favorite classes to teach. Outside of teaching. My research in my lab is focused on applications of deep learning and machine learning. And it's tied to the brands, so to the brain. So I am primarily a computational neuroscientist and a neural engineer. What that means is the neural engineering side. I'd built devices like brain machine interfaces or I drew, we were computing machine interfaces in that cycle I showed you earlier. And then on the computational neuroscience side, we apply machine learning techniques to try to understand how the brain represents information and how it does computations. But that information. Beyond that, I'd like to now introduce the TAs. So we have several TAs, but I'll do is I'll show define mental ask each TA who's quite as shown to just stand up and then you project to say a bit about yourself. And so we'll start off with envoy, who is our head TA for this class and before family. Two times before and also received a samuel the Excellence in Teaching Award for this class. And so he's very experienced here. I'm a PhD student. I'm working with Professor one here, I've drawn it. And my research interests are in computational neuroscience and it should be the documentation. So as Jonathan mentioned, this is my third time. This was in declining and overall, I have 11 times many posts within undergraduate and independent arguments. So my favorite courses that unfortunately is not disclosed. My favorite courses, 36 ft, which is the force on convex optimization. So one advice that I give to students every quarter is that you should take that course. That hasn't been the most rewarding course I have ever taken. It hasn't helped me understand all aspects of machine learning. So please feel free to talk to me if you have any questions about that person. I'm more than happy to help. So far we've been working to do all this wonder and I won't be directly with each other and get the best out of this TAs if you come up from here. So next we had a first-year kid. He's good bye, professor. Many models for all scenarios and models that are reliable and dependable. And I'm working on my model than the condition. So it is my first time. Second time, or the cross where I'm here to help you with any questions and concerns. You may rapidly quicker. Thanks so much for taking. The homeworks are going to be what I would do. But more than that, I'm looking forward to learning from one another. Hello, students. This is my first video that I apologize. And I do hope that Ali, The next day. She is not entirely economics. This is centered video to introduce herself. This is not Keynote. Hi everyone. Welcome to enter 23. My name is Hindu and I'm a second year master student in Electrical and Computer Engineering. Most of my coursework and projects involve machine learning. Scores last window and thoroughly enjoyed it. It's quite exciting to be on the other side. I'm looking forward to interacting with you all. I hope you'll find the course as rewarding as I did and wishing you a great quarter ahead. My master's student in computer science. So I hope you also find that you also to share my research involves using machine learning. And you see I've logged on using the lung cancer. We have two more hydrogen and then the secondary losses today. But this loss yet feel free to reach out to me with the advocate. Initially is elastic, which is student working on time series, Transformers and MATLAB and high conflict difficulty questions. And this is one of the greatest courses. There is still a great asset that you've taken this course and you would enjoy. Thank you to the TAs. I think it's appropriate. You all get in or out of a clause. They do all the hard work behind the scenes. We know from last quarter that the TAs are very important and directly work with for the TAs fire and teaching. And and I know that they care deeply about their learning, are ready to be really helpful during office hours. And for the four others that I haven't worked with, we've talked and I know that they feel that way also. So please be sure to use the TAs as a resource to get pumped for this class, we are really here to help. Alright? Any last questions before we dive into material? Alright, we're gonna get started then. So we're gonna start off the class with a machine learning perspective. This is something just to get us all on the same page. If you feel very comfortable with machine learning, then the first factor and then the next lecture. Maybe a bit of Congress, both sides for you, but we just ask for your patience. Again. I just want to get everyone refresh on the same page and have a common understanding of loss functions, parameters of gradient descent. And so we're going to refresh all of them. The reading for this receptor will be Chapter five up to 5.5 for the deep learning book. And then if you need a refresher of linear algebra and ability, please look at chapters 2.3 of the deep learning pluck from fifth dollar. You likely know that there are many types of machine learning. There's unsupervised learning. There is reinforcement learning. In this class, we're gonna be focused on supervised learning problems. Which means that in most of the settings we're going to give you both the data and the labels. And then this class is also going to focus largely on supervised classification problems. Which means that because classification means taking a data point and putting it into one of k potential classes. That's largely because we're gonna be working with this dataset called SeekBar time. And cost will be working primarily on supervised classification. We will do some regression in this class to motivate or to refresh machine learning. We'll do a regression problem later on this class when we talk about recurrent neural networks and other architectures, we'll talk more about regression. But at least for the majority of his class, we're going to be focused on this dataset. This dataset is called C4 and C4 ten. There are going to be images that are given to us. The images we can call a vector x. And each of these images of which were showing, I believe at ten by ten grid of them. Each of these images is going to be 32 by 32 pixels. So it's going to have a width and a height of 32. And then you all likely know that the way that we code for color is the R, G, and B values of an image. So in addition to having 32 by 36, every pixel is going to have three values for RGB. And so every single image is gonna be 32 by 32 by three. Now, each of these images is going to correspond to one of ten classes, so that's ten and C4 tag. And these are those tiny classes. So this is the possible classifications of each image. Each image will definitely be an airplane and automobile, et cetera. There are at least to start, we'll say one way that you can classify marriage is to take our data point x, which is this 32 by 32 by three objects. Object called a tensor. So it was just 32 by 32. We call it a matrix, right? We have rows and columns, but when we go to higher dimensions, but generally call them textures. And because this has three dimensions over which there's width, height, and depth, we would call this a three-dimensional tensor. But at least to start, what we could do is we can take these 32 by 32 by three values. If you multiply them together, you get 3,072. And so I could reshape this into a vector that has 3,072 elements. Right? And then the problem of classification would be to say the following. What I wanna do is I want to take my image x and I want to pass it through some function, I'm going to call it S. And the output of this function should tell me which of these ten classes have you lost it. All right, so the machine-learning problem takes my image x and tells me, is it a bird? Is it a cat? Is it a deer? So let me just write that clearly. The class refers to what the image belongs to. And C4 times a dataset you'll become very familiar with is actually going to be data set that we'll introduce to you in homework number two, where you'll start off by learning path that is just a linear via something called the softmax classifier. Then on homework number 34.5, you're going to start implementing neural networks and an augmentation to neural networks on the same dataset. And eventually, you're going to see the accuracy of classification on CFR can go up to several tens of percentages as a result of using neural networks. And so this will be a key dataset for this class. Any questions on this dataset? The question is, what are the hardware requirements for doing classification? Front seat part-time within this class? They are not demanding, so you should be able to do it on whatever computer you have. Will also put out a tutorial on how to use Google Colab for which you will also, if you choose to use, have access to it. A GPU. It's not a very powerful GPU, but it's watching you, but you'll be able to do all this on your standard X86 computer. Other questions. Great. Thomas question is, for this datasets, are the labels binary or are they the spores? Spores? Thomas question is basically, if I have an image of an airplane, are they going to tell you the probability that's an airplane and automobile, a bird, or is it just going to tell you an all or nothing? It's an airplane, not an automobile bird, and it's the latter. So both the dataset will tell you what is the class of that. Alright? So we are going to just again, give a refresher on supervised learning just to make sure that we all have stain fundamentals in terms of the homes of machine learning. But the complaints of the machine learning problem which I have lost function, the parameters, the gradients, and then doing gradient descent. Alright? So let's say we want to rent a home and rest. Good. This is all synthetic dated and I just conjured up. And we want to know if we're getting a good deal. So here we have sample homes where on the x-axis we have the square footage of the home. And the y-axis we have the rent of that. Alright, again, I can try example. The real data will be more complex, but square footage gives you a pretty good indication. Of course, there are other factors that will determine the amount of that. But in this case, what we'll have is, we'll have components of machine learning problems. The first will be the input to our machine learning algorithm. This example is going to be the square footage. Because in this case what we are going to want to do is we're going to want to say for home that I'm looking to rent, what is the square footage? And given its square footage, I want to predict what a good value for the Rex group. And so that tells me my output is going to be the rent. Not the goal of the machine learning problem is going to be learning some function f. Essentially the oracle of Westwood wrench, where F is going to take as input, square footage, and output to me what it predicts the rents ought to be read back home. Any questions on this setup? Alright, so considering the supervised learning problems, machine-learning, essentially, continent says, I have a goal, which is, I want to learn this function f that maps my square footage to rent. I know that my input, so many outputs. Alright, does it the square footage into retrospectively? And now machine learning, I have to start to make some design choices. Alright, so the first is, what is this function going to be? In other words, what model should I use for, right? So can someone tell me what the simplest model here might be that is reasonable? Yeah, Great. Linear regression. So I could choose that f of x is equal to align, right? So I could say this data, it looks like it's pretty well modeled by a line. I'll draw that line right here. Going through all these data points, alright? And we know that a line, It's described by two numbers, the slope and its y-intercept. And so f of x would equal a x plus b, where a is the slope of that line and b is the y-intercept. If I were to call the monthly rent some variable y, square footage is x. This will be the model. The red line would correspond to the model y equals a, x plus b. This isn't the only model we could have chosen. The model that we choose is up to us. I could have chosen to look at these little I guess I'll call it noise. Fernando isn't easy to say. I think a line might not be sufficient to model the complexity of the dataset. And maybe I might want to make the model that is a bit more complex. So maybe I want to make a model and where it's polynomial, so y equals b. And then let's say it's like 100 order polynomial. So I could say d plus 100, 100 plus 99 times x to the 99th, plus all the way down to a one x. Alright? And what a model like this would look like is one that actually pretty well goes through many of the data points, might have to make it a bit higher-order to go exactly through everything but a model like this orange polynomial talk more powerful in terms of fitting these points, right? So point-of-care, same. We get to choose the model. The model could be as simple as linear as more complex like this. Polynomial. Whatever we choose for the model, we step back. And then after this, we need to know a method to determine how to pick a model. Alright? So when I look at a model like the red line, and it's y equals a x plus b. The x and the y, our data. And then this a and this b are called parameters. Basically in this model, the parameters of the things that I get to change to make this model as good as possible. So I get to choose a and b, which are the slope and y-intercept for this line. To try to model this data, to try to get as good a prediction of the rents square foot as possible. Okay. Any questions so far? So how do you think? Yeah. Great. But I don't want to sit here. So the question is, yeah. So the question is asking you basically about computational time. This model is more powerful, right? It has higher capacity in terms of it can model more complicated things. But it may take longer time to compute. Now it turns out that this regression can be solved by something called the squares. But your point is still valid for neural networks where the training time you can be on the order of hours. In fact, it can be on the order of months, hopefully not years, but could be a really long time. So at what point do you trade off the simplicity of the model with few times and those are all optimisation you have to make as engineers from educated guesses. And there'll be a way to, you know, we'll talk about how we validate and test these models to get a good idea of what kind of performance benefits first thing. Any other questions here? Yes. Yes. So the senior was asking just to confirm, we're trying to find what the values of a and B should be, the optimal values of a and B so that this swine specific data as well as possible, right? And that's a good transition to our next slide. When we say that we're going to choose a and B to make this model as good as possible. We need a mathematically precise way to tell me how good a model, right? So if I were to draw it to linear models, there's gonna be one choice of a and b where the line looks like this. There'll be another choice of a and b where the line looks like, say this purple one. Alright, and then many of you will, what did this and you'll say the red line is obviously the purple line. Because the red line goes to the blue data points. Alright, but I needed to still this into something mathematically precise. So can someone, so think about it and they can someone raise their hand and tell me a mathematical way to know that the red line is better than the purple line. That didn't see any hands going up. Yeah, the students says, we can look at the average distance from the actual data points we get to the line. Alright, so that's correct it, and let's go ahead and write this out. So let's say that we have, I don't know how many dots there are here, but we're going to say that there are 20. Alright? And we have our model, which is this red one we'll call y equals y equals b1 plus a one x. And in purple, we'll call this Y equals b2 plus a two x, right? So they're both linear models, but they have different choices at the end. Alright, and then the students said, how I judge the model is, I'm going to look at my actual data points, like these blue points. So for each of these viewpoints, viewpoint is going to come with three pieces of information. The square footage, that's the x coordinate. The I'm sorry. It's only going to have tip is a progression as the square footage, the x-coordinate and the announcer fantastic work. Yes. So for the right data point, Let's say that this is the i'th data point and I'll index that by using a superscript i. The input is going to be the square footage. That's the variable x. And for particular data point, data points, that'll be x superscript parentheses. Alright? Then the output is going to be the rent, and we'll call that Y. So each data point is composed of the square footage of the data point and it's rent that is referring to this appointment. And what we wanna do then is we want to calculate said, which is the average distance of our prediction from these data points. And so what I'm going to do is I'm going to quantify the distance here as epsilon. All right? All right. So what is F19 mathematically, epsilon I is going to be the difference between the rent of this data point, which is why I minus the rent predicted by my red barn. So my red line is going to predict the rent of the data points as f of x pi. I'm going to call this y hat. So if I subtract off y hat of I, which is this value right here, that is going to give me the length and sine of this one. Alright. So we'll come back on Wednesday. 