So we're all done with the homework for this class. I have up here are back. So today we're going to finish covering recurrent neural networks, at least that's what a goal. And then the have made new slides on transformers. So it's like a little bit this quarter he would understand how checked GTT works. Alright, after transformers, we're going to cover variational autoencoders. And then after BAs will cover generative adversarial networks, Gans, and then finally we'll conclude with adversarial example. So we're gonna go through these architectures relatively quickly. But a philosophy moving forward is that even though they might be hard to understand the first time around, you already understand all the tools to know and even in cement all of these architectures. We know from prior lectures that for any deep learning model, what we have versus an architecture that is our connected network or CNN. We're going to talk about these orbits are all different architectures as well. We need to find the task difficult loss. We need a way to compute gradients with respect to that loss and that's backpropagation. Know we need a way to update the weights using these gradients. And that's the Catholic gradient descents with their favorite optimizer or momentum. Alright, so you understand those things for all of these architectures. The way that we're going to compute gradients is still with backpropagation. And the way that we're going to change the wage is still missing ingredient. You really understanding many of the ingredients for these different architectures. But then these architectures that will be applied in different settings like when the sequences are timed better. That's for RNNs and transformers. When we're doing unsupervised problems, like trying to find structure in data. We'll use VAEs and maybe we want to also generate data. We can use VAEs and gantry. Basically, what we're going to introduce with these different architectures is that each of these different topics, is there new architectures and how their needs for particular applications? All right, any questions on that? Okay. Friendly reminder that the project is the last part of this class and it's gonna be due Monday of finals week, which is March 20th. Please be referring to the colab notebooks back to Brandon and utilize prepared for you all that will help you to implement to recurrent neural networks even if you don't know how they worked out, but you still will get to you today. And then the TAs are also planning to go for some coding for the project for this week's discussion video. Last week, please submit any midterm regrade request has been tapped by the suspensory because we're not going to accept any exhibit. Any logistic questions. Alright, so revolt, get back into recurrent neural network for our last lecture on the criminal networks as a week ago. And refresh, remember that what we cared about and recurrent neural networks with this new setting, history matters. I care about how things evolved through time. So we said that the recurrent neural network would handle this by introducing a new variable called state. And that state has some value at time t. Alright? And what we can think of the state variable is a variable that contains all of the relevant information about your past historical inputs. Inputs are X1, X2, all the way up until time t, the state variable i sub t minus one would be a vector. That's a simply represents all the information I need from x one to x of t minus one to perform my task well, right, so it's a summary of the history. And then we're going to derive a recursion, or we're going to define a recursion for the state variables. So if I take, I take my state representing everything until I sub t minus one and I get my new input x of t. I could use that to update my state variable from S sub t minus one to S of T. And S of T contains all the relevant information about my templates from X1 to X2. And this will be implemented through a return neural networks. So this variable, concretely, of t minus one is going to be the hidden state h of t minus one of a recurrent neural network. Alright, any questions on the refresher, the motivation for RNNs? Okay, so then we ended last lecture going over how much the question is, will they not workbook output z of t and S of t? The answer is yes. So the network will, at every timestep compute S of T that the hidden state, which we're going to call this h of t, since that's what we've always called neural network hidden state. And then z of t will be the output of the network which the RNN will also. Okay. So we ended last lecture just going over some examples from this blog post by Andre capacity, the unreasonable effectiveness of RNNs. I wanted to just return to this example because this example is going to be super important to remember when you're talking about check GPT, Stanford generative pre-training transformer. And the pre-training part of captivity is exactly this task In the space of not characters but of words or tokens. In this task, what we wanna do is we want to generate text and generate texts by learning structure and the West Greenwich. So let's say that we just had four characters, which are H, E, L, 0, and we want to write hello, okay? So the inputs will be one-hot vectors, where if the first element is one, then h of the second element is going to be the third element. The fourth element. What happens is we inputting h into our network. Input into the network is going to update the state. And the state is going to be the hidden activations of this, sorry, he's gonna be the activations of this recurrent neural network. And then from the state, which here will be zero, we can put a softmax probability distribution over what the next characteristic, right? So currently is pulling up hello, then the softmax probability for each should be highest. Alright? You'll see that in this case it wasn't the softmax probability for 0 was the highest. We know that the target is E. And because we know that the correct target, and we know what an output, that would be a way for us to compute the cross entropy loss to update the parameters. Alright? So this is the same cross entropy loss that we would do for your homework. Let's say that he did have the highest public utility. What I would then do is I would sample from this distribution and hopefully pick up the next character is b. And I would take this character and I would pass it in as the second time step. Input into my R&R. And the RNN would hopefully output our correctly, that l would then go here. And I'll put another l that would come here. Now, I'm putting an output of one key thing to realize here is that In these two cases, the input character was exactly the same. Vector 0010, representing power. But the network was able to output something different in the first case, another L, and the other case 0. Because the hidden state was different. Because the state of the RNN was different. And so really, the state of the RNN again, succinctly captures my historical inputs, is what allows us to then generate texts because it's using information that is learned from historical characters. Any questions? Is there a reason? The question is, is there a reason why embark has three values? Just for ease of illustration, this will be the number of hidden units. Keep saying, Let me just say a number of artificial units in the RNN and it's usually on the order of hundreds of thousands. The question is, what will be the initial state? The initial state of the recurrent neural network can be learned through backpropagation. Any other questions? So all the blocks over here. Yeah, great. So tomboy saying all the blocks here, taking two inputs, which is the prior state and the current input. Is there something special that we have to mention here? Which is yes, there'll be an initial states with no input and that will be learned through almost I said that's not biasing the training process. Yes. So basically, the initial state can have quite a significant effect on the progression of the network. So you want this state to be judiciously chosen oral sex. Is there Any other questions? Okay, So then last lecture, we were showing these examples from Shakespeare. And so in this task, your goal is just to say, given my prior characters, predict the next character. So here if I stop Hello. Now if you say training on the works of Shakespeare, you train for many iterations. At first output shippers, but then learns to output things that aren't semantically meaningful. Like this is Jewish, but at least it follows capitalization, quotation basically grammar rules of the English language and capitalizes names. It puts a comma with an application. And again, it doesn't make sense, but it looks like. English language. And this is the final train numbered from this blog post where it learns the characters are fixed there and again, learns to output something that doesn't make sense semantically, but at least it looks like Shakespeare, right? You also talked about how you could find it too. I'll put Wikipedia articles and we'll learn how to do citations in Wikipedia. You can even give it low-tech code from an, from an algebraic geometry textbook. And it'll learn than to write LaTex code that looks like an algebraic geometry textbook. Were there any questions from these examples that we went through last lecture? Alright. So the recurrent neural network is able to learn long-term dependencies. What this means is it's able to learn things that occur over many timesteps, which is our goal with recurrent neural network. So if you look at one of the artificial neurons, blue to red corresponds to the value of the artificial neuron. And what we see is that whenever there's a quotation, this neuron has a low value. As soon as the quotation closes, the value turns white and red. And so you can see that this is essentially like a artificial neuron that learns that it stays silent until you close the quotation. So the base, the properties of how we use quotations. This is another example of when they use are an entity generate code. And so there was one neuron where they stand at the activation of this artificial neuron has to do with the amounts that you had indented. Alright, so this is a neuron that then learns to be sensitive to the depth of indentation. Your questions here. Alright. I'm going to show you a little preview of the PT network. Was it the general, which is the generative pretrain transformer? We're going to talk about that again at the end of this lecture slash next lecture. But things have come quite a bit since then. And so here's this really cool example. This is from 2018 with GPT-2, which is a precursor of TBT. And it's also trained to just generate instead of hex characters and generating next words. And so what they do is they seed this state of the network with some human texts that says in a shocking finding, scientists discovered a herd of unicorns to the theatre. Remote, previously unexplored valley in the Andes mountain. In the Andes Mountains, even more surprising to the researchers was the fact that the unicorns spoke perfect English. So something that is totally made up, never seen before. And here is GPT-2 completing that task within the same principles that we solve for x characters generation. And I'm not going to read through this right now, but I'll put this up during the five-minute break so you can read. But you'll see that they define a researcher, Dr. Dave Perez, University in the area. They named the unicorns. Unicorns. And then they even come up with a hypothesis for how these unicorns can speak English. And so I think they say the animals were believed to be a descendant, to be descendants of a lost race of people who lived there before the arrival of humans and those parts of South America. Alright, so actually it's generating something reasonable, but it has semantic meaning. I'll put that up during the five-minute break if you want to read a little bit further. Here's another example of a GPT-2, in this case, processing text about the 2008 Summer Olympics. And then being able to answer several questions about the passage. You read this and you wonder how you would do on the SATs. But that we're gonna get to the RNN, however texture, any questions on the examples? All right. I see. Tom boys question is, in this case, how does GET nodes and for bullet keep generating texts indefinitely. So when they train these things, there is both a start token as well as the end token. So it gets to this point and when it outputs again took investment. Other questions. Okay. Alright, so this is the vanilla recurrent neural network architecture. Vanilla. We've drawn it like this. This is how you should think of it, which is that there are a bunch of artificial neurons. And now instead of them before they are connected with the current connection. So there are loops within this network and their inputs that come into the network. Then been nephron kinda percolating on those inputs and an output variable z of t. So the equation of the, of the vanilla recurrent neural network is as follows. H of T is. It is the activity of these artificial neurons at time t. And it is equal to some activation function like ReLu. And they're going to be three matrices of interests. The first is W recurrent. And this is a matrix of connection weights that defines the values of these recurrent connections. And so this is going to be w recurrent times the hidden state at the last timestep, h of t minus one is also going to be an input matrix. And that tells me how my input is x of t map onto these artificial activities, HFT. So there's gonna be a plus wn times x of t. And then there's usually also a bias speeds. Any questions on that equation. And then after that there's a readout. So the rehab z of t is just going to be a function of my matrix W out and my artificial network activity. So z of t is going to equal, it's going to be a linear later, w times h of t plus some other buyers. So that is the equation or a vanilla recurrent neural network. Before we move on to then how we can train with this network. There are few things that I wanted to mention. The first is as soon as we can calculate outputs, we can start to define. So in the example that we showed before of generating the next character, right? You can think of these as, let me just pull it up. The Z of t is r like the softmax scores. So when you take softmax and z of t, it will generate a list item. This is the a t. You can think of. Z of t is the value of the output layer. And these are scores for each of the characters and it doesn't go through a softmax to turn it into a probability distribution. And then I could go into a cross-entropy loss. Alright? So one thing you'll notice is that there's a z of t and every single time step at time step 123.4. All right, so the way that we define the losses can be very analogous to what we've done so far. We'll take that z of t vector and turn it into softmax probabilities. In which case, you'd calculate the cross entropy loss at every single point in time. Any questions there? Alright, instead of cross-entropy loss, you could also imagine doing it, e.g. a squared loss, but we have overhears. Then there are also also had the liberty with an RNN to define exactly what time points matter to you. Alright? So even though we have a loss at every single time point t, we may not care about the loss and every single time point t. So let me give an example. Let's say that my goal is to input 50 characters into an RNN and then predict what the 51st character will be. I may only care about the accuracy of the 51st character and not the accuracies are the characters, one typically. So even though in this case I would have a loss for my first softmax at time step one, for my second soft much at time step two, all the way up until L 51, the cross-entropy loss for the 51st character I only in this context would care about Alpha-1. And so I can choose which losses at which time steps I care about backpropagating for. Any questions there. Makes sense to them. There's different hospital works. How do you fix the probability distribution and there can be any number. Yeah, great. Tom was asking a great question. So we've only shown right now next character prediction, which is really straightforward because there are only 26 characters. I've mentioned to you that checks you can t, isn't generating the next character, but it's generating the next word. And there are many more births than there are characters. So we're all happy on the stage to check digit is actually not generate the next word is generating the next quote, unquote token. Tokens can be worse, but more commonly the birds. So maybe if you've had the word minimise, this would be broken up into three tokens. Like there would be a token for men. There might be a token for n and then write your token for. Chaffey had, I believe, a vocabulary of 50,000 tokens on the order of 50,000 tokens. And by the way, the tokens include the individual characters a, b, c, d. Etc. So that through these 50,000 tokens, you can write any word in the English language, right? So I said before chat TBT is predicting the next word. Not totally accurate. It predicts the next token, but when you combine those two things being equal, birds and that vocabulary is about 50,000. So they're generating softmax probabilities over a 50,000 dimensional vector, right? Yeah, so the question is, during training, do I need to consider all of these bosses, L1, L2, all the way up to 51. So you can definitely compute them during training, but you don't have to use that. So what I'm saying is when you have a loss at the output, let's say l in green here is my total loss. I could have defined L to be L1 plus L2, or all the way up to 51. If I choose this to be my loss, then it will read my network to backpropagate so that all of these loss because our smallest possible. But I can also define in another setting that Alice just equal to L 51 and then back propagation but only change the weights. So that might fit the first correction is as good as possible. And it doesn't care about my first one to 50 predictions. Question is, can I give an example where that would be beneficial? Yeah, That's actually do use quite often for natural language processing. So even though we showed this setting where we're in this example, we do care about the loss at every single time step. Usually. We don't construct these in this way. We will construct them as you put an RNN, RNN, let's say 20 character inputs. And that allows the hidden state to update for 20 iterations. And then you only care about the classification at the last time step. And so that would be a problem of saying, I care about him putting 20th dark doesn't make it into 20 inverse correct distributions for the question is, when you do this, Do these intermediate character distributions of things before the 20th chapter ends up being correct? I'm actually not sure, but I would guess they're fairly accurate. Recall we should do exclusive, right? Exactly. Yes, It's homilies point here is related to Daniel's question, which is, even though we only care about the prediction of the 51st character, not the 50 before that. To get that 50, right, we would have had to do something reasonable. And that's why we would expect that the intermediate characters are also like. Alright. Okay, so that is the recurrent neural network loss using the loss functions that we've done before and character prediction Dolby cross-entropy loss. Alright, so let's take stock of what we know. We know the recurrent neural networks equations. And we can define a loss function. Alright, so immediately we know how they do it forward pass, right? We just compute this equation over and over again. And if we have a loss function, we know how to score how good our networks. We also know that when we have the gradients of the loss with respect to the parameters of the network. We know how to update those waves. And that's just using stochastic gradient descent, right? What's really then left for us to do is to answer this question of how do we compute the gradients of the loss with respect to weights. And in the case of fully connected networks and CNN is right for you that we did backpropagation. We've been at work. But this becomes a bit more challenging now because in the case of the recurrent neural network, there are loops. So the chain rule for derivatives that we just applied in each layer. In a feed-forward network becomes far more hairy when we consider a recurrent neural network. Alright? So how do we take care of this? What we do is we do the recurrent neural network, feed forward network in time. Okay, Let me unpack what I mean by that. So first off, I'm going to add one more thing to this drawing, which is that there's gonna be some initial state H zero, I guess into this network. Recurrent neural network in question was the following. H of t equals ReLu of w recurrent h of t minus one plus wn T. I'm going to drop off the biases. What this tells me is, if I start at time t equals one, then if I know the value of h zero and the X1, then I can calculate each one. So if I know the values of h to zero and x1, then through this equation I can get each one. And now I would know the value of X2. X1 and X2 would combine to give me a value H2. Alright, and I've drawn that right here. So we kept the cute H1 or H2. Know if h is zero and x one. To compute H2, I would just get to know each 1.2. Okay? So going along this axis here, we have time increasing. But now if I unroll this recurrent neural network equation in time, it becomes feed forward into time. And the number of layers of this network, however much time I'm using, my God. Any questions there? Yeah. So Tom Waits question is, can I say that one more time? How am I unraveling time? So what we're going to do is we're going to take this recurrence equation, right? Which in general will have groups because WE couples the various artificial units at time t minus one to time t, will draw that in the next slide. And I'll have another slide that says this more clearly. But if we look at this equation, we see that to compute the hidden state at time one, all I need to know is the initial hidden state h zero and my input X one. After I know each one, I can increment from t equals one to t equals two. So at time t equals two, I get a new input x2. And what this equation tells me is that I can compute H2 as long as I know what each one was. I got one already from each with their own X1 and X2. So from each one and x two, I can compute H2. Alright? So if I do this for every single time-step, then what I see is that when I write this equation, but I stand out time, right? This looks like a feed-forward network. There are no groups in this computation. Recall calculating the H1, H2, H3 is where all that I needed to compute the output of my network, because the output z of t is just a function of h of t. H of t. I can calculate z of t the output. And if I have z of t the output, then I can calculate my loss functions. I will unravel this further in the next slide. So let me just go to the next slide here. So let's take a concrete example where I'm going to have organelles. So here my hidden state h of t is going to be a 4D vector. And let's look at how this has recurrent by looking at the part of the equation ReLu of w recurrent times h of t minus one. If w recurrent or equal to this matrix. Then what this equation tells me is that I would add an h of t minus one. So I'm gonna write this as a 4D vector. Actually, let me, let me do the following. Let's, let's make this h of t plus one and make this thing h of t. So the four elements of this vector, I'm going to call h of t at index one, index two, index three, index four. Now what I know is if I go ahead and I take the ReLu, this W recurrent times h of t, That's going to give me h of t plus one. So this is going to give me h of t plus one. First element, h of t plus one, the second element. And so on. I'm going to call this vector h of t plus one. Way that we typically use is via a graph like the following. So if I look at how h of t plus one, the first unit. Is affected by all of the other units. We can see that h of t plus one in the first part official unit is gonna be one times h of t, the first unit. So there's a one here plus the value of the fourth unit at time t times 0.4. Okay? So these two values, I'm going to draw my graph via these lines. So the first unit has a connection to itself with value one. And the fourth unit affects the first unit with a weight of 0.4. So that's this connection right here. This thing here equals 0.4. Similarly, if I were to look at, Let's just look at the second unit, h of t plus one for the second unit is going to be 0.9 times the first unit. So there'll be a 0.9 over here. The third unit will have a weight of 0.6 to the second unit. The third unit here will affect the second unit with a weight of 0.6, and then the fourth unit will have a weight of 0.5. So that's this one right here. And if you do that for all of these and the networks, that's what this recurrent neural number. Any questions there? Okay? Now we're going to unroll this. And so what we're going to do is we're going to take this same network. And recall, we had this connection being a one and disconnection being 0.4. Alright? Now when I look at the values of my activations, so this will be in by h of t, which is a vector and R4, that's these four circles here of h of t minus one over here and h of t plus one over here. What we're saying is that h of t minus one affects the first unit with a weight of one. And therefore, this week here is a one. And then going from time t to t plus one is still affects it with a weight of warrants. So this is also equal to one. Then the fourth unit affects the first unit with a weight of zero point for this weight here is zero point for this weight here is also 0.4. Alright, I'm just going to do one more. I think on the prior slide we had that this value here was 0.9. And so now all of these connections, which tells me how h of one, sorry, the first unit affects your second unit. But each time step will be 0.9, right? So you can do that for all of these connections. And that takes my Recurrent Neural Network and unravel in time. Questions. Question is, is if I weight values are very small, isn't that really bad? Yes. So we'll talk about that in just a bit. What we'll find out that these numbers are really susceptible to vanishing and exploding gradients. Daniel's question raises another point, which is what you should have noticed also with a disability before neural network, where the number of layers is equal to the amount of time I run my number for. Every single weight connection is exactly the same. So you can think of this feedforward neural network, but all of the layers have the same exact way, which is defined by w recurrence. And now, like Daniel was saying, if W returned is small, that all of these wastes will be small and their gradients are going to vanish. And then if w wreck has any values that are both one, then after repeated application, your gradients are going to explode, vanishing and exploding gradients are really big challenge recurrent neural networks. It's tempting to train in between time steps. The first step, the question is, is w Rec, also a function of time? This W wreck ever going to be different than this w back. In return neural networks? Almost always. W Req will not change as a function of time. There have been some people who have thought about this, and there are some architectures where W direction change in time. An example is called the multiplicative recurrent neural network from Ilya Sutskever and Jasmine and James Martin's, where they do have a recurrent matrix that changes with time. But it's much more challenging problem. Other questions, Yes. Question is, when does the learning for w rank tracker? I will get to that on the next slide. So this is what the computational graph looks like for recurrent neural network. And what you see in this computational graph is that we start with an initial state and multiplies w rash, that we add a wn times X1. We add these things together and we applied our revenue, which is this function f. And I guess the H1. So this is just implementing that equation. Let me just write it down one more time. H of t equals. Instead of maybe I'm going to write f and it's going to be W times zero plus wn times, sorry, this should be h of t minus one times x, right? So this is just a computational graph for recurrent neural network. And then the hidden states are also used to compute the outputs Z1, Z2, Z3, etc. So the parameters up by recurrent neural network or this fall is there, w in W wreck and WWF? But in reality, the thing that's most challenging to learn is this w recurrent, right? Because this W recurrent is that one matrix that is applied at every single time step to update my current state. Okay? And this is the thing that is going to learn, those temporal dependencies over the history of your data. All right, any questions so far? Alright, so let's just consider a first setting where our loss is a cross-entropy loss. And we're doing next character prediction where we were, we care about the softmax probabilities and every single time. Alright, so here you would have that and l total l equals L1 plus L2, L3. In this case. This is a feed forward neural network. There are no loops. We know that when we have a feedforward neural network will have to do is back propagation the loss to the parameters to get my gradients. So if I were to start with my loss, I can backpropagate through this one path out three. That would give me a gradient with respect to z3. I can get a gradient with respect to h three. At this point. Let's say that I take this pathway to w, right? That would give me a gradient of the loss with respect to w recurrent. But I could have also at this point here, backpropagated through this pathway and then Gonstead W rack, I could have even gone one more time step ahead, and got to w. Similarly, I could have backpropagated through, Let's see if I have another. I could have backpropagated through L2 to get the gradient for w recurrence. I could have backpropagated through this capitalist to get ingredient. We know from our derivatives that you have multiple backup in the path-based object to a single parameter w. What happens is it a gradient is that they all add together. All right, So basically in this computational graph, if I want to compute DL, DW wreck, I'm going to have to be cognizant of every single pathway from the wasp to W recurrent, I'm going to have to backpropagate through every single pathway and then some other gradients. That'll give me a deal. Dw wretch. And then from there, I can just use adam or STD or my favorite optimizer to update the value of the current. Any questions here? Perfect. Yeah, The question is, are there also multiple Casper WN? The answer is yes. So Wn is gonna be the same at all of these wires. And so when we backpropagate, there'll be multiple passwords, W and skills. Great. Yeah, so this student was noticing that there is a growing, I think you say quadratic. I'm going to ask the TAs to just read about that to tell me what the complexity is. But as the number, as the amount of time steps grows, there will be a growing number of paths and that will, that will increase the complexity of the Talia is quadratic in time, but I'll perfect quadratic in time. They're trapped. Sorry, Another one at a time. The question is, when we talk about stochastic gradient descent, is that for one time or in other words, how do we use dashes here? So this DL DW rent is independent of time. So when we calculate DL DW rat will happen. You can think of it as like we're basically summing across every single possible path for the gradient to give us a single gradient of how changing w record a loss. But then because this is independent of time, then we can just use our favorite optimizer is just as GDA would be minus epsilon DL DW. Topic. Exploding gradient problem. And bring it back propagate again. We're getting great. Takeaway is asking a very deep question. Was I'm watching reserve for just the next slide. So let me first talk about what this backpropagation but looks like and then raise a problem. And then we're going to get it from right? So let's go ahead and just try to backpropagate through one of these w retry operations. So let's say that I had the activation or the gradient d L, d h three. And I want to backpropagate to hear DL TH2. And let's say that the function f was a railroad. So I want to know how to compute DL TH2 given my upstream gradient, d L, H three. Alright, you guys have done many times, so we'll just go ahead and do it. If I backpropagate through a ReLu right, we know that that means I'm going to take the Hadamard product with this indicator function of h to being bigger than zero. I'm sorry, No, it's not. H2. There'll be w wreck times H2 being bigger than zero. So that propagates through my ReLu. And then if I want to back propagate and DLD H2, I have to backpropagate through this matrix multiply. We know that this matrix multiply. We'll take the gradient because I'm backpropagating TH2. It'll be the value on this wire transpose w rack. So this will be a w transpose. So that's how we backpropagate from DLD street and DLB issue. I'm now going to change the reason to QC h of t and h of t minus one because we just see that if what the backpropagate one more time step to deal TH1. We're going through the exact same operations, alright? And so we would just have this equation compounds. So I'm going to change these CL, DH, DL, DHT minus one through this operation. Any questions? Okay, Here's where we see the problem of RNNs, which is, if I were to backpropagate through a really long path. This is just talking about what we already derived from the prior slide. That back propagation through this path involves the multiplication by W transpose. If I blocked out propagating. Now, I'm sorry. If I'm backpropagating from GLD H32 CLTS one, I'm going to have two of these w transpose is, if I'm backpropagating through even more timesteps, let's say ten timesteps, right? I'm gonna have to go through ten multiplications with WEBrick. I'm going to have w transpose raised to the power of ten. When I have a matrix raised to a power, because I'm doing repeated multiplications. I see that delta t is the number of steps in time we're going to backpropagate through. So let's say that we're backpropagating through ten times delta t equals ten. I'm going to multiply it by w transpose ten times. Let's say that w transpose has this eigenvalue decomposition. When I multiply it by itself, delta t times ten times. When you write this out, where you're going to notice is that the u minus one times use are all going to cancel to give the identity. This is going to therefore simplify to u times the eigenvalues raised to the power of ten times U inverse. When I say this matrix lambda, the eigenvalues raised to power of ten. What I mean is if I have a matrix that looks like this, lambda one, lambda two, all the way down to lambda n, like n eigenvalues and zero everywhere else. Then lambda raised attendance is going to equal. Lambda one, lambda two raise to attend all the way to lambda n, risk retention, looser as everywhere else. This is really bad because lambda or your eigenvalues, Let's say that we wanted to, I'm gonna make this even more exaggerated. And as you see, let's say that we want to backpropagate 100 times x, which means that I want 300 characters to predict the next word. If my first eigenvalue was even just slightly bigger than what? It was equal to 1.1. Then lambda one raised to the 100 equals 13780. And then if lambda one was just slightly less than one, or 0.9. Lambda one raise to the 100 would equal 2.65 times ten to the minus five. So if you wanted to backpropagate through a lot of time, your eigenvalues deviate from one at all. If they're less than one, you're going to have vanishing gradients that they're greater than one. We're going to have exploding gradients, right? Any questions there? Yeah. There's much. The question is, what are the eigenvalues corresponding to? These are the eigenvalues of like W returned, right? Great. Yeah, so generously, a great point, which is these are the gradients D L with respect to the hidden states. But they're still important because whenever I want to finally stopped going back in time and go to w rack, right? It is going to be like a TLD H2, H1 transpose DLD H2 is really big, then it's gonna be a really huge gradient. And if guilty exponentially small, start with DLD, H2 is really small, that's gonna be back in Britain. So let me just write that out here. This gradient would be pH2. There'll be multiplying spill the indicator for the atom product, as I've indicated for the ReLu. And then it would be times in each one tracks does. So. If this thing explodes or banishes, the whole thing, vanishes. Other questions. Alright, so this is really bad because as long as w wreck deviates from identity or something that has all eigenvalues are equal to one. We're going to have vanishing or exploding gradients. Now let's get to time-wise question earlier. Tom Waits. The question is, let's say that the gradients batch, alright? So let's see if all the eigenvalues are less than once, but brilliant Spanish. What that means is that if I back propagate through this path, right? Go through three W Rex, Right? Or in general, 20 W racks, that gradient will be equal to zero. However, there are all these other gradient past. And in particular there's this really nice gradient path that goes from l directly to W ranch. So once I still have a gradient to train a good w for this task, the answer is no, but I want somebody to tell me why the answer is. This is a hard question. Disproportionately takes into account the person's, right. So this student is saying disproportionately takes into account the first few tokens or words like, like the effective x, y, naught and power. There is also another gradient descent gradient path from L through L3 that goes straight to w. Recall. This would incorporate information about the third one. So that's a great idea. But in this case, there will be one, a gradient plot that only goes through one W rec for every week to replace the perfect. That's exactly right. So when I am wanting to learn how my characters 2020 vertigo affected my current work, I need to have a W Iraq April to find dependencies across 20 timesteps. When I look at the one W record gradient past that go directly from the loss of w records. Those are, those are good. Pastor telling me how the current token affects the loss, but it won't tell me how the first token through two timesteps effects the loss. Alright? So if you want to be able to learn long-term dependencies, you have to be able to backpropagate through those long temporal dependencies that tell me how one affects the third character, two times that fader. So if I want to know how X1, the first token affects my Word 20 times up Fader. I'd have to backpropagate through 20 of these new tricks. I've only consider these ones. The only, tell me how the current token. Yeah, the question is, let's say that w records really small, right? Sorry, Let's say that we had vanishing gradients. The gradients DL, DW requisite really small dental history effectively lose the recurrent return on average, the answer is yes, because we're just back off and getting through. They're really short paths blocked right? After this. Tom ways question is, well, I am going to have an age 50 e.g. and it is going to capture information about my past 50 inputs wanted. And what we're saying is, if you weren't able to backpropagate through through these 50 W wreck multiplies. Let's say that you only could relate that propagate through one. Then each 50 is essentially going to be a function of x 50. And very little of each 49. Any questions there? Jack? I'm just reiterating. If w is small, then W records not going to learn how to take X1 and effect outfit. It's not going to learn how a character 50 characters ago affects my current prediction. So W Req is small. It's basically just going to be like a feedforward neural network where I'm using my current character to predict my, my contacts something. Alright. These are, these are not easy concept. So if you're not following, but that's okay. It just takes some time, so please be sure to review the Spartans. Alright, so let me just do a few more things to finish off the yellow recurrent neural networks. How do people address this? Because this is a pretty big problem. And a story is, I was in graduate school for the first half of my graduate school. Learning wasn't yet experienced revival. And so we trained recurrent neural networks, but we never did backpropagation through time. I'm sorry, I should have mentioned this operation of unrolling the computational graph and backpropagating through it is called backpropagation through time. Backprop through time. The PTT. And we were writing a review paper and one of my colleagues to lead are all meant to be a scientistic brain. And then later, what he did is we're finding the UK for him. He wrote a sentence, essentially said that propagation through time is awful because of these vanishing and exploding gradients. And therefore, you always want to be using a different approach. We think we'd cut that sentence of the paper because after the deep learning revivals, people thought of tricks to trained with backpropagation through time and that's the prevailing way to train for recurrent neural networks. So how do people address this? The first way is we may just do truncated backpropagation through time. So let's say that I was bringing a neural network for Delta T equals 100 times steps. I need not be able to backpropagate through 100 time steps without having vanishing or exploding gradients. So I might just choose to backpropagate through 20 times x. If I do this, then what I'm basically saying is I'm never going to go through any path here that is more than 20 timesteps, even though I have 100 times steps. And we're basically conceding that because I can't go beyond 25. My neural network has never want to learn any historical dependency graders in 25 steps. I wouldn't have been able to train this incident, vanish or explode. That's the first way for exploding gradients. Gradients, I think I'm going to talk about this before for other networks and services, the gradient of the equation. And then for vanishing gradients, we could do a regularization topic. And so this is from husband pesky, Daniel and colleagues in 2012 where they add this term to the loss function and this regularization term is actually, it looks intimidating, but it's fairly simple. So if you look at this term right here in the numerator, this term right here, if you apply the chain rule is like DL, DHT, right? And then this term over here is the l th t plus one. And so DLT is t divided by delta H2 plus one is close to the value of one. And this regularizer is especially regularizing that. The norm of DL PhET is very close to the norm of T plus one. And this helps your gradients. You apply the current Neural Networks. Regularization is quite helpful in the case of the vanilla recurrent no numbers. Let me do one more fundamental. Think of it right? We started this off. Your initialization was really important for feedforward neural networks. For RNNs. If you're using a vanilla RNN clock and colleagues in 2015 suggested this initialization trick of settings W returned equal to the identity, has all eigenvalues of one. So just start, you're not going to have any exploding and vanishing gradients. And then there was another initialization in 2016 that has the same spirit, max, it initializes W, right? So that is Maps eigenvalue is equal to warn both of these perform. Let's take a five-minute break and then when we come back, we'll continue on with the LSTM architecture. I'm going to put up that GPT-2 text. It's going to spread. Regarding the readings for me. That's the direction. Right? Right. Yeah. Kelly, take some design that we use different fats like so what onetime step for one head and one time a truth instead of just kinda yeah, you could do that. I actually had a chance to give any paper. A lot of sense. Yeah, yeah. Another, another problem is that I just wondering, what's the time value of X t and h t n 0. It could be all different. So one can be n-dimensional, H could be n-dimensional. So then WE would be n Pi. Key to management on reading about wishes were based on me. Yeah. So like if you drink character predicting Vensim, It's going to be 26 dimensional, okay? Maybe a bit higher for other kinds. Okay, I see, So this is why they are all the same from my point of view, these two mice so important this week and this week, but they are different. This is really importantly. I can see that again, when the diamond use of x1 and z1 really different than those weights, that's due to anemia transformation for the output. Here, for the input. I really always important, even if, even if, even if their decision region, because you can think of like zero or like my teachers, this W added, It's kind of like the last linear layer of a neural network. So even if before the last layer, your features were already 1,000 and you have a topic. It's still good to put up linearly. Okay, I see. Thank you. So why do we add losses? Because in the press just put some as 51. Yeah. We don't have to. So it could have been that you only cared about L3, which gives you this backpropagate through L3. So I just give me a general effect on you're going to get oh, that's my bad. If I have to reply to shoot me an email, I just I just missing a minute. Okay. Yeah. Rnns. Right. We have we're assuming, like we have to wonder current page just represent all the dynamics as we evolve through time. Yes, tuple considered like cutting it off at a fixed contexts length and then doing tension on the states. So exactly similar, but some differences. What's this? You have self-attention, you have white global rights. I think I felt like if you want to see them at zero influences, I would think of attention. Exactly. Yeah, So we're going to find that, yeah, Basically for an opening and many other tasks, attention slash F farmers will make all okay. Because you don't have to global attention layer, then. This far easier to see. Okay, but is there like this, I guess, is there a difference between applying transformer on a sequence of hidden states that are predetermined price or exchange the projected in transformer encoder and walks of that on those spaces and reconstruct all the ones. I guess it's not really much different from an actual transformer. I mean, like if you think of Applying a transformer to Elmo and Betty's, that's kinda what it's doing because Elmo embeddings are using recurrence to try to figure out how the contexts later on at the start. So yeah, it could be studying, right. Okay. There's referring to my global attention on top of it. Okay. Yeah, yeah, super interested in. All right, everyone will get back to it. Any questions on anything from the first part of lecture or anything on vanilla RNNs. Now we're going to move on. So each step of the body. Yeah, yeah. So the question is how will recreate that? So basically, let me actually just give any staff or with Shakespeare texts. Let's say that we were trying to decode the 11th character from the fire ten. So basically the way that you would create a bachelor's, you would take ten characters. I'm not sure if this is ten, but we're just gonna say it's ten. This is going to be the inputs X1 to XN. And then the output will be the prediction of the letter. And then what I could do is I can shift this over just by one. This is another example where I have ten characters as input and then the correct output is whether, all right, so within a single article or a single block of text, you can extract from many different examples. And then these can be randomly sampled into your batch to then compute gradients. Chapter did he use his doctrines of white guy, several million million exactly. What goes into a single tenant. Right? Thomas question is, just to confirm the size of your bachelors, not the number of sub-sequences yet. Yeah, your batch size. It could be like this example like 256, it would, you would take 256 examples of ten characters followed by an 11 that answered the question. The question is, are all the examples in the batch of the same sequence length or would that bias training? In simpler examples that I've seen is typically the same sequence. Although I'm not sure if that's true. Intact GBT doesn't make sense to use different potentially to have better generalization. Does anyone know if Chuck E. Cheese is different than the sequence things like Daniel says, you may just padded if DFS. Okay, great. Yeah. Usually for these large-scale multiply the whole time. Any other comments on anything RNN related sequence? Yeah, so Russia was asking, you can take ten characters. And instead of competing and output at every time step. And then only catching the 11th, put it could take ten characters and just absolutely love into one. Basically, if you can find a feed-forward network where the input is at ten, character is three, and the output bit 11 characters. And yes, you can do that. Any other questions? Okay, So we're gonna get to the most common strategy to avoid vanishing and exploding gradients and RNNs. And that is to not use a vanilla recurrent neural network. But then actually changed the architecture into something called a Long, Short-Term Memory or an analogy here is that remember, the amount of time that we're going to have to work for is the number of layers that we have to backpropagate. We know that for convolutional neural networks, they fail when the number of players becomes large because gradients explode. But Resnick gives you a gradient highway and allows you to still train. You had any layers. The LSTM is also going to give us a gradient highway, although it's gonna be different than that, but it'll give us a gradient in the backpropagation through time. And that allows you to learn longer scale temporal dependencies. So we'll talk about the architecture. And the first thing that we're going to do is show you the equations are the arguments here. So these are the equations and it should look daunting and completely unintuitive. We're going to unpack what these are. So firstly, just a few things of notations. Remember that for every hidden state at time t, and an RNN is going to be a function of my input at time t and my hidden state at time t minus one. And so when I write up a matrix like this, WF, WF, you can think of as being comprised of a recurrence component and an input component. And this being multiplied by h of t minus one and x of t. H of t is a vector in R n and x of t. What's a vector in R, m? Then wf would be a matrix that was n, that is n by and parsing. All right? So that's what that notation means here. You'll see here that there are bunch of sigmoid. So this is the sigmoid function and it's gonna be applied element-wise. Montane Hs. We're going to see when a teenager is used. Usually that means that we're writing a value or we're calculating a value. Whereas when a sigmoid is used, we're going to see that that corresponds to a gate where one means the gate is active and zero means the gate is closed. I'll explain that in just a bit. So what I'm gonna do is I'm gonna take these equations and I'm going to write them into a diagram. So this is what the diagram looks like. To make this diagram look interpretable, I've omitted all of these matrices, WF, and the bias is WIDA, et cetera. But basically, what we're doing is in this diagram, we're going to start here. We're going to concatenate h of t minus one and x of t. That's computing this element, alright? Which is the, which is a part of all of these calculations. I'm not going to draw this WWF, but I will draw the sigmoid. So this sigmoid has an implicit WWF associated with it. And so the output of the sigmoid is going to be this value f of t. So f of t is sigmoid of wf times that concatenated and put a good candidate input, multiplies the WWF, goes through the sigmoid and I guess, alright, so just looking at the rest of these equations, I of t is this concatenation multiplied by an affine bare pass through a sigmoid. And this I of t is this right here. And t is the output of a tan h times this affine function of the input. And so this is v of t. Now there's one more variable of t which corresponds to this. Any questions on just how it is represented? All of these are scanning brains. Okay? We're going to talk about then how we intuitively think of all of these operations. Before we do that, I want to talk about how we should conceptualize this new variable, c of t. This t of t is called the cell state. And it's a critical feature of the osteon that allows us to learn really long temporal dependencies. So you should just think of the cell states here, t as a memory or two. I'm just going to call it a memory from there on out. When you have a memory, they're gonna be three operations that we can do to it. The first thing is that you could forget things from your memory, right? The second thing is that you could write in new information to the battery, so I want to add new memories. And then the last thing is that we could read out information from this memory or from this two. All right. Any questions there? Okay, so those are the actual three operations that these pathways to the LSTM. We have our memory or archaic state. And the first thing that we're going to do is you're going to multiply it by f of t, which is going to be called the forget gate. As of Tuesday, output at the output of a sigmoid. So that's what she's gonna be. Something between zero or one. If f of t equals zero, I multiply myself state by zero. I lost everything in the cell state, right? So f of t equaling zero. When you started watching, forget all the information in myself. The second function will be, let me do this in green. And I of t times v of t. And you'll see that we'll talk about this a bit more on the next slide. This thing is added to the cell state. So this is how we write information. If I want to add a memory to my cell state, then I write it in through this pathway. Then lastly, reduct information for myself states. So to get my hidden state at time t or the state of my Recurrent Neural Network at time t. What I will do is I will read out information through this pathway. And again, we'll talk about that in just a bit more detail when we talk about these individually. But basically, this multiplication is the operations for getting information. This addition operation to write information into the cell fate. And then this pathway that reads out is how I extract information from the self-care. The goals and the minus one. The same thing, 30,000 sequence. Yeah, so tomboy is saying there's a multiplication by a sigmoid here. This is going to be called the output gate, right? Can I explain how this doesn't cause gradients to vanish? I'm going to reserve that question for lamin talk about each of these cases individually, if that's the case. But it's still a binary event. Yes. Streets, especially bankruptcy. How much question is, is this a special kind of sigmoid other than the sigmoid that goes 0-1 notion typical sequence that we know, so it's an analog value. Any other questions? Alright, so let's talk about these gates that individually. The first is the forget gate, and I don't know why they chose this nomenclature that you should be called, but remember, the reason why is that the self status updated as c t equals to t minus one times the forget gate. So if the forget gate is close to one, then the cell state at time t is approximately a time, the cell state at time t minus one. And therefore the information and the tape and the memory is maintained. Alright? And that's where I think it should be called the remembered date because I think for debt is one that I want to forget is close to zero. I multiply C T, T minus one by zero and CT then becomes equal to zero. And therefore I'd forgotten any tax information. Okay, Any questions there? Alright, so that's the forget gate. And just to be absolutely clear, given Thomas question, f of t is going to be some analog value, 0-1. You should be practiced because we can take on those values. Alright? So f of t equals to one, we remember information of t plus to zero, forgetting. All right, the next part is running in information and text on the slides. So let me just tell you what's going on here. When we writing information, we have are two things that are computed. The first is called i of t, the input gate. And then the other thing, I actually call it something different than the paper, I'd call it a DFT. I think the papers might call this v of t, g of t. G of t. I call it that way because it's a valley. So whenever there's a tan h and another big effect tan h activation function, the output of a tan h is a value that I care about. And then any sigmoid is essentially a gate that tells me how much of that value I want to write. I am going to always be computing a value to write into my sulfate That's the empty. Then the amount that I write it in, it's going to be dictated by I of t. I of t is close to zero. I will not provide any information to my selfie. If I of t is close to one, then I will write in whatever value is computed by v of t. Alright? So that's the strike here. I of t, v of t is a value between -1.1. It tells us the value that we want to add to the cell state. But then how much did that value I actually add is determined by the input gate. So I have t is 0-1. It tells me how much of the value to write onto the soft food. So I take IMT and DFT, I multiply them together. Any questions? Great, yeah, The question is I of t and VMT process the same input but different productions of them. Is that right? Yes, there is, yes. So the matrix for the value date will be a W V, and the matrix for the input it will be a WIMPs will be totally different. The question is, is there any significance to have a negative value for V of T? T can be positive or negative. Some students sometimes ask, why do we need this IoT gate? Because can't write in zero information by setting d of t equals to zero right? Here, return to an answer which is that this architecture makes it easier to write than zero information because instead of tuning via TWO size value of zero when you don't want information. To be ready, man, all you have to do is get the sigma two equals zero, which is a lot easier because if this is a very large negative number of sigmoid, it back will be equal to zero. So it's easier to not reading information when you break it down. Alright, and then the last thing that's reading in information, last thing is reading out information and read that information. We take ourselves, we apply our activation function to it that can age. And then we multiply it by what is called the output gate. Just like the other ones, it should be fairly intuitive. 0 of t is equal to one. Then I'm going to read out everything from myself and all t equals zero, then I'm not going to react anything at all. And so 0 of t will be the output of a sigmoid. I don't have the equation here, but it was on the prior slide where the sigmoid of a W0 and toxic contaminated. Any questions there? All right, so that is the LSTM architecture is just a way of dynamically altering the street. Next. Next seat have some stored information from the district. I'm going to show you in just helping somebody that you're writing, so I'll do it. Right? Yeah. So that's right. Okay. I see where you're going. Thomas question is essentially, is this entire structure of the LSTM adding anything to the vanilla recurrent neural network? Or is it like primarily because it gave it additional capacity and computation? Or is it really just something that facilitates training by giving you a gradient highway? And the answer is, it's the ladder. So it actually turns out that a vanilla recurrent neural network is a universal dynamical systems approximator. A recurrent neural network can implement any dynamical system. The LSTM is a constrained version of a dynamical system. But with these architectures, however, it's enables you to practically changed because of the gradient, either which we'll talk about this next slide. The question is, when we backpropagate through this, we have saturation. The are you talking about the tan h over here? Oh, I see. I see. Yeah. So if you want to backpropagate through h of t minus one, we're going through a bunch of sigmoids, which we know can, can kill your gradients. And so that is one consideration of the LSTM is maybe not as great. Yeah, Perfect, Yeah, that's a perfect analogy. So this student said, Can we think of an LSTM to an RNN analogous to what a CNN is to avoid connecting number, the answer is yes. So point to Neptune numbered. Implement any function. The CNN is a constrained version with spatial locality like what you said. But that proximity trains veteran does better because it has so many fewer parameters and takes into account things related to vision and the LSTM by analogy then. Is like a constrained version of recurrent neural networks. So that's a really great example. Other questions. Alright, so let's fight for this. I don't. So if you look at the cell state, this tells me is the memory of the recurrent neural network. And you can see that he gets a gradient highway. Because if I calculate DL, really ugly L DL DCT, we know that when you go through a plus sign, just pass through. So backpropagating through a plus sign, we still have DLD CT, and then backpropagate through multiplication by a backup location I multiplied by the value of the other wire. So as long as f t is equal to one. So if f of t is approximately equal to one, then d l d c t minus one equals d l DCT. Alright? So the only way for my gradients to die in the LSTM is if the forget gate is close to zero, you should look at this and see that this is actually desired behavior. Because if I forget, I don't want to remember anymore temporal dependencies. And so as long as they're forget gate is not equal to zero, which means that I walked through, remember everything from this past history. My gradients are always going to survive and just gonna get a gradient by way to backpropagate the reference through any questions that you raise your hand if this is making sense. Okay, great. Yeah. Because of that property, PhD minus one. Yes. Backdrop to the minus one. By the, by the forget gate. Oh, sorry. You mean by, by this h of t minus one? Yeah. So Tom way of saying that when we backpropagate to h of t minus one, there's going to be several backpropagation is through sigmoids as well as other matrices. However, even though this is the case, it's not repeated multiplications of them. So if I want to know how much, say 50, related to X.25, right? As long as the forget gate was never a zero between time 25 to 50, I will have a strong gradient backpropagate through 25 of these. That thank you, goes to x 25. And it only goes through WWF warrantless. The pathway that relate to the t minus quantity has a gradient highway. Whereas the vanilla recurrent neural network, the pathway that relates t minus one to t, have to go through a matrix, multiply that matrix, multiplies what explodes or vanishing gradient. Gary, and only have to go through this. I might have misunderstood the question yet. There's a WF here, but this is only multiplied by warrants. So basically I go from C to D and we go like a d, l DC 5D. I would go through 25 of these to get a d L, d C25. And that will be capturing temporal 25 times stuff. But then if I were to backpropagate h of t minus one, I would only have to multiply by WL one. So Tom, I sang in this part here, this would be a DL DH 49. 49. Way that the memory is maintained is through the cell state. So this deal, DHS 49. It will have some impact on the sulfate and backpropagating. But it's okay if those gradients status because all of the information I want to remember is through illustrating pathway. So now what I was saying is that you replicate this pathway 25 times. I would get some DLD, see 52 DLD, C25. And now if I want to note the LDH 25, I would only have to backpropagate through one matrix multiply, right? Yeah, if you went from h t or h of t minus one, you would have more matrix multiplies. Yeah. But the memory is maintained by the CTUs. You can think of the HMC, that's just a readout of the CTs. And a CT has got really hard, right? Yeah. So Roxanne was saying if there's just one solid state, we're at the t equal zero, then all the gradients. But that's desired behavior because it forget it goes, is there evidence? Okay. Great. So let me just almost as it's clear now and I just want to reemphasize what he just said, which is we don't really care about or I mean, we're not trying to backpropagate from DLD HT, DH, t minus one. The thing that keeps the membrane is the cell state. So as long as we can backpropagate through c of t, then we have memory over time steps for the CMT is the key thing, the recurrent neural network. Other questions, Solving, vanishing and exploding gradients. We're solving both because the gradient can never get larger along this pathway. It can only get smaller. Oh, okay. Yeah. Yeah. That's a fair point. So here he is pointing out that there's a gradient that comes up through here that will also add there. In which case is possible for gradients to explode if at every single time step there is something being added. Longest pathway. Okay, Just in the interest of time because I want to at least start transformers since these days. But whenever you do natural language processing tasks, even though before we use recurrent neural networks. Beginning and I would say maybe it's like a 2016 or 2017, 2018, really replace these with transformers. So that's the LSTM. There's one more architecture called the GRU and I want to skip this. But basically in your project isn't LSTM or GRU. The GRU is basically like the LSTM, except instead of having four times the number of variables introduced, there's only three times, because basically they have the hidden state double up as a start state. And if you look at the equations of the GRU, just like the LSTM, there's gonna be a gradient highway on the h of t. So I'm just gonna leave that for you all if you're interested in looking at the details of the GRU. Alright, so we have training recurrent neural networks. If you're using a vanilla recurrent neural network, you should use gradient clipping. And if you're experiencing dash ingredients, you should use that past Daniel regularization that we've talked about. Also, you may consider using truncated backpropagation through time. That's where we say I'm only going to back propagate across 25 steps. I will never learn dependencies greater than 20 timesteps, but resect like gradients can vanish unexplored. If you're not constrained to use a vanilla RNN and that's almost all of you in most settings, you should just use an LSTM or a GRE. Alright? Any last questions about LSTMs? Grus? Alright, we will get to attention transformers and catch can see that these are new slides. Never talked to them before. I gave them this weekend. Hopefully they'll be clear, but he's asked they aren't. So our goal will be to understand how checks you can understand how attaching the tuberosity personally to understand what, how GPT works. Gpt stands for generative pretrain transformer, so that T is transformer. To understand that we need to understand transformers. Transformers, transformers, something called attention. So we're going to start off with trying to understand attention. And here are some really helpful resources that you can refer to in the future. If you want to. You want to see other people's takes on how they teach this material. I think that these are all great resources. So I wrote them from Jay LMR and then a few weeks ago onto a carpet, the Fc uploaded like a 1.5 to two hour video of him just implementing GPT from scratch. So if you want to, if you're someone like me who really understands things by, since anatomy, I recommend that video to you as well. Okay, I'm gonna keep the motivation of cats and BC relatively short because first vector I asked him and he had already played a bit, I'll say three examples. And these examples are homework questions. So this is the very first question that we asked you in homework number one. Let Q be a real orthogonal matrix so that q transpose and q should be a minus one are also orthogonal. I put this into chat and I'll tap good. And it generated lot of tech code compiled. This is compiled to answer. This is a correct answer. This is a question that is asked a lot in textbooks. And so I thought, Okay, well, you know, maybe it had memorized this from another textbook. I went to question number one where we asked you this probability question. And it says, we made up the numbers ourselves about the percentage of students in each of these natures. And we asked you, what is the conditional probability that a student is from Science, given that they liked the lecture and chats, you been outfitted this answer. And this answer is, in a numerical sense, incorrect because this is not the right number. But the only reason this is not the right number because it did this algebra wrong. So if it actually did the algebra correctly and got this number, 0.1, 443, but the actual number, it would have gotten the correct answer. All of the reasoning and logic that is used here was correct. And it just got this probability. Tom void. Putting this to check to see if this is the professor who teaches convex optimization here. And this is even more challenging problem where you have f being a convex function and you want to show that this set is convex. And this answer by chatting generated model is correct. All right, so what this is, this is gonna be a challenge for, for our cohorts in the future. But we know that chat TBT is able to do some pretty incredible things. Any questions? Alright, so let's go ahead and through out the ingredients. And so there's a few things I want to say, or just the first preliminary, which is we are going to be, you're going to be looking at chat GBG from the perspective of natural language processing. And that's really good. Things are working, right? Or detaching either tokens. Throughout the rest of this lecture, I'm going to be saying that we're going to be inputting the words into networks. And when I say words, what I really mean is something called a word embedding. And what you can think of this word embedding as being. And so the vector representation of a word, it actually captures semantic relationships between words. So these transformations are going to turn each word into a vector. And I have a nice illustration here from CSU 24 and a Stanford where basically they look at the word embeddings have several different words like sister, brother, niece, nephew, aunt, uncle. And what you can see is that this embedding all the different places, but the relationship of going from female to male occupies the same dimension, which is that the male in this embedding is below that female. So these word embeddings are themselves and entire topic that can take on lectures. And we're going to omit that for this lecture. But if you're interested in learning more, I recommend this course from Stanford. And first things to check out what the word to vec glove, which is an embedding techniques from Chris Manning who teaches this course at Stanford. And then sometimes words have different meanings, they really aren't. So there's a technique called Elmo that uses LSTMs to find these words. Alright, but from the rest of this lecture, I'm just going to say everything could award it to an upper, my favorite word that really means this word embedding any questions there. Alright, so we're gonna motivate attention first from this sequence to sequence problem. So we're going to consider the problem where our goal is to transform some input sequence to an output sequence. And here we want to use machine translation as an example. So let's say that I have in a sentence in English, I love watching UCLA basketball for you, but really that's the best bottom I'm getting Arizona the other day. And good luck on March Madness. If we had a sequence to sequence model working, then we would put this, in this case, translation to French. Day the basket, UCLA, all the quiet, the transformers. This problem was solved using recurrent neural networks. And the intuition for how this basalt was the following. We know that a recurrent neural networks hidden activity H is a succinct state represents a representation of the history of inputs. So if I were to pass in an entire sentence like this into an RNN and look at its state. After I put it, the entire sentence. This would be a representation of the history of the entire seconds. Let's write that down. So we start off with some initial state. We pass in i. And that updates the state at time one and love the state at time two, et cetera. So then this h of c is a single vector representation. Simply captures the information from this sentence. I love watching you say, all right, Any questions? Bear. Is it for the vanilla RNN? It can be any RNN, LSTM. Other questions. Just wondering, So we're doing sequence, a sequence number of tokens. Definitely happened stuff. We're doing machine translation. The number of output words might differ from the cupboard and provide for it. So how do we start? So I'll put the end token to note to say I'm done with this translation. So in addition to the tokens for words, there would also be a starting point. Any other questions here? Alright, so now It's gonna be something we call the context vector. After the activation of the artificial recurrent neural network neurons. And what I can then do is because this h of c contains all this information about the sentence. I could try to read out the translated sentence by using what is called an RNN decoder, where it gets that Kevin Spacey and basically unravels the information to produce the translate is seconds. So this is what that picture would look like. We would start off with some initial state and I would pass into this recurrent neural network, the HCI, that represents the entire sentence. And if it's trained well, then ideally it might have put the shell. Then I would take this and put it in as the input of the next time step, right? And then hopefully it would output a door, and then it would continue this autoregressive process until it outputted the translated sentence, the accident. Any questions there? Okay, So this is our machine translation used to be done prior to GBT and other transporters. When you look at this problem, you might imagine that there are actually quite a few limitations already, just intuitively. First is capacity. So in this example, we are reducing entire sequence, in this case is sentenced to one vector HCI. Alright? You can imagine that a sequence is getting longer, right? You might not be able to pay for ways to store all of the meaning of that sentence within a single vector of agency. So agency, because there's one consideration that I kind of glossed over here, but absolutely we know that the hidden state at time t, h of t is gonna be a function of h of t minus one and x of t. H of t actually most strongly represents the most recent work, x of t. And so this picture that I've drawn here is just different intuition. But actually what really happens is when you do a decode usually translate the sentence in reverse. So the first output would be pavilion. Then you could put the pavilion here and get Pauli. There is something which will be solved by transformers, which is, if I wanted to do this machine translation, any recurrent neural network-based architecture require sequential processing, sequential meaning sequential in time. To know EC2, I need to know s1, to know, really know, general to propagate there but require 0 of t. Any questions. Alright, so thinking about this question, which is, Why is it that when we do this translation, I'm summarizing everything into one vector, HSE. When when I do the encoder, I actually had a bunch of data. H1, H2, H3, H4 are corresponding to the different words. When I am translating the word regard there, e.g. right, that corresponds to watch it. So maybe if at this point in the sentence, instead of looking at HC, I looked at age three, which is the hidden state corresponding to when watching went into there. I'm going to do an even better job of translating. So the question then becomes why not look at every timestep? So instead of inputting h of c into this network, we would input H1, H2, H3, all of the hidden states at every single point in time. You can see this is the original synthesized translated, but it doesn't have very much other than shut doors. So that's why it can. Now easily, what that might look like is that when you have a recurrent neural network and now it receives inputs H1, H2. The note that no one ever actually implemented architecture because these are fully connected layers, linear layers. And you can tell me all the states are born with explosion of parameters. What might you think? Okay, maybe instead of going H1, H2 to HCI, I can be the average. Averaging is an operation that is going to be far from Austin. So instead, what we could do is instead of passing every single part tire hidden state, we can instead pass in particular inputs that we should have paid attention to. So again, there's gonna be one for I, for I love, H3 for I love watching. And when I translate radar dare, maybe I could have paid attention to this database through right. And then chador would have paid attention to the states of H1 and H2. If I do this and I'm teaching a particular point in time and using that to translate what that might do a lot better. So to do that, we're going to have to cover what attention is and what we're going to start with attention next lecture. Yeah. 