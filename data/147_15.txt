All right, everyone. All right, everyone, We're gonna get started. Here are our announcements. This is our third to last lecture. So today we're going to cover transformers and understand how it works. And then we're going to start variational autoencoders. Reminder that the projects are due March 20th, which is Monday of finals week. And we haven't created the grade scope submission portal yet. But just as a heads up, we're going to have you all submit just one project per team. So one person within your team will submit the project in grade scope and drove me an option to link your teammates. And so that's how we want you to submit the grade scope. The TAs are also going to be releasing a helpful coding video for the project this weekend. With respect to grading the project to provide clarity on performance. When we assign performance points, we're going to do that based off of your best-performing architecture. Even if your best-performing architecture happens to be a CNI. Any questions on any, just fix that. Alright. And then lastly, a friendly reminder that midterm regrade requests close tonight. So please get your midterm regrade request. I won't accept any request after that. The question is, for other architectures, for the project, we cite others architectures. So much If someone else's architecture, you should cite it, but you should do the implementation yourself. There questions. All right, so we will get back to where we left off last class, but we're starting to talk about how we understand check CBT. And we said that To do this, we first have to understand attention. These flies are new, so I had a bunch of typos in my prior slide, slide, I've updated some of them. One of the biggest ones is that I kept calling this H, or hidden state, which is just the artificial activity of neurons. In the updated slides in case you're confused by that. Alright, so just to recap, we were motivated by this machine translation question, which is to take sentence in English and translate it to French. And we motivated at first, this was done in the mid 2010s, which was their sequence to sequence models. Where we said, well, we know that recurrent neural network is able to encode mystery. So if I pass my entire sentence, I love watching UCLA basketball at 0.1000000000 to an RNN. Then his last day, which we'll call here hci, is one, Dr. activities. But adapter is that summarizes all of the words within the sentence. And so because HC contains history, we thought, well, we can go ahead and then use another recurrent neural networks. The traffic, the sentence I passing in this Hc into the recurrent neural network. And then the recurrent neural network was trained 12 and it can unravel the information within that sentence to translate this sentence into the trash. Alright, there's anyone have any questions on this idea? Just wanted. Some way ask this is just one layer of an RNA and not multiple layers. Is that correct? So you could have multiple stacked RNNs here, as we've done, is just a single aren't on there that you could organize their questions. The question is, what do we mean by stacking RNN layers? So what that means is that if I have some input X1 and it goes to an RNN that has hidden state H1, H2. This is an input x2. You can pass it to another RNN. I'll call it this RNN, hidden state S. So they're being S1 and S2. So you would have additional recurrent networks that are recurrent network is taking as input the hidden states, the activations of another recurrent neural membrane. The reason I'm saying it could do stay so much is because I've worked with dynamical systems all the time. And whenever I see an H in a temporal sequence, I always call it a business page. So if I stay hidden state it means artificial activation. But hopefully I will catch myself moving forward. All right. Other questions? Yeah. Correct? Yes. Yes. So here he's asked me to clarify. Hci is my context vector with summarizes my entire time because it just goes to an RNN. And then the information from it is basically unraveled through time. That's distinct from this. This is separate and it's just the architecture for if you want to stack multiple RNNs together to make them work. Telling me just call this NOT S1, which is a variable here. I'll call this like maybe the next states are called L1, L2, etc. Other questions here. This one, right? Yeah, So to be clear, in this example, there's gonna be one RNN for encoding. Actually let me just pull up the slides from last lecture where I had some dark here. There is going to be one RNN encoder that takes your sentence, encodes it into one vector. And then an RNN decoder that takes a vector and unravels the information to translate it something. Alright, so that's the setup that is going to motivate why we want attention. So we had talked last factor that elimination or there are several limitations with this setup. But the first is capacity. So basically I'm storing an entire sentence. I love working UCLA basketball sentence into a single vector. Hci, HCI, you know, maybe if I got 512 national factor, it can only store so much information. And so if we want to do longer sequences, then we'd have to make the order. But again, it's a lot, it's asking a lot out of this one vector H C to represent the entire sentence. One thing that we also know is that the context vector HC will move strongly represents the words at the end of the sentence. So HC most strongly represents pavilion and most weakly represents riboflavin chain because these inputs were far long ago. So actually when you do sequence to sequence this way, usually you're actually decoding the sentence in reverse because pavilion will be the most strongly represented things. So usually this will output 2 billion. Then Pauli, then at basketball of the seconds and recharges essentially. Alright? And then any recurrent neural network that wants to do this type of translation is also going to take order the number of words in the second space to do this computation because to get the next word to translate, have to have translated the first word. So to pass through this entire sequence, but require on the order of the Tiamat the time. So these are all limitations that are going to be addressed by transformers. Any questions here? The question is, if you miss translate the first word, would that be bad? Will that lead to crushing wherever it is, basically a bad translation because that's why you also usually like here, we're translating and reverse the most recent words. We can translate more faithfully and that'll hopefully get us back to the original receptors better. Alright, So we ended last lecture by saying, well, maybe instead of casting for this transformation just the last HCI, I might pass it H1, H2, all of the hidden states in between. So instead of just passing one Hc into this RNN decoder, I would give it all the hidden states. And the naive way to have all of our hidden states over time. And they are input into every single step of the recurrent neural network. But this we know is going to be incredibly expensive than parameters. This because these are fully connected layers, they're going to be occurring. So instead of doing this, well, we know that when we're translating each word, right? I really don't need the hidden state at every single point in time. If I'm translating regard, regard day, which is walking, right, I really just want to know what the hidden state h, three words, given that each one is, I issue is love, et cetera. And then to translate our door, right, Then maybe I just want to look at H1 and H2. Then of course, if I want to translate a basket here as basketball, I would have wanted to look at this part of the input. So we're going to be able to do this through an operation called attention. The bass part of the transformer. And we'll start off by asking, what is the tension. So I'm going to show you a picture. And if I asked you to caption this picture to see what's happening in this picture. You would say that this is a bird flying over water. When you made this determination is just how humans operate. Basically taking the whole image, their eyes would have focused on particular parts of D and H. So when I first showed you just manage your eyes, almost certainly focused on the bird. And I might ask you to capture the image. Looked around to see, okay, what is the bird flying over? This is very common or fact that our eyes are going to go from different parts of the images are called Chicago, like the Hoover I studied rapidly between different parts of the image, become very attending to different parts of the image at different parts and time. Alright, so what we can do is be constrained so that previous architecture translate this sentence in English. Right? So that done wrong, separate off the ground. I'm always asking a great question, which is what I just showed you, this image caption, the image. At the caption to the image, you would have to have observed the bird and then also observed what is flying over. Tomboy, ask the question instead of saying caption to the image, I couldn't give you the task to say what is the color of the bird. I've asked you, what is the color of the bird who are just paying attention to the bird? So depending on what the task is, what I'm asking you, you want to attend to different things. And that will be incorporated into training. Task is fundamental to understand what to pay attention to. Other questions. Alright, so this is from this paper by shoe and colleagues called show. It's an Intel Neural image caption generation with visual attention. And so what they're showing in this figure right here is they were able to successfully get a neural network to caption this, saying, a bird flying over a body of water using what we're going to describe mathematically work until the next slide is using attention. But basically, when you look at these images, tell you that when it's translating and outputting this word bird, the white areas are where the neural network is attempting to. So when it's translating bird, the neural network is looking at this part of the image of the bird and when it's flying, and then what it's translating, flying over a body of water. When it's translating body of water, It's basically looking everywhere around the bird. So that's the intuition you should have detected, which is that every single time step, whatever I'm trying to output, in this case water. They said I should attend to the relevant parts of that image. Any questions? First? Yes. Yeah. Perfect. Tom ways pointing out that in this caption, there is the word letter a here and here. In this case is modifying burden. So it's paying attention to the bird. Virus here is referring to a body of water. And so it's paying attention to the water around the bird. So that makes intuitive sense. Awesome. Was there another question? The question is, will the model? So in this case, the model has access to an image and then it translates or a captions it. But in other natural language processing, taskbar, if you want to translate a sentence, The question is, will it models have access to all the words or only like the preceding words. So we'll talk about this later on when we get to the transformer block. But if we're doing the sentence translation, and I've translated as John Doerr so far. When we do make our day, in terms of what's being translated, make our day will have access to the Zhai door, but it will also have access to the entire input sectors. So depending on what is the input and the output, what is the task? Any other questions here? Does visual attention come after the paper? All you need is attention. Actually, it does not. All you do is attention, is all you need is a tension. Introduces the transformer architecture that uses attention layers. And I believe that was 2017 or 2018, or is this paper in 2015, but there were earlier forms of it. Before the paper. And the student is referring to this paper which I recommend you read. This is the thing that talks about the transformer architecture that will be the base of GPT and birth and other language, some articles. Alright, so here are a few more examples of attention from this taper. So here the captioning, a woman is throwing a frisbee in a park. And the underlying word is what they're showing in here, what the visual attention is paying attention to secure a frisbee. The network is rightly paying attention to this region which contains a Frisbee, Hey everyone, their cats and dogs and paid attention to the dog stop sign. Stop sign or central visual attention is doing something very reasonable when find my caption these sentences. And the other question is, do they take a bounding box approach when doing image captioning with attention? I don't know the details off the top of my head, but I would guess no. Anyone knows this paper in more detail and can confirm or deny. Okay. So we can look at that paper just to be sure. Alright, attention, but also occur in the context of natural language processing. And that's what we're going to continue on with. These. This is a network that is taking Yelp reviews and essentially reading if they were positive or negative. And the red is where the network was paying attention to. The grocer was paying attention to you to determine if it was a positive or negative reviews. So here, I really enjoyed this restaurant, or sorry, not a restaurant. It's the hairstylists, the highlights, Perfect, Fantastic. That leads it to make a positive recommendation. And it's paying attention to these words or these phrases in particular. Same thing for this restaurant. So we are going to focus on Natural Language Processing going forward. And what we wanna do is we want to try to develop what the mathematical underpinnings of paying attention. That is, basically finding the things in the input that I want to extract so that I could make or I could do my task. Well, what that looks like mathematically. So, how do we implement attention? We're going to focus on natural language processing. Natural language processing. We're going to care about synthesis of this animal. This sentence is this. The animal didn't cross the street because it was too tired. And if attention is working well, then we come to a bird like it. You want to see what is it related to? What to pay attention to, to know what it is. It has the strongest attention for the word animal. It also pays a bit of attention to street. In this case. The animal didn't cross the street because it was too tired, right? We kind of know from the contexts that it should refer to animal and artists tree because it was too tired. But you can see that it does give some attention to the words there because in other settings it could be a bit. Alright. Any questions? Okay, so attention is going to come down to three quantities that we're going to need to compute from Martin inputs. And those quantities are called queries, the keys and the values. And these languages inspired from databases. So let's first look at what happens in a database. I'm not that familiar with databases. So sometimes conceptualize this as like a Python dictionary. In a Python dictionary or in a database, we know that there are particular keys. In this database. We may have n keys, and these keys are associated with different unique values, or they don't need to be associated with different values. And let's say I wanted to get the value associated with key three. What I would do is have a query, the number. The query would ask for Q3. Then I would look up in the database what cheats really was. And then I would return value to three. Alright, so from my clarity, I found that I want to know the key three and then the output of the database is a value associated with key questions better. Okay, so that's the basic gist of what these queries, keys, and values our database implemented. What deep learning, deep learning called quote unquote hard attention. Attention means that when we put in the query, we're only gonna get one value. So in this database, when I queried for T3, I got out one value, which is value three. Alright? But we know that when we do deep learning, we aren't currently database. We're wanting to know what to attend to. And sometimes you tend to do is almost surely not gonna be a single value in the input, but there's going to be. Some different dots, some different instances in the input. E.g. like this image where it should attend primarily to animal, but maybe I should also attended British tree because it could be some other contexts as well. Alright, so this leads us to this idea of staying instead of having database, I've put just one value. I'm going to output basically a linear combination of the values in the following way. This is going to be called softer section. So this is no longer database, but I'm still going to have queries, keys, and values. What I'm going to do now is because we're no longer that database sitting. I'm gonna take my query and I'm going to compare how similar it is to key one, key two, all the way down to maybe query is most similar to T3 and that I would assign it a similarity score. These are like the softbox with this force before the softmax layer in a neural network. So maybe query has a similarity score eight with k3, and then maybe after that with Q1. And it has a similar similarity score of five. Maybe with Q4 it is at least similar. And so there's some merit. Z-score is minus one. Alright. I'll then do is I'll translate these scores into a softmax distribution. So I'm just going to apply the softmax function to the scores. And we know that the highest score is going to have the highest probability that these we're going to sum to one. So after I do this operation, then the output of this soft attention is gonna be the softmax for T1 times value one plus the softmax for Q2 times W2, et cetera, et cetera. So the overall value produced for this query is going to be the linear combination of values here weighted by their softmax scores. Any questions here? The question is, can I explain again how the similarity scores are calculated? I actually haven't explained that yet. So I just give you the intuition of how the queries, keys and values within here. And then we'll come to how they're calculated in the next slides. On his question is, are the values advocate, as are the vectors that can be added together, meaning that vectors of the same dimensionality, they are additive. So all of these values are going to be factors that had the same exact size. So they might be on 100 dimensional vectors. And so each of these will be a different 100 dimensional vector, which I can add after out and multiply it by the salt score. Didn't answer the question. Sorry, can you repeat the question? In terms of word vectors or the attitude for the words. And this is a slide from last lecture. Remember that you're going to be B. Skip this, but we're going to translate each word into a vector embedding. Vector embedding always be 50,000 dimensional factors. And so all of the words are going to be translated into 50,000 dimensional vectors. I couldn't get out of the question. Others. The question is, what might be an example of a query? I'll get to this in the next slide, but the queries, keys, and values are all vectors. We'll talk about their sizes. But here I just watched you to understand how the query keys and values relate and how the queries and keys essentially gives you the weights based on the similarity scores. The waves are softmax distribution and they wait the values to get guilty. If we go back into the patients, always have a relationship and I buy exactly this thing without getting one. I mentioned, I recall this from others. In addition to punch is basically saying that this object in December, democracy position to the next one also, looking at those things that always happens. They can be abstract concepts of Thomas question is, will the attentions always be intuitive? But it is for the pictures I showed you earlier, and it is part of a sentence where it refers to animal and the answer is no. So we'll see later on that we're going to have any attention layers in parallel and we're also going to stack them. So this is like an intuitive explanation for where attention came from and what we hope it does. But oftentimes the network will be attending to the very interesting things and services, reducing your lawful and services better. But oftentimes it will be non-intuitive what it's attempting to. Alright, I mentioned this last lecture. This is the first time we're teaching this. So we appreciate any feedback on this type of please ask me any questions because I may not have optimized the slides or any other components here. Can you raise your hand if you understand queries, keys, values. Great. Alright, we're gonna move on then to how we implement this. So two, I'm first calculate the queries, keys and values. And actually I'm going to do a future slide here first. What we do is we look at a word and then put word. And the queries, keys and values are simply just factors that are generated as linear layers applied to the word. And so the queries, the keys and the values for the i'th word, maybe the word was it, right? That we were looking at in the prior example, would generate a query. This would be an R. We're going to use this variable to denote the dimensions of the queries and the keys. And then there's also gonna be a value, and this is gonna be in dimension d v, right? So these are all just factors that are generated by taking the word and passing it through a linear layer. Alright, so that any given that we know that the queries, keys and values are vectors. Question is, how do we measure the similarity score? I meant to have this wide plank and not have the answer. But you can see the x over here, which is if we want to measure the similarity of a very key or of two vectors in general. We know that one way to do that is to just take a dot product of them. So let's say that I had the query for the word it. I'm, if I'm looking at this sentence, I would have computed the query for the word. And then there would have been a key for all of these other values. So this would be k1, k2, k3. To compute the similarity scores, I would take the dot products of cube with all of the keys for all these other words. And that would tell me the similarity scores that I then turn it into a softmax probability distribution. So the way that we compute similarity, it's a dot product of vectors. There's one more detail. We will normalize the similarity scores by the square root of the dimension of the vector KI or Q. Remember, Q and K are vectors that had d k dimensions and we're going to normalize by the square root of DKA, this top part. Yeah, so his question is, are these embeddings in terms of translating the words into vectors. And those vectors have similarities based off of what those words are. So this is distinct from that. The embeddings, it's really referred, referred to instead techniques from natural language processing. So things like Word2vec, glove, and Elmo are those embedding techniques. Alright, One might ask, why do we write? Right? That's what a student is asking. Why are we adding these additional linear layers? Can we just directly measure the similarities by using the actual word embeddings and just took them their dot products. That's one form of detention. But by taking these linear layers, you can find other projections of the inputs that will give you additional modeling capacity. Said you could do attention without these linear layers, but it would be limited in this competition of capacity. Yes. Well, students are asking, have I gotten the indexes wrong? In terms of should it be like the IQ with the JFK? So we're gonna have to take the dot product between everything. But right now I just want to picture that we have one query for a word. And we wanted to compute a similarity, similarity to the keys every single other words. So I here is going to go from I equals one to n, where n is number of vertex. The question is, what do you, what do we have the same word, but with different meanings? The same verb and in different contexts can be something else. Would they have very high similarity scores? That's a great question. I'm going to bring up this thing one more time. So in things like Word2vec as well as glove, I believe they would have high similarity scores. Charlotte's exactly right. That depending on how the word is used, it can be a different thing. Like in the US means like footwear and boots. And the UK means like a trunk. And so could also even be location dependent. And so there are some embeddings like Elmo that take into account where that word appears in these contexts. And we'll find a different embedding to the same word depending on its context. And I'll note here that uses a bidirectional LSTM to do that. So it looks at words that come after it and where if the company were to give them that will be handed, that will be handled in the NLP side of things. Students asking where the normalization term comes from. Essentially, I'll go through them and wants to, but I think that there were question company. The question is what exactly is being embedded into the queries and the keys? So the queries and keys come from the burden Betty, this word embedding is that Word2vec, glove or MO embedding, where it translates for word into a vector. And then after that, those just go through a linear layer and that gives you your queries, keys and values. The question is, how are these linear mappings important in the context of attention? Or how are these embeddings important? Are you asking like, how do we make sure we learned by getting meaningful veneer? Okay, yeah. So these linear layers are gonna be parameters of your network that will be optimized for the network will learn to set these linear weights based off of what to attend to it. Yeah, So basically, we will have a loss function and will backpropagate to these linear layers. And the linear layers will change to make the loss of small as possible. And if a task is like cancellation, then you better be paying attention to the correct words to make them off as small as possible. So how the queries, keys, and values will ultimately come out from these linear layers is through optimization. Question is, how do we assign values to the keys? Same answer, which is through optimization. So, never handcrafts what should be the keys and what should be the values. These will be set by stochastic gradient descent. So to make the loss of power as possible, it'll find the features, the values correspond to the keys. So the network will learn attention. Okay, let me get back to this other student's question, which was, how does this square root of dk come about? So the way we do that is we're gonna make some assumptions. The first is that the queries are independent of the keys. So if the queries are independent the keys and further, the expected value of each element of the queries and each element of the keys. So I hear is indexing. Let me use a different, let me use the letter j here. J indexes elements of the vector. I'm doing this because I've already used I here to index words. Expected value of Q, j equals expected value of k, j equals zero and their variances equal to one. Then we compute Q transpose K similarity score, right? This is going to be a sum from j equals one to decay of q, j and k j. Because they're independent, right? Then. The variance of q transpose K is going to be equal to the variance of q j times the variance of k j. For j equals one to d k. This value follows as you can see, that the expected value is just gonna be zero because expected value of Q j times expected value of k j, when they're both zero is just gonna be zero. But this product of variances, what you call one. And the variance is going to grow as the nationality of the keys and values squared. So the variance of this spring is going to equal dk to normalize the variance of this dot product. So that's the scariest tastes. One, even when the vectors get barge divided by the square root of d k. Because we know that. One. Great. Daniel's question is, here, we're constraining the Q transpose K i's to be have mean zero and variance one. But Daniel saying we pass this through the softmax. So we want to be the case that they'll get normalized away. The answer to that is that if I added, so we know what the softmax, if we were to add a constant to all of these values. So this became 1008, 1003, 1005. He saw map, this would be the same. But if instead of adding, if we multiply them by something, which is what this variance scaling would do in the scale of the softmaxes would be different. So at this softmax is we're smaller. Sorry that this course we're smaller than a softmaxes would be closer to uniform. Whereas at the scores were larger the Softmax, it's looking closer to a one part and that's what we want to address. Right? It's keeping them at a similar scale so that you'll get closer to a one-hot just because you had just because you've had more than entrants figured. Great question. Other questions. Okay. Then you might ask why not use a neural network to compute similarity? Well, the dot-product does a really good job already and you couldn't do this. But it will require more parameters and more computation. Because the dot product already does a good job of measuring similarity. There. Just to make sure that we're clear. Here's an example again of us doing machine translation. This is a different sentence. This is at the input sentence. Economic growth has slowed down in recent years. This is the output. So attention by computing the queries for every single word in the output. In this case for machine translation, that would be for this work, economic Q3, right? And I would compute the dot product with the keys of every single burden on the employer. So I would want to compute Q3 transpose Q1, Q3 transpose k2, all the way up to Q3 transpose K has been commonly is referring to the translation for economic, I would hope that Q3 times Q3 transpose K1 is very large. But economic has little relationship to growth. And so hopefully this would be small. In fact, hopefully the dot products with all of the other keys would be small. In that case, then the value that would be assigned to the word economy would be primarily the value associated with K1, which is called V1. Okay? And then you would do this for every single part of the output. So there would also be no acute two. For this bird. I'm gonna get this croissants. And you would calculate Q2 transpose Q1, Q2 transpose K, two, etcetera. And compute their attention. Alright? I'm sorry, can you say that again? Right? The customer has a query related to each output word. How do we know how many words will have? So we get this transformer architecture lecture. We're going to, when we generate the output, essentially generate the output one word at a time sequence. And we're going to keep translating until we decode some Ns took them. So there's going to be an antigen that tells us to stop translating the decoding parties until sequential for machine translation. The question is, what's the example where we want Q transpose times k to be large for multiple values. Yeah, so in this example, which we call cross attention because the output and the input or difference. In machine translation. Oftentimes we're translating C over. So there's some, some words may have multiple values to attend to, but it comes up that work there and something called Self-Attention and soft the tension, the input and the output are both the same sentence. And I want to know what word in that sentence relate to other business happens. So if I look at this word, slowed, and this will be the fourth word. So it'll query will be four. Then, what has slowed? Well, economic growth has slowed. So we don't want to just pay attention to economic because it can be like, you know, economic depression has slowed and that would be a good thing. They're saying economic growth has slowed and that's a bad thing. So we would want in this case that for transpose K1 is and for transposed K2, both should be large. And there will be attending to multiple words. This question asks questions at every step. You get an answer that's appropriate. I'm not sure I follow your question entirely, Tom, like Can you please repeat that question? E.g. less than what I said. Yeah. You've answered as yes. It's always saying these ideas have crazy Keyes. Kind of seems similar to question answering and natural language processing. Like you can ask the question what has slowed? Paying attention to the word slowed and you will want to know that the answer is economic growth. That's correct. I mean, that's probably why the questions are called carries, right? It's like asking a database like I want the value or query right here. And the way that I find the values, I look up the key. It is correct that this is essentially saying, you know, what is Q for related to, and it should be related to K1 and K2 here. But all of this will be done using optimization, using the same tools as cast pretty descendant back propagation that we've already learned. All right, Question back there. Right? So I, I believe the question is and cross attention, like the queries are coming from, you know, French and the keys and the values are coming from English. So how do we calculate this? We'll get to that when we get through the transformer, sorry. So we'll say the architecture and how we get the keys and values for the input and the queries from the output or research question. The question is, are the dimensions for the quiz keys and values of the hyperparameters? Yes. Sorry. I explained how to generate a lot. Oh, yeah. So the students question is, can I explain how it is that degenerate the end token which stops the translation. That comes just from supervision. So basically in my training set, I would have had, if this was actually a dataset example, I would have a start token here. I would have an end token here. That's the starting end of my input. And I would have a start token here for my output and an end token here. And the way that these are training is autoregressive, same thing. So how we saw the RNN generated Shakespeare, I would take, let's call it. Five words. So it'd be economic growth has slowed, that's five words. And then it would want to predict. We want to predict. It would have, sorry, no, this is a bit different. So this is not a regressive. Well, we would do is we would take this entire sentence of economic growth has slowed down in recent years and then want to translate it correctly as this sentence over here. But in the training process, to get it correct, it would have had to. And it would have to predict the end token here. If it wasn't predicting an n tokens and it would have a large loss because then we'd get that token incorrect. So then stochastic gradient descent would change the weights for the tech and social determinants. Question is, what would be an example of a value? Like Do you want me D2. So the word economic, Would it be a vector? It would go through linear layer to output k1, and it would also go to another linear layer to output D1. The interpretation of this is difficult because it would be the same as what are the activations with an enrollment? Activation features to think of as a values here, but they're just coming from a linear layer applied to the embedding to work. Economically. It doesn't have a correspondence to an output or a controller? That's correct. It's like an intermediate feature and mineral and there'll be fully-connected with birth weight connected layers that will translate that and say, thank you all. I'm sorry, say that again. Right. Yeah. So the question is, how do we handle this cross attention? I need to attend to words in the output to the input. But there's also a relationship between birth and the input that probably require self attention and she also be self attention in the output. This transformer block wall into self-attention for both the outputs, the inputs as well as cost and tension between that. So I'll wait for architecture. So the question is, you may translate into English sentences, into two French sentences or another French sentence. Yeah. So the way that you would have a network that is that you would give it two sentences and having penetrate as the output two sentences. So check GBT. The inputs are 4,000 tokens, which translates to about 3,000 words. So every single example that chat TBT will see will have 3,000 words. So it's a lot more than just catch. You can see just looking at a lot of taxes. I put the input and the output. Okay, just for the interests of time, I'm going to keep moving on. Actually, I just saw the time and I see that. So let's go ahead and take a five-minute break and then we'll come back. So I think in the slideshow it's like a division where the body, the score by square root of n. The dimension. The reason because like the NeuroNode that we give witness possibly use, we just like normal left side simpler discourse outputs, softmax, yes. We're doing the division by square root of DT. It has to be one. It's less close to a one-hot vector. Or we don't want it to be dependent on the size and hope each hyperparameter and copy the two secretaries. Right? I see. I need to think more about neural networks. The message is always the number of classes, right? So that will be consistent across many different. Oh, I see. I see. Okay, cool. Thank you. Yeah. Like on one slide, we've seen as the key and value are just the opposite. What if I click Generate? Yeah, it's just a venue or later on flights in the video there is optimized. So it wouldn't mean also goes to monetize. What do you mean by context? Let's say four priority of, let's see, we have a court of law which asks you to write the query. One may attend to some sort of embedding. Like, how do you know what is actually already want? So which one do you like? Yeah. Yeah. This will be taken care of by something later on for the positional embedding. Oh, say can you just go to the slides? Yeah. Oh, yeah. So let's say that we have the value of the input embedding of the economic and we don't see with somebody in your matrices to go to k1 and you want. But let's say just here behind the history, how do you reach the linear layer? I plan on being applied on the word embedding for the economy. But we don't have it right input. And this is what we want to ask you at all. Because all let's say on the training we might have right? Corresponding. Right? Yeah, so that's a good point. So if the task is to translate this sentence, you're right, we don't want to meet yet. To get you to want to make we would have had to do you would only have one, as well as all of these pieces. So it would have to generate economy. But that would be from comparing these queries like that. How do we could say we don't want anything? They put all this. We can easily get the V1, T1 right at the start. It starts at. So the Q2 will be some things that aren't so good. Oh, okay. Yes. Okay. Alright. Can we make an assumption that they aren't especially important? Yeah, they're not independent set is just an approximation. But it's not like we're just thinking of something to say. What would be a reasonable thing to normalize. The only assumptions. It's more like a role of top side again, perfectly normalize. Okay, that's alright everyone. Let's get back to material. Lots of questions which are good. I'm going to take this into this vector for next year. I want to try another question from the graded. All right? I'm sorry. Right. Yeah. So for the next part is we're going to focus on self attention. Which is this idea of taking a sentence and seeing what word they're burst in this sentence I should pay attention to, like slow to pay attention to economic growth. Alright? So we mentioned that there are these linear layers. I'll take your word and change it into queries, keys, and values. The way that these will happen is with matrices. So let me first write it how you're going to put the notation because that's how the paper did it. So there will be some matrix w q, that will be my linear layer for the queries. Process XII. Let's say that XI was in r. So n is the dimension of my word embeddings. Then this would eat walk QI. Very simple. And therefore WQ would be a matrix that is DK, the dimensions of the queries by right? Similarly, KI would equal a W k times x. I. Tell my same time these subscripts instead of superscript, I'm going to keep them as superscripts since I'm going to show some screenshots later on from a different flag that I use and I superscripts. So and then there would also be a VI equals w b times xy. All right? Okay. That's hopefully intuitive to you all. I'm going to show these slides from j, our Mars blog on attention because I think he does a really nice job of visualizing these. In these cases, everything is flipped. So instead of over two, which is basically sand column vectors, these are going to be row vectors times matrices. So in this case, QI in purple is going to equal x I times WQ. And here the dimensions there's gonna be in our TK and in this example of decay is going to equal three X I is going to be the dimension of our input word embedding, that was our m. In this case, n is equal to four. Let me do this in green because inputs here in green, xy is going to be an RN. N equals four. And then therefore WQ has to be a three by four matrix. Okay? So if I have two input words, X1 and X2, I write multiply them by this matrix W Q, that is three by 0. Sorry, I did this wrong. This is four by 34 by three. And my four d vector, tons of four by three matrix will give me a 3D vector. So that's WQ, the linear layer that is multiplied by X1 and X2. To get nice curry when I'm alright, and then I'll have a different w v, w k for the keys and a WB for the lessons there. Notice something. We have to take the dot product of the queries and keys. So the queries and keys always have to have the same dimension, DK. The values can technically have a different dimension. But in practice, the dimension of the values is almost always going to equal the dimensions of the query is introduced and attaching Bt cotton stuff. Right? So now we're going to write this for all the inputs at once, the same formulation. So what we're gonna do is we're going to define big matrices, Q, K, and V. And these stores my queries, my keys, and my values for my n input tokens. So n here is equal to the number of input tokens. Following the convention of the last slide. Because in the actual attention paper, this is how they've already paid out. One is the query for the first word or the first token. And this being one, remember was equal to X1 times WQ, Q2 was equal to X1, X2 times WQ. And then Q n is equal to w to XN times w cubed. Same thing for the keys and the values. So then the operations that we talked about, where we take the query and the keys, we don't connect them together. And then we pass them through a softmax. And then we multiply the output by the value. This is done for all of the inputs at the same time through this operation, Q k transpose times v. Alright? When you do Q k transpose, then what you'll find is that this matrix Q k transpose. Actually, this will be a matrix softmax of Q k transpose divided by square root of Vk, each row of this matrix. So this thing here is going to be equal to this matrix here. Basically the first row is going to be the softmax distribution for query number of one. And so Q k transpose here is gonna be an n by k matrix, that's for q times n by n matrix, that's for k transpose. So the overall matrix is n by n. And what this n-by-n matrix looks like is in each row, we're going to have here softmaxes for query one compared to all of the keys. So this is what softmaxes for query one compared to all and keys, right? And so that's why they're going to be n columns here. Then they're going to be n rows because I'd had any queries, any questions there. The question is, this here is n by n. And that's only true in self-attention because in cross attention we could have a different number of keys and queries. So this might be a rectangular matrix and that's, that's correct. So then after this, we take this purple matrix where every single row sums to one and do the softmax is for each query. And we multiply it by this V matrix, right? And this D matrix, it looks like d v1, v2, all the way down to the n. And so when I multiply these two matrices together, I'm gonna get a matrix at the output. And the first row is going to be the softmax for query one and key v1 times w1 plus the softmax for query one and key two times value to the softmax of query one and Q three times W3, etc. So this value, this row here, is exactly going to be that output that I wanted, which is a sum from I equals one to n of the softmax I times value on this quantity here is what I had written in this slide over here, where the value for query one is going to be some vector that's a linear combination of all of these values that are weighted by their softmaxes. Okay? So if you go through all of these operations, you will find that every single row in this matrix will indeed be that output value for query one, the output value for query two, etc. Any questions there might be something that you might have to return to it just to make sure that you can convince yourself that we're doing the truck operation. It's not about why. Why did Jesus a good anchor squares to probably, to understand why he is probably the next incarnation. Right? Tom Weiss question is, how should we intuitively think of these values? So the way that I can get the values as the values are the features for each, for each word. So the word economics will have some features associated with, associated with it. And that feature will be analogous to the features in our neural networks. So the values are the features. And all this then tells me is attention tells me, how do I take a linear combination of those features to make sure that when I translate the word economy, I wanted to allocate attention to economic. So I wanted to value the feature for economic rather than the value or future for slow. Other questions to take. Question is, should this be the transpose on the right? It should be V. But if you, so, so, yeah, this is one of those things where you don't differ. Even when I wrote these slides, I wrote this out to convince myself it's true. It isn't true. I can just play around a bit. Sorry. You said again. Does this off expatriates property of what matrix? To be symmetric? No, it is not symmetric. The softmax matrix, the only property it has is that the sum of the rows are equal to warn, because every single row is a softmax distribution is not symmetric. Alright? So the way this looks like in picture, computing attention for everything at once. It's, we'll get our big matrices Q, K, and V, just like we had on our prior slide. This is query one. This is query to, this is value one, this is value to you and something analogous for cues as well. And then when we compute attention, we're doing this operation, which we already unpacked. So I just have to sit there for you if you wanted to see another visualization of it. And then it gives me an output z. And the z's are going to be the features which are going to be n, the number of tokens by dv, which is my feature size. So this is the feature size, the size of my balance. I'm going to have n features for however many employers I had. Remember here we only had. So that's why there are only two rows. Okay, Any questions there? Alright, so this equation is one attention layer. If you understand this, then the rest of transformers will hopefully make sense because it disappears, going to cascade many of these attention layers. So this is a single attention layer. The first modification that we make in a transformer is just like having convolutional neural networks. We had a filter. The filter extracts that feature, but we're not content to just have one feature, right? You usually have, you know, 512 filters are 256 filters. So you can think of this as outputting a single attention feature. And just like with layers, we have many filters, will also watch several parallel attention layers so that we can maybe attend to different things in my inputs. You can think that this might become even more relevant when say you have checked GBT, have 3,000 working, 43,000 work input. Your current word doesn't just depend on things in your past sentence. That will also depend on things written paragraphs or several sentences ago. And so having multiple attention layers allows you to first of all, where to attend to multiple things in the past in parallel. And so the act of concatenating several parallel attention layers is called multi-headed attention. Any questions? This is what multi-headed attention looks like. We already described. Attention had a single attention head, which is that we would have queries, keys, and values. Now, we would just have, instead of just one WQ, WA, WB, we'd have two sets of events. So this is w0w, sir, okay, w0d, and this is another set of attention parameters, w1. Yeah. The question is do we Ben had different queries, keys, and values? Yes, because these w's are different. So it's going to learn to optimize different queries depending on what your attention to attention heads looks like and don't pick up two sets of values. Yes. Sorry, Can you say again? Great. So students realizing that if we have two attention heads, if the output we call the z, we would now have two n times two. We would have two of these Z's. And so we would have doubled the number of z's. And so what we do to combine these down to be the same feature sizes. I'm going to index each attention layer with high. And so if we had eight multi-head attention with eight attention layers, then I would have output z1, z2. And each of these Z i's Z1, Z2 to z. These are going to be matrices that are n by d v. The way that I then concatenate them, or the way that I then get this down to a single z is that I'm going to concatenate them. So I'm gonna write this as z1, z2 to z. And so this is going to be a dimension n by eight times dV. I'm going to multiply it by a linear layer w. W out will be a matrix that is h d v by d v. And this is going to give me a final Z. And this Z is the same size as my original values and by dy. And that's just drawn here in this picture where my multi-headed attention is going to compute all of these different outputs. I'm gonna concatenate them in different columns multiplied by w zero and I'll give me a Z at my original dimensionality. Great. The question is, why is it that we want multiple attention pass rather than making decay the dimensions of the keys and values, or DD, dimensions of the output larger. Well, remember that no matter the size of D care, GG only gives you one similarity score. It's only going to give you one softmax distribution. So you're still limited in what you can attend to when you have these parallel paths. The softmax distribution can be, come up very fast and attentive different parts of the sentences. As opposed to if you have just one path for the large decane, you're flipping out what we had one attention distribution. Yes. The answer is yes. So the question is, when you do multi-headed attention, are you just taking the same attention? Layers and just replicating at a time? So the answer is yes, but each of the parameters and those eight attention layers are going to be different. Because they're gonna be County optimization. Just like how the different filters and the convolutional layer will pick out different features also. Alright. You mentioned, right? Yeah, Tom way bachelor's also, if dk becomes quite large, notions of similarity become less intuitive because the larger the volume that's occupied grows exponentially with the dimension of the vector. So if you take those very large vector space becomes a very sparse and distances similarities breakdown because of the curse of dimensionality. Alright? Can people raise your hand if you have followed us up to be detected? Okay. Any questions? Sorry. The question is if I can elaborate on yeah. So I didn't generate just figured this one is from Jay, our Mars plug. I actually don't know what the side note means. I, since I would need the context of the blog, but I think it's probably talking about structurally with the output of being killed her right below. I'm not sure. Yeah, so sorry, I'll have to refer you to the bar. Alright. Okay, so now let's come back to just the centromere is talking about that, how attention is used with the Recurrent Neural number for translation. This is also from JLL Mars blog. He has really great illustrations. That's why I put them in here. But basically what you can do is you can take each hidden state, right? Let's say I want to translate, sorry, obvious and hidden state. Let's say I want to translate the artificial activations at a time. Let's say I want to make the transformation for the first time step. What I do is I can turn this one into a query. I can turn all of these H 1s, two keys, and I can calculate the dot product, the similarity between this query and all of these keys. Compute my softmax. And then when they are least some bees. And that will tell me which of these hidden states, sorry, Which of these artificial activations to pay attention to? And that's how for a single point in time, my RNN can attend to one of these inputs using the same query key, value construction. Not going to pay too much time on this, since we wanted to then get to the transformer. So when we then look at what the architecture of this, when we wanna do this translation for I love watching UCLA basketball sentence, we would figure out which of the artificial activations H1 to H2 we pay attention to at time one, that's a one. And then we'll hopefully output. And we took the Azure as the input to the next time-step. See what we pay attention to at time step two, and then translate a door, etc. I want you to notice something, which is when I compute a one for this RNN, what I'm doing is I'm making a query one out of this activation at time one, S1. And then computing a similarity versus all of these artificial activations that came from the encoder. So here, k1 is going to equal H1. K2 is going to equal H2 all the way up to k, k equals h t. So when I compute this attention, I need to take the dot product of query one with k1 all the way to Katie. And what you'll notice is that attention one, this value here is a function of binding entire history. I had to have known the entire input sequence. I love watching UCLA basketball at 0.2 billion. This is kind of weird now because this is just repeating what I said. It's attention input is a function of every single input where this is weird because the way that we motivated the recurrent neural network is I want you to watch the entire history into a recurrent neural network. Recurrent neural network we wanted a succinct way to represent history. But if I'm already looking at the entire input, I don't need recurrence anymore because I'm passing on all of my history into the network. What's that? Because I have all of the history in A1, all the history and A2, I should be able to do this translation without these errors. Again, because every single input A1, A2, already contain all historical inputs x 1x2xt. Alright? So what I can actually do is I can remove these on relationships between the adjacent states. Instead. This now looks like a feedforward neural network where the input is the entire history. Any questions there? Alright, so in this case, because we're doing machine translation and I need to know that I translated to then get a door to then get regard day. In this instance, I would still have to calculate these sequentially, right? And so after kicking my current output and then making it the input of the next time step. This is called an autoregressive calculation. So if we're doing an autoregressive calculation, I still have to calculate these in sequence. But if I didn't have this autoregressive components where I didn't take the output from my prior time-stepping and put it up next time. So my network would look like this. And what you notice is that without not a regressive component, this computation does not rely at all on any of the others, A3, competing this block here doesn't require A2, A1 or something. So I can compute all of these paths in parallel. And that's really nice because then I don't have to go through this sequential Africa, the RNN, which is generally slow. So when there's no autoregressive component, calculating a local attention on all if you're history of inputs, you can actually parallelize your task, right? Any questions? All right, This gets us the transformer architecture with the next point, which is some of you may realize that if I have this architecture over here, there's a problem, which is that without recurrence, always have. Here are a bag of words, a bag of attention, and a bag of birds, but I don't know which word comes first, which second, because I have no notion of ordering a sequence. And so this gets us to the transformer architecture. In the first part of the transformer architectures solve this with what is called the positional embeddings. So just the transformer architecture. We're going to start off with the inputs to the transformer. This would be a sentence I love watching UCLA basketball. Then the outputs are going to be what we're eventually going to translate. The first minute. But transformer salts is this fact that if I'm doing everything in parallel, I've lost relationships with words relative to each other. And it does this by adding a positional encoding. But positional encoding, all it is is taking indexing, saying this is the first word, second part of the third word, translate that into a vector. So the vector for the first index is going to be this column right here. The vector for the second index is going to be the second column right here. And the values of these factors are calculated according to these equations. When you can see is that this vectors have particular structure. And by adding these inputs, you then get your inputs positional order rooms, the transmitting these indices to vector, you're basically putting the index as well, but in Texas represented as these vectors. Any questions? Alright, so that's the positional embeddings. The input embedding, which is a word embedding, cluster positional embedding. That was that green input side that we had before. And what happens is that xy is going to be translated into queries, keys, and values grew up in here later. So this multi-headed attention is the block that will compute my query q, my keys k, and my values V. That's why they're really going into that more attention block. And the output of this is going to be that variable Z that we talked about. Okay? And that's just going to follow that attention that you've already discussed. Multi-headed attention again means that we will have parallel, maybe eight, maybe 16 attempted walks. And then there's going to be an add-in dorm. All the ad is, is the residual connection. So that means that just like the ResNet, we are going to do a skip connection and that gives us a gradient Tywin. And then the norm here is going to be layer normalization. You'll recall this from the midterm exam normalization, but there's going to be a way to normalize across to fix the volume for activations to make them mean zero and variance one. Instead of doing that, which can be expensive because you have to calculate these across the bathroom, just gonna do normalization. Any questions there? Alright, so then after that, these activations are gonna go through a feed-forward neural network. And so that's going to be a fully connected neural network with a resume that connection. And also to have a non-linearity like array. It's going to compute some feature of your attention. Any questions? Alright, next is this block here, which is called masked or Titanic. Alright, so the motivation of this as the following. If I'm translating, I love watching UCLA basketball sentence. And if I were to pass in the entire transmission at the output. And let's say that I had already translated door, right? I want to translate radar day by referring to the inputs. But if I pass in entire translate a sentence, that door make our day, I'm essentially passing the answer into the input. What the transformer will then do is it'll just want to say I translated as chador and I know that Max works if you bake our day because it was inflicting to the network. So we need to do something called mass detention to say that if I've translated John Doerr, I can only pay attention to words that I've translated before Ashdod door. So I cannot look at any future words. And the way that we do that is with a mask multi-headed attention layer. All that is is it takes the attention layer and to it we define mass attention is taking the attention equation and adding a matrix M. And M is this matrix. Alright? Um, what this means is that remember that this is going to be for query one. This is going to be for query two. When I take the dot products of the queries and keys in Clery one, if I look at Q k transpose and I asked, and then I add mask and that hasn't negative infinity for T2, T3, all the way up to key. And then the softmax probabilities are gonna go to zero because if your score is minus infinity, your softmax probabilities. So basically, after mass attention, we know that the output softmaxes are going to look like 1000, all the way to zero. For query to maybe a slight 0.90, 0.100. So for the second word, I can only pay attention to the first word and it's the second word. But I can't get to the third, fourth or we're in the output. So if you go ahead and actually code this up, this is what the mass attention looks like. It's essentially a lower triangular matrix. And this guarantees that you will never pay attention to the future words. Your output is query one corresponding to this starts Okay? Yes. The question is, is masking only applicable at training time? Yes, at testing time, we haven't yet translated the future words. And so the testing time will look like that auto regressive graph that we had before. Other questions, All right. The question is, why are they doing sinusoidal positional embedding as opposed to some other function that represents division. There are people who have looked at other types of positional embeddings. You can even try to optimize a positional embedding. But in therapy, they either do similarly or if they do better, only marginally better than the sine and cosine. So that's an empirical results. Other questions. Alright. So that is the mask multi-headed attention. In this decoder block, we then get across attention layer. For our cross attention layer, the queries are going to come from. The decoder outputs, but then the keys and the values are going to come from my encoded sentence in English. I love watching UCLA basketball. So the keys and the values come from the encoder side. And what I pay attention to what I wanted then do the translation is I'll take the query for every single word in my output, compare it to the keys and my input, and compute a value according to the attention layer equation. Again, there's always wanted to be a residual skipped connection as well as our Layer Normalization. Any questions there? The question is, do we always make sure we have the same number of ones? Only thing that you have to guarantee between keys from being clutter and the queries from the decoder is that the keys and the queries are vectors that have the same dimension because they have to be dot-product together. But that's all. Are you talking about the sara? Yeah. So these are keys and values that I computed from being coded sentence. So, yeah, so the encoded sentence says they'll be, so the keys will be n by dk, let's say. And then these, these queries right here, Q can be N by dk. But all that has to match is the decay because we got to take the dog park. I'm sorry, I misunderstood your question. Yeah. Only the output here is connected to only the output of this network is connected to the decoder block. Sorry, I misunderstood your question. Alright. We're almost done with the transformer after the attention layer. All we need to do now is just get to classification because we're going to output a token or word from the English language. So we have a feedforward neural network to transform features. Then we get a linear layer that takes us to the size of our, I'll put the number of possible classes and then we do a softmax and then we draw from these output probabilities. Okay, so this is just transforming our cross attention features into a distribution over our tokens are our potential output for any questions there. What is the resolution of that? We cannot push off the back open. E.g. I have my money back. Yeah. Totally says it's asking what is the batched here? So let's say we had a batch of 512. If we were just doing self attention, like what should I pay attention to in my sentence, it would be 5.10, 12 sentences. Where if you are saying, let's take the past 3,000 words at the Duke of 3001st word. One example of that batch would be 3,000 words as the input, one guard as the output. And then I would have 500, 512 times 3,001 total words in my batch. I'm not sure people following this is falling off with and tell them to be shuffled. The second sentence, like, I love watching. If you turn it into like watching love UCLA, I probably doesn't make any sense. So the batches are distinct sentences. Now the question is, how do we do this in the testing phase? So then in the testing phase, we would do that autoregressive thing I drew earlier where we would have the starts token here. We would be able, we would have our entire input sentence. So we could do attention on bad. We take our start token, do our decoder blocks, and then I'll put a softmax distribution over the next characters. For the sentence we've been working with our deja, Zhu would come down and be the new input for which I would compute its queries. Then I would do the cross attention, and then I will get a softmax probability for whatever sample or the door. And I will keep doing that over and over again. This is why chat GBT takes some time to return output to you because it's doing this autoregressive component to pick out what's it back to you? Alright, that's the transformer layer. There were a lot of questions, so I know that there's a lot of can optimize these slides. If you have any feedback, feel free to send that to me. This is also, I think, just something where it helps to take a look at the paper and to spend some time on it. So please come to our office hours with any other questions on this question? Is there a question? Why do you need the positional encoding for the input? If I don't have this, then because all of these for self-attention just come in parallel. Like I love watching UCLA. There's no notion of order. They all go in parallel. We have to add the positional embedding to say I is index one, index two, watching his index three. So the positional and that is the only thing for the positional encodings that keeps the order of the words in the sentence. Alright, let's get the GPT. So now that we understand transformers, we can move on to trying to understand chat GTG. So there are several language models. We're only gonna do GPT because chat GTG is what we want to get to. But there are other important architectures like burst that we won't cover at a very high level. We're going to see GPT. It's just a stack of these transformer decoder blocks. And then an architecture like Bert is just a stack of these transformer encoder blocks. And they have different ways of training, which I'm not going to get into in detail here, but we'll talk about GTT. For GPT. Gpt-3. Gpt-3. They have the same basic architecture, which is a stack of decoder blocks with in stock a transformer decoder blocks. And then the most powerful ones have had a huge stack of them. So GPT-3, which is the precursor to chat TBT, is a network that has 175 billion parameters trained on 45 tb worth of data. You can see here how many decoder layers they have. So they have 96 encoders stacked. You can see that in each decoder they have 96. This is insane amounts of parameters that for a bachelor's using 3.2 million examples. So that's a 3.2 million sentences that have 3,000 words. Alright? Training these models is really expensive. I don't know the cost of sending three, but my lab was recently reading this device transformer paper from, from Google and just change the robotics transformer with over $1 million. So these things, I'd take quite a bit of time to compute. When GPT-2 came out, they were clear that one of the innovations was actually the datasets that they used to train it. So these things are learning natural glitches, but natural language processing. But tire data-sets which is greater the Internet and there's a lot of crap on the internet. So you want to make that dataset, Peter, so they will e.g. look at credit and then look at least posted on Reddit was written by humans for the most part, awesome at it, but somehow pulled out goodbye to post this also textbooks and books and Wikipedia that goes into these. So they have really made such a big dataset to train Chet. So protect GPT, GPT-3. For GPT, there is no encoder, so GBT does not have that at all. All that GPT is doing is taking this block here, which is the decoder block. Them. So GBT is just that. We're going to see my head because what CPT does is it just a word completion of Earth completion architecture, just like we had the text generation for, fixed here with recurrent neural networks. Let us all GPT is doing so for GPT, you pass in 3,000 words sentence. In this case, we just wrote nine words. And the sentence is, a robot must obey the orders given it. This is again from JLL Mars clock, and this is the second law of robotics. And the next word should be. And so if it's trained to roughly, what it would do is it would see the sequence of sentences. And it would generate a softmax distribution where buy is going to have a large probability. And if it doesn't, then it's going to have a high cross-entropy loss, which it can back propagate and update the parameters. To predict the next word. Well, alright, so the second law of robotics is a robot must obey the orders given by human beings, except where such orders with conflict, the first-order. So basically, this is one example, but then the next example, the input would be all shifted over by one. So it would be a robot dot-dot-dot given by. Then it would have to output the next sentence or it started the next word. This law of robotics, which is human. Right? So these are ways that we, that we train GPT to output the next word. So I should also mention GPT stands for generative pre-training task for her transformer. That just comes from the fact that we're using these decoder blocks from the transformer. And then generative pre-trained refers to this training process. We're about GPT does, is it's just trained to output the next word. Given some sentences that have been pulled from the internet, from textbooks, etc. Alright, that's why I showed this example. Last lecture or two lectures ago, where we saw that GPT is in its most basic form, distance often computer, you give it an input sentence or input sentences. And then it keeps generating more sentences to complete that. That's what, that's what, that's what GPT-2 is trying to do. But this isn't our experience of chats EBT. So a naive GPT model, but just simply complete tasks. But it's actually bt is something where we ask questions and it gives us answers. So how is this done? Sorry. Yeah, the question is, how do we know that GPT is not just memorizing things is learned from the English language. Rather than, you know, maybe put up, put, understanding it and putting something. This is a big philosophical debate in trying to understand. If GPT has some deeper semantic understanding. I'm going to leave that for office hours. Alright, let me take one more question and then I want to finish the last part of Great. Yeah, that's a great question. The question is GPG has so many layers of decoders. Gpt-3 is 96 decoders. Question is why doesn't have exploding and vanishing gradients? And that's because it has all these skip connections for restaurants. Alright, so let me get to my last slide, which is the way that we might go from GBT, a document either to a chat bot, is that we do an additional fine-tuning stage. So check. First learn structure from the heedless language through this generative pre-training process. This isn't what we know about attach to the t from the OpenAI blogs. But after that did you three steps. The first is they do some supervised fine-tuning, so they changed the task. You can give this as transfer learning. Instead of cleaning document that change the task now is to say, I'm going to ask you a question and it gives you a prompt. You have to give me an answer. So they give a prompt. The prompt might be explained reinforcement learning to the six-year-old. And then this is where humans coming to help fine-tune. Human will write an answer to this. That's the labeler who says, this is the kind of answer that you want to give to this question. Alright, so this is now a supervised data central questions and answers. You take that a chat GPT, GPT-3, pre-trained to just generate documents and you do some transfer learning. I sub T stands for supervised fine-tuning. To give such answers to such questions. That's the first step. But of course, you can ask if human labeler, to just generate arbitrary answers and questions, I would be really a Boreas. So the next thing that they do is they are able to further train chat TBT by reinforcement learning. The way this happens is that they will now present the same plot to check CBT, explain RL plus observable. Because ATP to generate sentences by generating softmax distribution is suffering from them. Every single time you prompt activity is going to give a different answer. So it's gonna give answers a, B, C, and D, where these are all possible explanations to this question or to the statement. Then a human labeler constant again. And then we'll rank them from best to worst. So they'll say D was better than CU is better than a was better than d. And what you can do from these labels is train another neural network called a reward model. And the reward model is going to tell you what the reward is or how good an answer that you get is, alright. Now become to the audit department no longer requires humans. They get the prompt to check, GET, and then they use reinforcement learning. So we want to talk about this in this class, but there's a really good optimizer and reinforcement learning called proximal policy optimization. Basically what happens is you answer the prompt, you give it to the Reward Model. Reward model tells you how good of an answer that was. And then you use that reward to train GBT to get even better. I went to bed and on overtime. So I'll just start the next lecture by reviewing this one more time you have any questions. 