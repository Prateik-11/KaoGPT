So we're all done with the homework for this class. I have up here are back. So today we're going to finish covering recurrent neural networks, at least that's what a goal. And then the have made new slides on transformers. So it's like a little bit this quarter he would understand how checked GTT works. Alright, after transformers, we're going to cover variational autoencoders. And then after BAs will cover generative adversarial networks, Gans, and then finally we'll conclude with adversarial example. So we're gonna go through these architectures relatively quickly. But a philosophy moving forward is that even though they might be hard to understand the first time around, you already understand all the tools to know and even in cement all of these architectures. We know from prior lectures that for any deep learning model, what we have versus an architecture that is our connected network or CNN. We're going to talk about these orbits are all different architectures as well. We need to find the task difficult loss. We need a way to compute gradients with respect to that loss and that's backpropagation. Know we need a way to update the weights using these gradients. And that's the Catholic gradient descents with their favorite optimizer or momentum. Alright, so you understand those things for all of these architectures. The way that we're going to compute gradients is still with backpropagation. And the way that we're going to change the wage is still missing ingredient. You really understanding many of the ingredients for these different architectures. But then these architectures that will be applied in different settings like when the sequences are timed better. That's for RNNs and transformers. When we're doing unsupervised problems, like trying to find structure in data. We'll use VAEs and maybe we want to also generate data. We can use VAEs and gantry. Basically, what we're going to introduce with these different architectures is that each of these different topics, is there new architectures and how their needs for particular applications? All right, any questions on that? Okay. Friendly reminder that the project is the last part of this class and it's gonna be due Monday of finals week, which is March 20th. Please be referring to the colab notebooks back to Brandon and utilize prepared for you all that will help you to implement to recurrent neural networks even if you don't know how they worked out, but you still will get to you today. And then the TAs are also planning to go for some coding for the project for this week's discussion video. Last week, please submit any midterm regrade request has been tapped by the suspensory because we're not going to accept any exhibit. Any logistic questions. Alright, so revolt, get back into recurrent neural network for our last lecture on the criminal networks as a week ago. And refresh, remember that what we cared about and recurrent neural networks with this new setting, history matters. I care about how things evolved through time. So we said that the recurrent neural network would handle this by introducing a new variable called state. And that state has some value at time t. Alright? And what we can think of the state variable is a variable that contains all of the relevant information about your past historical inputs. Inputs are X1, X2, all the way up until time t, the state variable i sub t minus one would be a vector. That's a simply represents all the information I need from x one to x of t minus one to perform my task well, right, so it's a summary of the history. And then we're going to derive a recursion, or we're going to define a recursion for the state variables. So if I take, I take my state representing everything until I sub t minus one and I get my new input x of t. I could use that to update my state variable from S sub t minus one to S of T. And S of T contains all the relevant information about my templates from X1 to X2. And this will be implemented through a return neural networks. So this variable, concretely, of t minus one is going to be the hidden state h of t minus one of a recurrent neural network. Alright, any questions on the refresher, the motivation for RNNs? Okay, so then we ended last lecture going over how much the question is, will they not workbook output z of t and S of t? The answer is yes. So the network will, at every timestep compute S of T that the hidden state, which we're going to call this h of t, since that's what we've always called neural network hidden state. And then z of t will be the output of the network which the RNN will also. Okay. So we ended last lecture just going over some examples from this blog post by Andre capacity, the unreasonable effectiveness of RNNs. I wanted to just return to this example because this example is going to be super important to remember when you're talking about check GPT, Stanford generative pre-training transformer. And the pre-training part of captivity is exactly this task In the space of not characters but of words or tokens. In this task, what we wanna do is we want to generate text and generate texts by learning structure and the West Greenwich. So let's say that we just had four characters, which are H, E, L, 0, and we want to write hello, okay? So the inputs will be one-hot vectors, where if the first element is one, then h of the second element is going to be the third element. The fourth element. What happens is we inputting h into our network. Input into the network is going to update the state. And the state is going to be the hidden activations of this, sorry, he's gonna be the activations of this recurrent neural network. And then from the state, which here will be zero, we can put a softmax probability distribution over what the next characteristic, right? So currently is pulling up hello, then the softmax probability for each should be highest. Alright? You'll see that in this case it wasn't the softmax probability for 0 was the highest. We know that the target is E. And because we know that the correct target, and we know what an output, that would be a way for us to compute the cross entropy loss to update the parameters. Alright? So this is the same cross entropy loss that we would do for your homework. Let's say that he did have the highest public utility. What I would then do is I would sample from this distribution and hopefully pick up the next character is b. And I would take this character and I would pass it in as the second time step. Input into my R&R. And the RNN would hopefully output our correctly, that l would then go here. And I'll put another l that would come here. Now, I'm putting an output of one key thing to realize here is that In these two cases, the input character was exactly the same. Vector 0010, representing power. But the network was able to output something different in the first case, another L, and the other case 0. Because the hidden state was different. Because the state of the RNN was different. And so really, the state of the RNN again, succinctly captures my historical inputs, is what allows us to then generate texts because it's using information that is learned from historical characters. Any questions? Is there a reason? The question is, is there a reason why embark has three values? Just for ease of illustration, this will be the number of hidden units. Keep saying, Let me just say a number of artificial units in the RNN and it's usually on the order of hundreds of thousands. The question is, what will be the initial state? The initial state of the recurrent neural network can be learned through backpropagation. Any other questions? So all the blocks over here. Yeah, great. So tomboy saying all the blocks here, taking two inputs, which is the prior state and the current input. Is there something special that we have to mention here? Which is yes, there'll be an initial states with no input and that will be learned through almost I said that's not biasing the training process. Yes. So basically, the initial state can have quite a significant effect on the progression of the network. So you want this state to be judiciously chosen oral sex. Is there Any other questions? Okay, So then last lecture, we were showing these examples from Shakespeare. And so in this task, your goal is just to say, given my prior characters, predict the next character. So here if I stop Hello. Now if you say training on the works of Shakespeare, you train for many iterations. At first output shippers, but then learns to output things that aren't semantically meaningful. Like this is Jewish, but at least it follows capitalization, quotation basically grammar rules of the English language and capitalizes names. It puts a comma with an application. And again, it doesn't make sense, but it looks like. English language. And this is the final train numbered from this blog post where it learns the characters are fixed there and again, learns to output something that doesn't make sense semantically, but at least it looks like Shakespeare, right? You also talked about how you could find it too. I'll put Wikipedia articles and we'll learn how to do citations in Wikipedia. You can even give it low-tech code from an, from an algebraic geometry textbook. And it'll learn than to write LaTex code that looks like an algebraic geometry textbook. Were there any questions from these examples that we went through last lecture? Alright. So the recurrent neural network is able to learn long-term dependencies. What this means is it's able to learn things that occur over many timesteps, which is our goal with recurrent neural network. So if you look at one of the artificial neurons, blue to red corresponds to the value of the artificial neuron. And what we see is that whenever there's a quotation, this neuron has a low value. As soon as the quotation closes, the value turns white and red. And so you can see that this is essentially like a artificial neuron that learns that it stays silent until you close the quotation. So the base, the properties of how we use quotations. This is another example of when they use are an entity generate code. And so there was one neuron where they stand at the activation of this artificial neuron has to do with the amounts that you had indented. Alright, so this is a neuron that then learns to be sensitive to the depth of indentation. Your questions here. Alright. I'm going to show you a little preview of the PT network. Was it the general, which is the generative pretrain transformer? We're going to talk about that again at the end of this lecture slash next lecture. But things have come quite a bit since then. And so here's this really cool example. This is from 2018 with GPT-2, which is a precursor of TBT. And it's also trained to just generate instead of hex characters and generating next words. And so what they do is they seed this state of the network with some human texts that says in a shocking finding, scientists discovered a herd of unicorns to the theatre. Remote, previously unexplored valley in the Andes mountain. In the Andes Mountains, even more surprising to the researchers was the fact that the unicorns spoke perfect English. So something that is totally made up, never seen before. And here is GPT-2 completing that task within the same principles that we solve for x characters generation. And I'm not going to read through this right now, but I'll put this up during the five-minute break so you can read. But you'll see that they define a researcher, Dr. Dave Perez, University in the area. They named the unicorns. Unicorns. And then they even come up with a hypothesis for how these unicorns can speak English. And so I think they say the animals were believed to be a descendant, to be descendants of a lost race of people who lived there before the arrival of humans and those parts of South America. Alright, so actually it's generating something reasonable, but it has semantic meaning. I'll put that up during the five-minute break if you want to read a little bit further. Here's another example of a GPT-2, in this case, processing text about the 2008 Summer Olympics. And then being able to answer several questions about the passage. You read this and you wonder how you would do on the SATs. But that we're gonna get to the RNN, however texture, any questions on the examples? All right. I see. Tom boys question is, in this case, how does GET nodes and for bullet keep generating texts indefinitely. So when they train these things, there is both a start token as well as the end token. So it gets to this point and when it outputs again took investment. Other questions. Okay. Alright, so this is the vanilla recurrent neural network architecture. Vanilla. We've drawn it like this. This is how you should think of it, which is that there are a bunch of artificial neurons. And now instead of them before they are connected with the current connection. So there are loops within this network and their inputs that come into the network. Then been nephron kinda percolating on those inputs and an output variable z of t. So the equation of the, of the vanilla recurrent neural network is as follows. H of T is. It is the activity of these artificial neurons at time t. And it is equal to some activation function like ReLu. And they're going to be three matrices of interests. The first is W recurrent. And this is a matrix of connection weights that defines the values of these recurrent connections. And so this is going to be w recurrent times the hidden state at the last timestep, h of t minus one is also going to be an input matrix. And that tells me how my input is x of t map onto these artificial activities, HFT. So there's gonna be a plus wn times x of t. And then there's usually also a bias speeds. Any questions on that equation. And then after that there's a readout. So the rehab z of t is just going to be a function of my matrix W out and my artificial network activity. So z of t is going to equal, it's going to be a linear later, w times h of t plus some other buyers. So that is the equation or a vanilla recurrent neural network. Before we move on to then how we can train with this network. There are few things that I wanted to mention. The first is as soon as we can calculate outputs, we can start to define. So in the example that we showed before of generating the next character, right? You can think of these as, let me just pull it up. The Z of t is r like the softmax scores. So when you take softmax and z of t, it will generate a list item. This is the a t. You can think of. Z of t is the value of the output layer. And these are scores for each of the characters and it doesn't go through a softmax to turn it into a probability distribution. And then I could go into a cross-entropy loss. Alright? So one thing you'll notice is that there's a z of t and every single time step at time step 123.4. All right, so the way that we define the losses can be very analogous to what we've done so far. We'll take that z of t vector and turn it into softmax probabilities. In which case, you'd calculate the cross entropy loss at every single point in time. Any questions there? Alright, instead of cross-entropy loss, you could also imagine doing it, e.g. a squared loss, but we have overhears. Then there are also also had the liberty with an RNN to define exactly what time points matter to you. Alright? So even though we have a loss at every single time point t, we may not care about the loss and every single time point t. So let me give an example. Let's say that my goal is to input 50 characters into an RNN and then predict what the 51st character will be. I may only care about the accuracy of the 51st character and not the accuracies are the characters, one typically. So even though in this case I would have a loss for my first softmax at time step one, for my second soft much at time step two, all the way up until L 51, the cross-entropy loss for the 51st character I only in this context would care about Alpha-1. And so I can choose which losses at which time steps I care about backpropagating for. Any questions there. Makes sense to them. There's different hospital works. How do you fix the probability distribution and there can be any number. Yeah, great. Tom was asking a great question. So we've only shown right now next character prediction, which is really straightforward because there are only 26 characters. I've mentioned to you that checks you can t, isn't generating the next character, but it's generating the next word. And there are many more births than there are characters. So we're all happy on the stage to check digit is actually not generate the next word is generating the next quote, unquote token. Tokens can be worse, but more commonly the birds. So maybe if you've had the word minimise, this would be broken up into three tokens. Like there would be a token for men. There might be a token for n and then write your token for. Chaffey had, I believe, a vocabulary of 50,000 tokens on the order of 50,000 tokens. And by the way, the tokens include the individual characters a, b, c, d. Etc. So that through these 50,000 tokens, you can write any word in the English language, right? So I said before chat TBT is predicting the next word. Not totally accurate. It predicts the next token, but when you combine those two things being equal, birds and that vocabulary is about 50,000. So they're generating softmax probabilities over a 50,000 dimensional vector, right? Yeah, so the question is, during training, do I need to consider all of these bosses, L1, L2, all the way up to 51. So you can definitely compute them during training, but you don't have to use that. So what I'm saying is when you have a loss at the output, let's say l in green here is my total loss. I could have defined L to be L1 plus L2, or all the way up to 51. If I choose this to be my loss, then it will read my network to backpropagate so that all of these loss because our smallest possible. But I can also define in another setting that Alice just equal to L 51 and then back propagation but only change the weights. So that might fit the first correction is as good as possible. And it doesn't care about my first one to 50 predictions. Question is, can I give an example where that would be beneficial? Yeah, That's actually do use quite often for natural language processing. So even though we showed this setting where we're in this example, we do care about the loss at every single time step. Usually. We don't construct these in this way. We will construct them as you put an RNN, RNN, let's say 20 character inputs. And that allows the hidden state to update for 20 iterations. And then you only care about the classification at the last time step. And so that would be a problem of saying, I care about him putting 20th dark doesn't make it into 20 inverse correct distributions for the question is, when you do this, Do these intermediate character distributions of things before the 20th chapter ends up being correct? I'm actually not sure, but I would guess they're fairly accurate. Recall we should do exclusive, right? Exactly. Yes, It's homilies point here is related to Daniel's question, which is, even though we only care about the prediction of the 51st character, not the 50 before that. To get that 50, right, we would have had to do something reasonable. And that's why we would expect that the intermediate characters are also like. Alright. Okay, so that is the recurrent neural network loss using the loss functions that we've done before and character prediction Dolby cross-entropy loss. Alright, so let's take stock of what we know. We know the recurrent neural networks equations. And we can define a loss function. Alright, so immediately we know how they do it forward pass, right? We just compute this equation over and over again. And if we have a loss function, we know how to score how good our networks. We also know that when we have the gradients of the loss with respect to the parameters of the network. We know how to update those waves. And that's just using stochastic gradient descent, right? What's really then left for us to do is to answer this question of how do we compute the gradients of the loss with respect to weights. And in the case of fully connected networks and CNN is right for you that we did backpropagation. We've been at work. But this becomes a bit more challenging now because in the case of the recurrent neural network, there are loops. So the chain rule for derivatives that we just applied in each layer. In a feed-forward network becomes far more hairy when we consider a recurrent neural network. Alright? So how do we take care of this? What we do is we do the recurrent neural network, feed forward network in time. Okay, Let me unpack what I mean by that. So first off, I'm going to add one more thing to this drawing, which is that there's gonna be some initial state H zero, I guess into this network. Recurrent neural network in question was the following. H of t equals ReLu of w recurrent h of t minus one plus wn T. I'm going to drop off the biases. What this tells me is, if I start at time t equals one, then if I know the value of h zero and the X1, then I can calculate each one. So if I know the values of h to zero and x1, then through this equation I can get each one. And now I would know the value of X2. X1 and X2 would combine to give me a value H2. Alright, and I've drawn that right here. So we kept the cute H1 or H2. Know if h is zero and x one. To compute H2, I would just get to know each 1.2. Okay? So going along this axis here, we have time increasing. But now if I unroll this recurrent neural network equation in time, it becomes feed forward into time. And the number of layers of this network, however much time I'm using, my God. Any questions there? Yeah. So Tom Waits question is, can I say that one more time? How am I unraveling time? So what we're going to do is we're going to take this recurrence equation, right? Which in general will have groups because WE couples the various artificial units at time t minus one to time t, will draw that in the next slide. And I'll have another slide that says this more clearly. But if we look at this equation, we see that to compute the hidden state at time one, all I need to know is the initial hidden state h zero and my input X one. After I know each one, I can increment from t equals one to t equals two. So at time t equals two, I get a new input x2. And what this equation tells me is that I can compute H2 as long as I know what each one was. I got one already from each with their own X1 and X2. So from each one and x two, I can compute H2. Alright? So if I do this for every single time-step, then what I see is that when I write this equation, but I stand out time, right? This looks like a feed-forward network. There are no groups in this computation. Recall calculating the H1, H2, H3 is where all that I needed to compute the output of my network, because the output z of t is just a function of h of t. H of t. I can calculate z of t the output. And if I have z of t the output, then I can calculate my loss functions. I will unravel this further in the next slide. So let me just go to the next slide here. So let's take a concrete example where I'm going to have organelles. So here my hidden state h of t is going to be a 4D vector. And let's look at how this has recurrent by looking at the part of the equation ReLu of w recurrent times h of t minus one. If w recurrent or equal to this matrix. Then what this equation tells me is that I would add an h of t minus one. So I'm gonna write this as a 4D vector. Actually, let me, let me do the following. Let's, let's make this h of t plus one and make this thing h of t. So the four elements of this vector, I'm going to call h of t at index one, index two, index three, index four. Now what I know is if I go ahead and I take the ReLu, this W recurrent times h of t, That's going to give me h of t plus one. So this is going to give me h of t plus one. First element, h of t plus one, the second element. And so on. I'm going to call this vector h of t plus one. Way that we typically use is via a graph like the following. So if I look at how h of t plus one, the first unit. Is affected by all of the other units. We can see that h of t plus one in the first part official unit is gonna be one times h of t, the first unit. So there's a one here plus the value of the fourth unit at time t times 0.4. Okay? So these two values, I'm going to draw my graph via these lines. So the first unit has a connection to itself with value one. And the fourth unit affects the first unit with a weight of 0.4. So that's this connection right here. This thing here equals 0.4. Similarly, if I were to look at, Let's just look at the second unit, h of t plus one for the second unit is going to be 0.9 times the first unit. So there'll be a 0.9 over here. The third unit will have a weight of 0.6 to the second unit. The third unit here will affect the second unit with a weight of 0.6, and then the fourth unit will have a weight of 0.5. So that's this one right here. And if you do that for all of these and the networks, that's what this recurrent neural number. Any questions there? Okay? Now we're going to unroll this. And so what we're going to do is we're going to take this same network. And recall, we had this connection being a one and disconnection being 0.4. Alright? Now when I look at the values of my activations, so this will be in by h of t, which is a vector and R4, that's these four circles here of h of t minus one over here and h of t plus one over here. What we're saying is that h of t minus one affects the first unit with a weight of one. And therefore, this week here is a one. And then going from time t to t plus one is still affects it with a weight of warrants. So this is also equal to one. Then the fourth unit affects the first unit with a weight of zero point for this weight here is zero point for this weight here is also 0.4. Alright, I'm just going to do one more. I think on the prior slide we had that this value here was 0.9. And so now all of these connections, which tells me how h of one, sorry, the first unit affects your second unit. But each time step will be 0.9, right? So you can do that for all of these connections. And that takes my Recurrent Neural Network and unravel in time. Questions. Question is, is if I weight values are very small, isn't that really bad? Yes. So we'll talk about that in just a bit. What we'll find out that these numbers are really susceptible to vanishing and exploding gradients. Daniel's question raises another point, which is what you should have noticed also with a disability before neural network, where the number of layers is equal to the amount of time I run my number for. Every single weight connection is exactly the same. So you can think of this feedforward neural network, but all of the layers have the same exact way, which is defined by w recurrence. And now, like Daniel was saying, if W returned is small, that all of these wastes will be small and their gradients are going to vanish. And then if w wreck has any values that are both one, then after repeated application, your gradients are going to explode, vanishing and exploding gradients are really big challenge recurrent neural networks. It's tempting to train in between time steps. The first step, the question is, is w Rec, also a function of time? This W wreck ever going to be different than this w back. In return neural networks? Almost always. W Req will not change as a function of time. There have been some people who have thought about this, and there are some architectures where W direction change in time. An example is called the multiplicative recurrent neural network from Ilya Sutskever and Jasmine and James Martin's, where they do have a recurrent matrix that changes with time. But it's much more challenging problem. Other questions, Yes. Question is, when does the learning for w rank tracker? I will get to that on the next slide. So this is what the computational graph looks like for recurrent neural network. And what you see in this computational graph is that we start with an initial state and multiplies w rash, that we add a wn times X1. We add these things together and we applied our revenue, which is this function f. And I guess the H1. So this is just implementing that equation. Let me just write it down one more time. H of t equals. Instead of maybe I'm going to write f and it's going to be W times zero plus wn times, sorry, this should be h of t minus one times x, right? So this is just a computational graph for recurrent neural network. And then the hidden states are also used to compute the outputs Z1, Z2, Z3, etc. So the parameters up by recurrent neural network or this fall is there, w in W wreck and WWF? But in reality, the thing that's most challenging to learn is this w recurrent, right? Because this W recurrent is that one matrix that is applied at every single time step to update my current state. Okay? And this is the thing that is going to learn, those temporal dependencies over the history of your data. All right, any questions so far? Alright, so let's just consider a first setting where our loss is a cross-entropy loss. And we're doing next character prediction where we were, we care about the softmax probabilities and every single time. Alright, so here you would have that and l total l equals L1 plus L2, L3. In this case. This is a feed forward neural network. There are no loops. We know that when we have a feedforward neural network will have to do is back propagation the loss to the parameters to get my gradients. So if I were to start with my loss, I can backpropagate through this one path out three. That would give me a gradient with respect to z3. I can get a gradient with respect to h three. At this point. Let's say that I take this pathway to w, right? That would give me a gradient of the loss with respect to w recurrent. But I could have also at this point here, backpropagated through this pathway and then Gonstead W rack, I could have even gone one more time step ahead, and got to w. Similarly, I could have backpropagated through, Let's see if I have another. I could have backpropagated through L2 to get the gradient for w recurrence. I could have backpropagated through this capitalist to get ingredient. We know from our derivatives that you have multiple backup in the path-based object to a single parameter w. What happens is it a gradient is that they all add together. All right, So basically in this computational graph, if I want to compute DL, DW wreck, I'm going to have to be cognizant of every single pathway from the wasp to W recurrent, I'm going to have to backpropagate through every single pathway and then some other gradients. That'll give me a deal. Dw wretch. And then from there, I can just use adam or STD or my favorite optimizer to update the value of the current. Any questions here? Perfect. Yeah, The question is, are there also multiple Casper WN? The answer is yes. So Wn is gonna be the same at all of these wires. And so when we backpropagate, there'll be multiple passwords, W and skills. Great. Yeah, so this student was noticing that there is a growing, I think you say quadratic. I'm going to ask the TAs to just read about that to tell me what the complexity is. But as the number, as the amount of time steps grows, there will be a growing number of paths and that will, that will increase the complexity of the Talia is quadratic in time, but I'll perfect quadratic in time. They're trapped. Sorry, Another one at a time. The question is, when we talk about stochastic gradient descent, is that for one time or in other words, how do we use dashes here? So this DL DW rent is independent of time. So when we calculate DL DW rat will happen. You can think of it as like we're basically summing across every single possible path for the gradient to give us a single gradient of how changing w record a loss. But then because this is independent of time, then we can just use our favorite optimizer is just as GDA would be minus epsilon DL DW. Topic. Exploding gradient problem. And bring it back propagate again. We're getting great. Takeaway is asking a very deep question. Was I'm watching reserve for just the next slide. So let me first talk about what this backpropagation but looks like and then raise a problem. And then we're going to get it from right? So let's go ahead and just try to backpropagate through one of these w retry operations. So let's say that I had the activation or the gradient d L, d h three. And I want to backpropagate to hear DL TH2. And let's say that the function f was a railroad. So I want to know how to compute DL TH2 given my upstream gradient, d L, H three. Alright, you guys have done many times, so we'll just go ahead and do it. If I backpropagate through a ReLu right, we know that that means I'm going to take the Hadamard product with this indicator function of h to being bigger than zero. I'm sorry, No, it's not. H2. There'll be w wreck times H2 being bigger than zero. So that propagates through my ReLu. And then if I want to back propagate and DLD H2, I have to backpropagate through this matrix multiply. We know that this matrix multiply. We'll take the gradient because I'm backpropagating TH2. It'll be the value on this wire transpose w rack. So this will be a w transpose. So that's how we backpropagate from DLD street and DLB issue. I'm now going to change the reason to QC h of t and h of t minus one because we just see that if what the backpropagate one more time step to deal TH1. We're going through the exact same operations, alright? And so we would just have this equation compounds. So I'm going to change these CL, DH, DL, DHT minus one through this operation. Any questions? Okay, Here's where we see the problem of RNNs, which is, if I were to backpropagate through a really long path. This is just talking about what we already derived from the prior slide. That back propagation through this path involves the multiplication by W transpose. If I blocked out propagating. Now, I'm sorry. If I'm backpropagating from GLD H32 CLTS one, I'm going to have two of these w transpose is, if I'm backpropagating through even more timesteps, let's say ten timesteps, right? I'm gonna have to go through ten multiplications with WEBrick. I'm going to have w transpose raised to the power of ten. When I have a matrix raised to a power, because I'm doing repeated multiplications. I see that delta t is the number of steps in time we're going to backpropagate through. So let's say that we're backpropagating through ten times delta t equals ten. I'm going to multiply it by w transpose ten times. Let's say that w transpose has this eigenvalue decomposition. When I multiply it by itself, delta t times ten times. When you write this out, where you're going to notice is that the u minus one times use are all going to cancel to give the identity. This is going to therefore simplify to u times the eigenvalues raised to the power of ten times U inverse. When I say this matrix lambda, the eigenvalues raised to power of ten. What I mean is if I have a matrix that looks like this, lambda one, lambda two, all the way down to lambda n, like n eigenvalues and zero everywhere else. Then lambda raised attendance is going to equal. Lambda one, lambda two raise to attend all the way to lambda n, risk retention, looser as everywhere else. This is really bad because lambda or your eigenvalues, Let's say that we wanted to, I'm gonna make this even more exaggerated. And as you see, let's say that we want to backpropagate 100 times x, which means that I want 300 characters to predict the next word. If my first eigenvalue was even just slightly bigger than what? It was equal to 1.1. Then lambda one raised to the 100 equals 13780. And then if lambda one was just slightly less than one, or 0.9. Lambda one raise to the 100 would equal 2.65 times ten to the minus five. So if you wanted to backpropagate through a lot of time, your eigenvalues deviate from one at all. If they're less than one, you're going to have vanishing gradients that they're greater than one. We're going to have exploding gradients, right? Any questions there? Yeah. There's much. The question is, what are the eigenvalues corresponding to? These are the eigenvalues of like W returned, right? Great. Yeah, so generously, a great point, which is these are the gradients D L with respect to the hidden states. But they're still important because whenever I want to finally stopped going back in time and go to w rack, right? It is going to be like a TLD H2, H1 transpose DLD H2 is really big, then it's gonna be a really huge gradient. And if guilty exponentially small, start with DLD, H2 is really small, that's gonna be back in Britain. So let me just write that out here. This gradient would be pH2. There'll be multiplying spill the indicator for the atom product, as I've indicated for the ReLu. And then it would be times in each one tracks does. So. If this thing explodes or banishes, the whole thing, vanishes. Other questions. Alright, so this is really bad because as long as w wreck deviates from identity or something that has all eigenvalues are equal to one. We're going to have vanishing or exploding gradients. Now let's get to time-wise question earlier. Tom Waits. The question is, let's say that the gradients batch, alright? So let's see if all the eigenvalues are less than once, but brilliant Spanish. What that means is that if I back propagate through this path, right? Go through three W Rex, Right? Or in general, 20 W racks, that gradient will be equal to zero. However, there are all these other gradient past. And in particular there's this really nice gradient path that goes from l directly to W ranch. So once I still have a gradient to train a good w for this task, the answer is no, but I want somebody to tell me why the answer is. This is a hard question. Disproportionately takes into account the person's, right. So this student is saying disproportionately takes into account the first few tokens or words like, like the effective x, y, naught and power. There is also another gradient descent gradient path from L through L3 that goes straight to w. Recall. This would incorporate information about the third one. So that's a great idea. But in this case, there will be one, a gradient plot that only goes through one W rec for every week to replace the perfect. That's exactly right. So when I am wanting to learn how my characters 2020 vertigo affected my current work, I need to have a W Iraq April to find dependencies across 20 timesteps. When I look at the one W record gradient past that go directly from the loss of w records. Those are, those are good. Pastor telling me how the current token affects the loss, but it won't tell me how the first token through two timesteps effects the loss. Alright? So if you want to be able to learn long-term dependencies, you have to be able to backpropagate through those long temporal dependencies that tell me how one affects the third character, two times that fader. So if I want to know how X1, the first token affects my Word 20 times up Fader. I'd have to backpropagate through 20 of these new tricks. I've only consider these ones. The only, tell me how the current token. Yeah, the question is, let's say that w records really small, right? Sorry, Let's say that we had vanishing gradients. The gradients DL, DW requisite really small dental history effectively lose the recurrent return on average, the answer is yes, because we're just back off and getting through. They're really short paths blocked right? After this. Tom ways question is, well, I am going to have an age 50 e.g. and it is going to capture information about my past 50 inputs wanted. And what we're saying is, if you weren't able to backpropagate through through these 50 W wreck multiplies. Let's say that you only could relate that propagate through one. Then each 50 is essentially going to be a function of x 50. And very little of each 49. Any questions there? Jack? I'm just reiterating. If w is small, then W records not going to learn how to take X1 and effect outfit. It's not going to learn how a character 50 characters ago affects my current prediction. So W Req is small. It's basically just going to be like a feedforward neural network where I'm using my current character to predict my, my contacts something. Alright. These are, these are not easy concept. So if you're not following, but that's okay. It just takes some time, so please be sure to review the Spartans. Alright, so let me just do a few more things to finish off the yellow recurrent neural networks. How do people address this? Because this is a pretty big problem. And a story is, I was in graduate school for the first half of my graduate school. Learning wasn't yet experienced revival. And so we trained recurrent neural networks, but we never did backpropagation through time. I'm sorry, I should have mentioned this operation of unrolling the computational graph and backpropagating through it is called backpropagation through time. Backprop through time. The PTT. And we were writing a review paper and one of my colleagues to lead are all meant to be a scientistic brain. And then later, what he did is we're finding the UK for him. He wrote a sentence, essentially said that propagation through time is awful because of these vanishing and exploding gradients. And therefore, you always want to be using a different approach. We think we'd cut that sentence of the paper because after the deep learning revivals, people thought of tricks to trained with backpropagation through time and that's the prevailing way to train for recurrent neural networks. So how do people address this? The first way is we may just do truncated backpropagation through time. So let's say that I was bringing a neural network for Delta T equals 100 times steps. I need not be able to backpropagate through 100 time steps without having vanishing or exploding gradients. So I might just choose to backpropagate through 20 times x. If I do this, then what I'm basically saying is I'm never going to go through any path here that is more than 20 timesteps, even though I have 100 times steps. And we're basically conceding that because I can't go beyond 25. My neural network has never want to learn any historical dependency graders in 25 steps. I wouldn't have been able to train this incident, vanish or explode. That's the first way for exploding gradients. Gradients, I think I'm going to talk about this before for other networks and services, the gradient of the equation. And then for vanishing gradients, we could do a regularization topic. And so this is from husband pesky, Daniel and colleagues in 2012 where they add this term to the loss function and this regularization term is actually, it looks intimidating, but it's fairly simple. So if you look at this term right here in the numerator, this term right here, if you apply the chain rule is like DL, DHT, right? And then this term over here is the l th t plus one. And so DLT is t divided by delta H2 plus one is close to the value of one. And this regularizer is especially regularizing that. The norm of DL PhET is very close to the norm of T plus one. And this helps your gradients. You apply the current Neural Networks. Regularization is quite helpful in the case of the vanilla recurrent no numbers. Let me do one more fundamental. Think of it right? We started this off. Your initialization was really important for feedforward neural networks. For RNNs. If you're using a vanilla RNN clock and colleagues in 2015 suggested this initialization trick of settings W returned equal to the identity, has all eigenvalues of one. So just start, you're not going to have any exploding and vanishing gradients. And then there was another initialization in 2016 that has the same spirit, max, it initializes W, right? So that is Maps eigenvalue is equal to warn both of these perform. Let's take a five-minute break and then when we come back, we'll continue on with the LSTM architecture. I'm going to put up that GPT-2 text. It's going to spread. Regarding the readings for me. That's the direction. Right? Right. Yeah. Kelly, take some design that we use different fats like so what onetime step for one head and one time a truth instead of just kinda yeah, you could do that. I actually had a chance to give any paper. A lot of sense. Yeah, yeah. Another, another problem is that I just wondering, what's the time value of X t and h t n 0. It could be all different. So one can be n-dimensional, H could be n-dimensional. So then WE would be n Pi. Key to management on reading about wishes were based on me. Yeah. So like if you drink character predicting Vensim, It's going to be 26 dimensional, okay? Maybe a bit higher for other kinds. Okay, I see, So this is why they are all the same from my point of view, these two mice so important this week and this week, but they are different. This is really importantly. I can see that again, when the diamond use of x1 and z1 really different than those weights, that's due to anemia transformation for the output. Here, for the input. I really always important, even if, even if, even if their decision region, because you can think of like zero or like my teachers, this W added, It's kind of like the last linear layer of a neural network. So even if before the last layer, your features were already 1,000 and you have a topic. It's still good to put up linearly. Okay, I see. Thank you. So why do we add losses? Because in the press just put some as 51. Yeah. We don't have to. So it could have been that you only cared about L3, which gives you this backpropagate through L3. So I just give me a general effect on you're going to get oh, that's my bad. If I have to reply to shoot me an email, I just I just missing a minute. Okay. Yeah. Rnns. Right. We have we're assuming, like we have to wonder current page just represent all the dynamics as we evolve through time. Yes, tuple considered like cutting it off at a fixed contexts length and then doing tension on the states. So exactly similar, but some differences. What's this? You have self-attention, you have white global rights. I think I felt like if you want to see them at zero influences, I would think of attention. Exactly. Yeah, So we're going to find that, yeah, Basically for an opening and many other tasks, attention slash F farmers will make all okay. Because you don't have to global attention layer, then. This far easier to see. Okay, but is there like this, I guess, is there a difference between applying transformer on a sequence of hidden states that are predetermined price or exchange the projected in transformer encoder and walks of that on those spaces and reconstruct all the ones. I guess it's not really much different from an actual transformer. I mean, like if you think of Applying a transformer to Elmo and Betty's, that's kinda what it's doing because Elmo embeddings are using recurrence to try to figure out how the contexts later on at the start. So yeah, it could be studying, right. Okay. There's referring to my global attention on top of it. Okay. Yeah, yeah, super interested in. All right, everyone will get back to it. Any questions on anything from the first part of lecture or anything on vanilla RNNs. Now we're going to move on. So each step of the body. Yeah, yeah. So the question is how will recreate that? So basically, let me actually just give any staff or with Shakespeare texts. Let's say that we were trying to decode the 11th character from the fire ten. So basically the way that you would create a bachelor's, you would take ten characters. I'm not sure if this is ten, but we're just gonna say it's ten. This is going to be the inputs X1 to XN. And then the output will be the prediction of the letter. And then what I could do is I can shift this over just by one. This is another example where I have ten characters as input and then the correct output is whether, all right, so within a single article or a single block of text, you can extract from many different examples. And then these can be randomly sampled into your batch to then compute gradients. Chapter did he use his doctrines of white guy, several million million exactly. What goes into a single tenant. Right? Thomas question is, just to confirm the size of your bachelors, not the number of sub-sequences yet. Yeah, your batch size. It could be like this example like 256, it would, you would take 256 examples of ten characters followed by an 11 that answered the question. The question is, are all the examples in the batch of the same sequence length or would that bias training? In simpler examples that I've seen is typically the same sequence. Although I'm not sure if that's true. Intact GBT doesn't make sense to use different potentially to have better generalization. Does anyone know if Chuck E. Cheese is different than the sequence things like Daniel says, you may just padded if DFS. Okay, great. Yeah. Usually for these large-scale multiply the whole time. Any other comments on anything RNN related sequence? Yeah, so Russia was asking, you can take ten characters. And instead of competing and output at every time step. And then only catching the 11th, put it could take ten characters and just absolutely love into one. Basically, if you can find a feed-forward network where the input is at ten, character is three, and the output bit 11 characters. And yes, you can do that. Any other questions? Okay, So we're gonna get to the most common strategy to avoid vanishing and exploding gradients and RNNs. And that is to not use a vanilla recurrent neural network. But then actually changed the architecture into something called a Long, Short-Term Memory or an analogy here is that remember, the amount of time that we're going to have to work for is the number of layers that we have to backpropagate. We know that for convolutional neural networks, they fail when the number of players becomes large because gradients explode. But Resnick gives you a gradient highway and allows you to still train. You had any layers. The LSTM is also going to give us a gradient highway, although it's gonna be different than that, but it'll give us a gradient in the backpropagation through time. And that allows you to learn longer scale temporal dependencies. So we'll talk about the architecture. And the first thing that we're going to do is show you the equations are the arguments here. So these are the equations and it should look daunting and completely unintuitive. We're going to unpack what these are. So firstly, just a few things of notations. Remember that for every hidden state at time t, and an RNN is going to be a function of my input at time t and my hidden state at time t minus one. And so when I write up a matrix like this, WF, WF, you can think of as being comprised of a recurrence component and an input component. And this being multiplied by h of t minus one and x of t. H of t is a vector in R n and x of t. What's a vector in R, m? Then wf would be a matrix that was n, that is n by and parsing. All right? So that's what that notation means here. You'll see here that there are bunch of sigmoid. So this is the sigmoid function and it's gonna be applied element-wise. Montane Hs. We're going to see when a teenager is used. Usually that means that we're writing a value or we're calculating a value. Whereas when a sigmoid is used, we're going to see that that corresponds to a gate where one means the gate is active and zero means the gate is closed. I'll explain that in just a bit. So what I'm gonna do is I'm gonna take these equations and I'm going to write them into a diagram. So this is what the diagram looks like. To make this diagram look interpretable, I've omitted all of these matrices, WF, and the bias is WIDA, et cetera. But basically, what we're doing is in this diagram, we're going to start here. We're going to concatenate h of t minus one and x of t. That's computing this element, alright? Which is the, which is a part of all of these calculations. I'm not going to draw this WWF, but I will draw the sigmoid. So this sigmoid has an implicit WWF associated with it. And so the output of the sigmoid is going to be this value f of t. So f of t is sigmoid of wf times that concatenated and put a good candidate input, multiplies the WWF, goes through the sigmoid and I guess, alright, so just looking at the rest of these equations, I of t is this concatenation multiplied by an affine bare pass through a sigmoid. And this I of t is this right here. And t is the output of a tan h times this affine function of the input. And so this is v of t. Now there's one more variable of t which corresponds to this. Any questions on just how it is represented? All of these are scanning brains. Okay? We're going to talk about then how we intuitively think of all of these operations. Before we do that, I want to talk about how we should conceptualize this new variable, c of t. This t of t is called the cell state. And it's a critical feature of the osteon that allows us to learn really long temporal dependencies. So you should just think of the cell states here, t as a memory or two. I'm just going to call it a memory from there on out. When you have a memory, they're gonna be three operations that we can do to it. The first thing is that you could forget things from your memory, right? The second thing is that you could write in new information to the battery, so I want to add new memories. And then the last thing is that we could read out information from this memory or from this two. All right. Any questions there? Okay, so those are the actual three operations that these pathways to the LSTM. We have our memory or archaic state. And the first thing that we're going to do is you're going to multiply it by f of t, which is going to be called the forget gate. As of Tuesday, output at the output of a sigmoid. So that's what she's gonna be. Something between zero or one. If f of t equals zero, I multiply myself state by zero. I lost everything in the cell state, right? So f of t equaling zero. When you started watching, forget all the information in myself. The second function will be, let me do this in green. And I of t times v of t. And you'll see that we'll talk about this a bit more on the next slide. This thing is added to the cell state. So this is how we write information. If I want to add a memory to my cell state, then I write it in through this pathway. Then lastly, reduct information for myself states. So to get my hidden state at time t or the state of my Recurrent Neural Network at time t. What I will do is I will read out information through this pathway. And again, we'll talk about that in just a bit more detail when we talk about these individually. But basically, this multiplication is the operations for getting information. This addition operation to write information into the cell fate. And then this pathway that reads out is how I extract information from the self-care. The goals and the minus one. The same thing, 30,000 sequence. Yeah, so tomboy is saying there's a multiplication by a sigmoid here. This is going to be called the output gate, right? Can I explain how this doesn't cause gradients to vanish? I'm going to reserve that question for lamin talk about each of these cases individually, if that's the case. But it's still a binary event. Yes. Streets, especially bankruptcy. How much question is, is this a special kind of sigmoid other than the sigmoid that goes 0-1 notion typical sequence that we know, so it's an analog value. Any other questions? Alright, so let's talk about these gates that individually. The first is the forget gate, and I don't know why they chose this nomenclature that you should be called, but remember, the reason why is that the self status updated as c t equals to t minus one times the forget gate. So if the forget gate is close to one, then the cell state at time t is approximately a time, the cell state at time t minus one. And therefore the information and the tape and the memory is maintained. Alright? And that's where I think it should be called the remembered date because I think for debt is one that I want to forget is close to zero. I multiply C T, T minus one by zero and CT then becomes equal to zero. And therefore I'd forgotten any tax information. Okay, Any questions there? Alright, so that's the forget gate. And just to be absolutely clear, given Thomas question, f of t is going to be some analog value, 0-1. You should be practiced because we can take on those values. Alright? So f of t equals to one, we remember information of t plus to zero, forgetting. All right, the next part is running in information and text on the slides. So let me just tell you what's going on here. When we writing information, we have are two things that are computed. The first is called i of t, the input gate. And then the other thing, I actually call it something different than the paper, I'd call it a DFT. I think the papers might call this v of t, g of t. G of t. I call it that way because it's a valley. So whenever there's a tan h and another big effect tan h activation function, the output of a tan h is a value that I care about. And then any sigmoid is essentially a gate that tells me how much of that value I want to write. I am going to always be computing a value to write into my sulfate That's the empty. Then the amount that I write it in, it's going to be dictated by I of t. I of t is close to zero. I will not provide any information to my selfie. If I of t is close to one, then I will write in whatever value is computed by v of t. Alright? So that's the strike here. I of t, v of t is a value between -1.1. It tells us the value that we want to add to the cell state. But then how much did that value I actually add is determined by the input gate. So I have t is 0-1. It tells me how much of the value to write onto the soft food. So I take IMT and DFT, I multiply them together. Any questions? Great, yeah, The question is I of t and VMT process the same input but different productions of them. Is that right? Yes, there is, yes. So the matrix for the value date will be a W V, and the matrix for the input it will be a WIMPs will be totally different. The question is, is there any significance to have a negative value for V of T? T can be positive or negative. Some students sometimes ask, why do we need this IoT gate? Because can't write in zero information by setting d of t equals to zero right? Here, return to an answer which is that this architecture makes it easier to write than zero information because instead of tuning via TWO size value of zero when you don't want information. To be ready, man, all you have to do is get the sigma two equals zero, which is a lot easier because if this is a very large negative number of sigmoid, it back will be equal to zero. So it's easier to not reading information when you break it down. Alright, and then the last thing that's reading in information, last thing is reading out information and read that information. We take ourselves, we apply our activation function to it that can age. And then we multiply it by what is called the output gate. Just like the other ones, it should be fairly intuitive. 0 of t is equal to one. Then I'm going to read out everything from myself and all t equals zero, then I'm not going to react anything at all. And so 0 of t will be the output of a sigmoid. I don't have the equation here, but it was on the prior slide where the sigmoid of a W0 and toxic contaminated. Any questions there? All right, so that is the LSTM architecture is just a way of dynamically altering the street. Next. Next seat have some stored information from the district. I'm going to show you in just helping somebody that you're writing, so I'll do it. Right? Yeah. So that's right. Okay. I see where you're going. Thomas question is essentially, is this entire structure of the LSTM adding anything to the vanilla recurrent neural network? Or is it like primarily because it gave it additional capacity and computation? Or is it really just something that facilitates training by giving you a gradient highway? And the answer is, it's the ladder. So it actually turns out that a vanilla recurrent neural network is a universal dynamical systems approximator. A recurrent neural network can implement any dynamical system. The LSTM is a constrained version of a dynamical system. But with these architectures, however, it's enables you to practically changed because of the gradient, either which we'll talk about this next slide. The question is, when we backpropagate through this, we have saturation. The are you talking about the tan h over here? Oh, I see. I see. Yeah. So if you want to backpropagate through h of t minus one, we're going through a bunch of sigmoids, which we know can, can kill your gradients. And so that is one consideration of the LSTM is maybe not as great. Yeah, Perfect, Yeah, that's a perfect analogy. So this student said, Can we think of an LSTM to an RNN analogous to what a CNN is to avoid connecting number, the answer is yes. So point to Neptune numbered. Implement any function. The CNN is a constrained version with spatial locality like what you said. But that proximity trains veteran does better because it has so many fewer parameters and takes into account things related to vision and the LSTM by analogy then. Is like a constrained version of recurrent neural networks. So that's a really great example. Other questions. Alright, so let's fight for this. I don't. So if you look at the cell state, this tells me is the memory of the recurrent neural network. And you can see that he gets a gradient highway. Because if I calculate DL, really ugly L DL DCT, we know that when you go through a plus sign, just pass through. So backpropagating through a plus sign, we still have DLD CT, and then backpropagate through multiplication by a backup location I multiplied by the value of the other wire. So as long as f t is equal to one. So if f of t is approximately equal to one, then d l d c t minus one equals d l DCT. Alright? So the only way for my gradients to die in the LSTM is if the forget gate is close to zero, you should look at this and see that this is actually desired behavior. Because if I forget, I don't want to remember anymore temporal dependencies. And so as long as they're forget gate is not equal to zero, which means that I walked through, remember everything from this past history. My gradients are always going to survive and just gonna get a gradient by way to backpropagate the reference through any questions that you raise your hand if this is making sense. Okay, great. Yeah. Because of that property, PhD minus one. Yes. Backdrop to the minus one. By the, by the forget gate. Oh, sorry. You mean by, by this h of t minus one? Yeah. So Tom way of saying that when we backpropagate to h of t minus one, there's going to be several backpropagation is through sigmoids as well as other matrices. However, even though this is the case, it's not repeated multiplications of them. So if I want to know how much, say 50, related to X.25, right? As long as the forget gate was never a zero between time 25 to 50, I will have a strong gradient backpropagate through 25 of these. That thank you, goes to x 25. And it only goes through WWF warrantless. The pathway that relate to the t minus quantity has a gradient highway. Whereas the vanilla recurrent neural network, the pathway that relates t minus one to t, have to go through a matrix, multiply that matrix, multiplies what explodes or vanishing gradient. Gary, and only have to go through this. I might have misunderstood the question yet. There's a WF here, but this is only multiplied by warrants. So basically I go from C to D and we go like a d, l DC 5D. I would go through 25 of these to get a d L, d C25. And that will be capturing temporal 25 times stuff. But then if I were to backpropagate h of t minus one, I would only have to multiply by WL one. So Tom, I sang in this part here, this would be a DL DH 49. 49. Way that the memory is maintained is through the cell state. So this deal, DHS 49. It will have some impact on the sulfate and backpropagating. But it's okay if those gradients status because all of the information I want to remember is through illustrating pathway. So now what I was saying is that you replicate this pathway 25 times. I would get some DLD, see 52 DLD, C25. And now if I want to note the LDH 25, I would only have to backpropagate through one matrix multiply, right? Yeah, if you went from h t or h of t minus one, you would have more matrix multiplies. Yeah. But the memory is maintained by the CTUs. You can think of the HMC, that's just a readout of the CTs. And a CT has got really hard, right? Yeah. So Roxanne was saying if there's just one solid state, we're at the t equal zero, then all the gradients. But that's desired behavior because it forget it goes, is there evidence? Okay. Great. So let me just almost as it's clear now and I just want to reemphasize what he just said, which is we don't really care about or I mean, we're not trying to backpropagate from DLD HT, DH, t minus one. The thing that keeps the membrane is the cell state. So as long as we can backpropagate through c of t, then we have memory over time steps for the CMT is the key thing, the recurrent neural network. Other questions, Solving, vanishing and exploding gradients. We're solving both because the gradient can never get larger along this pathway. It can only get smaller. Oh, okay. Yeah. Yeah. That's a fair point. So here he is pointing out that there's a gradient that comes up through here that will also add there. In which case is possible for gradients to explode if at every single time step there is something being added. Longest pathway. Okay, Just in the interest of time because I want to at least start transformers since these days. But whenever you do natural language processing tasks, even though before we use recurrent neural networks. Beginning and I would say maybe it's like a 2016 or 2017, 2018, really replace these with transformers. So that's the LSTM. There's one more architecture called the GRU and I want to skip this. But basically in your project isn't LSTM or GRU. The GRU is basically like the LSTM, except instead of having four times the number of variables introduced, there's only three times, because basically they have the hidden state double up as a start state. And if you look at the equations of the GRU, just like the LSTM, there's gonna be a gradient highway on the h of t. So I'm just gonna leave that for you all if you're interested in looking at the details of the GRU. Alright, so we have training recurrent neural networks. If you're using a vanilla recurrent neural network, you should use gradient clipping. And if you're experiencing dash ingredients, you should use that past Daniel regularization that we've talked about. Also, you may consider using truncated backpropagation through time. That's where we say I'm only going to back propagate across 25 steps. I will never learn dependencies greater than 20 timesteps, but resect like gradients can vanish unexplored. If you're not constrained to use a vanilla RNN and that's almost all of you in most settings, you should just use an LSTM or a GRE. Alright? Any last questions about LSTMs? Grus? Alright, we will get to attention transformers and catch can see that these are new slides. Never talked to them before. I gave them this weekend. Hopefully they'll be clear, but he's asked they aren't. So our goal will be to understand how checks you can understand how attaching the tuberosity personally to understand what, how GPT works. Gpt stands for generative pretrain transformer, so that T is transformer. To understand that we need to understand transformers. Transformers, transformers, something called attention. So we're going to start off with trying to understand attention. And here are some really helpful resources that you can refer to in the future. If you want to. You want to see other people's takes on how they teach this material. I think that these are all great resources. So I wrote them from Jay LMR and then a few weeks ago onto a carpet, the Fc uploaded like a 1.5 to two hour video of him just implementing GPT from scratch. So if you want to, if you're someone like me who really understands things by, since anatomy, I recommend that video to you as well. Okay, I'm gonna keep the motivation of cats and BC relatively short because first vector I asked him and he had already played a bit, I'll say three examples. And these examples are homework questions. So this is the very first question that we asked you in homework number one. Let Q be a real orthogonal matrix so that q transpose and q should be a minus one are also orthogonal. I put this into chat and I'll tap good. And it generated lot of tech code compiled. This is compiled to answer. This is a correct answer. This is a question that is asked a lot in textbooks. And so I thought, Okay, well, you know, maybe it had memorized this from another textbook. I went to question number one where we asked you this probability question. And it says, we made up the numbers ourselves about the percentage of students in each of these natures. And we asked you, what is the conditional probability that a student is from Science, given that they liked the lecture and chats, you been outfitted this answer. And this answer is, in a numerical sense, incorrect because this is not the right number. But the only reason this is not the right number because it did this algebra wrong. So if it actually did the algebra correctly and got this number, 0.1, 443, but the actual number, it would have gotten the correct answer. All of the reasoning and logic that is used here was correct. And it just got this probability. Tom void. Putting this to check to see if this is the professor who teaches convex optimization here. And this is even more challenging problem where you have f being a convex function and you want to show that this set is convex. And this answer by chatting generated model is correct. All right, so what this is, this is gonna be a challenge for, for our cohorts in the future. But we know that chat TBT is able to do some pretty incredible things. Any questions? Alright, so let's go ahead and through out the ingredients. And so there's a few things I want to say, or just the first preliminary, which is we are going to be, you're going to be looking at chat GBG from the perspective of natural language processing. And that's really good. Things are working, right? Or detaching either tokens. Throughout the rest of this lecture, I'm going to be saying that we're going to be inputting the words into networks. And when I say words, what I really mean is something called a word embedding. And what you can think of this word embedding as being. And so the vector representation of a word, it actually captures semantic relationships between words. So these transformations are going to turn each word into a vector. And I have a nice illustration here from CSU 24 and a Stanford where basically they look at the word embeddings have several different words like sister, brother, niece, nephew, aunt, uncle. And what you can see is that this embedding all the different places, but the relationship of going from female to male occupies the same dimension, which is that the male in this embedding is below that female. So these word embeddings are themselves and entire topic that can take on lectures. And we're going to omit that for this lecture. But if you're interested in learning more, I recommend this course from Stanford. And first things to check out what the word to vec glove, which is an embedding techniques from Chris Manning who teaches this course at Stanford. And then sometimes words have different meanings, they really aren't. So there's a technique called Elmo that uses LSTMs to find these words. Alright, but from the rest of this lecture, I'm just going to say everything could award it to an upper, my favorite word that really means this word embedding any questions there. Alright, so we're gonna motivate attention first from this sequence to sequence problem. So we're going to consider the problem where our goal is to transform some input sequence to an output sequence. And here we want to use machine translation as an example. So let's say that I have in a sentence in English, I love watching UCLA basketball for you, but really that's the best bottom I'm getting Arizona the other day. And good luck on March Madness. If we had a sequence to sequence model working, then we would put this, in this case, translation to French. Day the basket, UCLA, all the quiet, the transformers. This problem was solved using recurrent neural networks. And the intuition for how this basalt was the following. We know that a recurrent neural networks hidden activity H is a succinct state represents a representation of the history of inputs. So if I were to pass in an entire sentence like this into an RNN and look at its state. After I put it, the entire sentence. This would be a representation of the history of the entire seconds. Let's write that down. So we start off with some initial state. We pass in i. And that updates the state at time one and love the state at time two, et cetera. So then this h of c is a single vector representation. Simply captures the information from this sentence. I love watching you say, all right, Any questions? Bear. Is it for the vanilla RNN? It can be any RNN, LSTM. Other questions. Just wondering, So we're doing sequence, a sequence number of tokens. Definitely happened stuff. We're doing machine translation. The number of output words might differ from the cupboard and provide for it. So how do we start? So I'll put the end token to note to say I'm done with this translation. So in addition to the tokens for words, there would also be a starting point. Any other questions here? Alright, so now It's gonna be something we call the context vector. After the activation of the artificial recurrent neural network neurons. And what I can then do is because this h of c contains all this information about the sentence. I could try to read out the translated sentence by using what is called an RNN decoder, where it gets that Kevin Spacey and basically unravels the information to produce the translate is seconds. So this is what that picture would look like. We would start off with some initial state and I would pass into this recurrent neural network, the HCI, that represents the entire sentence. And if it's trained well, then ideally it might have put the shell. Then I would take this and put it in as the input of the next time step, right? And then hopefully it would output a door, and then it would continue this autoregressive process until it outputted the translated sentence, the accident. Any questions there? Okay, So this is our machine translation used to be done prior to GBT and other transporters. When you look at this problem, you might imagine that there are actually quite a few limitations already, just intuitively. First is capacity. So in this example, we are reducing entire sequence, in this case is sentenced to one vector HCI. Alright? You can imagine that a sequence is getting longer, right? You might not be able to pay for ways to store all of the meaning of that sentence within a single vector of agency. So agency, because there's one consideration that I kind of glossed over here, but absolutely we know that the hidden state at time t, h of t is gonna be a function of h of t minus one and x of t. H of t actually most strongly represents the most recent work, x of t. And so this picture that I've drawn here is just different intuition. But actually what really happens is when you do a decode usually translate the sentence in reverse. So the first output would be pavilion. Then you could put the pavilion here and get Pauli. There is something which will be solved by transformers, which is, if I wanted to do this machine translation, any recurrent neural network-based architecture require sequential processing, sequential meaning sequential in time. To know EC2, I need to know s1, to know, really know, general to propagate there but require 0 of t. Any questions. Alright, so thinking about this question, which is, Why is it that when we do this translation, I'm summarizing everything into one vector, HSE. When when I do the encoder, I actually had a bunch of data. H1, H2, H3, H4 are corresponding to the different words. When I am translating the word regard there, e.g. right, that corresponds to watch it. So maybe if at this point in the sentence, instead of looking at HC, I looked at age three, which is the hidden state corresponding to when watching went into there. I'm going to do an even better job of translating. So the question then becomes why not look at every timestep? So instead of inputting h of c into this network, we would input H1, H2, H3, all of the hidden states at every single point in time. You can see this is the original synthesized translated, but it doesn't have very much other than shut doors. So that's why it can. Now easily, what that might look like is that when you have a recurrent neural network and now it receives inputs H1, H2. The note that no one ever actually implemented architecture because these are fully connected layers, linear layers. And you can tell me all the states are born with explosion of parameters. What might you think? Okay, maybe instead of going H1, H2 to HCI, I can be the average. Averaging is an operation that is going to be far from Austin. So instead, what we could do is instead of passing every single part tire hidden state, we can instead pass in particular inputs that we should have paid attention to. So again, there's gonna be one for I, for I love, H3 for I love watching. And when I translate radar dare, maybe I could have paid attention to this database through right. And then chador would have paid attention to the states of H1 and H2. If I do this and I'm teaching a particular point in time and using that to translate what that might do a lot better. So to do that, we're going to have to cover what attention is and what we're going to start with attention next lecture. Yeah. All right, everyone. All right, everyone, We're gonna get started. Here are our announcements. This is our third to last lecture. So today we're going to cover transformers and understand how it works. And then we're going to start variational autoencoders. Reminder that the projects are due March 20th, which is Monday of finals week. And we haven't created the grade scope submission portal yet. But just as a heads up, we're going to have you all submit just one project per team. So one person within your team will submit the project in grade scope and drove me an option to link your teammates. And so that's how we want you to submit the grade scope. The TAs are also going to be releasing a helpful coding video for the project this weekend. With respect to grading the project to provide clarity on performance. When we assign performance points, we're going to do that based off of your best-performing architecture. Even if your best-performing architecture happens to be a CNI. Any questions on any, just fix that. Alright. And then lastly, a friendly reminder that midterm regrade requests close tonight. So please get your midterm regrade request. I won't accept any request after that. The question is, for other architectures, for the project, we cite others architectures. So much If someone else's architecture, you should cite it, but you should do the implementation yourself. There questions. All right, so we will get back to where we left off last class, but we're starting to talk about how we understand check CBT. And we said that To do this, we first have to understand attention. These flies are new, so I had a bunch of typos in my prior slide, slide, I've updated some of them. One of the biggest ones is that I kept calling this H, or hidden state, which is just the artificial activity of neurons. In the updated slides in case you're confused by that. Alright, so just to recap, we were motivated by this machine translation question, which is to take sentence in English and translate it to French. And we motivated at first, this was done in the mid 2010s, which was their sequence to sequence models. Where we said, well, we know that recurrent neural network is able to encode mystery. So if I pass my entire sentence, I love watching UCLA basketball at 0.1000000000 to an RNN. Then his last day, which we'll call here hci, is one, Dr. activities. But adapter is that summarizes all of the words within the sentence. And so because HC contains history, we thought, well, we can go ahead and then use another recurrent neural networks. The traffic, the sentence I passing in this Hc into the recurrent neural network. And then the recurrent neural network was trained 12 and it can unravel the information within that sentence to translate this sentence into the trash. Alright, there's anyone have any questions on this idea? Just wanted. Some way ask this is just one layer of an RNA and not multiple layers. Is that correct? So you could have multiple stacked RNNs here, as we've done, is just a single aren't on there that you could organize their questions. The question is, what do we mean by stacking RNN layers? So what that means is that if I have some input X1 and it goes to an RNN that has hidden state H1, H2. This is an input x2. You can pass it to another RNN. I'll call it this RNN, hidden state S. So they're being S1 and S2. So you would have additional recurrent networks that are recurrent network is taking as input the hidden states, the activations of another recurrent neural membrane. The reason I'm saying it could do stay so much is because I've worked with dynamical systems all the time. And whenever I see an H in a temporal sequence, I always call it a business page. So if I stay hidden state it means artificial activation. But hopefully I will catch myself moving forward. All right. Other questions? Yeah. Correct? Yes. Yes. So here he's asked me to clarify. Hci is my context vector with summarizes my entire time because it just goes to an RNN. And then the information from it is basically unraveled through time. That's distinct from this. This is separate and it's just the architecture for if you want to stack multiple RNNs together to make them work. Telling me just call this NOT S1, which is a variable here. I'll call this like maybe the next states are called L1, L2, etc. Other questions here. This one, right? Yeah, So to be clear, in this example, there's gonna be one RNN for encoding. Actually let me just pull up the slides from last lecture where I had some dark here. There is going to be one RNN encoder that takes your sentence, encodes it into one vector. And then an RNN decoder that takes a vector and unravels the information to translate it something. Alright, so that's the setup that is going to motivate why we want attention. So we had talked last factor that elimination or there are several limitations with this setup. But the first is capacity. So basically I'm storing an entire sentence. I love working UCLA basketball sentence into a single vector. Hci, HCI, you know, maybe if I got 512 national factor, it can only store so much information. And so if we want to do longer sequences, then we'd have to make the order. But again, it's a lot, it's asking a lot out of this one vector H C to represent the entire sentence. One thing that we also know is that the context vector HC will move strongly represents the words at the end of the sentence. So HC most strongly represents pavilion and most weakly represents riboflavin chain because these inputs were far long ago. So actually when you do sequence to sequence this way, usually you're actually decoding the sentence in reverse because pavilion will be the most strongly represented things. So usually this will output 2 billion. Then Pauli, then at basketball of the seconds and recharges essentially. Alright? And then any recurrent neural network that wants to do this type of translation is also going to take order the number of words in the second space to do this computation because to get the next word to translate, have to have translated the first word. So to pass through this entire sequence, but require on the order of the Tiamat the time. So these are all limitations that are going to be addressed by transformers. Any questions here? The question is, if you miss translate the first word, would that be bad? Will that lead to crushing wherever it is, basically a bad translation because that's why you also usually like here, we're translating and reverse the most recent words. We can translate more faithfully and that'll hopefully get us back to the original receptors better. Alright, So we ended last lecture by saying, well, maybe instead of casting for this transformation just the last HCI, I might pass it H1, H2, all of the hidden states in between. So instead of just passing one Hc into this RNN decoder, I would give it all the hidden states. And the naive way to have all of our hidden states over time. And they are input into every single step of the recurrent neural network. But this we know is going to be incredibly expensive than parameters. This because these are fully connected layers, they're going to be occurring. So instead of doing this, well, we know that when we're translating each word, right? I really don't need the hidden state at every single point in time. If I'm translating regard, regard day, which is walking, right, I really just want to know what the hidden state h, three words, given that each one is, I issue is love, et cetera. And then to translate our door, right, Then maybe I just want to look at H1 and H2. Then of course, if I want to translate a basket here as basketball, I would have wanted to look at this part of the input. So we're going to be able to do this through an operation called attention. The bass part of the transformer. And we'll start off by asking, what is the tension. So I'm going to show you a picture. And if I asked you to caption this picture to see what's happening in this picture. You would say that this is a bird flying over water. When you made this determination is just how humans operate. Basically taking the whole image, their eyes would have focused on particular parts of D and H. So when I first showed you just manage your eyes, almost certainly focused on the bird. And I might ask you to capture the image. Looked around to see, okay, what is the bird flying over? This is very common or fact that our eyes are going to go from different parts of the images are called Chicago, like the Hoover I studied rapidly between different parts of the image, become very attending to different parts of the image at different parts and time. Alright, so what we can do is be constrained so that previous architecture translate this sentence in English. Right? So that done wrong, separate off the ground. I'm always asking a great question, which is what I just showed you, this image caption, the image. At the caption to the image, you would have to have observed the bird and then also observed what is flying over. Tomboy, ask the question instead of saying caption to the image, I couldn't give you the task to say what is the color of the bird. I've asked you, what is the color of the bird who are just paying attention to the bird? So depending on what the task is, what I'm asking you, you want to attend to different things. And that will be incorporated into training. Task is fundamental to understand what to pay attention to. Other questions. Alright, so this is from this paper by shoe and colleagues called show. It's an Intel Neural image caption generation with visual attention. And so what they're showing in this figure right here is they were able to successfully get a neural network to caption this, saying, a bird flying over a body of water using what we're going to describe mathematically work until the next slide is using attention. But basically, when you look at these images, tell you that when it's translating and outputting this word bird, the white areas are where the neural network is attempting to. So when it's translating bird, the neural network is looking at this part of the image of the bird and when it's flying, and then what it's translating, flying over a body of water. When it's translating body of water, It's basically looking everywhere around the bird. So that's the intuition you should have detected, which is that every single time step, whatever I'm trying to output, in this case water. They said I should attend to the relevant parts of that image. Any questions? First? Yes. Yeah. Perfect. Tom ways pointing out that in this caption, there is the word letter a here and here. In this case is modifying burden. So it's paying attention to the bird. Virus here is referring to a body of water. And so it's paying attention to the water around the bird. So that makes intuitive sense. Awesome. Was there another question? The question is, will the model? So in this case, the model has access to an image and then it translates or a captions it. But in other natural language processing, taskbar, if you want to translate a sentence, The question is, will it models have access to all the words or only like the preceding words. So we'll talk about this later on when we get to the transformer block. But if we're doing the sentence translation, and I've translated as John Doerr so far. When we do make our day, in terms of what's being translated, make our day will have access to the Zhai door, but it will also have access to the entire input sectors. So depending on what is the input and the output, what is the task? Any other questions here? Does visual attention come after the paper? All you need is attention. Actually, it does not. All you do is attention, is all you need is a tension. Introduces the transformer architecture that uses attention layers. And I believe that was 2017 or 2018, or is this paper in 2015, but there were earlier forms of it. Before the paper. And the student is referring to this paper which I recommend you read. This is the thing that talks about the transformer architecture that will be the base of GPT and birth and other language, some articles. Alright, so here are a few more examples of attention from this taper. So here the captioning, a woman is throwing a frisbee in a park. And the underlying word is what they're showing in here, what the visual attention is paying attention to secure a frisbee. The network is rightly paying attention to this region which contains a Frisbee, Hey everyone, their cats and dogs and paid attention to the dog stop sign. Stop sign or central visual attention is doing something very reasonable when find my caption these sentences. And the other question is, do they take a bounding box approach when doing image captioning with attention? I don't know the details off the top of my head, but I would guess no. Anyone knows this paper in more detail and can confirm or deny. Okay. So we can look at that paper just to be sure. Alright, attention, but also occur in the context of natural language processing. And that's what we're going to continue on with. These. This is a network that is taking Yelp reviews and essentially reading if they were positive or negative. And the red is where the network was paying attention to. The grocer was paying attention to you to determine if it was a positive or negative reviews. So here, I really enjoyed this restaurant, or sorry, not a restaurant. It's the hairstylists, the highlights, Perfect, Fantastic. That leads it to make a positive recommendation. And it's paying attention to these words or these phrases in particular. Same thing for this restaurant. So we are going to focus on Natural Language Processing going forward. And what we wanna do is we want to try to develop what the mathematical underpinnings of paying attention. That is, basically finding the things in the input that I want to extract so that I could make or I could do my task. Well, what that looks like mathematically. So, how do we implement attention? We're going to focus on natural language processing. Natural language processing. We're going to care about synthesis of this animal. This sentence is this. The animal didn't cross the street because it was too tired. And if attention is working well, then we come to a bird like it. You want to see what is it related to? What to pay attention to, to know what it is. It has the strongest attention for the word animal. It also pays a bit of attention to street. In this case. The animal didn't cross the street because it was too tired, right? We kind of know from the contexts that it should refer to animal and artists tree because it was too tired. But you can see that it does give some attention to the words there because in other settings it could be a bit. Alright. Any questions? Okay, so attention is going to come down to three quantities that we're going to need to compute from Martin inputs. And those quantities are called queries, the keys and the values. And these languages inspired from databases. So let's first look at what happens in a database. I'm not that familiar with databases. So sometimes conceptualize this as like a Python dictionary. In a Python dictionary or in a database, we know that there are particular keys. In this database. We may have n keys, and these keys are associated with different unique values, or they don't need to be associated with different values. And let's say I wanted to get the value associated with key three. What I would do is have a query, the number. The query would ask for Q3. Then I would look up in the database what cheats really was. And then I would return value to three. Alright, so from my clarity, I found that I want to know the key three and then the output of the database is a value associated with key questions better. Okay, so that's the basic gist of what these queries, keys, and values our database implemented. What deep learning, deep learning called quote unquote hard attention. Attention means that when we put in the query, we're only gonna get one value. So in this database, when I queried for T3, I got out one value, which is value three. Alright? But we know that when we do deep learning, we aren't currently database. We're wanting to know what to attend to. And sometimes you tend to do is almost surely not gonna be a single value in the input, but there's going to be. Some different dots, some different instances in the input. E.g. like this image where it should attend primarily to animal, but maybe I should also attended British tree because it could be some other contexts as well. Alright, so this leads us to this idea of staying instead of having database, I've put just one value. I'm going to output basically a linear combination of the values in the following way. This is going to be called softer section. So this is no longer database, but I'm still going to have queries, keys, and values. What I'm going to do now is because we're no longer that database sitting. I'm gonna take my query and I'm going to compare how similar it is to key one, key two, all the way down to maybe query is most similar to T3 and that I would assign it a similarity score. These are like the softbox with this force before the softmax layer in a neural network. So maybe query has a similarity score eight with k3, and then maybe after that with Q1. And it has a similar similarity score of five. Maybe with Q4 it is at least similar. And so there's some merit. Z-score is minus one. Alright. I'll then do is I'll translate these scores into a softmax distribution. So I'm just going to apply the softmax function to the scores. And we know that the highest score is going to have the highest probability that these we're going to sum to one. So after I do this operation, then the output of this soft attention is gonna be the softmax for T1 times value one plus the softmax for Q2 times W2, et cetera, et cetera. So the overall value produced for this query is going to be the linear combination of values here weighted by their softmax scores. Any questions here? The question is, can I explain again how the similarity scores are calculated? I actually haven't explained that yet. So I just give you the intuition of how the queries, keys and values within here. And then we'll come to how they're calculated in the next slides. On his question is, are the values advocate, as are the vectors that can be added together, meaning that vectors of the same dimensionality, they are additive. So all of these values are going to be factors that had the same exact size. So they might be on 100 dimensional vectors. And so each of these will be a different 100 dimensional vector, which I can add after out and multiply it by the salt score. Didn't answer the question. Sorry, can you repeat the question? In terms of word vectors or the attitude for the words. And this is a slide from last lecture. Remember that you're going to be B. Skip this, but we're going to translate each word into a vector embedding. Vector embedding always be 50,000 dimensional factors. And so all of the words are going to be translated into 50,000 dimensional vectors. I couldn't get out of the question. Others. The question is, what might be an example of a query? I'll get to this in the next slide, but the queries, keys, and values are all vectors. We'll talk about their sizes. But here I just watched you to understand how the query keys and values relate and how the queries and keys essentially gives you the weights based on the similarity scores. The waves are softmax distribution and they wait the values to get guilty. If we go back into the patients, always have a relationship and I buy exactly this thing without getting one. I mentioned, I recall this from others. In addition to punch is basically saying that this object in December, democracy position to the next one also, looking at those things that always happens. They can be abstract concepts of Thomas question is, will the attentions always be intuitive? But it is for the pictures I showed you earlier, and it is part of a sentence where it refers to animal and the answer is no. So we'll see later on that we're going to have any attention layers in parallel and we're also going to stack them. So this is like an intuitive explanation for where attention came from and what we hope it does. But oftentimes the network will be attending to the very interesting things and services, reducing your lawful and services better. But oftentimes it will be non-intuitive what it's attempting to. Alright, I mentioned this last lecture. This is the first time we're teaching this. So we appreciate any feedback on this type of please ask me any questions because I may not have optimized the slides or any other components here. Can you raise your hand if you understand queries, keys, values. Great. Alright, we're gonna move on then to how we implement this. So two, I'm first calculate the queries, keys and values. And actually I'm going to do a future slide here first. What we do is we look at a word and then put word. And the queries, keys and values are simply just factors that are generated as linear layers applied to the word. And so the queries, the keys and the values for the i'th word, maybe the word was it, right? That we were looking at in the prior example, would generate a query. This would be an R. We're going to use this variable to denote the dimensions of the queries and the keys. And then there's also gonna be a value, and this is gonna be in dimension d v, right? So these are all just factors that are generated by taking the word and passing it through a linear layer. Alright, so that any given that we know that the queries, keys and values are vectors. Question is, how do we measure the similarity score? I meant to have this wide plank and not have the answer. But you can see the x over here, which is if we want to measure the similarity of a very key or of two vectors in general. We know that one way to do that is to just take a dot product of them. So let's say that I had the query for the word it. I'm, if I'm looking at this sentence, I would have computed the query for the word. And then there would have been a key for all of these other values. So this would be k1, k2, k3. To compute the similarity scores, I would take the dot products of cube with all of the keys for all these other words. And that would tell me the similarity scores that I then turn it into a softmax probability distribution. So the way that we compute similarity, it's a dot product of vectors. There's one more detail. We will normalize the similarity scores by the square root of the dimension of the vector KI or Q. Remember, Q and K are vectors that had d k dimensions and we're going to normalize by the square root of DKA, this top part. Yeah, so his question is, are these embeddings in terms of translating the words into vectors. And those vectors have similarities based off of what those words are. So this is distinct from that. The embeddings, it's really referred, referred to instead techniques from natural language processing. So things like Word2vec, glove, and Elmo are those embedding techniques. Alright, One might ask, why do we write? Right? That's what a student is asking. Why are we adding these additional linear layers? Can we just directly measure the similarities by using the actual word embeddings and just took them their dot products. That's one form of detention. But by taking these linear layers, you can find other projections of the inputs that will give you additional modeling capacity. Said you could do attention without these linear layers, but it would be limited in this competition of capacity. Yes. Well, students are asking, have I gotten the indexes wrong? In terms of should it be like the IQ with the JFK? So we're gonna have to take the dot product between everything. But right now I just want to picture that we have one query for a word. And we wanted to compute a similarity, similarity to the keys every single other words. So I here is going to go from I equals one to n, where n is number of vertex. The question is, what do you, what do we have the same word, but with different meanings? The same verb and in different contexts can be something else. Would they have very high similarity scores? That's a great question. I'm going to bring up this thing one more time. So in things like Word2vec as well as glove, I believe they would have high similarity scores. Charlotte's exactly right. That depending on how the word is used, it can be a different thing. Like in the US means like footwear and boots. And the UK means like a trunk. And so could also even be location dependent. And so there are some embeddings like Elmo that take into account where that word appears in these contexts. And we'll find a different embedding to the same word depending on its context. And I'll note here that uses a bidirectional LSTM to do that. So it looks at words that come after it and where if the company were to give them that will be handed, that will be handled in the NLP side of things. Students asking where the normalization term comes from. Essentially, I'll go through them and wants to, but I think that there were question company. The question is what exactly is being embedded into the queries and the keys? So the queries and keys come from the burden Betty, this word embedding is that Word2vec, glove or MO embedding, where it translates for word into a vector. And then after that, those just go through a linear layer and that gives you your queries, keys and values. The question is, how are these linear mappings important in the context of attention? Or how are these embeddings important? Are you asking like, how do we make sure we learned by getting meaningful veneer? Okay, yeah. So these linear layers are gonna be parameters of your network that will be optimized for the network will learn to set these linear weights based off of what to attend to it. Yeah, So basically, we will have a loss function and will backpropagate to these linear layers. And the linear layers will change to make the loss of small as possible. And if a task is like cancellation, then you better be paying attention to the correct words to make them off as small as possible. So how the queries, keys, and values will ultimately come out from these linear layers is through optimization. Question is, how do we assign values to the keys? Same answer, which is through optimization. So, never handcrafts what should be the keys and what should be the values. These will be set by stochastic gradient descent. So to make the loss of power as possible, it'll find the features, the values correspond to the keys. So the network will learn attention. Okay, let me get back to this other student's question, which was, how does this square root of dk come about? So the way we do that is we're gonna make some assumptions. The first is that the queries are independent of the keys. So if the queries are independent the keys and further, the expected value of each element of the queries and each element of the keys. So I hear is indexing. Let me use a different, let me use the letter j here. J indexes elements of the vector. I'm doing this because I've already used I here to index words. Expected value of Q, j equals expected value of k, j equals zero and their variances equal to one. Then we compute Q transpose K similarity score, right? This is going to be a sum from j equals one to decay of q, j and k j. Because they're independent, right? Then. The variance of q transpose K is going to be equal to the variance of q j times the variance of k j. For j equals one to d k. This value follows as you can see, that the expected value is just gonna be zero because expected value of Q j times expected value of k j, when they're both zero is just gonna be zero. But this product of variances, what you call one. And the variance is going to grow as the nationality of the keys and values squared. So the variance of this spring is going to equal dk to normalize the variance of this dot product. So that's the scariest tastes. One, even when the vectors get barge divided by the square root of d k. Because we know that. One. Great. Daniel's question is, here, we're constraining the Q transpose K i's to be have mean zero and variance one. But Daniel saying we pass this through the softmax. So we want to be the case that they'll get normalized away. The answer to that is that if I added, so we know what the softmax, if we were to add a constant to all of these values. So this became 1008, 1003, 1005. He saw map, this would be the same. But if instead of adding, if we multiply them by something, which is what this variance scaling would do in the scale of the softmaxes would be different. So at this softmax is we're smaller. Sorry that this course we're smaller than a softmaxes would be closer to uniform. Whereas at the scores were larger the Softmax, it's looking closer to a one part and that's what we want to address. Right? It's keeping them at a similar scale so that you'll get closer to a one-hot just because you had just because you've had more than entrants figured. Great question. Other questions. Okay. Then you might ask why not use a neural network to compute similarity? Well, the dot-product does a really good job already and you couldn't do this. But it will require more parameters and more computation. Because the dot product already does a good job of measuring similarity. There. Just to make sure that we're clear. Here's an example again of us doing machine translation. This is a different sentence. This is at the input sentence. Economic growth has slowed down in recent years. This is the output. So attention by computing the queries for every single word in the output. In this case for machine translation, that would be for this work, economic Q3, right? And I would compute the dot product with the keys of every single burden on the employer. So I would want to compute Q3 transpose Q1, Q3 transpose k2, all the way up to Q3 transpose K has been commonly is referring to the translation for economic, I would hope that Q3 times Q3 transpose K1 is very large. But economic has little relationship to growth. And so hopefully this would be small. In fact, hopefully the dot products with all of the other keys would be small. In that case, then the value that would be assigned to the word economy would be primarily the value associated with K1, which is called V1. Okay? And then you would do this for every single part of the output. So there would also be no acute two. For this bird. I'm gonna get this croissants. And you would calculate Q2 transpose Q1, Q2 transpose K, two, etcetera. And compute their attention. Alright? I'm sorry, can you say that again? Right? The customer has a query related to each output word. How do we know how many words will have? So we get this transformer architecture lecture. We're going to, when we generate the output, essentially generate the output one word at a time sequence. And we're going to keep translating until we decode some Ns took them. So there's going to be an antigen that tells us to stop translating the decoding parties until sequential for machine translation. The question is, what's the example where we want Q transpose times k to be large for multiple values. Yeah, so in this example, which we call cross attention because the output and the input or difference. In machine translation. Oftentimes we're translating C over. So there's some, some words may have multiple values to attend to, but it comes up that work there and something called Self-Attention and soft the tension, the input and the output are both the same sentence. And I want to know what word in that sentence relate to other business happens. So if I look at this word, slowed, and this will be the fourth word. So it'll query will be four. Then, what has slowed? Well, economic growth has slowed. So we don't want to just pay attention to economic because it can be like, you know, economic depression has slowed and that would be a good thing. They're saying economic growth has slowed and that's a bad thing. So we would want in this case that for transpose K1 is and for transposed K2, both should be large. And there will be attending to multiple words. This question asks questions at every step. You get an answer that's appropriate. I'm not sure I follow your question entirely, Tom, like Can you please repeat that question? E.g. less than what I said. Yeah. You've answered as yes. It's always saying these ideas have crazy Keyes. Kind of seems similar to question answering and natural language processing. Like you can ask the question what has slowed? Paying attention to the word slowed and you will want to know that the answer is economic growth. That's correct. I mean, that's probably why the questions are called carries, right? It's like asking a database like I want the value or query right here. And the way that I find the values, I look up the key. It is correct that this is essentially saying, you know, what is Q for related to, and it should be related to K1 and K2 here. But all of this will be done using optimization, using the same tools as cast pretty descendant back propagation that we've already learned. All right, Question back there. Right? So I, I believe the question is and cross attention, like the queries are coming from, you know, French and the keys and the values are coming from English. So how do we calculate this? We'll get to that when we get through the transformer, sorry. So we'll say the architecture and how we get the keys and values for the input and the queries from the output or research question. The question is, are the dimensions for the quiz keys and values of the hyperparameters? Yes. Sorry. I explained how to generate a lot. Oh, yeah. So the students question is, can I explain how it is that degenerate the end token which stops the translation. That comes just from supervision. So basically in my training set, I would have had, if this was actually a dataset example, I would have a start token here. I would have an end token here. That's the starting end of my input. And I would have a start token here for my output and an end token here. And the way that these are training is autoregressive, same thing. So how we saw the RNN generated Shakespeare, I would take, let's call it. Five words. So it'd be economic growth has slowed, that's five words. And then it would want to predict. We want to predict. It would have, sorry, no, this is a bit different. So this is not a regressive. Well, we would do is we would take this entire sentence of economic growth has slowed down in recent years and then want to translate it correctly as this sentence over here. But in the training process, to get it correct, it would have had to. And it would have to predict the end token here. If it wasn't predicting an n tokens and it would have a large loss because then we'd get that token incorrect. So then stochastic gradient descent would change the weights for the tech and social determinants. Question is, what would be an example of a value? Like Do you want me D2. So the word economic, Would it be a vector? It would go through linear layer to output k1, and it would also go to another linear layer to output D1. The interpretation of this is difficult because it would be the same as what are the activations with an enrollment? Activation features to think of as a values here, but they're just coming from a linear layer applied to the embedding to work. Economically. It doesn't have a correspondence to an output or a controller? That's correct. It's like an intermediate feature and mineral and there'll be fully-connected with birth weight connected layers that will translate that and say, thank you all. I'm sorry, say that again. Right. Yeah. So the question is, how do we handle this cross attention? I need to attend to words in the output to the input. But there's also a relationship between birth and the input that probably require self attention and she also be self attention in the output. This transformer block wall into self-attention for both the outputs, the inputs as well as cost and tension between that. So I'll wait for architecture. So the question is, you may translate into English sentences, into two French sentences or another French sentence. Yeah. So the way that you would have a network that is that you would give it two sentences and having penetrate as the output two sentences. So check GBT. The inputs are 4,000 tokens, which translates to about 3,000 words. So every single example that chat TBT will see will have 3,000 words. So it's a lot more than just catch. You can see just looking at a lot of taxes. I put the input and the output. Okay, just for the interests of time, I'm going to keep moving on. Actually, I just saw the time and I see that. So let's go ahead and take a five-minute break and then we'll come back. So I think in the slideshow it's like a division where the body, the score by square root of n. The dimension. The reason because like the NeuroNode that we give witness possibly use, we just like normal left side simpler discourse outputs, softmax, yes. We're doing the division by square root of DT. It has to be one. It's less close to a one-hot vector. Or we don't want it to be dependent on the size and hope each hyperparameter and copy the two secretaries. Right? I see. I need to think more about neural networks. The message is always the number of classes, right? So that will be consistent across many different. Oh, I see. I see. Okay, cool. Thank you. Yeah. Like on one slide, we've seen as the key and value are just the opposite. What if I click Generate? Yeah, it's just a venue or later on flights in the video there is optimized. So it wouldn't mean also goes to monetize. What do you mean by context? Let's say four priority of, let's see, we have a court of law which asks you to write the query. One may attend to some sort of embedding. Like, how do you know what is actually already want? So which one do you like? Yeah. Yeah. This will be taken care of by something later on for the positional embedding. Oh, say can you just go to the slides? Yeah. Oh, yeah. So let's say that we have the value of the input embedding of the economic and we don't see with somebody in your matrices to go to k1 and you want. But let's say just here behind the history, how do you reach the linear layer? I plan on being applied on the word embedding for the economy. But we don't have it right input. And this is what we want to ask you at all. Because all let's say on the training we might have right? Corresponding. Right? Yeah, so that's a good point. So if the task is to translate this sentence, you're right, we don't want to meet yet. To get you to want to make we would have had to do you would only have one, as well as all of these pieces. So it would have to generate economy. But that would be from comparing these queries like that. How do we could say we don't want anything? They put all this. We can easily get the V1, T1 right at the start. It starts at. So the Q2 will be some things that aren't so good. Oh, okay. Yes. Okay. Alright. Can we make an assumption that they aren't especially important? Yeah, they're not independent set is just an approximation. But it's not like we're just thinking of something to say. What would be a reasonable thing to normalize. The only assumptions. It's more like a role of top side again, perfectly normalize. Okay, that's alright everyone. Let's get back to material. Lots of questions which are good. I'm going to take this into this vector for next year. I want to try another question from the graded. All right? I'm sorry. Right. Yeah. So for the next part is we're going to focus on self attention. Which is this idea of taking a sentence and seeing what word they're burst in this sentence I should pay attention to, like slow to pay attention to economic growth. Alright? So we mentioned that there are these linear layers. I'll take your word and change it into queries, keys, and values. The way that these will happen is with matrices. So let me first write it how you're going to put the notation because that's how the paper did it. So there will be some matrix w q, that will be my linear layer for the queries. Process XII. Let's say that XI was in r. So n is the dimension of my word embeddings. Then this would eat walk QI. Very simple. And therefore WQ would be a matrix that is DK, the dimensions of the queries by right? Similarly, KI would equal a W k times x. I. Tell my same time these subscripts instead of superscript, I'm going to keep them as superscripts since I'm going to show some screenshots later on from a different flag that I use and I superscripts. So and then there would also be a VI equals w b times xy. All right? Okay. That's hopefully intuitive to you all. I'm going to show these slides from j, our Mars blog on attention because I think he does a really nice job of visualizing these. In these cases, everything is flipped. So instead of over two, which is basically sand column vectors, these are going to be row vectors times matrices. So in this case, QI in purple is going to equal x I times WQ. And here the dimensions there's gonna be in our TK and in this example of decay is going to equal three X I is going to be the dimension of our input word embedding, that was our m. In this case, n is equal to four. Let me do this in green because inputs here in green, xy is going to be an RN. N equals four. And then therefore WQ has to be a three by four matrix. Okay? So if I have two input words, X1 and X2, I write multiply them by this matrix W Q, that is three by 0. Sorry, I did this wrong. This is four by 34 by three. And my four d vector, tons of four by three matrix will give me a 3D vector. So that's WQ, the linear layer that is multiplied by X1 and X2. To get nice curry when I'm alright, and then I'll have a different w v, w k for the keys and a WB for the lessons there. Notice something. We have to take the dot product of the queries and keys. So the queries and keys always have to have the same dimension, DK. The values can technically have a different dimension. But in practice, the dimension of the values is almost always going to equal the dimensions of the query is introduced and attaching Bt cotton stuff. Right? So now we're going to write this for all the inputs at once, the same formulation. So what we're gonna do is we're going to define big matrices, Q, K, and V. And these stores my queries, my keys, and my values for my n input tokens. So n here is equal to the number of input tokens. Following the convention of the last slide. Because in the actual attention paper, this is how they've already paid out. One is the query for the first word or the first token. And this being one, remember was equal to X1 times WQ, Q2 was equal to X1, X2 times WQ. And then Q n is equal to w to XN times w cubed. Same thing for the keys and the values. So then the operations that we talked about, where we take the query and the keys, we don't connect them together. And then we pass them through a softmax. And then we multiply the output by the value. This is done for all of the inputs at the same time through this operation, Q k transpose times v. Alright? When you do Q k transpose, then what you'll find is that this matrix Q k transpose. Actually, this will be a matrix softmax of Q k transpose divided by square root of Vk, each row of this matrix. So this thing here is going to be equal to this matrix here. Basically the first row is going to be the softmax distribution for query number of one. And so Q k transpose here is gonna be an n by k matrix, that's for q times n by n matrix, that's for k transpose. So the overall matrix is n by n. And what this n-by-n matrix looks like is in each row, we're going to have here softmaxes for query one compared to all of the keys. So this is what softmaxes for query one compared to all and keys, right? And so that's why they're going to be n columns here. Then they're going to be n rows because I'd had any queries, any questions there. The question is, this here is n by n. And that's only true in self-attention because in cross attention we could have a different number of keys and queries. So this might be a rectangular matrix and that's, that's correct. So then after this, we take this purple matrix where every single row sums to one and do the softmax is for each query. And we multiply it by this V matrix, right? And this D matrix, it looks like d v1, v2, all the way down to the n. And so when I multiply these two matrices together, I'm gonna get a matrix at the output. And the first row is going to be the softmax for query one and key v1 times w1 plus the softmax for query one and key two times value to the softmax of query one and Q three times W3, etc. So this value, this row here, is exactly going to be that output that I wanted, which is a sum from I equals one to n of the softmax I times value on this quantity here is what I had written in this slide over here, where the value for query one is going to be some vector that's a linear combination of all of these values that are weighted by their softmaxes. Okay? So if you go through all of these operations, you will find that every single row in this matrix will indeed be that output value for query one, the output value for query two, etc. Any questions there might be something that you might have to return to it just to make sure that you can convince yourself that we're doing the truck operation. It's not about why. Why did Jesus a good anchor squares to probably, to understand why he is probably the next incarnation. Right? Tom Weiss question is, how should we intuitively think of these values? So the way that I can get the values as the values are the features for each, for each word. So the word economics will have some features associated with, associated with it. And that feature will be analogous to the features in our neural networks. So the values are the features. And all this then tells me is attention tells me, how do I take a linear combination of those features to make sure that when I translate the word economy, I wanted to allocate attention to economic. So I wanted to value the feature for economic rather than the value or future for slow. Other questions to take. Question is, should this be the transpose on the right? It should be V. But if you, so, so, yeah, this is one of those things where you don't differ. Even when I wrote these slides, I wrote this out to convince myself it's true. It isn't true. I can just play around a bit. Sorry. You said again. Does this off expatriates property of what matrix? To be symmetric? No, it is not symmetric. The softmax matrix, the only property it has is that the sum of the rows are equal to warn, because every single row is a softmax distribution is not symmetric. Alright? So the way this looks like in picture, computing attention for everything at once. It's, we'll get our big matrices Q, K, and V, just like we had on our prior slide. This is query one. This is query to, this is value one, this is value to you and something analogous for cues as well. And then when we compute attention, we're doing this operation, which we already unpacked. So I just have to sit there for you if you wanted to see another visualization of it. And then it gives me an output z. And the z's are going to be the features which are going to be n, the number of tokens by dv, which is my feature size. So this is the feature size, the size of my balance. I'm going to have n features for however many employers I had. Remember here we only had. So that's why there are only two rows. Okay, Any questions there? Alright, so this equation is one attention layer. If you understand this, then the rest of transformers will hopefully make sense because it disappears, going to cascade many of these attention layers. So this is a single attention layer. The first modification that we make in a transformer is just like having convolutional neural networks. We had a filter. The filter extracts that feature, but we're not content to just have one feature, right? You usually have, you know, 512 filters are 256 filters. So you can think of this as outputting a single attention feature. And just like with layers, we have many filters, will also watch several parallel attention layers so that we can maybe attend to different things in my inputs. You can think that this might become even more relevant when say you have checked GBT, have 3,000 working, 43,000 work input. Your current word doesn't just depend on things in your past sentence. That will also depend on things written paragraphs or several sentences ago. And so having multiple attention layers allows you to first of all, where to attend to multiple things in the past in parallel. And so the act of concatenating several parallel attention layers is called multi-headed attention. Any questions? This is what multi-headed attention looks like. We already described. Attention had a single attention head, which is that we would have queries, keys, and values. Now, we would just have, instead of just one WQ, WA, WB, we'd have two sets of events. So this is w0w, sir, okay, w0d, and this is another set of attention parameters, w1. Yeah. The question is do we Ben had different queries, keys, and values? Yes, because these w's are different. So it's going to learn to optimize different queries depending on what your attention to attention heads looks like and don't pick up two sets of values. Yes. Sorry, Can you say again? Great. So students realizing that if we have two attention heads, if the output we call the z, we would now have two n times two. We would have two of these Z's. And so we would have doubled the number of z's. And so what we do to combine these down to be the same feature sizes. I'm going to index each attention layer with high. And so if we had eight multi-head attention with eight attention layers, then I would have output z1, z2. And each of these Z i's Z1, Z2 to z. These are going to be matrices that are n by d v. The way that I then concatenate them, or the way that I then get this down to a single z is that I'm going to concatenate them. So I'm gonna write this as z1, z2 to z. And so this is going to be a dimension n by eight times dV. I'm going to multiply it by a linear layer w. W out will be a matrix that is h d v by d v. And this is going to give me a final Z. And this Z is the same size as my original values and by dy. And that's just drawn here in this picture where my multi-headed attention is going to compute all of these different outputs. I'm gonna concatenate them in different columns multiplied by w zero and I'll give me a Z at my original dimensionality. Great. The question is, why is it that we want multiple attention pass rather than making decay the dimensions of the keys and values, or DD, dimensions of the output larger. Well, remember that no matter the size of D care, GG only gives you one similarity score. It's only going to give you one softmax distribution. So you're still limited in what you can attend to when you have these parallel paths. The softmax distribution can be, come up very fast and attentive different parts of the sentences. As opposed to if you have just one path for the large decane, you're flipping out what we had one attention distribution. Yes. The answer is yes. So the question is, when you do multi-headed attention, are you just taking the same attention? Layers and just replicating at a time? So the answer is yes, but each of the parameters and those eight attention layers are going to be different. Because they're gonna be County optimization. Just like how the different filters and the convolutional layer will pick out different features also. Alright. You mentioned, right? Yeah, Tom way bachelor's also, if dk becomes quite large, notions of similarity become less intuitive because the larger the volume that's occupied grows exponentially with the dimension of the vector. So if you take those very large vector space becomes a very sparse and distances similarities breakdown because of the curse of dimensionality. Alright? Can people raise your hand if you have followed us up to be detected? Okay. Any questions? Sorry. The question is if I can elaborate on yeah. So I didn't generate just figured this one is from Jay, our Mars plug. I actually don't know what the side note means. I, since I would need the context of the blog, but I think it's probably talking about structurally with the output of being killed her right below. I'm not sure. Yeah, so sorry, I'll have to refer you to the bar. Alright. Okay, so now let's come back to just the centromere is talking about that, how attention is used with the Recurrent Neural number for translation. This is also from JLL Mars blog. He has really great illustrations. That's why I put them in here. But basically what you can do is you can take each hidden state, right? Let's say I want to translate, sorry, obvious and hidden state. Let's say I want to translate the artificial activations at a time. Let's say I want to make the transformation for the first time step. What I do is I can turn this one into a query. I can turn all of these H 1s, two keys, and I can calculate the dot product, the similarity between this query and all of these keys. Compute my softmax. And then when they are least some bees. And that will tell me which of these hidden states, sorry, Which of these artificial activations to pay attention to? And that's how for a single point in time, my RNN can attend to one of these inputs using the same query key, value construction. Not going to pay too much time on this, since we wanted to then get to the transformer. So when we then look at what the architecture of this, when we wanna do this translation for I love watching UCLA basketball sentence, we would figure out which of the artificial activations H1 to H2 we pay attention to at time one, that's a one. And then we'll hopefully output. And we took the Azure as the input to the next time-step. See what we pay attention to at time step two, and then translate a door, etc. I want you to notice something, which is when I compute a one for this RNN, what I'm doing is I'm making a query one out of this activation at time one, S1. And then computing a similarity versus all of these artificial activations that came from the encoder. So here, k1 is going to equal H1. K2 is going to equal H2 all the way up to k, k equals h t. So when I compute this attention, I need to take the dot product of query one with k1 all the way to Katie. And what you'll notice is that attention one, this value here is a function of binding entire history. I had to have known the entire input sequence. I love watching UCLA basketball at 0.2 billion. This is kind of weird now because this is just repeating what I said. It's attention input is a function of every single input where this is weird because the way that we motivated the recurrent neural network is I want you to watch the entire history into a recurrent neural network. Recurrent neural network we wanted a succinct way to represent history. But if I'm already looking at the entire input, I don't need recurrence anymore because I'm passing on all of my history into the network. What's that? Because I have all of the history in A1, all the history and A2, I should be able to do this translation without these errors. Again, because every single input A1, A2, already contain all historical inputs x 1x2xt. Alright? So what I can actually do is I can remove these on relationships between the adjacent states. Instead. This now looks like a feedforward neural network where the input is the entire history. Any questions there? Alright, so in this case, because we're doing machine translation and I need to know that I translated to then get a door to then get regard day. In this instance, I would still have to calculate these sequentially, right? And so after kicking my current output and then making it the input of the next time step. This is called an autoregressive calculation. So if we're doing an autoregressive calculation, I still have to calculate these in sequence. But if I didn't have this autoregressive components where I didn't take the output from my prior time-stepping and put it up next time. So my network would look like this. And what you notice is that without not a regressive component, this computation does not rely at all on any of the others, A3, competing this block here doesn't require A2, A1 or something. So I can compute all of these paths in parallel. And that's really nice because then I don't have to go through this sequential Africa, the RNN, which is generally slow. So when there's no autoregressive component, calculating a local attention on all if you're history of inputs, you can actually parallelize your task, right? Any questions? All right, This gets us the transformer architecture with the next point, which is some of you may realize that if I have this architecture over here, there's a problem, which is that without recurrence, always have. Here are a bag of words, a bag of attention, and a bag of birds, but I don't know which word comes first, which second, because I have no notion of ordering a sequence. And so this gets us to the transformer architecture. In the first part of the transformer architectures solve this with what is called the positional embeddings. So just the transformer architecture. We're going to start off with the inputs to the transformer. This would be a sentence I love watching UCLA basketball. Then the outputs are going to be what we're eventually going to translate. The first minute. But transformer salts is this fact that if I'm doing everything in parallel, I've lost relationships with words relative to each other. And it does this by adding a positional encoding. But positional encoding, all it is is taking indexing, saying this is the first word, second part of the third word, translate that into a vector. So the vector for the first index is going to be this column right here. The vector for the second index is going to be the second column right here. And the values of these factors are calculated according to these equations. When you can see is that this vectors have particular structure. And by adding these inputs, you then get your inputs positional order rooms, the transmitting these indices to vector, you're basically putting the index as well, but in Texas represented as these vectors. Any questions? Alright, so that's the positional embeddings. The input embedding, which is a word embedding, cluster positional embedding. That was that green input side that we had before. And what happens is that xy is going to be translated into queries, keys, and values grew up in here later. So this multi-headed attention is the block that will compute my query q, my keys k, and my values V. That's why they're really going into that more attention block. And the output of this is going to be that variable Z that we talked about. Okay? And that's just going to follow that attention that you've already discussed. Multi-headed attention again means that we will have parallel, maybe eight, maybe 16 attempted walks. And then there's going to be an add-in dorm. All the ad is, is the residual connection. So that means that just like the ResNet, we are going to do a skip connection and that gives us a gradient Tywin. And then the norm here is going to be layer normalization. You'll recall this from the midterm exam normalization, but there's going to be a way to normalize across to fix the volume for activations to make them mean zero and variance one. Instead of doing that, which can be expensive because you have to calculate these across the bathroom, just gonna do normalization. Any questions there? Alright, so then after that, these activations are gonna go through a feed-forward neural network. And so that's going to be a fully connected neural network with a resume that connection. And also to have a non-linearity like array. It's going to compute some feature of your attention. Any questions? Alright, next is this block here, which is called masked or Titanic. Alright, so the motivation of this as the following. If I'm translating, I love watching UCLA basketball sentence. And if I were to pass in the entire transmission at the output. And let's say that I had already translated door, right? I want to translate radar day by referring to the inputs. But if I pass in entire translate a sentence, that door make our day, I'm essentially passing the answer into the input. What the transformer will then do is it'll just want to say I translated as chador and I know that Max works if you bake our day because it was inflicting to the network. So we need to do something called mass detention to say that if I've translated John Doerr, I can only pay attention to words that I've translated before Ashdod door. So I cannot look at any future words. And the way that we do that is with a mask multi-headed attention layer. All that is is it takes the attention layer and to it we define mass attention is taking the attention equation and adding a matrix M. And M is this matrix. Alright? Um, what this means is that remember that this is going to be for query one. This is going to be for query two. When I take the dot products of the queries and keys in Clery one, if I look at Q k transpose and I asked, and then I add mask and that hasn't negative infinity for T2, T3, all the way up to key. And then the softmax probabilities are gonna go to zero because if your score is minus infinity, your softmax probabilities. So basically, after mass attention, we know that the output softmaxes are going to look like 1000, all the way to zero. For query to maybe a slight 0.90, 0.100. So for the second word, I can only pay attention to the first word and it's the second word. But I can't get to the third, fourth or we're in the output. So if you go ahead and actually code this up, this is what the mass attention looks like. It's essentially a lower triangular matrix. And this guarantees that you will never pay attention to the future words. Your output is query one corresponding to this starts Okay? Yes. The question is, is masking only applicable at training time? Yes, at testing time, we haven't yet translated the future words. And so the testing time will look like that auto regressive graph that we had before. Other questions, All right. The question is, why are they doing sinusoidal positional embedding as opposed to some other function that represents division. There are people who have looked at other types of positional embeddings. You can even try to optimize a positional embedding. But in therapy, they either do similarly or if they do better, only marginally better than the sine and cosine. So that's an empirical results. Other questions. Alright. So that is the mask multi-headed attention. In this decoder block, we then get across attention layer. For our cross attention layer, the queries are going to come from. The decoder outputs, but then the keys and the values are going to come from my encoded sentence in English. I love watching UCLA basketball. So the keys and the values come from the encoder side. And what I pay attention to what I wanted then do the translation is I'll take the query for every single word in my output, compare it to the keys and my input, and compute a value according to the attention layer equation. Again, there's always wanted to be a residual skipped connection as well as our Layer Normalization. Any questions there? The question is, do we always make sure we have the same number of ones? Only thing that you have to guarantee between keys from being clutter and the queries from the decoder is that the keys and the queries are vectors that have the same dimension because they have to be dot-product together. But that's all. Are you talking about the sara? Yeah. So these are keys and values that I computed from being coded sentence. So, yeah, so the encoded sentence says they'll be, so the keys will be n by dk, let's say. And then these, these queries right here, Q can be N by dk. But all that has to match is the decay because we got to take the dog park. I'm sorry, I misunderstood your question. Yeah. Only the output here is connected to only the output of this network is connected to the decoder block. Sorry, I misunderstood your question. Alright. We're almost done with the transformer after the attention layer. All we need to do now is just get to classification because we're going to output a token or word from the English language. So we have a feedforward neural network to transform features. Then we get a linear layer that takes us to the size of our, I'll put the number of possible classes and then we do a softmax and then we draw from these output probabilities. Okay, so this is just transforming our cross attention features into a distribution over our tokens are our potential output for any questions there. What is the resolution of that? We cannot push off the back open. E.g. I have my money back. Yeah. Totally says it's asking what is the batched here? So let's say we had a batch of 512. If we were just doing self attention, like what should I pay attention to in my sentence, it would be 5.10, 12 sentences. Where if you are saying, let's take the past 3,000 words at the Duke of 3001st word. One example of that batch would be 3,000 words as the input, one guard as the output. And then I would have 500, 512 times 3,001 total words in my batch. I'm not sure people following this is falling off with and tell them to be shuffled. The second sentence, like, I love watching. If you turn it into like watching love UCLA, I probably doesn't make any sense. So the batches are distinct sentences. Now the question is, how do we do this in the testing phase? So then in the testing phase, we would do that autoregressive thing I drew earlier where we would have the starts token here. We would be able, we would have our entire input sentence. So we could do attention on bad. We take our start token, do our decoder blocks, and then I'll put a softmax distribution over the next characters. For the sentence we've been working with our deja, Zhu would come down and be the new input for which I would compute its queries. Then I would do the cross attention, and then I will get a softmax probability for whatever sample or the door. And I will keep doing that over and over again. This is why chat GBT takes some time to return output to you because it's doing this autoregressive component to pick out what's it back to you? Alright, that's the transformer layer. There were a lot of questions, so I know that there's a lot of can optimize these slides. If you have any feedback, feel free to send that to me. This is also, I think, just something where it helps to take a look at the paper and to spend some time on it. So please come to our office hours with any other questions on this question? Is there a question? Why do you need the positional encoding for the input? If I don't have this, then because all of these for self-attention just come in parallel. Like I love watching UCLA. There's no notion of order. They all go in parallel. We have to add the positional embedding to say I is index one, index two, watching his index three. So the positional and that is the only thing for the positional encodings that keeps the order of the words in the sentence. Alright, let's get the GPT. So now that we understand transformers, we can move on to trying to understand chat GTG. So there are several language models. We're only gonna do GPT because chat GTG is what we want to get to. But there are other important architectures like burst that we won't cover at a very high level. We're going to see GPT. It's just a stack of these transformer decoder blocks. And then an architecture like Bert is just a stack of these transformer encoder blocks. And they have different ways of training, which I'm not going to get into in detail here, but we'll talk about GTT. For GPT. Gpt-3. Gpt-3. They have the same basic architecture, which is a stack of decoder blocks with in stock a transformer decoder blocks. And then the most powerful ones have had a huge stack of them. So GPT-3, which is the precursor to chat TBT, is a network that has 175 billion parameters trained on 45 tb worth of data. You can see here how many decoder layers they have. So they have 96 encoders stacked. You can see that in each decoder they have 96. This is insane amounts of parameters that for a bachelor's using 3.2 million examples. So that's a 3.2 million sentences that have 3,000 words. Alright? Training these models is really expensive. I don't know the cost of sending three, but my lab was recently reading this device transformer paper from, from Google and just change the robotics transformer with over $1 million. So these things, I'd take quite a bit of time to compute. When GPT-2 came out, they were clear that one of the innovations was actually the datasets that they used to train it. So these things are learning natural glitches, but natural language processing. But tire data-sets which is greater the Internet and there's a lot of crap on the internet. So you want to make that dataset, Peter, so they will e.g. look at credit and then look at least posted on Reddit was written by humans for the most part, awesome at it, but somehow pulled out goodbye to post this also textbooks and books and Wikipedia that goes into these. So they have really made such a big dataset to train Chet. So protect GPT, GPT-3. For GPT, there is no encoder, so GBT does not have that at all. All that GPT is doing is taking this block here, which is the decoder block. Them. So GBT is just that. We're going to see my head because what CPT does is it just a word completion of Earth completion architecture, just like we had the text generation for, fixed here with recurrent neural networks. Let us all GPT is doing so for GPT, you pass in 3,000 words sentence. In this case, we just wrote nine words. And the sentence is, a robot must obey the orders given it. This is again from JLL Mars clock, and this is the second law of robotics. And the next word should be. And so if it's trained to roughly, what it would do is it would see the sequence of sentences. And it would generate a softmax distribution where buy is going to have a large probability. And if it doesn't, then it's going to have a high cross-entropy loss, which it can back propagate and update the parameters. To predict the next word. Well, alright, so the second law of robotics is a robot must obey the orders given by human beings, except where such orders with conflict, the first-order. So basically, this is one example, but then the next example, the input would be all shifted over by one. So it would be a robot dot-dot-dot given by. Then it would have to output the next sentence or it started the next word. This law of robotics, which is human. Right? So these are ways that we, that we train GPT to output the next word. So I should also mention GPT stands for generative pre-training task for her transformer. That just comes from the fact that we're using these decoder blocks from the transformer. And then generative pre-trained refers to this training process. We're about GPT does, is it's just trained to output the next word. Given some sentences that have been pulled from the internet, from textbooks, etc. Alright, that's why I showed this example. Last lecture or two lectures ago, where we saw that GPT is in its most basic form, distance often computer, you give it an input sentence or input sentences. And then it keeps generating more sentences to complete that. That's what, that's what, that's what GPT-2 is trying to do. But this isn't our experience of chats EBT. So a naive GPT model, but just simply complete tasks. But it's actually bt is something where we ask questions and it gives us answers. So how is this done? Sorry. Yeah, the question is, how do we know that GPT is not just memorizing things is learned from the English language. Rather than, you know, maybe put up, put, understanding it and putting something. This is a big philosophical debate in trying to understand. If GPT has some deeper semantic understanding. I'm going to leave that for office hours. Alright, let me take one more question and then I want to finish the last part of Great. Yeah, that's a great question. The question is GPG has so many layers of decoders. Gpt-3 is 96 decoders. Question is why doesn't have exploding and vanishing gradients? And that's because it has all these skip connections for restaurants. Alright, so let me get to my last slide, which is the way that we might go from GBT, a document either to a chat bot, is that we do an additional fine-tuning stage. So check. First learn structure from the heedless language through this generative pre-training process. This isn't what we know about attach to the t from the OpenAI blogs. But after that did you three steps. The first is they do some supervised fine-tuning, so they changed the task. You can give this as transfer learning. Instead of cleaning document that change the task now is to say, I'm going to ask you a question and it gives you a prompt. You have to give me an answer. So they give a prompt. The prompt might be explained reinforcement learning to the six-year-old. And then this is where humans coming to help fine-tune. Human will write an answer to this. That's the labeler who says, this is the kind of answer that you want to give to this question. Alright, so this is now a supervised data central questions and answers. You take that a chat GPT, GPT-3, pre-trained to just generate documents and you do some transfer learning. I sub T stands for supervised fine-tuning. To give such answers to such questions. That's the first step. But of course, you can ask if human labeler, to just generate arbitrary answers and questions, I would be really a Boreas. So the next thing that they do is they are able to further train chat TBT by reinforcement learning. The way this happens is that they will now present the same plot to check CBT, explain RL plus observable. Because ATP to generate sentences by generating softmax distribution is suffering from them. Every single time you prompt activity is going to give a different answer. So it's gonna give answers a, B, C, and D, where these are all possible explanations to this question or to the statement. Then a human labeler constant again. And then we'll rank them from best to worst. So they'll say D was better than CU is better than a was better than d. And what you can do from these labels is train another neural network called a reward model. And the reward model is going to tell you what the reward is or how good an answer that you get is, alright. Now become to the audit department no longer requires humans. They get the prompt to check, GET, and then they use reinforcement learning. So we want to talk about this in this class, but there's a really good optimizer and reinforcement learning called proximal policy optimization. Basically what happens is you answer the prompt, you give it to the Reward Model. Reward model tells you how good of an answer that was. And then you use that reward to train GBT to get even better. I went to bed and on overtime. So I'll just start the next lecture by reviewing this one more time you have any questions. All right, everyone. Can people hear me use the system? Alright, give me get started for today. Before we begin. The first announcement is to look at where are we gave instructions on how to sign up for my scope, as well as Piazza and we add a colon, the substance gets centralized scheduling. The TAs and I are still figuring out the discussion sections and office hours. ******* balls. We're almost done, but we don't have them right now. So we're going to send a ruined learn announcements when the discussions and office hours are all configured. Said, we anticipate doing that likely this evening, tomorrow. Alright. We uploaded a few supporting files to Bruner. This is a PDF on setting up Python and two books. And then if you wanted to use Google Colab, we also put up a PDF on that. Welcome to use whatever setup in desire. But if you want to know what I personally would prefer, if I was thinking this class, I would personally install Anaconda and effort each assignment I would create a conda virtual environment to do that assignment. All right. Any questions there? I still here but to shattering cell, so if you can hear me, can you just go? Also uploaded the formal notes of his classic examples dating back to 2018. Alright. Any questions on any logistics or anything from the syllabus? I think that needs to be clarified or anything that I prefer. All right. No questions. So we're gonna get back to material. So we were again doing this review of the basics of machine learning just to make sure that we're all on the same page. And last time we talked about this very simple regression problem, where our inputs are going to be the square footage of booms and busts for x and y is gonna be the rents. And we want to train an algorithm that is going to predict the rent y from the square footage. And we said you're going to model this. This, again, with a linear model. The choice of the model is entirely ours. And so we chose to use a model Y equals AX plus B to model this data. Where a and b are the parameters, are the things that I get to set to make this linear model and read the data as well as possible. All right, Any questions there? Okay, so we were then, I've created a second. So actually in our machine learning problem, which is the bind, the loss or the cost function for the problem, right? And be motivated it from if I have the model Y equals b, the y-intercept plus a x, where a is the slope of the line. I could set a to whatever value I want a certain setting up being a gift to be this red line. Another setting of a gives me this purple line. And the red wine is better than the purple line. But they needed to come up with a rigorous mathematical way to say red is better than purple. And after we come up with a rigorous mathematical way or the loss function to quantify how good our model is. Then we can optimize that loss function with respect to our parameters to choose the best or this model. So at the end of last lecture, we said that each of these data points, of which I have, let's say 20 of them will label each of these data points is x and x superscript I. So that's the five data points. I am putting the output. And for the five data points, what I can do is I can measure the error of my model in predicting this particular data point. So if my model is the red line, the error epsilon I, which we'll define as the distance from our prediction to our actual data point will be epsilon equals the actual y value, the true y value from the data, this data point minus the value predicted by my model and red. And that's going to be this here. That's gotten by computing v1 plus A1. Okay? Any questions there? Alright, so I have, let's call this 20 data points. And so I would need to compute Epsilon for all 20th, 13 of points. And then what I could do is I can get a loss function by summing them all up together. So my boss, which is also called the cost function. So we'll say loss, which is equal to a cost, will be the sum of these errors. Alright, so I'm gonna write that as. The sum from I equals one to big N, the number of data points I have, in this case 20. And then it's going to be my actual data point value. That is the y superscript i minus what's predicted by my model, which is this y hat i, which is equal to f of x. So it's gonna be x. That's how I can quantify how good the model is. Alright, there's one thing that's still wrong with this. Can someone tell me what's wrong with this? Great, yeah, so students mentioned, right now, this error is sorry. Alright, so for this boot data points, the error will be positive. But if I were to take the error for this data point down here, this would be negative. And if I have both positive and negative values being summed together, they will cancel out each other, right? But this error in the negative direction is just as meaningful as an error in the positive direction. So I need to make these errors positive. One way I can do that is by taking the absolute value of these. Another way I could do that is just by squaring this, which is what we're going to choose to do, at least to start. So if we were to quantify this loss function, can usually, we will write this as a script L, right? We will see that this L to be a lot lower for the red line and the purple line. Because if we look at these data points, they have a really large epsilon i in magnitude to the purple line, but relatively small. Right? So that's gonna be the mathematical quantity that we will optimize to choose the best values of a and B. By defining this, you can now get to questions. So the student asked, we will oftentimes put a one-half in front. Really, we could put any constant in front, oldest continuous scale the losses so it won't change, but the optimal. Usually people put a one-half here because we're going to see that we have to differentiate this quantity. And when I bring down the two, that'll cancel out this one. Another thing that people will often do, and we probably ask you to do this in the homework, is to normalize this by big N. So it's like a lost e.g. but you can optimize this quantity without this scaling factor that was still get the same optimal and it just scales the value lost. Other questions. Alright, so this loss function is the next important component of the machine learning problem that we will derive later on for neural networks. But it tells us how good our model is. We have to choose this very judiciously. Otherwise, they may be optimizing for something that doesn't actually tell us how good a model is. So in our simple example, we'll have our model y-hat equals a x plus b. We call again that these y's and the x's. These are the output and the input data. These are data that are less. So there are values are known. Then what is unknown parameters? In this case a and B. These are the parameters, which means I get to choose the values of a and B to make the model as good as possible. When I say, I get to choose, I'm not saying like I actually like I am going to set the value of AB that's going to be chosen, or that's going to be set by an optimization process. But these are the free, these are the things that we can to make this model. And so this is going to be determined by making all of our parameters, in this case AMD, as small as possible. Okay? Any questions on that setup? So the question is, could also define a loss where the error is the projection onto this red line. And you could do that, you would just have to thank quantify what these projection distances are and then sum them. Or some distributors. Oh, okay, the question is, how do you know which loss function is a better one to use? So this is a bit more nuanced when we get to neural networks, we both talked about the different types of loss functions that you could choose and how some of them are better choices for searching the hyper parameters of our neural network. So I'm going to table that for when we get to neural network loss functions. But if it's still there, please pick it up again. Yeah, let us follow up on that. Okay. So I'm gonna do one more thing, which is I'm going to rewrite this AX plus B as a simple dot product. The way then once you do this is the machine learning using the parameters of your model are going to be denoted, denoted by vectors theta. So theta is going to contain my parameters, which are a and B. And to make this equation equal AX plus B, what I'm going to do is I'm going to define an x hat. And x hat is going to equal our original x and one. Alright? So when I do Theta transpose x hat, I'm going to do a x plus b, right? Question, sir. Alright, so then with our loss function that we question. Okay, yeah, so tomboy and remain are saying that if we were to take a loss function that's the perpendicular distance, it would end up being the same. Alright? So for this loss function, I'm going to now take my wife hat for the i'th example. I replace that by theta transpose hat. Sorry. Alright. So this again tells me how good my model is. And you'll notice that now the loss function is going to be a function of my parameters theta. And I again get to choose or I get to optimize the values of beta AND to make this loss as small as possible. And what's possible. That means that all of these errors are minimized. Then, now comes the question, how do I choose the Theta to minimize this elevator? And the way that we do this as w constructed as an optimization problem. So our goal is to choose the parameters, the values of beta, to make a boss, I love Beta as small as possible. Okay, so the diagnostic have in mind when we get to this, when we think of this optimization problem, is that on the x-axis, we have our choice of theta or parameters. On the y-axis is our loss function. I love data. And as I change the values of a and B, I'm going to move where my line is, and that's going to lead to a different boss. And so L of theta is going to look something like this. For this particular problem that we're doing. L of theta is going to be convex with respect to beta, which means that we're going to have one global optimum. And if that is the case, how do we find what the minimum value or how do we find the value of Theta that minimizes this convex loss function L of Theta. Perfect, Yeah, so in the case where there's just one global minimum, we can use our result from calculus that at the global minimum, the derivative is equal to zero. And so what we can do is we can go ahead and calculate the derivative of the loss with respect to Theta and set that equal to zero. When we solve for that data, that'll tell me the optimal setting up data that minimizes loss. Alright? Any questions there? Yeah. The question is, how do we know that the loss function is convex or not with respect to the parameters. So there's going to be an entire class called convex optimization that we'll go over the definition of this. For this class. I'm not going to go into the details of that would be beyond the scope because actually this is just a very simple contrived example. This actually motivates me to say something else, which is that in general, for neural networks and optimizing them, the loss function will never look. Convex, is going to have many local minima, like crazy loss function electron here. And even for these examples though, we're going to learn how to optimize these with an algorithm called gradient descent. And for those you still need to know DLD theta, which tells you the direction that you stepped in to make this possible small as possible. Alright? So we are going to go ahead and differentiate this with respect to theta. And then for this simple problem, we'll take the derivative and set it equal to zero. For later on, problems are going to know what the gradient is and use that to, to optimize the plastic region. Right? So this gets us then to vector and matrix derivatives. So in machine learning, we are often going to have to take derivatives of quantities with respect to vectors and matrices. And these are typically called gradients. All right, we're going to discuss three of these today. The derivative of a scalar with respect to a vector, derivative of a vector with respect to a vector and just derivative of a scalar with respect to a matrix. In general, if a variable is lowercase italicized is going to be a scalar. This lowercase bold, there's gonna be a vector. Then if it's uppercase fold is going to be a matrix in this class. So if I want to know the gradient or the derivative of a scalar or little y with respect to a vector x. I can denote it in two ways. I might write DY DX. That's pretty common notation from capitalists. Or we might also use this nabla operator. This means I'm taking the gradient or the derivative of a scalar y with respect to the vector x, right? So we're going to make a few definitions. The first will be the gradient of a scalar y with respect to a vector x. So first off, if y is a scalar and x is a vector, the vector x is an n-dimensional vector. But wants to do is we're going to define the gradient DY, DX or this other nabla notation that we used to also be an n-dimensional vector. And the elements of that vector are as follows. The first element is d y with respect to dx one where X1 is the first element of the vector x and d y, d x2, or x2 is a second element of the vector x, etc. Any questions on that definition? Alright, so x is a vector, derivative of a scalar with respect to a vector is a vector of the same size as x. And these are the elements of that texture. So if I go ahead and I tell you e.g. so let me first write out that x is going to be this vector elements X1, X2, all the way down to Zen. I tell you DY, DX is equal to 10.5 000, et cetera. This DY DX, this gradient tells me that d, dy dx1 equals one and d y dx2, it was 0.5. Then we know that in calculus, basically the delta y change in y, change in x is approximated by the derivative. So if I go ahead and I wiggle x one, I know that that's going to lead to a wiggling in Y as well. And that we're going in y delta y due to changing at x1 is approximately equal to d-y, d-x One times an x y. So if I wiggle X1 by 0.01, I would expect Y to wiggle by 0.01 as well. Given that d-y, d-x is equal to one, I wiggled by 0.01, I would expect y to be equal by 0.00. Any questions here? The question is, will there be cases where we cannot take the derivative of the loss function with respect to the parameters in this class. So when we build neural networks, will be careful to use operations that we can always differentiate. And then there will be operations that are strictly not differentiable. What did they have like a discontinuity. But what we'll do is formally, we're going to take the gradient of those informally in this class, but they do have e.g. a. Function. The absolute value. When x equals zero, it is derivative is undefined or equal to zero. If you don't follow that, no worries, we can get some more detail when we get to ingredient to second, what does it mean for the operation to? What does it mean for something to be differentiable? Here I'm saying the derivative exists. And so like for the absolute value, the limits become defined when I have the two lines intersecting. Sorry, can you repeat that? Operations are analogous to pump in this case. Yeah. Good rocks with the setting for it to be differentiable, the left limit and the right equal to each other. So this is recapping what we just said. A vector, the gradient of a scalar with respect to a vector. It's gonna be a vector that has the same size as what we're differentiating with respect to each dimension of the gradient tells us how a small change in x and that dimension is going to be to a small change in y. Okay? So now let's say that I were to give you a vector. Let me give you, let me say that I give you a vector x. Maybe this vector x is equal to 0.050, 0.010, 0.02, et cetera. And I want to know if I wiggled x by delta x is small vector. How much does, why change? And in this case, I know what the gradient of y with respect to taxes. Okay? Raise their hand and tell me how much this delta y change, or how much does y change given that I know the gradient and I know what Delta x is. Great. The students says he just talked out with delta x. That is correct. So the change in y, right, is going to be approximately equal to the sum from I equals one to big, to little n, where n is the dimension of X. And then it's going to be DY dxi times delta x. So basically the change in y is going to be how the changes in x and every single dimension modify, live all of those together. And we can see is that this is a dot product of the gradient where our change in x. Any questions there? The question is when I write approximately here, is it because this mathematical relationship is approximate? The answer is yes. So this approximation becomes more and more true as delta x goes closer and closer to zero. But when Delta x I is large, it's going to be a linear approximation of how much the change. Other questions. Alright, so we're gonna go ahead and take our first gradient. So let's say that f of x equals theta transpose x wasn't know the gradients of f of x with respect to x. And here I'm just going to write that y equals f of x. So I'll use my Excellent Boy notation. Alright, so first, we should always check dimensionality, so make sure things make sense. So let's say that x is a vector that is little n dimensional, right? Since I'm dotting it with Theta, Theta also has to be an n-dimensional vector. And I know that if they've got an extra n-dimensional vectors and the dot product is just going to be a scalar. So y is going to be a scalar in R. And therefore, by gradient of y with respect to x should also be a vector in R n by definition. Any questions? Alright, so let's go ahead and take this gradient. So y is equal to Theta transpose x. So I'm just gonna write out this dot product as Beta one x1 plus beta two x2, all the way to Theta and x, right? In this case, then the gradient of y with respect to x is going to be. Dy, dx1, dx2, all the way down to d y, d x. And this is going to equal. So for the first one, right? I take the derivative of this quantity with respect to X1. X1 only appears in one place. That's this expression and it will apply theta one. So if I differentiate with respect to x one, I just get beta one. Similarly for theta-2 all the way down to theta N. So I can say that the gradients of my function f of x with respect to x is just theta. Data is a vector in RN. So dimensionally, things still work out because my gradient, I expected to also be a vector. Any questions there? All right, that was hopefully the ones. We're going to just do, a bit more of a complex one. So what we're going to do is we're going to take the derivative of a vector. Sorry. Never mind. This is still a derivative of a scalar with respect to a vector, but it'll be a bit more mortality. So here we're going to have f of x equals x transpose a times x. We want to know what does the gradient of f of x with respect to x. Alright? So here again, x is going to be a vector in R n. X is a vector or n, then x transpose is n by one, sorry, is one by n. And x is n by one. Which means that a has to be an n by n matrix for everything to work on. So a is going to be an n by n matrix. Then we know that a transpose a x is just going to be a scalar. Therefore, we also expect that the gradient of effects with respect to x, It's gonna be a vector with the same side as x, will be a gradient, which will be a vector. And any questions there on the dimensions also track something I said earlier, I said today we're talking about the derivative with respect to a Dr. and that isn't sure if we'll get to that later on. That's called the Jacobian. But today we're just going to do derivatives of scalars with respect to a vector or matrix. So this matrix a is going to be m by n, and its elements are going to be A11. All the way up to A1, just gonna be a, N1, going to be a. So with this, let's go ahead and try to compute the gradient. To compute the gradient, I will need to compute how y changes, or I'll need to compute the derivative of y with respect to all the elements of x, x one to x. Then I have my X equals X transpose a X. Someone to tell me what the first step trying to get this gradient would be. You're confronted with this problem. Perfect. Yeah, We should do something. That's correct. We should do something similar to what we did. The last problem, which is to expand things out in terms of our X1, X2 so that we can differentiate this expression with respect to the x ones, the size in general. So we change the color back to black. We know that x transpose a x can be written as double sum over I and J AIJ. That's just the definition of matrix vector multiplication applied to this quantity here. Given by not have this, some are constructed, try to compute the elements of the gradients. So why don't we start off by trying to compute the derivative with respect to x one. Alright, so I'm gonna try to compute d f of x dx one. When I do that, I'm going to go ahead and I'm going to expand this expression. Simple quite a bit. Well, note two things in this expression, right? All right. Is Iterating from one to n and j is iterating from one to n. But what I wanted to take the derivative with respect to x one, all I care about are terms that have x in them, right? So one term where X1 is when I have I equals one and j equals one, right? So if I equals one and j equals one, then this summation will have a term that is equal to a 11 times x one squared. Because again, i and j are both equal to one. Any questions there? So when I differentiate this with respect to x one, this is going to give me a term to A11 times X4. And so that's going to be one element that comes out of the derivative. So I'm going to have a two A11 times x for now. And let's just consider the next set of terms that has an a one. So I can consider an excellent, I mean, when I equals one and j does not equal one. So this summation I equals one, this term will be an X1. This will be an a one j times x j. Here j is not equal to one. Alright? So what this is going to give me is this is going to give me a summation from j equals two to n, where I have a one j times x, one times x j. All that's doing is taking this term, let me rewrite this term really quickly so that we have it as ai j times x times x j. These are all the terms where i equals one. Mj does not have any questions there. Alright? And then if I differentiate this with respect to x one, so if I go ahead and do derivative of this whole thing with respect to x one, that is going to just give me the X1 will be differentiated away and I'm just gonna be left with the summation from j equals two to n of A1 j times x j. So I'm going to write that term over here in purple would be the sum from j equals two to n a j times x j. Similarly, they're going to be terms where I does not equal one, but j equals one. And that's going to give me a summation over I equals two to n a i one times x times x one. To get this expression, I'm accepting at the terms with x one in them when j equals one. But I just started. Now if I were to differentiate this term with respect to x one, I would get a plus sum from I equals two to n a i one times x. Any questions there? Yes, thank you. I from two to n. Any questions? Just asking you, this is just a scalar. The answer is yes. So f of x is a scalar, x one is a scalar, so the derivative will be a scalar. And all these terms are also just scalars. So now I'm going to simplify these expressions. You can see that these sums are from j equals two to n and from I equals two to n. And then I have, if this works to b from j equals one to n, right? I would have a term, A11 times X1. That term A11 times X1 is here in pink, and I have two of them. So I can put one of these A11 X1 into this sum, making this song. Sum from j equals one to n of A1 j times x j. I can put the other A11 X1 into this summation, which would make this plus the sum from I equals one to n of I one times x by simplification, by consolidating terms. Any questions there? Alright, so we're gonna continue on. We have that. I'm going to copy and paste this onto the next page. We have that D f of x with respect to X1 is equal to this quantity. The quantity is in terms of matrix silence in factors. So if I have my matrix a and its A11, A12, all the way up to A11, A21, A22 to n a n, a n and my matrix a. I were to take my vector x, which is X1, X2, all the way down to x. Then what I can see is for the purple some. What we're doing is we're taking a one j for j equals one to n. So we're taking this first row of the matrix a, and we're taking this dot product with the vector, okay? And so one way that I can write this term in purple is to just say that what the term purple is is if I do a matrix vector, multiply a times x, I take the very first element of that resultant vector. So this is equal to the matrix multiply a times x, taking the first element. Similarly, for the green term, we have that is a I1 from I equals one to n times x psi. Which means that we're going to be taking this entire column here in green and dawdling it with the vector. And this can be written as the first element of the matrix vector multiply a transpose Ax. Okay? So here what I'm doing is I'm taking our summation notation and converting it back into just vector and matrix notation. Questions here. Alright, so then I'm not going to do this, but you're going to find is that if you were to do this, we did this for X1. But D f of x dx and dy f of x dx, three, et cetera, all follow the same pattern. So if I were to take d f of x dx, the result would be this. But now, so the first element, the second element is, alright, so in total, if I wanted to write my gradient, which would be d f of x dx one, D f of x dx two, all the way down to D f of x dx. And this is going to equal. So the first term, VFX dx1, we got it already would be a x one transpose x one brace there. So I have a bit more space. And similarly for the second element, it would be a two plus a transpose x two, all the way down to a x n plus h transpose x M. Alright? And then what that tells me then is the overall gradient f of x with respect to x. Remember this function was x transpose a x. This is equal to a X plus a transpose x. Right? Any questions there? You raise your hand if you're following. Okay, that's everyone or almost everyone. Great. So I'm gonna do a few laptops. I'm gonna write this as equal to a plus a transpose x. And there's one more simplification case. And you can make if a is symmetric, then this simplifies to two a x. Alright? Whenever we can answer like this, we should do a few sanity checks. Especially if you're doing it for the first time. The first is again dimensionally, does it make sense? So we said that the gradient scalar with respect to a vector that is of size n should be a vector of size, right? This is a vector of size n. You betcha, because a is n by n and x is n by one, right? There's another sanity check that you can often run when you're doing matrices and vectors. Which is to consider what happens when the dimensionality little n reduces toward the scalar case. Alright? So can someone tell me if this is true in the scalar case and if so, why? Perfect, Good. Yeah. So Blake says, when n equals one, our original function was x transpose a x. But when n equals one X matrix big aid, they're both scalars, right? So when n equals one, then f of x equals x times a times x, which is just a times x squared. And if I differentiate a times x squared and D f of x, dx is equal to two a x, right? And then that matches what this gradient is. Alright? Any questions? Alright, so if you look at the formal notice that we posted on ruler, they will have the LaTex version written up of how we calculated this gradient. And so this is just a recapitulation of that will define one more thing, which is a matrix derivative. So let's say we're going to find here the derivative of a scalar with respect to a matrix. We're going to say that some function y is equal to z transpose a x, e.g. and in this example, let's say that Z is a vector that is n dimensional. X is a vector that is n-dimensional. I said the major dimensions work must be a matrix that is n by n dimensional. Then we define the gradient of y with respect to a in the following way. D y, d big a, which can also be written with our nabla notation, is equal to a matrix that is the same size as a. And its elements are d y, d A11, A12, Dui, dA1. And this will be d y, d a d y with respect to d, Then the element. So basically it's analogous to the vector case, right? The first element, the one-one element here tells me how wiggling a 11, the first row, first column entry of a, how are we going? This changes? Why? Then in general, be wiggling? If I wiggle the AIGA term, a, d y, d a I, j will tell me how much I expect. Okay? So if a is a matrix that is m by n, then the gradient of y with respect to a will also be a matrix that is n by n, where every single element of the matrix again tells me how much I wiggled up elements of a. How much will widen. Any questions here. Okay, so the next thing I'm going to say is really important, because I already know that even though I'm saying it here, students are going to be asking this on Piazza and what they do, whatever they could respond if you're paying attention right now. Which is that this thing here is called denominator layer. In denominator, the x is a vector that is n by one. Then the gradient of a scalar with respect to x is also a vector that's n by one. And if a is m by n, and the gradient of y with respect to a is also a matrix that is m by n. When we do questions, written questions in this class, we are always going to follow these conventions of denominator layer. There's another layer that is commonly used called numerator layout. And numerator layout is simply the transpose is of the denominator value definitions. So enumerator layout, the gradient of y with respect to x. Instead of being a Dr. the same size as x column vector is going to be a row vector. It's gonna be this transfer. So enumerator layout. The gradient is going to be a row vector which is R1 by n. And the gradient of y with respect to a is gonna be this matrix transpose. So it's going to be a matrix that is R. And by both conventions can be used. But basically in a class where we give out questions, we have to choose one and stick with it. So in this class we're going to choose denominator Leah, and stick with it. A lot of other classes may use numerator layout and there are good reasons to use numerator layout. So enumerator layout, the Jacobian, which is the derivative of a vector with respect to a vector, I will not have an extra transpose symbol. And when we derived the chain rule, the numerator that is going to read from left to right versus in denominator Leah, and reads from right to left. Those are easy to choose. The Amrita that the reason I liked denominator layout is because I know that the gradients are going to be the same size as what we're differentiating with respect to. So basically, moving forward in this class, we're going to use the denominator lab. And when you do, right or when you look up things on Google gradients, you may see everything transpose. And that's because whatever we are looking at would have been using numerator. They are. Right. Any questions here? I'm sorry, can you repeat the question? Yes. I got the student is asking is the numerator layout just the transpose of the gradients and the denominator. The answer is yes. Any other questions? All right. Let's take a five-minute break and when we come back, we're going to use this to solve our pharmacy money problem. This isn't like a silly question, but I suppose we find that it follows rotate and became, Yeah, so can you make a machine on the send button and mathematically then yeah, can you just exactly like yeah, that's not a silly question at all. So that is a critical thing for how these neural networks for the container. So in week seven or eight of class, we're going to show you my torch and TensorFlow, exactly working that fourth named Steve works by composing gradients. So basically for every operation by a process of multiplications, et cetera, tensorflow or Pi-fourths, will know what the derivative of that is along the computer to give it a programs might be, could be. Does that make decent every woman, I didn't just make healthy thinking. And you don't have to calculate radius or is this term AI can be trained to do that. But I don't know, I don't know if something back to that, but I don't think it would be I think would be possible to train. Would be classified as yeah. Yeah. You mentioned that you showed the derivation. So I was wondering for you to post your right out of the class. Yes, those will all be posted here. Great, Awesome. Either way, I'm asking you to do fast at it. From my advice there. I just thought that this week. Watch afterwards, I'll walk into your office. Yeah. Yeah. Yeah. Let me check. Oh, yeah. Exactly. Of course. If it's turned on because ultimately I will match the names to what's on the right. The last thing for the project. All right, everyone will get back to you. There are a few logistics questions during the break. All right. The first question was if these annotated notes, the notes that I'm making right now on despite what I'm ready done, will be posted to Boomer and the answer is yes. Everything that I write down here will be posted to grow and learn. And then there was a question about the final exam for this class. If there's no final exam for this class, there'll be a final project that will be due Monday of finals week. All right? Okay. Any questions on any of the vector or matrix derivatives we talked about? The question is, can I explain the difference between the denominator and numerator of my app? Yes. So the only difference is that they're transpose. And so in the numerator layout, let me just add a slide here. In numerator layout, I would define d y, d k. So in denominator layout, this was d y, d A11, A12. Numerator layout. Everything would be transposed. So numerator layout would find this to be d y, d A11, A12 down to d y, d a V1 n, right? So this row will now become a column. Then this element would be d y, d a two-one, cetera. So it's just the definition of the denominator layout matrix. The question is, why would you use one over the other? Just as a matter of whether these two are what's convenient, mathematically correct. You just have to remember that they will have the chain rule written either left to right or right to left. There will be less transpose is it gives the numerator lab, which is why people prefer, especially for which we will discuss later on in this class. So we're going to be then solve our simple supervisor an example. Remember the entire goal of these gradients so that I can differentiate a loss function with respect to my parameters Theta, right? So let's go ahead and do that. We have our loss function. Our loss function is going to be a function of our parameters theta. And recall that in our example data where the values of a and B that we get to optimize for to make the model is fit as possible to reduce the loss as much as possible. And I remember that this x hat and x hat was our input square footage and the value one. So what we're gonna do is we're going to extend this expression and then we're going to differentiate with respect to Theta. All right? So then after that we're going to set the derivative equal to zero. So the first thing I'm going to do this, I'm going to write out the squared as a quantity transpose itself. So this is equal to one-half sum from I equals one to big N y i minus Theta transpose x hat by this whole thing, transpose itself. Alright, and that's just running out. The squared. Remember these superscript eyes refer to data points, right? Examples. So each of these would be a different example. So this is summing across all of my data points. That's what this summation. Alright, I'm gonna make this question. Great. Yeah, the question is, why is there this is 1/2. It doesn't have to be there. Because when we differentiate, this two is going to come down and cancel it out. So it comes out to be nicer. You don't have to write the two everywhere. The question is, why do I need to write a transpose here? If this thing is a scalar, you are right, that we don't have to write the chance because, because the transpose of a scalar is just itself. However, we're going to do something called vectorization, but we're going to write this in terms of vectors and matrices without a summation so that we can easily differentiate it. So it's on the way towards that factorization, which we'll see in two steps. All right, so the next thing I'm going to do is I'm going to write this as one-half sum from I equals one to big N. And I'm going to do something. But I'm going to do is I'm gonna write this as y minus. I'm going to make this instead of Theta transpose x hat, I'm going to make it x hat transpose theta. So what I've done is I've just taken this expression here and I flipped the order of the accident that data. Why am I allowed to do that? Yeah, I agree. They're the same thing. And so this is actually related to the question that was just asked. Theta transpose x i is a scalar, right? It's a number. Let's say that the number was fine, okay, if I transpose phi is still equal to five. So you can always take the transpose of a scalar and it will equal itself, right? To go from here to here, I use this rule from linear algebra where if we have a, B, C transpose, this is equal to c transpose, B transpose a transpose. So when I take the transpose of theta transpose x, i, ai transpose each individual element and I took their orders, that's going to give me x transpose times theta. Any questions? Alright, Now this is a key step called vectorization and you're going to do this on homework number two, when you implement an algorithm called K-nearest neighbors. So this is called vectorization. Basically in vectorization, if we were to write this in Python, right? Would be that we could write it is with a for loop. But we know that for loops are relatively slow. And so there's actually a way to speed up computation by running. This is just a matrix vector operation. We know that a song can be written as a dot product of two vectors. So what I want to do is I'm going to write this sum as a dot-product by in the following way. So what I'm going to do is I'm gonna write this as one-half. And then I'm going to take my white eyes from I equals one to big N for my big and examples and stack them in a vector. So this is going. 
Okay, I'll record the second half of this. Makes sense that once you cross connection, okay, I'll record the second part of this. Thanks Jeremy. Alright. So to turn this into a summation, I will transpose this quantity with itself. I'm going to share a bit in writing and just copy and paste. This thing I wrote, copy and paste. So if I take this thing transpose, but the cell is going to implement this summation. Alright? We're going to check the dimensionality. So I'm gonna call this vector of the water wants to buy anyways. I'm going to call this quantity a big matrix, Y. And Y we can see is a big m-dimensional vector. We'll call this thing here big X. Someone told me what the dimensionality of the axis. Once students have any other takers, heard, someone say n by two. Yes. So remember that because I'm one step here, that each x i is a two by one vector. When I transpose it, it becomes a one-by-two. And I start to begin again. So this is in our big N by two. And then I theta isn't art here. So this big X times beta will indeed gives you just an m-dimensional vector back. Each element of an n-dimensional vector is one of these terms. And so if i.it with itself that implements this entire nation, any questions on that vectorization? Alright, so this will be like playing around with something in a vector or a matrix for that removes the formative and it'll automatically scale computation. You'll do this in homework number two. Alright, so this is going to be equal to one-half. And I'm going to write this as big y minus x Theta transpose y minus x Theta. Lastly, I could just do oil to write out all these terms. So this is equal to one-half, will have a Y, Y, Y transpose Y minus Y transpose X Theta minus Theta transpose X transpose y minus Theta transpose X transpose X Theta. Doing like foil for these two expressions. Sorry. Thank you. Questions here. And just say, thank you for the questions ahead of time. I definitely will make mistakes while I'm lecturing up here. So if you ever see some mistakes that I make, feel free to just call it an ***. Alright, so what I'm going to do is I'm going to take this expression. Remember that this is 0. And you can see here I combined these two inner terms into just one to y transpose X Theta, right? The reason that I can do that is again, because these expressions are scalars and do the transpose of each other so I can combine them. The same exact logic that took us from here. Alright, so this thing is equal to ls data. Tomboy is referring to this expression here. Yeah, tomboy is pointing out one way to write this notationally is as follows. The two norm of y minus x Theta squared. And Tom, I also raised something which is, this is called the squares. And probably many of you have seen this example so far. And I know that this will give us the least square solution. Thank you, a tomboy. Alright, so we're going to differentiate this guy with respect to Theta. So I'm going to compute DL. D Theta. And to do this, you're going to use the matrix and vector derivatives that we just derived, actually just a vector of derivatives. So if I have d theta transpose theta d theta, we know from the prior slide that this is equal to a plus a transpose theta. Alright? And then I'm going to write this in terms. If we have a w equals z transpose Beta, then dw d theta is going to equal z. I'm going to differentiate with respect to Theta. I'm going to have my one-half to turn come out. Y transpose Y has no data in it, so gradient with respect to theta is equal to zero. All right, For this next term, I have a two, y transpose X times Theta. And y transpose we know is going to be a one by big N vector. X is going to be an n by two matrix. And so overall, y transpose x, which I'm going to call z transpose, is going to be a one by two. I'm going to call this thing z transpose. So this is gonna be a two times z transpose Theta. Gradient of z transpose Theta with respect to theta is just z. So the gradient of this term with respect to Theta, it's going to just be z and z is x transpose y. Me questions there. Okay, and then the last question I had is this Beta transpose X transpose X Theta X transpose X. I'm going to call a matrix a. And I know the derivative, the gradient of theta transpose theta with respect to Theta is gonna be a plus a transpose Theta. So this is going to equal X transpose X plus X transpose X times questions, question, what is the gradient of w with respect to z? Oh, so the question is, why is this gradient to z and z transpose? Because we're using denominator over. Yeah, so here theta would be a vector that is in R2. So this should also be a vector in R2. It could be Z transpose if you're using numerical that. But in the past people used denominator, they will also still BC. So for that, just play around with and then use the definition of the gradient. Yeah, it's, it's something that we actually derived an order. It doesn't conference but because we derived theta transpose x instead of x transpose beta, but using the same exact logic you'll, you'll get because in denominator. Alright, so I'm going to continue simplifying. This is equal to minus X transpose y because the 2's cancel out. Then here I have Q X transpose X is, but then that two will cancel out with this one-half b plus X transpose X times beta. Then to get the optimal Theta, I, set this bin to be equal to zero. So now if I set this thing equal to zero, I'm going to give me the equation, X transpose Y equals X transpose X Theta. As long as X transpose X is invertible. This gives me my solution, theta equals X transpose X inverse times X transpose y. Any questions? Alright, so if you've had linear algebra and machine learning before, right? This is something that you'd like to you already know. We went through this example. So again highlight portions of machine learning problem, which is that we need to first define a model. In this case, linear model means to find an extra or loss function that tells us how good or bad. That's my Theta. And then I'm going to need to know how to differentiate L of theta with respect to Theta. So that's something that we've done here. And in this case, we can solve directly for theta. The more general case where there are many minima and maxima, you will never be able to solve for theta because you won't be able to write down a closed form analytical solution. Hover, will use gradient descent when we aren't those cases. Questions. Next question is, is it because of denominator layout that X transpose X is a matrix instead of a scaler. Note in both numerator and denominator, we have x transpose x will be a matrix. They'll just be the transpose of the chapter. Did that answer your question? Yeah. The question is, what is X transpose X a dot product with itself? It is, but actually call is an n by two matrix. So it's a two by n times n by two. I think it's a two-by-two. Oh great. Roxanne is asking here. We stepped it up in terms of rows, can be stacked them in terms of the columns. For x. Change data for the other side. Yeah. As long as everything works dimensionally and you haven't orders partner, you just stack them however you want. Question is, can I remind you what x is an n by two matrix? Yes, because remember this X hat is r square footage x and a value one. So it's a two by one vector. And I stacking up and have them. So each of these is a two dimensional row vector and I'm Stephanie. The question is, can I stay wide? X transpose X is invertible. X transpose X may not always be invertible, but you will see that x, right? In this example is X transpose X could be a two-by-two matrix, right? So it will only be non-invertible if the rank is foreigners thereof. But when we do x transpose x, we have big N examples. And as long as big N is larger than two, usually we only multiply them together. It would be high probability. There'll be likely that it'll be full. If you have many more dimensions of this too became ten and you only had two good examples like big N equals two, then it wouldn't be invertible. Other questions, actually, a transpose X, transpose X. And the reason that is is because if a, I'm just gonna write this as b, c, right? If I do a transpose Ax equal to c transpose B transpose. So if I transpose this quantity, I take this thing and transpose it and put it in front. That gives me the first X transpose. And I think take this x transpose, a transpose if I just put it to the bathroom, That's just one Other questions. Alright? So this solution is called the squares. It appears in a variety of linear application, but we're also going to see in just a few slides have non-linear polynomial models. Okay? So this is code and kept up the code from what I prefer the password was in, I believe 20 2017. So some of the syntax is a bit outdated, like Python, you can use an app symbol for doing matrix vector. Matrix multiplies as opposed to dark tones. But in this code, which you'll do something similar in your homework, why is gonna be a big n-dimensional array of rents? And x is going to be a big N-dimensional array of square footage is. And then this code is going to implement the least squares solution that we saw on that we just derived in part with the model isn't that model will be this green line. Alright? Alright. See, you might look at this thing. Can't we do better, right? We talked about before how they could make e.g. polynomial model that maybe does a better job of going through these data points. Alright, so here's a question for you all to get on, and I'll ask them to answer the question. The question is, how does our current least squares formula allow for learning non-linear polynomial models? Like, let's say I wanted to be an nth order polynomial instead of just the law. Alright? So I'm telling you that what we derived already can do polynomial. And I want something to think about how that's possible for their current machinery. I see some hands going up already on here, w1, like 20 s to think about this and then I'll call someone to answer it. Received one. All right. Someone raise your habits. How many? Polynomial regression? Yeah. Perfect. Yeah, so this student says, what I can do is I can make a new attack feature right? Before, it was just x and one. But now I can make this be, let's say I wanted a third order polynomial fit, right? I can make it one, x squared and x cubed. If I do that, then let me write it more generally here. What I can do is I can define X hat to be one, x squared all the way to XN. If I define theta to be B, then what multiplies x one is a one multiplies x squared is A2. So it'd be, be A1, A2, all the way down to AN. Then I've written this wide as a theta transpose x, where I get to choose the values of B, A1A2 am, that are the coefficients of my polynomial powers of x. Any questions here? So this is one way in which you can make a nonlinear model, which is that you choose what the nonlinear features are there ever choosing that are nominator features are squared all the way up to x n. And these are their coefficients. Alright? And using the exact same these squares formulation we derived, we derived how to minimize that loss function, the squared loss respect to theta of this form. You can now fit polynomials. So all of these data points. Oh, thank you. Yeah, the student raises that I missed my factor. Other questions. The question is just num pi used denominator or numerator. So NumPy is a collection of like a Foxconn's operations. But you can keep track of whether like how you define the gradient with num pi citation. It's up to you. And so you define the derivative of a scalar with respect the conductivity, conductivity denominator reaction. And the rest would even later. So don't fight doesn't have a preference if it's just a way for you to do these compensations. The question is, what is linear regression? So this solution here that we derived for the data is the least squares solution. But then a linear regression or linear model, or an affine model would be y equals b plus a one, x one. So the linear part refers to the model. This gear is not a linear model because it has non-linear functions of x. The question is Theta transpose x that we derived? We can generalize the higher parts of that. Yes, and that's what we're doing here. Oh, the question is, what if x were apertures? Does this still work out? The answer is yes. Oh, yeah. Error on despite here, there should not be a one here. Alright, so use this to polynomial functions of x to your data. And I have now a question on the slide. A higher degree polynomial will always get the provided data as well as the lower order polynomial. What I'm saying is that if you were to measure the error from these blue points to your model, prediction, error will always be higher. If I make my polynomial higher order or not better, it will always be worse than it will always perform as well as someone told me why this is true. Perfect gas or the student says, in the worst case, you could always set the coefficients of the higher-order polynomial term Cicero and model lower one. And that's exactly right. So if I had the model, let's say I had a third order polynomial is the plus A1, B1, A1 plus A2 x squared plus a3 x cubed. This is a third order polynomial. It can always do, as well as model B plus A1, A2, and A3 to be equal to zero. All right, so when I add more polynomial terms, As long as A2 and A3 could be set to a value, a non-zero value, that makes the error smaller and therefore it will make the loss. Any questions there? Yeah, Tom, we asked you can set a you can make the loss of the road by fitting a polynomial exactly to these data points. That's correct. If you make n large enough, eventually you're going to get a polynomial. With that, we'll go through every single data point. Alright? So it is possible to get the losses the road by making a really, really high order polynomial. Which leads us to our next question. Why don't you want to do this before? That's the question. The question is, is there a ever only one solution to the least squares problem when the loss function is convex in the parameters as it is in this case. Yes, there's one solution which is the global minimum. The question is, can I clarify the notation for this class? When would I want to write y hat and x hat? There will be a general notation for the uses of hats here. Because I wanted to say that this vector of features is related to x. I want to keep the variable x, but I just chose to make it different than x. So there won't be anything consistent with these attacks. I could have called the other questions. Okay. So if you were to look at this data and I want some to answer this without using the word overfitting. Why is it not? Gets to make the polynomial n arbitrarily high? The provided data very well, but it may not fit the underlying distribution well. That is correct. Another way to say that is there could be data points from the underlying distribution that we haven't yet seen. It's just one data point here that wasn't in the training data. So the high order polynomial fit this data really well. But if we were to predict which model actually has the lowest average for this data point over here, it would be the linear model. So in general, when we build machine learning models, right, we don't want them to only work on the data that we provided it. We wanted to generalize well to data that it hasn't seen. So this leads us to the concepts of overfitting and underfitting, which I understand it again, obviously for most of you. But to finish it up on the same page, we want models that are general, that will generalize well. This leads us to first, so first I'm going to talk about training and testing data, but then in a few slides for them to just another notion called validation data. The idea of generalization can be made more formal by introducing two datasets, the training data and the testing data. The training data is data like these blue points here. There are used to fit the parameters of your models, but theta's, the a and the b. After that, testing data is excluded into training and testing data are like these red axes that weren't used to learn the model. Through which we can compute the error of our model by measuring what the law says with respect to these red x's. Again, there's also validation data and we'll talk about that in just a few slides. But at the highest level, we define this notion called overfitting. Overfitting is when a model has very low training error, which means that it models the blue points really well. But it has high testing error, which means it does not model the red axis, right? So that models are training data very well, but it doesn't generalize if you are in that scenario than your model has. One thing which we'll see later on is that more data helps to avoid overfitting. You think about polynomial that can go through every single data point. But when you have a ton of data points, that polynomial will no longer go through every single data point. In fact, if we look at the models for the polynomial is going from n equals one to n equals five, almost close to linear, right? So even for the polynomial with n equals five, even though it has x squared, x cubed, x to the third coefficients are closer to zero and the actual fitted model is linear. So more data helps to avoid over fitting. There is also somewhat more estimation underlying distribution. Distribution. So let me repeat what I said that she wanted to clarify. Another way to say this, which is having more data helps you to learn the underlying distribution better. Model the underlying distribution is better at a training and the testing data have the same distribution, then you should also generalize better to testing. Great, yeah, so Andy's question is, when I say more data, what is that relative to? Because presumably you increase the order of the polynomial, we're going to be even more me, that's good. In general, when we have many parameters as we will have neural networks, you will need commensurately more data to avoid overfitting. Right? So in addition to over-fitting, there is something called underfitting. Underfitting is the idea that we cannot think of model really simple. So there could be e.g. data that comes through a distribution that is polynomial. So this is data that comes from a distribution that is up to third order polynomial. Here's the Python code for that. If I were to assume my model was y equals a x plus b, so that I can draw a line through it. No matter even if I find the optimal values of a and B, I won't do a good job at predicting the, from the y-values, the outputs of these blue points, because this model is overly too simplistic and can never capture some of these interesting nonlinearities in the data. So going through the same method that we derived, you can fit these polynomial models, this data, and this n equals one model won't capture the curvature and this distribution and it's in these points. And so a model is called underfitting if it has both loads of both high training error and high test error. Since on those definitions of over 100. Alright, so then that leads us to one more thing which is called cross-validation. So in our model, you'll have noticed that there's something that we had to choose beforehand, which was the order of the polynomial, right? I had to set what little n was equal to, say n equals four. And then after I chose little n equals four, I did my gradient of the loss with respect to theta to calculate the F1, A1, A2, A3, A4, right? So the B, A1, A2, A3, A4 are my parameters. But this thing that I chose beforehand, little n, is called a hyperparameter, is something that I choose before I do the optimization. And this hyperparameter is quite important because it defines the model. How do we choose our hyperparameters for? The question? Yeah, great. So the question is, how do you decide that? What is meant by the training error as high as the validation testing error is high or low. So we'll talk about this more when we get to gradient descent, but it's usually a relative thing. So when we look at our loss function, and this is for training generally as a function of theta, but as a function of the amount of time that we trained for, the loss function will decrease and the slope decreases will be in front and we'll talk about that. But then for testing or validation, let me write validation because in general we won't be in this situation, although actually not having introduced validation here, let me call this test. The test error will also decrease, but then there'll be a point where it begins to increase again. So it's a relative comparison. In this regime here we would say that we are overfitting now because we haven't gotten your training error but higher, higher. Alright? So again, we were saying that a model will have hyperparameters. Why? The order of the polynomial, okay, I have to choose the forefront to the optimization. And we may be wondering how do I choose the optimal polynomial order. To do that, we also introduced this notion of validation data. And validation data is data that is used to optimize the hyperparameters of your model. So it allows you to choose what is the best little. Alright? How do we do that? What we're going to do is we're going to divide our training set, or sorry, we're going to divide our data into three sets. The first is the testing data. The testing data you should think of as with pelvic samples, Christina and set aside. And you basically get to look at this testing data once after you've set all of your hyperparameters and then the parameters and the testing data is just satisfied. You get to query it wants to ultimately scored or model. Okay, so that's what the test data is. Partitioned away from your original data set aside and use wants to just get a score on your model at the very end. Okay? Any questions there? Okay, So that's testing data, training data and validation comes from the remaining data that you have leftover. And what you will do then is you'll withhold some data and validation data someday. That's training data. Training data used to train your model. And then validation data is used to score your models generalization on data. This might be confusing, so we're going to write this out. So there's a process called k-fold cross-validation. This k-fold cross-validation is going to assume that you already have a separate testing set. And what we're going to do is we're going to take the remaining data that we have, separate it into a training and validation data set. Let's say that for training dataset contains and examples. Let's say that is 800 examples. Okay? So we have big N equals 800. Let's say we're going to do four fold cross-validation, which we often call CBD. Okay? Then what we're gonna do is report to split the data into k equals four equals sets, each with 200 examples. So we're going to have four folds, each worth 200 examples. Of this force will be used as training data. And then one of those poles is going to be used score model, which is called validation data. Alright? And then you can swap between which are the training data, which are the validation data. And what that looks like is the following. So I've drawn two examples here. The first example is very rare, so I'm always showing this to you as something that you can do. So in the case where your model has no hyperparameters, you can split your data, your original data into train and folds in green, testing fold in red. And because you have no hyperparameters, you can just train on these four folds of the data and then test on the fifth fold. And testing on this pitfall will give you a score of how good your model is. Then after that, you can make fold one, the red testing data and make folds two to five degree trended data. And you can do this five times together. An estimate of what your average test error. This is very rare. You will never see it in the class. So I've removed it off this slide. And this is what k-fold cross validation will look like. So let's say that our original data has 1,000 samples. Alright? What I will first do is I'll make it successful. This test fold will have 200. Examples in the remaining training data will have 800 examples. Now this tough fold is put aside. I'm never going to touch it until I'm done with all of my machine learning optimization. And then I get to query the test fold, wants to score my model. My remaining 800 examples. I want to make a bottle as good as possible, which means I didn't choose the best value of this hyperparameter little pen. And then you'd also do my optimization of the coefficients, a one, A2, A3, et cetera. So what I do is with my 800 examples, I'm gonna do, in this case, four fold cross-validation, which means I'm going to split this into four folds, each with 200 examples. Yeah, tomboy saying, do you stumble the data before you assign the folds disease, you could idealize shuffle your data before you assigned to e.g. if you just kept in order and this was like SeekBar time unfolds. One might contain a lot of cars and horses, but the cats and dogs mail will be important when you want somebody to sample all the different classes in each of these faults. So what we do with these oracles is this is used to learn theta. Then this is used to assess how well does it generalize to unseen data. So this is our validation fold. Right? So I would set M equals one. Then learn AND from my green training data. And then I would assess its performance on the withheld data. And my validation. Set n equals to training again, assess performance on the validation fold. I was at n equals three and then assess performance again on the validation. Right? And so you can repeat this procedure first, then choose what the best setting up little n is. That would be the model that, and to achieve the lowest validation error, let's say it was the n equals three bottle, little n equals three. So the third order polynomial, I got the best validation error, right? Little n equals three. Train a model on all of my training data. And then at the end I can query it on the test together by one measure of what the losses on this test. All right, Let's talk with them and then let's generalize it to do it. But then I didn't make us. Great. Yeah. Alright. Yes, That's perfect. Yeah, So tomboy is mentioning something that I think back then, which is choosing the best value of n when we do four fold cross-validation. But we're also going to do is we're going to swap which ones are the validation in which went to the training set. So in my second go round, I'll make fold one. The validation and then falls to four will be training. And then I could also do one where fold 13.4 are training but for two or validation. So by swapping out each of these, I will calculate it for validation errors for every single hyperparameter, and that'll give you a better estimate of what the actual question. Yeah, so the question is for doing optimization of the hyperparameters on the validation datasets. That's correct. And then after you have the optimal hyperparameters, you could set them and then just train across all of your green data. Could just get the best out of all of your training data to build your ultimate model. The question is, how's a number of both optimize? A very common thing is to do five-fold cross-validation, where you've got 80% shining in 20% test. There's also something called beat one out cross validation, where your validation is one example. And if the rest of the training data, and they'll depend on your datasets. I said, How do you choose that? There's a book. That's how much does it's a whole field called next, which is devoted to this hyperparameter optimization. All right, so I want to say one more thing before we end. Which is, students will often ask, why is this testable left out? And what do I have a different validation dataset so that your model is not Crawford demise or fine-tuned to do well on the test. It could be that if you were to optimize the hyper parameter middle n by repeatedly looking at your passport, you might get a value of n equals three. But then you would have chosen the hyperparameter based off of predicting this particular dataset. And that might not be the model that generalizes the best out there interviewing. And so the test fold again, just think that you should clearly once so that you don't learn the particularities of it. They will come back. Yes. Recording. 
All right, everyone. We'll get started for today. So we have several announcements. Didn't hear me, I think he goes. Did you have several announcements? The first is that homework number one is due tonight, upload it to Gradescope by 11 59. And for the code, this is a friendly reminder that you need to submit the code that you wrote. So if the grazers evaluated, and what that means is that we would want to print your notebooks to PDF with their solutions and clocks built in. Homework number two is going to be uploaded today by the TAs, and it's gonna be due in a week on Monday, January 30th. Upload it to grades together. A heads up for homework number two and also assignments moving forward is that the assignments are going to have a good amount of Python coding in them. And so we really encourage you to get started early in this homework number two, you are going to be commenting k-nearest neighbors as well as the softmax classifier, which we will finish discussing it and derive it in details up today. Any questions on these first three announcements? Yeah. The question is, can I clarify what it means to submit dot PY files as PDFs? So in homework number one, there are no dot PY files, but later on in homework number two, moving forward, there'll be classes that are in d2y path and just wanting to be. So ignore that for homework number one, but burrito numbers. Are there questions on the first three-and-a-half sense? Alright. So there's a detail on TA office hours notes, but I don't believe either pronounced so because we have so many CA office hours. The way we tried to organize the uploading of thumb is that the TAs are going to upload basically a consolidated office hours notes on Friday along with the discussion video for that week. So we will upload notes from office hours for the TAs, but they will all go off together on Friday. Any questions that alright. And then lastly, I'm just due to some administrative reasons, we haven't yet hired the graders for this class. And so, apologies on my end. Homework number one is going to, there's going to be the array and returning your grades to you since we haven't get fired, our graders. However, we will, of course, I put the homework number one solutions on Wednesday so that you can check your work and understand it. On Thursday. Great. Yeah, we will upload the homework number one, which is on Thursday because they'd go off after the late deadline, which is Wednesday, right? Any questions about any logistics for the course? Alright, we're gonna get back into material back. So last lecture we discussed this simple classifier, k nearest neighbors. And then we talked about some of its weaknesses. And then we mentioned that we would then look at a classifier based on linear classification. This classic buyer ends up being at the output of most classifier neural networks today. Also, it comprises something called a linear layer or linear building block, which is a key component of neural network architectures, which we should also get to likely by the end of lecture today. So we mentioned that a linear layer will implement this function if x is your input and y is your output, they'll do wx plus b. And then what we said is that this vector y, if we're doing classification, can be interpreted as a texture of spores. So y will be a C dimensional vector, C being the number of classes. So in C4, ten, that's ten classes, so it'll be attendee vector. And then the first element is a score opinion classic one. The second element is the score of being in class two, etc. We talked about how this will compute a score for every single class. And we also talked about how this wx plus b is implementing linear classifier and ends up finding solutions if you just have wx plus b. When w, because we're going computing like a w1 transpose x, right? The score will be highest when the weight wi looks most similar to x. So if you've looked at the weights, e.g. if the car cost, you can see that they kind of present all cars. Were there any questions from the last lecture? All right, We'll move on then. So after we get the scores, what we need to do is we need a way to translate them into a loss function L. Because we don't wait to say for some setting of the weights w and the bias vector b, how good is my Softmax classifier this money earlier at predicting the correct class. So I need a loss function L to evaluate how good it is. And then after that, I need to know how to make w and b minimize that loss, right? So we're going to fill in those gaps today. And so like we talked about how we're going to do this. There's something called maximum likelihood. What maximum likelihood does is it says for some observed data, what we're going to do is we're going to find a way to evaluate the probability of having observed this data given the parameters of my models. We've talked about this coin flipping example where the parameter is the probability that it comes out to parents. And then we're going to apply this today to our softmax classifier. So when we change the softmax scores, which in general can be positive negative numbers, into a setting where we're going to use maximum likelihood is going to be critical that we are able to interpret the scores as probabilities. Alright, so to use maximum likelihood, I need to be able to calculate a possibility to begin with. So that's going to be done by the so-called softmax. So we're going to do is we're going to define score for class II. I'm going to call that a as a function of x. X is my image. And then Ai of x is gonna be the score that the image belongs to classify. So this is a variable that we want or just use the I element of that vector y. And we know that the score is given by W transpose X plus some scalar bias beyond. So I want to know the score of the image X being classified. All I have to do is calculate wi transpose x plus b. Again, the score could be any number as negative if you want or as positive as you want depending on the values. Is that wi. When we define the softmax function, the goal will be to change the scores for each of these classes into probabilities. And for probabilities, we know that there are two constraints. Probability has to be 0-1. The sum of all the possible probabilities for each class at the sum up to one. The way that we do that is with the softmax function. But we're going to do is the softmax, the image X being in class. I, would later find out that this is gonna be interpreted as the probability of class I definitely image x is gonna be e to the score of class I divided by the sum of e to the scores of every single class. Alright, so in a very simple example where we just have two classes, the score for class one is going to be a one, x is going to equal w1 transpose x plus b one. I'm going to talk through these just for the sake of writing. A score for class two will be A2 of x equals W2 transpose x plus, I'm going to drop these. Alright? So those are my scores for class one, class two. The softmax function says softmax for class one of x is going to be e to the w transpose x divided by the sum of the exponentiated sports broadcaster. So it's gonna be e to the w transpose x plus e to the W2 transpose x. And then softmax for class E of X is going to be key to the W2 transpose x divided by e to the w transpose x plus e to the w transpose x, right? So you can see for every single softmax, the denominator is always the same. It's going to be the sum of all the scores. The numerator changes is going to be the exponentiated score for that class. You can see that if I sum up softmax one and Softmax two, they add up to one. All right? You can also see that these are all going to be values 0-1, right? Because I'm dividing a score by, by some larger non-negative score. And so therefore, because we know that each softmax for class II is going to be 0-1. And we know that across all classes, they sum up to one. And what the softmax function does is it turns my vector of spores into a valid probability distribution over each class. Said a lot there, I want to pause and ask if there are any questions. I said Jake's question is, what is the advantage of using the exponential instead of doing one score divided by the sum of all scores. That's a really great question. So the first thing to note is that Any function that makes the scores positive will work as a normalizing that's already normalized probabilities. If they were negative, then we might not satisfied all of the constraints. The probability distribution you could do like absolute value of Ai divided by absolute value. But then if you asked me that it has a negative score, large positive score, right? But you can choose other functions that are monotonic. So some people say like, Oh, you can do to, to the AIX instead of e to the x. That's totally valid. Also, you could do E to the two to the AIX that's also valid. All of these will create a valid probability distribution. When you use ie. This is very common, but also has some relationship to information theoretic quantities. And so that's one reason why it's preferred. Excuse for perfect gas. It's always just another thing, which is when we derived by people, like we talked about last lecture, you see we take the log likelihoods to turn all those products into sums. If you have logs of ease, we know that they canceled after just give you an exponent and it helps to make some of the math simpler. Right? Yeah, so remain as mentioning that also has a strong nonlinearity. We're gonna see how that leads to his name softmax, that this function is almost by taking the maximum of the neck, is almost something that returns the maximum score. Although it's gonna be differentiable. The question is, would I repeat the first advantage of that information theory? I'm going to leave that for beyond the scope of this class, but nobody talked about it with me in office hours. Basically, if you choose the softmax function, you'll see that several information theoretic terms fallout. Another name for the loss function, both the ride today, the maximum I can put boss watching is called entropy. Entropy is a concept from information theory. All right, we'll continue on that. So that's the softmax function. And you're going to do is we're going to define the probability of an example. So this is going to be e.g. J. We're going to say the probability that example j has a label that is I. So I will be from one of the tank classes. Given that I know the image, which is x, j, and I know what my data is. Data are my weights and biases like gave me the scores. We're going to define this probability to be the softmax applied to the image for that class. Alright, so let me just write this out in words because whenever we see probabilities, we have a good intuition over them and what they universe. So this is going to be the probability that x j, j, this is just with Jacob, my training set. This is probability that image x j belongs to class. All right? Any questions there? Yeah. Great. The question is in the data matrix, x is each column. So if we're just talking about C4, C4 can we may have any images. And then remember for these first examples could restate them to the 30 72 vectors. So each X is gonna be a 30 72 vectors. So you can imagine we might have some big matrix X of all of our data. And it's going to be 50%, sorry, big N, big N images by, by 30, 72. So then every single row here would then just be one example. This would be x superscript one. The second row would be x superscript two. So x superscript j is just the state of new tricks. Great question. Other questions here. Alright. So let me just say it one more time. This is, we're going to assign, we're going to define that the probability of your image belonging to class I is going to be equal to the softmax function applied to your image for that class already. Remember this is going to, this is going to be based on the scores for Phi-Psi AIX divided by the normalized, divided by the exponentiated sum scores for the classes. Alright? Any questions there? Alright, then we're going to go ahead and derive what the maximum by, because it was. So what we wanna do is being able to take our data. In this case, we have datasets where this is our first example, right? Again, remember X1 will be some image. Then y one will be a stable. So let's say that the first image has got a dog. So X1 is the image of the dog. And then Y one is the label that tells me it's a dog. And then epsilon would be my image and maybe white and as a cat. So what I wanna do to do maximum likelihood ready? As I wanna do, I want to compute the probability of having observed all of my data. Which means I want the probability of having observed image one. And image one was a dog having observed image and that image and was account. Alright, and this is going to be conditioned on my model theta. So remember that Theta then is going to comprise my matrix W and my biases. And those are the weights and biases that control the scores for each class. And those scores in turn affects the overall probability of my bottles. Want to pause here and just ask if there any questions on this. Just writing out the most naive. Just running up the phone. I'm sorry. Great. The question is, what does it mean by the likelihood of having seen the data? So what we're going to do is we're going to have a model, w and b, right? And that controls the score for each class, right? So let's say that image one looks like a dog and it's payables and dog. So for image one, our model would have high probability if the w's and b's assign it a highest score for dog. If the w's and b's told me that image one is a car, then I'm going to have a lower probability. Because the probability of my model saying that image one is the dog is going to be very low. So this is a way for me to write out what my model to say. Image one is a dog and a cat, et cetera. Analogously, I always like to go back to the simple example when I'm thinking about this, the data is our observations. But we're giving like the sequence heads, tails, heads, heads, tails, tails for that point where we want to make sure that our model does a good job at predicting that we see a sequence of four heads or tails. Similarly, in this case, our model parameters, but tell us that each one has a high probability of being a dog. That's a high-quality bean, et cetera. Good question. Any other questions? Great. Yeah, so Rockford is going to hit that magic, something which will bring us for our first step. So in general, when you see a probability with many terms, that is going to be something that's very, it could be something that's difficult to compute. We have to start making some simplifying assumptions to be able to actually write an expression for this profitability. Assumptions might not be true and the board, they are untrue, likely the term that we compute will be more off, but many of the assumptions that we make are not totally unreasonable. So the first assumption that we're going to make is that image is conditionally independent of dimension j given my parameters. What that is saying is that what that is saying is that I treat each data point as my prediction for data points to is going to be conditionally independent of my prediction for Data Platform image to image one. Okay. Any questions there? Yeah, from each one? Based on that. The question is, isn't that basically an approximation? Because images can have some correlated features of sorts that my model might pick up on. And being able to do an associate that might make this assumption not totally valid and not That's correct. Yeah. Given data, they are independent. Given my model. Alright, so at this point, we're not going to make our next simplifying step, which is ever going to apply the chain rule for probability. So I'm going to break this in to the probability times the probability of y given x psi. Alright? Remember that in words, is probability of y given x psi is saying, this is the probability that my image x belongs to the class. Why? Alright, and this thing is going to be our softmax probability. I didn't even pass rate e.g. the thousand samples. Samples or samples from the bulk glass. And Muslims. Tomboy asking you, what is the dependence? Hear me. So the Independents here is the identical fire sorry. The identical distribution is referring to the samples. X i, y are. When you're seeing the samples from the same class type of distribution, are you talking about p of x given y 0? I'm talking about, let's say. But ultimately, if you wanted to raining or snowing identically distributed according to this training samples. Even less, all the samples we go get the same distribution that's provided. So when we see identically distributed here, what we're saying is that p of x i comma y given Theta will all have the same distribution. P of y given x, I will always be the softmax. And I think if I understand what you're saying, tomboy, you're saying that if you have the probability of an image given its class, why I equals k or something, that this will also be a different distribution in general for each class. And that is also true. So general the distribution of exercise, which are the images given the classes will also be different, but the identical distribution refers to this term here. Actually this is a, this is a great question to make sure that we're all following. So when we apply the chain rule for this term, we do the probability of the image and then the probability of YI, the label given the image. We know that in layman's terms, this is the probability that X belongs to the class. Why? I could have also rewritten this in the following way. I could have written product from I equals one to n of p of y i given beta times this term. Next time I mentioned p of x given y i. And fair, right? I could have used the chain rule in the other direction. Why don't I do what's right? Great. So the student thinks like from a practical point of view, the goal of a classifier should go from x to y. And because we're going from x, y to y, I didn't say this, but this is kinda where the actors going. We're going to have to calculate a term that takes us from x-i to y-i is gonna be our classifier for which we have a term is soft match probability already computed. So in essence, we have this term to find because that's what the problem that we're doing. And that is correct. That's one reason we want to go with this expression. How about another reason we prefer this one over this one in terms of complexity. Wonderful. Yeah, so this student is saying, for a given class, there are many different images that belong to that class, which is true. And that's kind of getting at this answer, which is, if I were to write out this probability P of x given y, right? What does that mean in words? It's saying, let's say that the class is dogs. So let's say that the Y is the label for dogs, is saying, what is the probability of how images look like, right, of seeing this image of a dog, which is a 3,072 dimensional vector. When, what is the distribution over images that come from the class dog, right? And in general, this will be a high dimensional complex distribution. There'll be a 3,017 and I shall distribution. Whereas the softmax vector saying over my ten classes, right, which is a simple distribution with ten possible outcomes. This is much more easier to estimate or approximating this thing, which can in general be intractable. Any questions that you recently had a good following. Okay, great. Majority of class question. Intractable. When I use that phrase, music, We can't write it there. So it would be saying, well, what kind of distribution can we write down? That's 3,072 dimensional, which would model the distribution of what the dog images, stuff like. Tom Waits question was just pointing out which distributions are identical. And Tom, I was making the point that this distribution, which is if k is equal to the dogs, the distribution of but dogs look like there's going to be different than the distribution of what Casper quite et cetera. We have nothing that those aren't ethical. We've assumed that these distributions are identical. So we haven't assumed that p of x given y is identical, right? Yes. Thank you. So the student says it's a big idea, but the probability of all the pictures being classified correctly is the product of each individual image being classified correctly? And then multiplying it all come together. The answer is yes. All right. Last question. I'm sorry. You said we didn't tractable, Why would it? Oh, yeah. The answer is, are we using Naive Bayes? So yeah, if you know that I Bayes classifier, that will work. But the more troubling thing is that you will need to assume a distribution over the natural images. And probably the approximation you make there would be very poor distribution. Yes, it's almost making one last point here. If you were to actually learn this distribution, that would be very cool, right? Because then you can say, I'm going to set y to be the dogs. And if I can draw from this probability distribution that I can generate images of dogs. We'll talk about some of these generating distributions at the end, the classroom effects of games. And okay, let's move on for now. So what we're gonna do is I'm going to take this expression over here. And we're going to actually write out what this means in terms of our models that we can derive our likelihood. So I'm going to take this expression just copy and pasted from the last slide. And when we do optimization, our goal is to make w and b, which on my parameters theta, to optimize them to make this likelihood as big as possible. The first thing that I'm going to do and going from this step to this step is I am going to just erase this term. Alright? So from going from here to here, I just erased the first term. And someone raised their hand and tell me why I'm allowed us to do. So. Great. Yeah, so let me write out what, let me write out what this probability P of XI given theta is. First off, let me start off actually with a simpler question which is, can a student told me in words what this probability distribution means? You will go with, you got that. Wonderful. Accept this. The probability that you see this image or this image occurs given are setting up my w and b matrix and vector. I'm going to say that this is equal to p of x, sorry, and discuss the student answer. Because the probability that you're going to see an image of a dog, right? Or the probability of an image of a dog in a hurry. It's going to be the same irrespective of my bias is one or two, right? The been totally unrelated things like the parameters of my model to whatever I want them to be. That's not going to affect anything about how the images, images are data that are given to us in this setting of W and B have no impact whatsoever on what the images look like. And therefore, P of XI given theta equals p of x sine. This is true. Then P of XI given theta has no dependence on theta. And because I am going to be maximizing this expression over theta and this term, does it impact data? You can imagine that it comes up, that I can just remove it because it won't change what my final setting up data is. Alright, any questions there? All right, and just in general, always helpful to make sure that when we write gotten distributions, they aren't mysterious to us. For the distributions that we write down, we should always be able to say intuitively what it means in words. And that'll help people with simplifications like this. All right. Alright, we're going to continue our work here. So this is going to be equal to ArcMap data. And recall that we define this probability distribution here to be softmax side and now voting, overloading variables. So this probability here is going to be assigned softmax. I'm just gonna be the softmax score for class. Why are they? So I'm going to give us superscripts right here. So I'm going to write this in terms of my bug, in terms of my softmax is one other thing I want to do is I'm going to take the log of this so that the product turns into summation. So I'm going to turn this product over examples n into summation. So it's gonna be argmax I equals one to n. And then have log of these probabilities. And these probabilities are gonna be those softmax course. There's going to be var log softmax. That example, I belongs to class label y i. All right, we're going to then go ahead and expand this. So I'm gonna write that this is equal to arg max over theta sum from I equals one to m. And then I'm just going to plug in the definition for softmax. So this is gonna be bogged. And then the numerator is gonna be the score of the correct class. So the score of the correct class will be e to the w transpose x plus BA II Plus BYUI. Then it's gonna be divided by this for the rest of the class, it's just gonna be divided by the sum. I'm going to use, I'm Jay to index the rest of the classes. So j equals one to see. And that's going to be E to the W j transpose x plus b j. Now sometimes people get confused by this notation, y being in the subscript. So remember y is one of the numbers 1-10. Let's say that y equals 4.4 is the class label for dogs. And so this is saying, compute the score for the dog exponentiated and then divide it by the score for the rest of the classes. So example I tells me what the correct score is, dog and cat, etc. This is saying compute the score for that correct class. Any questions there? All right, so we're gonna go ahead and simplify this even further. At this point, I'm just going to go back to using that notation that we had, where this w j transpose x plus b is just going to be the score. Ha, so I'm just gonna do that. So I have plus routers today. So this is gonna be argmax theta. We're going to have a sum from I equals one to m. Now I'm just going to simplify log experts. So we're going to have log E to the score of class YUI. That's just going to equal a YI x phi. Alright? And so this AY XI says this exponent over here. Then it's going to be minus log of the sum of scores across all. It's gonna be minus log. Sum from j equals one to c, e to the w j transpose X. Score notation e to the x. Then usually one other thing that we do is we'll make this an average across example. So I'm gonna put a one over m term here. This hasn't changed the actual answer because one over m is just a constant and that won't change the state that maximizes this term. Any questions here? The question is, why don't we take involved with Chicken balls to turn all of these products into Sunday? And so in homework number two, you're going to have to take the derivative of softmax with respect to w and v easy. And That'll be easier when these are songs and our products. Raised the question is, why did we use, if I understand correctly, y in the numerator and the denominator, this is so close, has this question. Okay. Usually, you see this is the most frequently asked question. Yes, sir. Um, remember that what we wanna do is we want to compute the probability of the correct class. Let's see, let me just write it out explicitly. Let's just say e.g. all right. The correct class is at y equals four. If we look at our students are ten, we can see that the fourth example is cat. So we're saying that example, I as a cat, y equals four. And this means I belongs to class four, which is a cat. And the probability image xy being in class for, right, we'll just write as softmax. Softmax for objects. Alright? So this would be the probability that xy belongs to class board. I wanted to write this more generally then for all of my examples from I equals one to m. So instead of putting this explicit subject for, I'm just going to say whatever YI is for that example. That will be the softmax publicly. The question is, what is this probability? What is this probability? This is the probability that given my model, you observe that the image xy. We observed the image XI and its label such, such as like y equals four mini that this is an image of a cat and this is cat, Right? Yeah, so y is going to be the label that I asked. And I want my softmax to set the parameters of Theta such that y1y was equal to four. Softmax for an xy would be a large number, something closer to what? Yeah, softmax for class three would ideally be zero or something close to zero. The question is, is the probability of the incorrect class? Can you go in this expression? It is not, however, it is implicitly in there because this is a distribution that sums up to one. So the probability of cat is high. The probability of dog and everything else has to be less. Okay? So we're gonna go from this expression. This is again just plugging in the softmax. And we're going to do one last thing, which is this is maximum likelihood, but we know that in general, the convention is that we have a loss function that we want them in MRI. So to make this an RNN problem, we're going to use that property where if I have some function of Theta, now want to match it over theta, this is equal to minimizing negative f of theta over theta. So this thing here equals argument over theta, one over n. Sum from I equals one to n. Remember this over my examples and then I'm just going to flip these two terms together, my negative sign. So this is going to be log sum over all my classes. E to the h minus y. Alright? This function here is going to be my likelihood that I will desire to minimize. Honor. Yes. The question is confusion over this notation here. Oh, yeah, it's confusing over the notation W subscript y. So let me write this also more explicitly. So if I were to write, this is important because you all need to implement this on homework number two, a phi of x phi, which is this term here. This is defined to be W for the correct class Y i transpose x i plus B, y, and z. For ten, we have ten weight vectors, W1, W2, all the way up to w ten. And then we also have ten biases, v1, v2, all the way up to be taxed, right? That's an art have been here model. When I say a YUI, what I'm saying is that when you get Example I is gonna be an image. You're going to get a data example with XI YI image. I'm just going to come with a label and that label y will be labeled image. And I will be some number 1-10. So if e.g. I, they face a cat, right? And he saw before a cat corresponds to y equals four. Then a y i x i will be a four. So this will be W4 transpose x plus b for e.g. are e.g. I. Plus one. It can be that y equals ten. Then this numerator would be e to the w transpose x plus b times et cetera, et cetera. Right? Right. The student is asking, so let's say that y was equal to four, so it's a cat. What is this expression actually equivalent to in terms of thinking of like five linear classifiers. So what it would correspond to is what you can do is you could take your road W4 transpose here, dotted with x and then add your fourth price here. Or you could just take the fourth element of this Y matrix, a wide vector. Both of those would give the equivalent thing, just the score of the image being in class for. Yeah. The question is, why did I add the one over n? You don't have to add the one over n. Remember the one over m is just a scaling term. We added so that this loss is basically the average loss per example. So it's just a normalizing term that says, okay, this is going to be like physically my loss, e.g. but it isn't necessary. Alright. Go ahead and take our five-minute break. Given the number of questions, I think I didn't do the best job of explaining this, so please take some time to look over it with the five-minute break and then when we come back, I will be happy to take other questions too, hopefully. Two questions. The first one is directly given theta equals P x, y term. You say that the px py doesn't matter because even though we get, we are dealing with different Thetas. Dxi is still the same. But I wanted to ask like, we, you have a whole data set like say 60 or I could call a hundreds of patients dataset. Split. This patient's ear training set and testing set. Doesn't matter. It doesn't, because remember, this is the true distribution over images or whatever data that you have. That distribution is given to you and you get examples. Distribution. You're not going to get, you're not going to just won't look different based off of your weights of one or two to the images aren't just something true before you do any. Assuming this is the true distribution of the population we are analyzing, we are splitting Reimer with, right? Yeah, so that's the probability of drawing samples from that distribution matters here. Not for optimization. In general, it's going to be like something very complicated. But its parameters. Oh, yeah, awesome question. And you're like, what do you exactly mean by like this? P, X and Y follows the same ID. So that I can think of distribution means that this P is the same for every single bar. So it isn't that like for XI and XJ and YJ, the keys are different. What does that probability is that even the fetus and probability we are observing this hair. The same distribution across all of the sample to Halloween. The softmax distribution. We took this off. Yeah. Okay. Often. Alright. Everyone, any other requests on a softmax classifier? Right? Yes, that's correct, but it's gonna be negative effect. Remember we took the log scale because of the loss, can be any number, any number that is zero or higher. Okay, Other questions on this top path, this is super important and will be on homework number two. Yeah. Great question is, if the dataset was thought identically distributed, we happy to give weights to different problem. Well, actually let me just answer the first part. Identically distributed means that this P is the same irrespective of example. So this P was not the same. There are two answers to your question. One is we would still assume them to be the same because otherwise we won't be able to be very public and we wouldn't be authored by an analytical expression. And therefore we would just keep penalize on out for our assumption is. There might be some forums where they're not identically distributed. And you know, like they have some distributions that are different. You can define them, but when you multiply them together, they both comfortable. Almost surely a monster so-called conjugate priors. So when we say identically distributed, I'm saying that this P is the same for all the examples. And then when we break this down, that means that all of that means that P of Y given X, Y is always going to be assault. It answered the question. Oh, okay. The question is saying, if you do a good job in decoding, does that mean that there are some things are essentially valid that it was posted conditional independence. I'm not sure if that's always true, but I would strongly believe that the requisitions dependent. Sorry, I'm still stuck. The question is, what does it mean that these examples are identical? I should have just written this out given the confusion. So what it means is if I have p of X1 and Y1, X2 and Y2 given theta. I'm not saying that they have the same probability. I'm saying that they come from the same. I'm saying that the values of X1 and Y1 don't impact the values of X2 and Y2. So I'm saying that this breaks down to be the product of x1, y1 given theta times the probability of X2 and Y2 given theta. Which means that knowing that image X2 is say, a dog has no bearing on what X1 is and what it was. Last question here. The question is, why did I write this line and what does it mean? So here, what I was saying is, you could use the chain rule to break this distribution down in a different way. And we can try to optimize this, but we don't want to do that. Why is that the case? Well, in the first decomposition, this is a simple probability over ten possible outcomes. So we can model with softmax. This case. This is a distribution that's 3,072 dimensional. And it's very complex and it's very difficult to model in general because we can't model this distribution well, it's better to go with this decomposition. I still want to caution is on the independent identically distributed. So know that this is a very common assumption. They overdid examples to simplify machine learning problems. If you don't have the assumption of data being ideas generally difficult to write down any likelihood. So this is sometimes an assumption that we live with, even if it's not true, because without it, we probably couldn't simplify our expression any further. Okay? Alright, we'll move on for now. So I have a loss function, and now this is our softmax loss function. We haven't yet figured out how to get the optimal parameters, but I'll be just a few slides here. That just means it's a time I'm going to ask you to review on your own. But these are essentially, we'll say that the softmax classifier, it gives a reasonable score or loss to every example. So if you worked through just plugging into that loss function that we derived the actual values of the scores for each class. You can see that the car loss is close to zero, which is good. Remember for a lost, Zero is good, far from zero is bad, so hires worse. So in this example, the corporate car was 5.1 and it was indeed a car. So the law should be low. In this example, which is a bird. It turns out this score for car was higher than for her to associates that. But she gave us a large loss. Right. Running through, plugging in some numbers the sanity check. Then I wanted to also give some intuitions over this loss function and the softmax. Afterwards, then we'll get into gradient descent. Alright, so a few months that the softmax classifier first is where does the name actually first? Tried to break down the terms and see what happens. So if I want to look at the probability that the label is why I give him some image X, right? We know that this is going to be that term log of the probability that X is in class YI. If we're looking at log probabilities. When I simplify this expression, I get this term which is a YI x minus log of the sum of the exponentiated scores. This is the exact same term that we had on the slide. This slide right here. This is softmax for class Y-bar. What I want you to notice about this is that if we want to maximize this term in the maximum likelihood setting, we make this term as big as possible by making this, this term here, a y axis biggest possible. Alright? The reason or the way that you can see this is that if a y x produces the largest score, which means that we have the largest score corresponding to the correct class YUI. This term is almost equal to zero. The way to see that is because the exponential exaggerates small differences. So let's say that the correct class was, let's say that we had three classes. And let's say the scores were 120 and minus one. What this term is saying is take the log of the sum of the exponentiated scores. E to the 100 is a lot bigger than each of the 20 is a lot bigger than e to the minus one. E to the 100 plus 20 plus e to the minus one. It comes close to equaling e to the 100. And then what is the only take the log of e to the 100, I'm gonna get back out the score of 100. So this term right here is approximately equal to the maximum score. As long as x is core dominates the other scores. So in the case where you have the correct for us, then that means that, sorry, in the case where your classifier's predicting the correct classic, it's the largest score to the correct class label. This B will be equal to AY x and this is equal to AY AX, and therefore, this log probability comes out to be approximately zero. However, an incorrect class produces the largest score, right? Then this value will be more and more negative. And so your log-likelihood will be more and more negative and your model anymore and probably, alright, so once we do that, I need to check these received that this thing will indeed be maximized when it assigns the highest score to the correct class. That's wanting to. Another practical thing to consider is something called the overflow because softmax, so let's say that I got a vector, I had three classes. And my scores were as follows. They were 500, 404, 50. If I go ahead and start to exponentiate 500, I'm gonna get an enormous number. And sometimes these scores will be so large that we won't be able to represent them with floating-point precision. Alright? So when the score of a class is very large, each be AIX, may be very large, and that will cause numerical overflow of my five 35%. So one standard way to address this is to normalize the softmax functions by just subtracting a constant score from everything. So in this case, subtract the largest score. So I would do -500 so that the scores shifts is zero, -100 -50. And now if I exponentiate the scores, I won't have overflow. Calculating the softmax on this vector. And this vector will give you the exact same probabilities. And I will leave this since it's a very simple exercise to you all to see that this is the case that you could take your score and just add some constant to it, and they'll give you the same softmax probability. Alright? So usually, again, what we do is we take our scores and then we'll subtract the largest score from every single element to renormalize all the scores back to the same song, hence probabilities. Alright? Any questions there? All right, so there is one more classifier that we used to teach in the past called the support vector machine. And it's going to support vector machine uses something called the hinge loss. This was back in a time where maybe you might want to put an SVM instead of a softmax classifier at the end of your neural network. But these days everyone puts a softmax classifier that record. So this is relevant. That said, we've still putting these slides. To it in case you were curious to see how we teach the support vector machine and the loss function, but you won't be tested on it in an exam. Homework number two, we are going to have you play around with this hinge loss just to test your understanding of loss functions. And if some of the operations that we've talked about, but we won't test you on the support vector machine on exams. Let me scroll. Keeps going through these. All right. Okay. Any questions on softmax? Alright, so we're back to this place where we have a gain or loss functions. And we know that the parameters of our softmax are going to be that the parameters beta will be the big matrix W and this bias vector B. And we want to optimize to choose w and b to make this loss as small as possible, right? So now the question is, how do we find the setting of w and b that make this loss of small as possible. And then this will be now or refresher on gradient descent. And an after that we will get to neural networks. So this is the last from 146 are your first machine learning class. So you'll recall that in the first example that we did in class, we took a gradient. But the loss with respect to your parameters you did on homework number one to that case, you are able to set the derivative equal to zero and solve for theta. Alright? And the reason that was, was because for those problems, it turns out that if you look at our data as a function of theta, it was so-called contracts. There's one solution. And if I set the derivative equal to zero, I'm going to get the optimal value of Theta that makes that a smallest possible. In other settings, particularly for neural networks, the loss function is never look this nice. They are in general are going to have many local minima. So if you go ahead and you take this loss function, you take the derivative with respect to theta, you set it equal to zero. You're not going to get a closed-form solution because you can even just see from the image there are many such solutions where Theta equals zero and usually you won't be able to come up with something. In these cases which can fight for neural networks. We're going to get us to still get to the bottom of his class functions. And that solution is called gradient descent. So we want to minimize some function f with respect to our parameters Beta. Here we call that x. And what we know or at some setting of my parameters x. Alright. I can apply my function, a neural network or a softmax classifier. They'll give me f of x. We know from a prior lecture that the gradient tells me that if I change x by a small amount, in this case epsilon. If epsilon is sufficiently small, then the amount that f of x will change can be approximated to be linear. And that's going to be f of x plus epsilon times the derivative of f with respect to x. So this is a linear approximation. We'll draw this out in just a few slides. So this derivative, f prime of x tells me how I can make my f of x smaller. And if we are eventually able to reduce f of x such that f x is equal to zero, then we would have reached a stationary point of the loss function, which we'll talk about in the next slide. In this class, we're going to be interested in doing gradient descent, where our parameters are vectors or matrices. And we'll talk about, we've talked about some gradients already and you've done several of them homework. Number one, we'll have a few more rules that both arrive at the neural networks. Great gas a timely it's also raising the point that you might see in the blue case where you have one global minimum. But you may still need to use gradient descent because you may not be able to get a close form solution by setting for DLD beta equals. Alright, so to do this, we're going to use gradient descent. First. Some terminology that I hope is a review for all of you. If we have some parameter, I'll call this w1 for wait one. Alright? And our loss function looks something like this. The global minimum of this loss function corresponds to the value of the loss that is the minimal over every selection of every potential value of w1. So there's no point at the blue curve that goes lower than this point, then we would call this the global minimum. Will also have local minima. And these correspond to places where the derivative is equal to zero and the curve. The curve has a second derivative that is positive. And at these points, local minimum if your app is such a critical point. But the value of the loss is violet and a global Min com, alright? And then there are analogous definitions for global maximum and local maximum. Now lastly, there are things called saddle points. These corresponds to points where the derivative is equal to zero, but it isn't a minimum or maximum. So this point here, the derivative is equal to zero. If I increase my way, I will lower the loss function. And if I, I'm decreasing my weight, I will increase the loss function. So that's called a saddle point. Any questions there? Hopefully review for everyone and every call them. But we also have defined this gradient in lecture two, where the gradient of f of x with respect to x is going to be a vector that's the same size as x, right? Where each element is D f of x dx with respect to that, with respect to that index offense. Alright? We all talked about how if I were to move x but still not delta x. So let's say that delta x is comprised of these small changes, delta X1, X2 down to delta XN. Then if I know x results in a value f of x, then if I were to add delta x, Delta x is sufficiently small, like first-order approximation of how it affects what change is going to be given by f of x plus h transpose the gradient x. Remember this term here is a dot product that's doing delta X1 times d Fx dx1 plus delta x2 times d f of x dx to saying, I'm going to wiggle x one. This is how much I expect f of x to wiggle. If I change x one, I'm going to sum this across the contribution from x1, x2, etc, all the way up. Got any questions there? All right, and then what we're also going to do is we're going to do fine something called a directional derivative. This is how visible we're going to use to derive the gradient descent rule. The directional derivative is going to tell us the direction, or it's going to be this quantity, U transpose gradients. Alright, so this term looks very similar to this term. The only difference is that the directional derivative has a unit vector u whose norm is equal to one. And so it's saying essentially if I stuck in this direction, how much do I expect my loss function to change? Apply? Any questions there. All right, so if I want to make my f of x, which in general against the loss function as small as possible. What we will want to do is we want to step in directions that make f of x smaller and smaller. I remember when I step in a direction, you write the value of f of x I expect to change by U transpose gradient. Alright? So what we can do is to minimize f of x. We wanted to find the direction to step in to make f of x decrease the fastest. So the amount that will change is given by this expression. That's this term right here, with delta x replaced by you. And I want to choose the direction you to make this as small as possible, meaning that my other vaccine is going to decrease as much as it can. If I go ahead and read that this dot product to be equal to the norm of u times the norm of the gradient times cosine theta, where theta is the angle between them. Alright? Because you have norm one, this is just going to equal, there should be agreed norm sign here. This is going to equal the norm of the gradient times cosine Theta. I remember, I get to choose a direction you to make this as small as possible. This first term doesn't have a u. So all I can really control is cosine theta, which is the angle between u and by gradients. And this quantity is minimized when you points in the opposite direction of the gradient, is that cosine theta equals minus one, right? So this is a rule that for gradient descent, if I want to make my function f of x becomes smaller, I will watch a step in that direction. I will watch it changes direction as opposed to the gradient, which means I take my gradient and I multiply it by minus one. This gives us our gradient descent rule, which is to say I'm going to take the gradient on my function f at some location x. And then I'm going to change x by subtracting off. Some small number times the radius. One is called the learning rate here, and we'll talk about it in the next slide to talk about having to use their questions. Right? Yeah, so I should have actually changed this. X here is analogous to theta. And the vector is analogous to our loss function. Sorry, sorry. Oh, yeah, sorry. Rocks at this saying this Beta is not the same as district or the state of carrots angle between the previous night. Tomboy is asking the question, why did I get this optimization problem of I want to choose a direction that minimizes this, right? So remember, my goal is, this is a loss function f of x. I want to make it as small as possible. So if I change x by Delta x, my first-order approximation, I expect Exit changed by Delta x transpose gradients. Alright? So I wanted to, f of x is smallest possible. I want to make this thing here as small as possible. I can make these things as small as possible by making delta x smaller norm. But what I really want to know in which direction to take a step into make this thing as small as possible. So that's why we constrain the spectra you to have unit norm. So we're just trying to derive if I were to go in any direction, what direction would be best. And so the direction would be best mixed epitaxy small as possible, which means that this number is a smallest possible. And that's why we end up minimizing this expression. Find the derivative of a scalar. Yes, it is, yeah, So the loss function L of theta will be a scalar. And then our parameter, such as like the vector w1 will be a vector. That's correct. Right? I'm a student. I think you're replacing the point that direction that would in which you descend the fastest may not be the best direction to go into. Yeah, so there are practical considerations here. We're going to get to that when we talked about optimization for neural networks. So let's talk about different optimizers that needs momentum and historical gradients. But for now, we're going to just consider the simplest form, which is if you just ask the classic, if you just have gradient descent and you can stop anywhere, What's the direction that gets me to increase the function the most? Oh, yeah, great. So the student may be referring to oversleep. We'll talk about that in just a few spikes. Other questions. Okay, So we're going to just show you a few examples. So here, let me change these two losses. So let's say that X and Y were two parameters, W1 and W2. And then G here was my loss function as a function of W1 and W2. And so this would be a setting where we have a loft surface and our goal is to change W1 and W2 to make this loss as small as possible. And the lower you are, the more blue you are. And there's a global minimum which is given by this star here. So this is just a very simple contrived example to give you intuition. This is a view of the same function just looking from above. And now we're doing a contour plot. So this here would be W1, this y here would be W2. And then the contours correspond to places where the value of the loss is the same. And then the color blue is a better loss of lower loss, and red is a higher cost. To implement this gradient descent and by conflict would be very straight forward. You will do it in homework number two. There's a bunch of code here, but it's actually mostly just for saving values of things. There's going to be essentially in homework two, some function that you implement. This function is going to be softmax, loss and grade grad gradient. Basically, this loss ingredient will compute both the loss function of the soft max that we just derived, as long as it's gradients with respect to your parameters w. And so this returns the loss at setting of W as well as the gradient. And then to update w, you would just subtract off epsilon times the gradients. So that's all. These two lines here. Implements. So it's a very simple algorithm. I have two videos that show stochastic gradient descent where this example that we have, the loss global minimum is at the red star. And in this video, what we've done is we've started off in this bottom right quadrant. And what we're doing is we're continuing to gradient at every single point. Something negative that gradient direction. And you can see that over time, although slow, it'll eventually get to the red backs. Alright, here's one more example where I'm going to start off the optimizer somewhere else in the bottom left quadrant and just follow the gradient of the contour lines. And we'll slowly through gradient descent to the global minimum or a local minimum. Any questions here? Yeah. Great. The question is, how does gradient descent avoid local minimums? It does not. So this reminds me that there's usually an intuition that I gave on the slide that I skipped over. When we optimize neural networks are going to be using stochastic gradient descent. And gradient descent. Once you get to a local minima, right, you just stay there. Stay there. Then there's a chance that for neural networks, we may do gradient ascent and arrive at these local minima that actually have high bosses. And if they do, then we haven't learned a good setting of the parameters. In practice, neural networks usually do really well. Alright, so what's an intuition for why this might happen? One intuition to carry with you is that neural networks have a ton of parameters, but let's say we are looking at neural network with 100 million parameters. Alright, here, and this loss function, I've drawn just one parameter, w warn. Alright? But you could imagine that there was a second dimension, W2. W2 was along here, and that this was now that there was also values of the loss function along other dimensions. So maybe along W2. The loss function looks like this. So if you look at this example just going from one to Mexican see dimension. What we see is that this point here, It's no longer a local minima because even though if I change w1 only, the loss will only increase. If I change W2, there's a dimension in which my boss can decrease further. All right, with two-dimensions. For neural network, there are 100 million for ambitious e.g. what that means is when you're at a local minima across 100 million different search dimensions, there is no dimension that you can step in that would make the loss even lower. And that's pretty bad because with 100 billion stars interventions, there's probably some dimension that you can go and that will make a hospital or an order. So if you just do this very simple approximation, Let's say that for every single variable at a saddle point, there is a coin flip at 50, 50 chance that this variable moving, increasing, it will either decrease or increase the loss. If you have 100 million parameters, then the probability of you finding a local minima where there are low directions. You can go in that increased the loss becomes exponentially small. And what that means is that in neural networks, it is very rare. It's exponentially rare to find a local minima with high loss relative to the global minimum. Or said differently in neural networks, most of the local minima ends up being close to the global minima. Again, because of local minima occurs, but you can't search one of 100 million connections for another way to decrease your loss further. In that case, actually having a lot of parameters is a good thing. Any questions there? Great. Question is, does gradient descent ever stop or does it just get closer and closer? I mean, basically it will just make it into somewhat velocity decreases are decreasing by very much. Yeah. Yeah. So the question is, is it just one loop that runs forever? We will either set some number of max iterations or else we'll set some tolerance that says if the loss hasn't changed by some small amount, sorry, the loss change is less than some amount. The determinate gradient descent. Yes, it's always saying that in this class practically, you likely won't even be implementing some of these checks because they could be more time-intensive, will just stop after some number of iterations. Okay, let me get back to gradient descent and I want to do one more thing to talk about what the next step size. So again, hopefully review for most of you and I want to draw this picture just to make sure we're all on the same page on what's happening in gradient descent. So let's say I just have one parameter w and I have lost Alec W. And then let's say that I lost surface looks like the following, like this. And then let's say that my additional parameter setting is this value over here. So let's say that this is my value of my weights. I'll call that w one because it's the first iteration. So at w1, the value of my loss function. So this here is the value L of W1. Gradient descent says take the derivative of the gradient. So the gradient at this point would be the line tangent to this curve. Let's for simplicity, just say that this curve here has a slope of minus one. So the gradient of w with respect to W is going to be equal to minus one. So this is just a line of slope minus one. This is a line of slope minus one. Then what gradient descent tells me is that if I want to update to get the value W2, that's going to be equal to w1 minus epsilon times the gradient. And the gradient is equal to minus one. So in this case is just w1 plus epsilon. Consider 2 s. The first is that epsilon is big. So let's say that I choose epsilon so that one, so that W2 is equal to epsilon, so that now W2 is equal to this value, right? Because W2 is a gradient descent will be w1 plus four. If I choose a font to be large, the amount that I expect my lost to have decreased. It's gonna be given by this difference. Alright, so this is my expected decrease in l of w, right? From my linear approximation using gradient descent. However, because I took a step too large, but as the actual value of LOW, it's going to be equal to this value over here. So if we take a really large step, we're in bad shape because my approximation no longer holds. It only holds locally. So I could be in a totally different part of the loss function. And then here I have emerged to take a gradient, right? This would be a very high slope area and you can see how they're intersecting could go wrong quite quickly. Then of course, the other option is to choose an epsilon that's relatively small. So we can choose epsilon small, which is epsilon small. Maybe epsilon is now this big. So this is my new epsilon and the purple setting and this would be my new W2. Then my loss at this point would be relatively closely approximated by. Okay. So any questions on that example? We just a nice refresher on how gradient descent works. What we could do is we can look at that example that I showed you a video of. And I could set epsilon many different values. If I said x quantity larger 0.1, we can see that actually here in this case, we're gonna be starting from the top right over here. When we take us up to 0.1, we stepped too much in this direction. We go to a region where the gradient is very high. And then with this far step size, we actually suck off the page. And so gradient descent but not converge for this choice of epsilon. If we make epsilon very small, we, as in this gold color, we go pretty directly to the global minima. And you only comment would be that because epsilon is so small, we have to do many, many iterations of gradient descent, so it's going to be slower. Then there might be some intermediate values like 0.08. And this is what tomboy and the students are there customer mentioning which is 0.08, you know, you do, you do eventually get to the minimum. But because it's effect size is large, you might have some zigzagging phenomenon as you'd get there because you keep overshooting a bit in different directions. So in a later lecture when we talk about optimization for neural networks, we're going to talk about techniques that help to ameliorate some of this zigzagging. But at least for now we're just gonna go with vanilla gradient descent. The question is, if epsilon is small, does that mean that we may need more training data? I don't believe so. I think if epsilon is small, it's just the case that you will take up various though we're looking at. Alright. So why not always use smaller learning rates? This is the part that says the learning rate with a smaller on the left and larger on the right. And the amount of time it takes to reach the minima is going to be a lot longer for larger, a smaller learning rate. Then how do you interpret or how do you choose the right learning rate? This is largely an careful exercise. The axon will be a hyperparameters. These are some example characteristic curves that you might want to add phosphate you do optimization. If you see the loss may be decreased or maybe it doesn't decrease but it explodes. That's a setting where you set epsilon to be too large and you definitely need to decrease it. You see the loss decrease very quickly, but then tactile. After just a few iterations, then you probably set the learning rate too high. And what this could reflect is that the loss faced similar because you took a step in the correct direction, but now it's like yours zigzagging or rapid County. And you're never really going down to value this zigzagging around a similar level of the gallery. So this corresponds to a high learning rate. You want to decrease it by more. If your learning rate is just for losses just decreasing to slowly, then it's probably to vote and why go somewhere intermediate where the agreements, alright, so that's just all heuristic at something to keep an eye on as you are because you're looking at these loss curves here. Sorry. Can you repeat the question? Right plane along the LV. Even knowing that, what do you mean by moving along the curve? Okay. Yeah. Yeah. So rupture the saying. And we'll talk about this when we get to the optimizer structure. Instead of a first-order approximation, you could do something called Bike. Their second-order approximation. We're just approximating the parabola that fits the loss curve over here. This is something called Newton's method or you can step, you can take. This leads us to able to take more livestock sizes and not have to worry about epsilon. But we're going to see that this has some challenges because it is going to increase our computational time by a lot. So in general, we won't pay for it. Alright? Any questions on gradient descent? You raise your hand if you're following. All right? Okay, So I just have some last things to say about gradient descent. First is a question for you all. When we asked you to implement gradient descent. On homework number two, you are going to lead to compute the actual derivative, the gradient of the loss function of the soft mask with respect to w. And several of you is going to be something that takes some time. You have to do a bunch of algebra to actually derive the gradient. There's some of you in the context of thinking about this. They think, Okay, I can compute an analytical expression for the gradient, or I could just actually using numerical gradient, right? I could, on my computer, take my function f, my loss function. I could just wiggle theta by a bit and calculate a numerical gradient and use that as the gradient that I used for stochastic gradient descent to the correct direction. Alright? Why is this not a good idea? Parameters. Perfect gas. So the student says, when you have millions of parameters, you have to assess this numerical gradient and millions of dimensions. That's the main hurdle, which is that to do that will be computationally expensive. And there's another factor here, which is that whenever you evaluate this, you have to evaluate the loss function. And at that loss function, the loss function for a neural network. Evaluating S means that you have to evaluate the output of the neural network. And that can be expensive. So evaluating may be expensive. So for this reason in the homework we're going to ask you to actually compute the analytical gradient. Then you're going to actually write code that implements what you could do it on paper. You won't always have to do this. Later on, after the midterm, we're going to talk about software packages like quite fortunate TensorFlow. And really there. What are their key capabilities is that they can automatically take readings for you. But at least to begin, we're going to be doing these gradients by hand. All right. Any questions there? May be expensive. Yeah. Ok. Involved eventually. Yes. A tomboy saying that when we do gradient descent later on with back propagation will still have to make evaluations through the network. Number two here is tied to number one, which is that if we have millions of parameters, we'll have to evaluate this numerically for each of those dimensions. And that'll be many, many more evaluations of that. Alright. So a few less points on gradient descent. So in this example that we've shown you the videos on renew the loss function F Exactly. And that's like every point in space we can calculate the gradients exactly. But in terms of building networks to classify CIFAR ten, we are going to be differentiating or loss function with respect to the parameters. From these loss functions with respect to the parameters will be a function of training data. You'll see that when you work on homework number two. And therefore, the gradient that you actually stuck in will depend on the actual examples for training them to use. And therefore, you can think of each data point is providing an estimate of the gradient. So then the question becomes, what kind of data to use to estimate the gradient. So there are three distinctions based here, although we generally will be focusing on the middle one here. So let's say that we have EM images and see ***** can, one way that we can compute the gradient, but say that n is equal to 1,000, is that we could compute the gradient by using all 1,000 examples to estimate the gradient. That's a batch algorithms. That means using alter data to get one gradient. But you could also estimate the gradient from a subset of example. Let's say you have 1,000 examples, but you're going to estimate the gradient from just 50 of those examples. Alright? That will allow you to compute the gradient more quickly because you're using your examples. However, because you're using your examples, the gradient will be noise here. Alright, and so this is called mini-batch gradient descent. There's one more thing called stochastic gradient descent. And actually formally when it was defined, this corresponds to using just one example to compute the gradient. But in general, we always use mini-batch gradient descent, and we call it the classic gradient descent. So whenever you hear someone say stochastic gradient descent, they're usually doing Mini-batch gradient descent and they'll specify what that is. So don't call this the castle. That's the most common thing to do because and territory. One interesting thing is that even though you think mini-batch should do worse than batch, because they're using fewer examples and therefore your estimate of the gradient is first, it actually turns out that when you have Boise gradients, this will provide a regularizing effect for your algorithms. So actually minibatches faster and oftentimes will generalize better. We'll talk more about regularization, a few lectures from now, but that's something to keep in mind so well, almost always to mini-batch gradient descent. Any questions here? The question is, let's say k equals 50. Well, we want five images from every class. That would be the best stuff, that'd be the best thing to do as opposed to like jabbing 50 images of dogs and you might have a very poor some of the other questions. This is our last slide on gradient descent. So any questions on gradient descent in general? Where do you have this randomly sample? From positive integers? That's correct. Yeah. The first one. Yeah, that's good. So Tom way is asking, when I sample from this 1,000 on the next iteration, my sampling 50 again, so that there could be repetitions using what will happen is you want to make passes through your dataset. So you will start off with 1,000 examples with sample a mini batch. If you take a greedy this next step, you're left with 950 examples. Then you would sample 50 of those times where you would go through every single data point once before you then repeated. Alright, so that's gradient descent and, um. You will be implementing this in homework number two. There's going to be a lot more to be said about optimization. And so in a future lecture, when we talk about neural networks, we're gonna be talking about first and second quarter methods, momentum, adaptive gradients to everyone to see that this is going to help a lot when we get to the implementation of gradient descent for neural networks. In the remaining 4 min that we have, I just wanted to intro how a neuron works and then we'll talk about them in neural network architecture on Wednesday. The reading for this lecture will be Chapter six of the deep-learning. I just want to talk about how a neuron works. I've shown you here is an image of an actual physical neuron. So what we are doing is in this image where the sending it an electrode and actually listening into the activity of that neuron. You can see the neuron is connected to all these other neurons as well. And the neuron has several components, but I want to just focus on a few for the purposes of how to build connections neural networks. So the first thing I want you to pay attention to is that the neurons have these called dendrites. And dendrites are the inputs of the neuron. And the dendrites will correspond to these arborist regions of the neuron. What happens is that at these regions, there are other neurons that are connected to these dendrites. So this is an upstream neuron. If the upstream neuron has a signal, it will convey that the dendrite, and there are dendrites all over the body of the axon, all over the body of the neuron. So these are where input signals are taken. There's also something in the neuron called the axon. The axon here corresponds to this long tubule structure of the neuron. You can think of this as the output of the neuron. So you can see that in this image, what I've drawn is that this axon takes a signal from the neuron and it actually connects to the dendrites of downstream neurons. So these are three downstream neurons and the axons dendrites. There's one more component. I'm going to label this with the number 2.5 here. And this is called the axon hillock. The axon hillock is over here. And the axon hillock is the integrative part of the neuron. So what do you can imagine this is the axon hillock does, is that there's all these signals coming from the dendrites and what they do, they propagate down the neuron until they eventually hit the axon hillock. Let's say the signals where I'll call them like Z1, Z2, Z3. A very simple version of the axon hillock is that it implements a function f. And f songs all of these inputs, Z1, Z2, Z3 is bigger than the threshold, then the neuron generates something called a spike. So if f exceeds our threshold, then it generates a signal called the spike. And so it fires a spike. You can think of this as sending a signal one. And then if f is below a threshold, then nothing happens. There's no signal feminine, so that's like a zero. And the signal will be sent down the axon and then conveyed to other neurons. Alright, so when come back on Wednesday, we'll recap this and then we'll go into how to control my forgot. Let me first. 
All right, everyone, We're gonna get started for today. So a few announcements before or the day. If you are an MS Online students on good learned, we just such an announcement maybe an hour ago about the exam time. So it's going to be Saturday, February 25th, 1-250 PM. So if you're gonna sound like to me that you didn't see this massive. Please be sure to check through him as a reminder that homework number one is I'll put it in there and it's going to be due this Monday. I'll birth to grade scope. In homework number one, there is the coding components. And so to submit the code and components, please be sure to print out your Jupyter Notebook so that all of your code is because the auto-grader and the grader will look at that. Alright. Also had their solutions and Clarksville better. Readers need to see your code to give you credit for your car. So also in future assignments, you'll be editing d2y files and for any files. And yet it felt to my file to also have to print to PDF that codes of the Greater can see any questions there. Alright, so on Wi-Fi and how it affects the recordings. So we saw on Piazza that some of the Zoom recordings may not be totally reliable and that's probably because of the Wi-Fi issues that we couldn't happen. Today. We're having our MSM one TA can join the room, both be reliable Wi-Fi connection, hopefully the top of the quality. But please also remember that crew and cast upwards lectures. They're under UCLA to get reserved some Boomer. And those are also, we ran discussions for the first time last Friday. And for discussion once c, which is one of collides discussions, there is no reliable Wi-Fi connection in that room. And so for discussion once C, we will not have a simultaneous Zoom meeting for that. Any questions on this or discussions? Okay. And then the last comment is that we'll go to discussion video regarding the coding part of homework number one and that will be posted this Friday. Alright, any questions generally about logistics for this class? Alright, we're gonna get back into material. So last lecture we were talking about how to assess whether a model is overfitting or underfitting. So this is recap. We talked about this procedure called k-fold cross-validation. Where what we'll do is we'll start off with our original data, right? The first thing that we do is we split the original data into a testable than extreme fold. The test bulb is set aside pristine dataset that you clarify once at the end to score your model. Alright, but then for the remaining training data, for 24 fold cross validation. And what we've reduced, we would split it into four segments, all of equal number of trials. And then we would designate three of them for training and one of them for validation. Validation is to optimize our hyperparameters. Remember that these are the choices in the discipline, the algorithm that you as a designer get to choose beforehand. And what you would do is you would train your model on the green training folds and then you would evaluate its performance on the yellow withheld validation fold. It gives you a validation accuracy to score your model. And then you can keep doing that for different sources of hyperparameters. And then you can also switch which folders, the validation fold and which folds of the training. Just to make this as concrete as possible. Here's an explicit example that will happen in your homework. So resting for ten data set, which we're going to talk about today, is a dataset that contains 60,000 images. There are ten classes, so depth ten different categories of images and there are 6,000 images for each category. What we're going to do is with these 60,000 images at the onset, we're going to set aside 10,000 images, and that comprises the test set. And we will never touch this test set until one time at the end, when we overall score how good our model is. Right? After setting aside these 10,000 test set examples, we still have 50,000 images. What we use for training and validation. So let's say that instead of four fold cross validation study into 4.4 holes without you fight for cross-validation. Five-fold cross-validation is put the 50,000 images into fibrin. So each fold contains 10,000 images in five-fold cross-validation, one called this validation. So one fold is my validation dataset with 10,000 images. The remaining four folds are what I used to learn the parameters of my model containing the 40,000 images. Then I can repeat this training process five times, each time swapping out, which is the validation. And then we can average those five validation accuracy to determine the best hyperparameters. After you do that, then you can train one more time across all of your images for training and S-Corp once on your test set model question. The question is, is it just a coincidence that the test set is the same size of the polls that we used for training and validation. In this case, we chose them to be the same. Oftentimes, the test set should be relatively larger. So it makes sense to make it similar to an old size, but it doesn't ask the question. The question is, when we do five-fold cross-validation for each of the hyperparameters are going to have five validation accuracies. So then how do we choose the best hyperparameters? It's not a question, right? So if you are doing, actually I had this example on the next slide. So this is code that will optimize the polynomial order for our linear regression example. And you can see us a polynomial order gets higher and higher. The training set error will decrease like we talked about last time. But when you average those five validation accuracies for each polynomial order from order one to order five, you can see the accuracy, sort of validation or the validation set error increases, which is bad. So in this case, you would choose that you would use a first-order polynomial. It's the best performance of the other questions here. Wonderful. So time-wise, making the point that once you split the images into folds, those folds, you shouldn't shuffle them in between repeated validation. Otherwise you're going to be, you're going to have data-set week where across different validations you're gonna be using the same data for training or for validation. Alright, so that's cross-validation. You're free to take a look at this code, which is what you'll also incremental hallmark number one, that we'll do a cross-validation for our polynomial example. Like we were just saying. If you do to compute the best hyperparameters using the validation set with health data. You'll find that for the data that we were showing you earlier for the linear regression example, is best to use a linear form. Or one thing you need to talk about. Each of the validation sets just not profitable. Yeah, you're welcome to shuffle. What does he would say things, but then you can shuffle your examples. But you shouldn't change the examples in these folds across each cross-validation. Question is, can I explain what the difference is between the left and the right click or yes, the left finger. Sorry, I went through this quickly, is training set error. So this we should expect to get better and better because you'd expect lower error as you increase the polynomial order. Because you can use the expressive, the higher capacities at the data points better. But then this one here is validation set error. So this is how it generalizes to new data that was not used to learn the parameters. Just to keep the colors consistent. I think the training set we were doing in green and validation set we were doing in yellow or orange. Right? Any last questions on this simple example? Yes, this student is saying, just to clarify this secret clarification, we tested 25 models here, and that is correct. So every single validation fold, and we didn't five-fold cross-validation here. So there are five folds. Folds. So we're going to have five validation accuracies. Per hyperparameter, setting of hyperparameter. And then here we have five study for the hyperparameter, which is the polynomial order that goes 1-5. And so this would be five times five, and that will be a total of 25 mortals. Alright? So hopefully again, these past few lectures, these, the last lecture was a review for most of you. If you haven't had a machine learning background, please be sure to become very familiar with these. Will still talk about some things that you probably learned in your prior machine learning classes. But now there'll be relevant for neural networks. So in this example, we talked about minimizing the mean square error. It turns out for neural networks, we're going to need to use different loss function. One that is based on something called maximum likelihood. We're going to talk about this later today when we talk about and derive the softmax classifier. The softmax classifier we're going to learn is the most commonly used output of a neural network. Alright, so we're gonna be going over the softmax classifier today. So this is the topic of supervised classification. The deep learning book, it will correspond to these chapters are reading. So today we're gonna talk about supervised classification. We're talking about two classifiers. They're gonna be k nearest neighbors, which will just use as a motivational example really. Then after that, we're going to talk about the softmax classifier. And again, like I was just saying, this is the output of most modern neural networks today that do classification. And the softmax classifier we're going to train using a loss function derived from maximum likelihood. And then the way that will optimize this loss function is with gradient descent. Alright? And so these are the next steps that again may be review for some of those who had machine money back down. But this will be basically the first component of our neural network, which is the output. Right? So the motivation for this is, we're going to talk about image classification. That's because computer vision is really the field that gave rise to this past decade revolution in neural networks. And like I've mentioned before in our homeworks, are going to be looking at the image classification, which is T4 tag. And so I want to give some background on why this is a challenging problem. So we look at this image and we can clearly see it's a cat. But what does the computer C. So if you were to lose this captains your computer, you all probably know that every single pixel of an image contains three numbers. One that tells you the amount of red, green, and blue in it. And these values for red, green, and blue are all numbers I can go 0-255. If you have sympathy for red, green, and blue, that means you have a lot of red, green and blue together. And together they mixed to become white. If you have zero for red, green, and blue, then you have no read it didn't prove the absence of color is black. So if we were to zoom in on this part of the cat right here and look at how it's stored on the computer. Because it's what you would see numbers that are really close to 255 across the swath of the cat. So the computer is essentially just storing numbers there at 02:55 to represent this cat. Zero to 255. Again, just for the RGB of which there are three for every pixel. And then there's of course, a width and a height of the image. So the images are stored ultimately as a 3D array or a 3D tensor. And each of those values in that 3D texture range 0-2. Alright. So this is gonna be the input data or your computer vision algorithm for your neural network. And there's clearly a semantic gap here because we looked at this and we think a computer looks at this and sees all of these numbers. And we have to go from these numbers to determine what makes a cat a cat. All right, Any questions there? Alright, so the image is sorted array of numbers. And so you think about this a bit. So notice that there are a lot of challenges. One of the challenges is viewpoint variation. So these are different at the same object, which is a statue of David. And even though these are all the same object, if you look at what their numbers aren't any period, they're very different. So we just take the bottom left corner of this viewpoint. All of these values are close to zero because the image is comprised of black. Whereas if we were to take a look at the bottom left for these images, and the numbers will be closer to 100, 1,600% because it's not black. Alright? So just even a different angle can dramatically change the values stored for that condition. Even though they're all the same object, you point variation become awesome effect in the form of CO2. So just like the last thing WE showed, same image just zoomed in. It's still a cat. But again, the values are very different. If you're trying to decode. And cats in general, they can come in different illuminations. And here you can see again, the pixel values will be as different as they can get. Because here we're going to have a bunch of 200s or the captain bright illumination. But for the cat and dark elimination is going to be cluster two zeros. Any questions there? Alright, so then at this point you might potentially, okay. Obviously we can't use just the absolute value of the numbers making classification. We can start to maybe in trying to think of features that define what makes it. So you might say, a cat has ears, too pointy ears, two eyes, and four legs. But then there are going to be images where there's deformation. And so you see an image like here. We see three of the lens of attack, but then they're all inter, twined, entangled. Even if you were to say, Okay, it can have two eyes and image can have occlusion. And so you might have a cat hiding behind. You're here and you might only see one I get for all of these, we understand that these are the care. They can be cluttered by background. I chose this image because this cat bears or as almost as if camouflaged and the trees, and the trees have very similar values to let the capitalists like. Then lastly, we want to know how does attack. But then in fact Clement all different shapes and sizes. And we would still want to be able to classify that all of these are cats, right? So these are some of the challenges that come with having a classifier that takes an image and is able to identify it as a cat or a dog or something else. Any questions here? All right, So when trying to classify the image, there are several approaches that one could be located. One of them is this one that I just mentioned, where perhaps we can define the features of what makes the cardiac cath. And I look for those features in the image. But then this is going to require a lot of human expertise, right? Or a human has to first determine what Mason Katic cat, and then design an algorithm to look for those features with an attack. And that overall is time-consuming and not the most efficient use of resources. So the other way is to take the data-driven approach where you kinda throw up our hands and we say, Hey, I'm not exactly sure algorithmically, what makes a cat a cat. But I wanted to train a neural network, a machine learning algorithm to do it for me. And basically in the process of training based off of what the data looks like, the neural network, the machine-learning algorithm is going to learn what are the features that allow you to classify data. So in this setting, just like we've talked before, we're going to be to train an algorithm. We train an algorithm in this approach, we're given data. That data will comprise images of cats, dogs, airplanes, automobiles. And what we will do is we will take this data and train a model that takes the image and reports the class. And so this will give us in the training phase model parameters that we learn from data. Alright? So for deep neural networks, we're going to find out that these parameters result in learning. The features. Let me remove, digest learning features that are optimized to classify an image. In other words, to say that this image of a cat is in fact the cat. We might not know what the speech or talk to the algorithm will learn. After that, we'll have then our test phase. So in our test phase, we're going to deploy this model. So we start off with our model that was learned from training. So this model comes here. And now from this model, what we can do is we can put in a new image into this model. And it's going to tell me what class of students that's going to tell me is this image of a cat, a dog? It's alright. Any questions here? Alright, this is all set up. So like I mentioned earlier, we are going to be using this department dataset. The dataset is going to comprise 60,000 images, each of them or 32 by 32 pixels. And therefore, each image is going to be. I'm a 3D tensor that is 32, 32 by three. And at least to start for this lecture, we're going to go ahead and we're going to just restrict this into a vector. So our image, which was 32 by 32 by three, we're going to shape into a single vector, which is 3,070, two-dimensional. Thousand 72 is 32 times 32 times three. There are ten classes. These are the tank classes with 6,000 images per class. And then we're going to withhold 10.10 thousand of them for testing. 50,000 of them will be used for training. And I think I was looking at the homeworks. Sometimes we change these numbers and this sometimes it's gonna be like a fauvism for testing and if thousand for validation, but in the home or school. And so again, the goal of our machine learning or deep learning, will be to take in the input image and then predicts the class. I'll call this y. And remember what that we've been using the superscript I to denote an example. So why I think sample is going to correspond to the label of that image and it's going to be one of these ten classes from airplanes. Got Dr. Chuck. This is a code that will provide for you on our comforts two to five. And this will go to S4. And like I mentioned before, we're going to be taking these 32 by 32 by three images. So with this mode, seek part-time scripts. It'll give you x train, which are the images. X train is going to be 50,000 images. You took them 32 by 32 by three. Y train will then be the labels of those 50,000 images. So y train will be a number 1-10 for every single example. And class one would correspond to their plane. The y was equal to two, of course, automobile, bird, et cetera. And then we'll have our test data and our test labels, and they will be in the same dimension as the suspect there attend. These will be input data to our training. The question is, what we refit that 32 by 32 by three tensor into a 3,072 dimensional vector, we lose the positional information. Are you mean like, like e.g. like facial correlation, immunity. But when we get to later neural networks, just to motivate the first algorithm, the softmax classifier load mean of vector input. Other questions. That's a great question because there's a lot of information that spatial, spatial relationship. Alright, so we are going to talk first about a simple algorithm that doesn't require machine learning called K-nearest neighbors. Who here has seen k-nearest neighbors before? Most of you. So this should be hopefully straightforward. And dental. A different approach, which will be our softmax classifier. Alright, so consider a setup wherever using the maximum, oh, sorry. We're going to discuss maximum likelihood classifier in this lecture. We're given input vectors. These are my seat bar ten images, each of them in our 30, 72. And then for spawning classes, so each of these y's correspond to a label of that. And that'll be some number 1-10 or maybe it's Sarah tonight. Let me do one to ten just for the sake of simplicity in the center. Now, with this, we can train a classifier. And our goal is to now be given a new data point x you. And again, sorry, ignore this part. The probabilistic model, we decide to remove this from the structure based on past chairs. Feedback. We want to do it in a way that doesn't, that is kind of intuitive. And so that'll be something like k nearest neighbors. And intuitively what K-Nearest Neighbor says is, I can look at my new data point and look at the labels of the images that are closest to me. And then I'll think of both. And that'll be what I guess my images. Let me draw this out. Let's say that we had 2D data. So our examples at xy are all two-dimensional. And then this will be the first dimension of x. This will be the second dimension of x. And then let's say that I just had two classes, so I had cats and dogs. So let's say that orange circles are caps. Then we'll say that green X's are dogs. Right? So maybe my examples of each of these is a different example in my dataset. Looks like this, and then my dogs look like. So now if I were to give you a new data point that we have never seen before. And I put it, are the x values of this data point right here, this red square. And they asked you, what is the class of this new data point? What boat I could take it to be, you would say is probably going to be a dog because the things that are closest to it are all dogs. So K nearest neighbors formalizes this intuition. What we do in k-nearest neighbors is we set the hyper-parameter k. K is the number of nearest neighbors we're going to just consider. So let's say that k equals five. What this algorithm says is take my new data point and look at what my five nearest neighbors are based off of distance. And then whatever the majority or the plurality of those neighbors are, is what my class is going to be. So e.g. if my x nu was over here, alright, I will look at my five nearest neighbors. And it would be three cats and two dogs. And because of cats have three and salts have to then either classified that this point here is a any questions there or even any tenures neighbors. If you have a tie, then you can just choose randomly one of the classes that was time for the most votes. So more formally, and K-nearest neighbors, what we do is we choose a distance metric. Because ultimately we need to find our k-nearest neighbors. So we need a distance metric to assess what the distance is. All of my other data points, most commonly, we're going to use the 2-norm between two vectors, XI and XJ. Although in homework number two, we'll ask you to try also the one norm and infinity norm. Yeah, the question is on the slide, are the values X1 and X2. X1 and X2 will correspond to my example x psi. What are the two values there? So e.g. we're going to have some number of data points where each data point, data point is going to be X-i, Y-i. But then for this constructed data and I've just defined excited be some 2D vector. And maybe the values are 1.0, 0.5, which means that for this example, I would recite it one on x coordinate 0.5 and x2. Yes. We choose the values of K so that we never have to write good time, e.g. but the thing is if we have more than two classes, it can be like, let's say, added cats, dogs, and horses to this my nearest neighbor. But if you'd like to cats, to dogs and then one course and then in that case you just choose cat or dog. The question, yeah. The question is, if you make K larger, will that lead to a better classifier? I'm weren't accurate classifier. There isn't. Making cave bigger in general will help to ameliorate overfitting. But the optimal value of k will not be the biggest value of k. And that's something that you will find k-fold cross-validation. I will ask you to do that on homework number two. The question is, would you say is common to treat the distance metric as a hyperparameter as well? Yes. So in homework two, you will get back. Actually, I have a slide later on that was asking that question. Oh, yeah. Soon as asking. It could be that you have an example where let me draw this out. Maybe there's an x over here. And maybe we're doing like, let me make this. I'll do some different color. We'll call this threefold and purple. And maybe you're new example is, is over here. So in this example is much closer to the dog, but then the CAT score when by boat. And so in three, not threefold, sorry, I'm three nearest neighbors. K equals three. If you're doing a three nearest neighbors, that purple square will be classified as a cat. You can make modifications on this algorithm to, instead of doing the neighborhood kids like compute the distances, e.g. we're going to talk a bit industries size. Why not do it? Right? Perfect, Yes, a tomboy is raising the point that in this case, you actually probably want to guess that this is a cat, because in this green x here is an outlier because the screen x is very far away from the other dogs. Alright, last question. Yeah. Yeah. Wonderful. Yes, I start to write that L2 distance. In this case, this would equal the square root. And so x is a vector in RN. This distance would be the sum from k equals one to n. And then we're comparing example XI and XJ. We're comparing all n dimensions between them. So this would be x k minus x j k. This whole quantity squared. Sorry, I, right, if it's small because I'm at the edge. Standard Euclidean distance. But again, in homework number two, you'll try also the L1 norm and the infinity norms. We will also choose our nearest neighbors k. Again, notice that for the distance metric and the nearest neighbors, I said cheese, right? And so these are hyperparameters. You get to optimize over. We take our new data point, which was that red square. And we calculate the distance between that red square and every single data point that I have. And then we choose the k nearest neighbors, which have the case smallest distances. And we choose the class to be the thing that went for plurality vote. So the class that occurs most frequently amongst its nearest neighbors will be the class that we got to guess. And if there's a tie than any of the classes can be selected. That's the formal description of what we just described an image, the last slide. Any questions? Yeah. Right. Yeah. The question is, in the case of a tie, is fair randomization to choose. The time. That tells me what I just said. There were four pockets and I withdraw uniform random variable amongst four. Okay. That's great. Yes, it's one-way says that it's sometimes the case of ties. Instead of doing randomization, you can also use the actual distances to break the time escalate idea. Alright, so here's example dated at this time we'll have three classes, the green triangles, the red Xs, and the purple circles. And we're going to build this classifier. Alright, so the first question I have for you all now is how do we train the classifier? I'll give you all 15 s to think about it and then I'll ask someone for an answer of how to find the classifier. Hoping to talk to neighbors to the lungs. All right, It's disgusting. Someone told me how to train this classifier. Wonderful. That's all come back. So I just asked, how do we train a classifier? And Jacob answered, all you gotta do is save the data, and that's correct. So you were to make a k-nearest neighbor class? I need to classify by just comparing. You did appoint all of my existing data points. And then I also have to know the labels wise so that I can do the book for K Harris papers. Alright, so this is all the training purposes which is to memorize your data. Alright. What are the pros of this was 200. So it's the book. And it's also fast, right? Presumably already in memory. So you just have to make a pointer to that memory. Someone told me what a con of this training approaches as slow as well. In what sense? The testing us though, yes, and we'll come back to that. Wonderful. Yeah, so it's memory, and especially because I'm gonna write memory intensive, as your dataset becomes bigger, you have to store more and more examples. And in general, if we have large datasets like ImageNet, right, that pelvic gigabytes and that will not be a wise store. So it's memory intensive because we need to store all the input data. This will be in contrast to machine learning algorithms, where we have a predefined set of parameters. And all we have to do is store those values of Theta. So keep that in mind when we talk about the softmax classifier. How do we test the new data point? So let's say that we are working in R2 just to have a concrete example. So our data points that are two-dimensional, well we would do, is we would get a new data point by x tests. And then when I calculate this distance here, what I'm doing is I'm Kathy calculating the Euclidean distance between X test the new data points, and self-talk x train. Remember x train was my, was just my entire dataset. So if you haven't used Python before, you've likely not heard of broadcasting. Even if you use Python, maybe you haven't heard of broadcasting. This is going to be really important to make your code more efficient. So let's say that we had an examples in my training set. Then train might be, wouldn't be then a two by n array. Then my new data point is just one-sample. Z-test would be a 2D array. When I calculate the two norm, I have to subtract from every single example in the training set. Alright? One way that you could do is do this is to write a for loop. Alright? But writing a for loop is going to make your code into walk longer. And the procedure of subtracting this 2D array from every single 2D array of which there and at them in X train should be something that you can do just like a carrier. If you come from that background and you know that the function you would use DSX fun. In Python, this will be done automatically through something called broadcasting. In broadcasting, what'll happen is if I kick self dot x train dot transpose, that makes this term here. And n by two array because it transpose is the array, so it's n by two. Then X test here is a 2D array. And if you do an operation like minus, and the trailing dimension of the first variable matches the leading dimension of the second variable. Then Python will know to perform this operation minus subtracting this 2D variable from each of the N examples in which each one is two-dimensional and self-talk extreme dot transpose. Alright, so this is a quick way where as long as the inner dimensions match for your operation, Python, we'll apply this operation minus for every single example in extra. And that saves you from having to write a for loop. And it's also a lot faster time. You'll use this in the homework. Any questions there? Okay, the question is, why is x trained to buy in to begin with as explicit and bite to? It might be more common. This was just a preference for me. When I wrote this code, I defined x train to have the Fincher is being the rows and the number of examples you can call it, but it could accompanied by two, in which case you wouldn't have had to have this dark transpose here. The question is, why is X test two comma empty as opposed to two comma one or one comma two. So in Python, it's better when things aren't bacteria to just define them as two comma empty. Because then Python will know to treat it as a column vector or a row vector depending on the context in which he sees. So it's more flexible to define it as two comma as opposed to two comma 12 comma one, then a has to be a column vector. The question is, when do you define it? You will define it to be at, to come after me. You could write it that way or you could just write an array. And bettering will be like an array with two numbers, then you don't need to cite to comment. That's right. Right. Tas? Far less than I used to when I was in graduate school. So sometimes, sometimes I had to check with the TAs for syntax. All right. Other questions. Yes. The question is, isn't this square root unnecessary? Yes. That's correct. Yeah. You could have used the two norm squared because you just care about the relative distances. That would give you the exact same answer. Alright, so this is testing the testing function for our k-nearest neighbors class. The pros, again is a simple. Let's go through these lines, but these lines will just sort the distances and then choose the k nearest ones and then do the plurality. What's the chronosystem? Our students already said it. It's really slow because whenever you want access to new data point, you have to compare its distance to every single point in the dataset. And that is going to be slow. And not only is it going to be supposed, going to scale with the amount of training data and not of training data. This is really bad if e.g. we're looking at real-time applications. If you're building a neural network that isn't a self-driving car, right? It has to be able to process the image. And it just is receiving a real fun and make decisions, right? And it cannot be waiting a long time to process a new image. X tests. Alright? So in general, for machine-learning, and this will motivate our next algorithm, softmax. We want to flip this. We would tolerate the training phase being very long. Instead of a second here, or like Toby, microseconds here, nanoseconds. We're fine with the training process being wrong. What's talking about neural networks later on, where even with many GPUs they take over three weeks to train. But then after you train the neural network, the testing phase is very fast on the order of milliseconds. And that's really important because then it can be used in real time. Alright, so we kinda want the flip the situation. K-nearest neighbors, sir. Any questions? Alright, so this is what the solution looks like for one nearest neighbor. So basically what I've done is in this code, I've iterated over every single point in a grid like fashion, and I just computed what the k-nearest neighbors are. And you can see that there are some of these islands over here, where from my outliers and X, these points are all nearest to actin, so they get colored red. And that can be a sign of overfitting. This is ameliorated when you increase the number of nearest neighbors to three. And you can imagine, again, interesting this further. Ultimately then the question is, how do you optimize these k-nearest neighbors? Well, it wasn't hyperparameter, as well as the distance metrics. So the answer to this question, what are the hyperparameters of k-nearest neighbors? They are K and the distance metric. And then how would you optimize them? Well, you would do that with k-fold cross-validation and go do that again in homework number two, you might have this question, or maybe you don't have to specify. What might you say nearest neighbors not be a very good idea for image classification beyond what we've talked about, about protecting time, being very long. High-dimensional data, there may not be explicitly boundaries between classes. Perfect gas and the students said they're collapsing into a vector, so busy spatial information. And then lumbar drain k-nearest neighbors we're measuring and the neighbor distances based off the values. We've already talked about how the pixel values aren't. Good enough semantic sense, because e.g. cat can be in a light image or an image or a dark image, and it would still be a cat. To that point. We have this image where we have an original images image. And then if we do three modifications on this image, what is boxing out? Part of the features shifting and another is just by thinking everything. All three of these images will have the same L2 distance to the original image. Getting again at this point back on pixel distance does not equal syntactic or semantic understanding. Alright? And then there was also a student who mentioned difficulties when it comes to high dimensional data. So there's something that you've likely heard the Machine Learning class called the curse of dimensionality. And when we have a C4 TEM image being a 3,070 two-dimensional vector. This is. A point in a 3,072 dimensional space. While k nearest neighbors makes sense when we're drawing into d, distances or intuitions that we have in 2D break down when we go to a very high dimensional space. The first thing about a high-dimensional space is that every single dimension that we add increases the volume of that space exponentially. So in going from 2D to 3D and 2D still have the same number of data points, but they are in an exponentially larger volume. And this means that there's a lot of empty space in that volume. The first thing is, it's implies is that the nearest neighbors may not be served there. Then within this really large volume, there are 3,072 dimensions that you can compute a distance across. Some distances may matter more than others. So our intuitions over distances of proximity in 2D and 3D really break down what we get. So high dimensional data, making k-nearest neighbors, not a great algorithm for high dimensional data. Right? Any questions? Right? Yeah, so the city is asking, can you do some feature normalization, e.g. to whiten the features so that the features are hopefully more uniform space. Still our intuitions of distance breakdown because we have soaking points in this really high, really large volume space. Mr. Mention that it's very small. Great. Yeah, so tomboy said, when you go to high dimensional space, because there's such a large volume, the variances between data points actually decreases. And so the distances become even less meaningful than they were with the word dimensional space. So these are just the intuitions. We won't test you on the curse of dimensionality, but these are institutions to carry with you guys to why. This is not a great idea for image classification. So perhaps a better way would be to take an image. And we know that the image and see if our time has to be one of ten classes. So what we can do some particularly image and then basically develop a score for each image onto a class. And so we'd have an image. Maybe the score dipping an automobile is ten for it being in the airplane is 200th birthday. Attack is negative five. And we would take the highest score, which would be for the airplane classy cheese. And we're going to do this base off of linear classifiers. We do this for two reasons. First, linear classifiers aren't major building block for neural networks and so it's important for us to understand them. In particular, we're going to see that each layer of a neural network because of a linear classifier followed by non-linear function. Then lastly, like I mentioned at the start of fracture, the classified ever going to talk about which is called the softmax classifier is going to be the most calming classifier at the end of the neural network. And so in any, in most applications where you are going to be doing classification, I drove the a softmax classifier at the output, and that's what we're going to derive next. Alright, so let's go ahead and take a five-minute break. When we come back, we'll do that softmax classifier, right? Right. Yes. If this is because of the plaque operation of broadcasting, because I want to take this subtracted from every single 2-dimensional. Yeah. So when you do security as a row vector apartment, just want also this one yet subtracted for every single example that I just had a question. Yeah. Terrific axes which are right back. Yeah. I see reconstructed warrants and the ones after that. Let me ask Tom way rocks get right now. Thanks. Hi. Generally there for these things. Hello. Alright, if I had a couple of questions, questions asking about podcasting. If you can hear me, you go. Student questions about broadcasting. So I want to say broadcasting is something specific to Python. So if you try to do that operation, that subtraction operation that we did in a different language, it would fail. The Python is programmed to know that, but the inner dimensions of your arrays naturally to do the broadcasting operation. Great. So tomboy says that the TAs are going to cover broadcasting, encoding video to go out, not this Friday but next Friday. Alright, so we're going to now do classical classifier, the softmax classifier based off of linear classification. And here we're going to imagine that we're still in the classification problem. So we have data is n-dimensional. So in C4, ten middle Andrew here and 3,072. And then we're going to have some number of classes. So first CFR, they're going to be the numbers 12 all the way to ten, because we have ten different class seconds. We're going to do is we're going to define a matrix, big W. Big W equals the spring. So big W is going to be c. C is the number of classes. So MC4. See what equal ten. By the snappy big and this should be little n. Little n is the dimension of my inputs. So what we're going to do is we're going to calculate y equals wx plus b, where b is a vector of bias terms. Alright, so let's just go ahead and draw this out. So we're gonna have a matrix W, W. I'm just going to copy down. It's gonna be w1 transpose, That's the first row of w. W2 transpose the second row of w all the way down to WC transpose the case of CIFAR ten equals ten. So this would be a ten by dimension of X matrix. So this would multiply x. This is w times x. That's this part of the operation. Let me just put the dimensions for C4. Ten would be ten. X is 3,070 two-dimensional. So this vector here is 3,072 by one. Why is gonna be a number 1-10? Actually, no, sorry. Why is not? Why here is gonna be a vector and arctan y is gonna be a vector in R2. So c is equal to ten for C4 ten, which means that this W matrix is gonna be ten by 30, 72. We're going to add that matrix B. So this is going to be b, and b is going to be at ten by one matrix. B is or ten also, there's a bias for each class. Then that's going to boom. When we do them this operation to wx plus b, we're going to get an output vector y. And y is simply going to be this vector of ten numbers, w1 transpose x plus b one, W2 transpose x plus b2 all the way down to WC transpose x plus b, c. And this is going to be a ten by one. Alright? And then what were bunch of zeros? In this case is we're going to interpret the scores or each element of y as the score for that class. So this is going to be a number which is going to be interpreted as the score of being in class one. And then this is going to be the score of being in class C. We have a score for all of our sea clocks. This is the part that would be ten scores corresponding to ten classes. So therefore, our predicted y. For some example, I maybe it looks like respect your 300, 500 minus 150 dot, dot, dot. Let's say that these are all negative numbers here. In this case, then we would say that the score for the second class is the largest because it's 500 and it's bigger than everything else. So then we would predict that example I belongs to class two. Any questions on that linear setup? Yes. Great. So Tom, we asked a really great question, which is, this looks very similar to the linear regression we did for the housing prices. What is different here? Great, yes. So in the housing example before, our output, Y, in that particular example was a scalar value because it was quantifying and rent and rent it to be any number. Whereas here our output y is actually gonna be a score for every single class. And the way that we choose the correct classes to take the argmax of the scores. Just see how the space to lead us into a different loss function. Because you can go for the housing example we used to be square error loss, right? Where we compare the predicted versus the true rent. Let's say that in this example. The answer is that example I belongs to class two, alright. So that's our prediction. If it was actually class too, and we use the mean square error loss, right? We would have like a two minus the true class. I say the true class, I'll red and orange was too. If we take the mean squared error, that equals zero, and that's what we did. You actually will see really quickly that if I use this encoding scheme for the crossover breaks down quite quickly. Because maybe two is the class for automobiles, one is the class for airplanes and tenants of class for trucks. So let's say that this was actually an airplane. If I do two minus one squared, that would be an error of one. But if this was actually a truck which was class ten to minus ten squared, this would equal 64. In this case, you can see that this is already not a good boss competency to use because if I predicted that this was a plain class too, and it was a sorry, I think I said clusters and automobile, but it was actually a claim class one versus the truck past, we've had very different losses, but we shouldn't because the Wasserstein are different by this much. So we'd definitely at least some new loss function, which we will be shortly. Great, yeah, so this student says, can we apply a one buddy, buddy, It's the classes and then you use a squared error loss, you could do that. So that's, that's one thing that you could use it. We're not going to talk about this, but what this is saying is we could make y hat attendee vector where everywhere is zero except for the class with the highest score. In this case, it would be 01000 is the reserve natural one month after representing the correct class and we wouldn't have this carbon. I would talk to me about you didn't follow that. Don't worry, we're not going to do that. Okay? Any questions on just the linear center? Great question is in this case, we really need to know these vectors W1 to WC. We also have to know these prices, B1, B2 to BC. How do we get them? We will show you how to get them. So we'll have to tell the loss function and it will have to know how to optimize it. Which will be, the loss function will be introducing maximum likelihood and then optimizing it won't be as doing gradient descent. Let's move on for now. So because what we're comparing in this course is essentially w transpose x plus b, right? And W transpose X is a dot product that measures the similarity of two vectors. Then there's actually an intuition for how you should interpret each row of W. Let's say that w, I'm going to actually try to get in class two is automobile or a car. If W2 transpose times picture a, W2 transpose times pictures of cars has a high score in W2. So book like what a car looks like. So you can think of for cars, automobiles here. If you take that 3,070 two-dimensional W2 vector and you reshape it. You look at what it looks like. You can actually see it kinda looks like a car, right? Like this is the hood of the car. This is the one intuition is that you can think of each row of w as a template which captures how, which captures the average book of the examples from that class. Any questions there? All right, and then just be a refresher for most of you saw it, go through it a bit. I'll go through how we get an intuition of what we're doing here is linear classification. So that's again, consider the case that we're gonna be working in TV because that's all I can draw. So that means x is going to be comprised of scalar components, x1 and x2. If I have a 2D space where X1 and X2, when I do my linear transformation, I'm computing y equals, let's say Y1. The score for class one is equal to w transpose x plus b. I'm just going to jump the biases for now. Just in the name of simplicity, let's say that we had y equals w1 transpose x. W one is going to be a two-dimensional vector. So let's say that w1 is this 2D vector that looks like this. Then let's say our data point x is going to be some data point here. So let's say that this is the example X and it resides right here. When I continued w1 transpose x, this equals the norm of w v1 times the norm of x times cosine theta of the angle between them. Theta corresponds to this angle right here. Then for this intuition example, I'm just going to make this assumption. Assume that the norm of w one is equal to one. Right? If I didn't assume that we said we just scale my scores. So this simplifies to norm of x times cosine theta. If we look at this image, I've drawn, right, what I can do is I can draw for drop a perpendicular line from excited to w1. So this is a right angle. To draw this, even though it doesn't look like a right angle to me. Actually let me redraw this dotted line so it's more like a right angle. Okay? So this is a right angle. If we're taking the norm of x, norm of x is the length of this vector. Cosine of theta is going to be for this red triangle, the adjacent divided by the hypotenuse, right? So this is hypotenuse adjacent over hypotenuse, which means that this value y will be the length of the adjacent side. So this distance from the origin to here is my value y, so that is my score. What you can notice then is that for every single point x-i that lies on this dotted line here, all of this normally access times cosine Theta are going to be equal to this point. Alright? So this dotted line corresponds to when w1 transpose times any point along this line x side will be equal to some constant, I'll call that constant K. Any questions there? All right, so then let's say we have our second class, W2 transpose x. Let's say that W2 is over here. For the same exact reason, there's going to be a line perpendicular to W2. I'm going to draw that over here. And this perpendicular line defines a line where W2 transpose XI is equal to some constant. Let me call this x j. So it's just a different example. So now we know that if we were to put in a new example, I'll draw this as an orange square here. I wanted to do it in class one or class two. All right, just from this image you can tell me what it is. Because if I look at its core for classic one is going to be larger than K, right? Anything along this line is a score equals k. If you're lower than the spine and the score will be less than K. And if you're greater than the five is called the period m and k. Whereas for W2, if you're vivo despite your score is greater than K, And if you're above this line, discourse less than k. So this orange triangle, we will have a higher score in the red class one than the blue class two. I can therefore classify it as being in class. And so when you think about how these spaces are essentially partitioned by these lines, you can see that what the linear classifier is doing is it's going to partition the space into linear boundaries. And so this is going to be a point of indecision where the score for W1 and W2, class one and class two are going to be equal and there's going to be equal to k. There are also all these other parallel lines. So this will be when w1 transpose X transpose. So smaller value than k, we'll call it p. And then there'll be a similar line here for W2. And where they intersect will be another point of indecision. And if you connect all of these lines, this becomes, this becomes a linear boundary separating class one and class two. I can't draw a line very well from that purple line, but that's supposed to be a line that separates class one and class two. Generalizes as you go from two classes all the way up to ten classes, you're going to have essentially ten separate lines have been nearly cord that had partition the space within your boundaries. Any questions here? All right. Hopefully again, review for most of you. Back there. The question is, what are the blue and red dashed lines? The blue and red dashed lines correspond to points where any value on these dashed line. Um, we'll have the same score for the class W2. And any point on this red dashed lines will have the same score for class one. So there are the lines of the same z-score and these lines will always be perpendicular to. This. Question is, is why just a scalar in this case? Yes, that's correct. So y will always be a scalar for each class. We're going to get the score from classical and the score for class two, et cetera. And just comparable. Tom I says, do these w i's have a relationship to the mean of the feature vector belonging to class I, e.g. does this wi have a relationship to the mean automobile? I don't know off the top of my head. I think the answer is no because it'll be something that general to standard via an imperfect optimization process. But I would say that the mean feature is a really good initialization for your gradient descent is to find something kinda close to the mean feature. Austin. Yeah. Yeah, totally raise a good point, which is several of you won't follow this. Nowhere is, this is not going to be covered in this class. But if you're doing maximum likelihood with Gaussian conditional Gaussian distribution, the answer ends up being the beads and they put their answers in this distribution. In that case of the poem is also convex, whereas in this case is because no question. Oh great. The question is, in this slide, what should be the values of k for the blue lines and the lines is arbitrary. I could have picked by, this could be like k equals five. And then, then for this one it might be like k equals four, e.g. so it's just a line of where everything on here has the same value. That value is arbitrary. So I knew somebody said, Oh, yeah, just select that senior. Thanks everyone. So why don't we also normally lineup that we along with a picture. I see. Yeah. So the student is asking, you know, there could be an image that is highly aligned to W1. Let's say it's just example in purple here. And then there could be another image which looks a lot like or which is more aligned with another vector. But maybe it's over here. And it has a larger score on w1 and the purple. Yeah, so there are two things to say here. I said proceeded with an asking, you should be normalized. So two things to say here. The first is, while the score for this blue point will be higher than the purple point for W1. Remember that when we're classifying, we're not looking at the absolute value of the stores scores for a class. We're looking at how this compares to the score in another class. For another vector, this course will be higher than it was for W1, whereas for this purple one, it will only defies the company one. That's the first thing. The second thing is, yes, it is a good idea to normalize and we'll do that in the big sets. Yeah, I think the question is, is this fine for different XII or the same excite? It is for a different one, yes. So I I kinda overloaded. I had this would be for x j. So this would be like for a point like this. Next question. Sorry, can you say that again? Yeah. So the question is, if the score for two classes is the same, what happens? So the image, I'll just say for now, the image can be classified as either a class. You're going to see in a few slides that we're going to change these scores to probabilities. And so if they were equivalent scores would be saying that the probability of any of these classes. And if you were to do like e.g. text generation, you could sample from that distribution. And some percent of the time you get the font-awesome. And after some time you get to the other. All right. Last question here. Yeah. Yeah. In response to Tom was a question where Tom Waits asking what these conversion to me. So just some practical optimization. It won't converge to the mean because we're gonna be doing gradient descent. However, because we have this intuition that, you know, the features should look like the images than it would be a good idea to initialize these features. Will have, you will have him Clark this on homework number two. You're going to have to change the Softmax classifier there. Alright, so let's move on. Um, where my McCarran classifiers fail. This is pretty straightforward, also probably be kept for many of you, but if you have problems where the boundaries of the medically cannot be linear, then the tail. So one example is the XOR problem. So maybe our data points look like this and there's no linear boundary that can separate the red from the blue loads ordered. You might have radial data. So maybe your data looks like this red axis in a circle and then blue O's in another bigger circle. And there's no linear classifier that you can build this data. Hello, can I give this example? Because several of you may see that if you just do a change of coordinates from Euclidean into polar coordinates, instead of x and y, we're going to have r and theta, right? Then the red axis will reside here, and the blue circles will reside here. The reason I'm showing this example is to say data may not be linearly classifiable. But oftentimes there's gonna be a transformation that you can make such that they are linearly classified. And I told you that a neural network usually has a softmax classifier, which is a linear classifier at the output of the neural network. And so one way to think of a neural network, knowing now that is output is linear. Even though it's doing a very highly non-linear task, is that the earlier layers of a neural network performed the non-linear transformations to make the data is linearly separable for a softmax classifier as possible. I just intuition to keep in mind as we continue on. We have now a way to transform our input data X output scores. Why? And what we want to do now is we want to be able to know them what a good model is. So machine learning recipe, that means we need a loss function. We need a way to tell us how good asserting W and a BR. And then we need a procedure, a learning method to learn W and make this loss function as small as possible. So this will now hearken back to what we were talking about earlier or what some students raised in terms of what the loss function is here. So we're going to optimize this via procedure called maximum likelihood estimation. Maximum likelihood optimization, optimization. The idea is that you're going to have data like x's and y's. And we want to maximize or wanted to be able to compute the probability of having observed data. This can be kind of a tricky to understand. So I always motivate this with the simplest coin flipping example, actual softmax classifier. So let's say that we have a coin given to us. And the coin has a parameter, which is the probability that it comes up. Alright? You don't know what the probability of this coin comes up heads is that you're allowed to flip the coin as many times as you want to determine that probability of coming up heads, right? So when I phrased that to you all, do you know the answer? To get the probability of coming up heads, you flip the coin 1 million times and kept him to the portion of tax, right? How does this come from? A more rigorous mathematical approach? So this experiment where you flip the coin 1 million times or what's the state eight times? Because I don't want to write 1 million heads and tails. Let's say you flip the coin eight times and you get heads, tails, heads, heads, tails, tails, heads, tails. That's your data from a one plus, and you want to determine what the probability of coming up heads, right? And so we need a model for this. And our model is that our data are exempt or our samples X, which represents the outcome of Bitcoin. These are either going to be heads, which I'll assign the number one, or tails, which I'll assign the number zero. So this is a Bernoulli random variable and it's gonna be heads with probability theta. And I'll write this Theta in red. And then if it's heads with probability theta, then it's going to be tails with probability one minus theta. Theta is my parameter. And I get to choose Theta or get to optimize for Theta to best explain the sequence of observations, heads and tails. In other words, I get to choose theta to maximize the probability of having observed this experiments occur. So the way that we do this is we have to write a problem, ability to expression because the likelihood, the likelihood is simple. It's simply the probability of having observed or data given your parameters. Okay? So let me just start off by saying, let's say I give you three models to choose from. In model one. We have that your parameter theta is equal to one. In model two, model three, theta is equal to 0.75 and model three is equal to 0.5. I've given you these three models to choose from. I want you to tell me which one is most likely. The way that you would do that is you would calculate the probability of observing your data under these models assumptions. So let's start off with model one. Model one says the probability of coming up heads is one. And if that's true, then there would never be any tails. So the likelihood should be zero. Let's confirm that the model one likelihood. The probability of observing this sequence would be the probability of heads. The probability of the first hedge is a one. The probability of tails is zero. Heads, one head. This one tells us their hotels is Sarah edge, this one tells us around. And if I multiply these altogether, the probability of model one being correct or what I want to claim this data is there so I can reject model one. It definitely does not explain this. For model2, this would equal 0.75, the probability of coming up heads raised to the fourth because I have foreheads and then 0.25, the public coming up tails raised to the fourth. And this equals 0.0 0124. Then for Model three, it would be 0.5 raised to the fourth times 0.5 base to the fourth equals 0.0, 0239. So we can see a faucet models 12.3. The probability or the likelihood of having observed this data is highest under Model three. So if I asked you to choose at these three models, what is the best model you should choose? Model three, because it has the highest likelihood of explaining the data. This is a really critical concepts. So any questions here? Alright, again, hopefully review for most of you. For me, you know, sometimes perceive any problems getting really complicated and it's helpful for me to come back and pick up the simplest example. And this is the example that I usually returned to you for maps provided in a tomboy says, I'm sorry, can you say it one more time? Columns on the right. So tomboy is saying, essentially, we were going to make an assumption. You'll see this later on that P of D given theta, every single trial is going to be independent. The question is, is P of D are those child's independence also? To get to observe the sequence? Yes, that is true. Yeah. So that is not an independent. Yeah, thank you so much. Alright. So this was just sourcing from three models. In general. For optimization, I wouldn't ask you to choose between three models. I would ask you to choose what is the best value of Theta. All right? So in this case we would actually write out the equation for the likelihood. So the likelihood l, This is the likelihood would be the likelihood of observing your data given your parameters Theta. So this would be theta to the fourth times one minus theta to the fourth, because I have four heads and four tails. And if I wanted to then just solve for the best data. Then we'd have to do is take the derivative of the likelihood with respect to Theta, set this equal to zero. And if you do this, you just solve for Theta equals one-half. I'll leave that as an exercise. It's all. One quick note if you do this exercise is that generally in machine learning, oftentime for going to take the log likelihood. So instead of looking at just L, we're gonna look at Lagerfeld. Maximizing the log likelihood is equivalent to maximizing the likelihood because log is a monotonic function. If the employment for longest larger, the output will be larger. But then the nice thing about taking a log is it turns all of these multiplications into summation. So generally I makes our life easier, so we'll usually be working with log likelihoods. So tomboy is raising another point, which is that optimization data, because it's a probability is constrained 0-1. And so you could go in, and you said, do this with overdrawn two multiplier. I believe that for this example, if you just solve this unconstrained data, so pumped up to 0.5 because he shook me. But yeah. The question is, the data here is just one experiment. Why are we trying to fit Theta to just this one experiment I gave? You might have had it, maybe too little data to infer data reliably. So in general, yes, more data will lead us to a better a better estimate of the underlying fabric data. But sometimes we just won't have that much data and so we kind of have to, I'm going to figure it out and probably the next form. A tomboy also raised the point that this is a independent coin flips. It can be thought of as a experiments, not one. But the point is, just more data will be better. All right, other questions. Yeah. The question is, we don't have the equation for the likelihood. How can we solve this? So you need an expression for the likelihood to be able to use maximum likelihood. And we're going to derive the expression for the likelihood that a soft classifier if you're in this situation, but if you can't derive the likelihood and that's sometimes arises, then you may use other machine-learning interests. And so this is truly a preview of what comes into play in week eight, OB GYN, we're going to learn about an architecture called the variational autoencoder architecture. The likelihood is intractable, it cannot be written down. However, what we'll do is we'll be able to write down a lower bound and we can optimize the lower ends up working. So there are a few other tricks that we can use in the case. We can write out the likelihood. For both classification problems we do with neural networks. We're going to use a softmax classifier at the output, and that will give us a fair likelihood. We're not something that is going to correspond to something called the cross entropy loss. That public who here is the cross entropy loss for like half the class. Some of you have heard of that before. Alright? Any other questions? Alright. That's maximum likelihood. I'm great. So there's one more thing. I wanted to go over this, which is the chain rule for probability. I don't over this because I want to make sure we're all on the same page, but this might just be me. I took several probability classes and it wasn't until I took information theory, but I really understood the chain rule for probability. So I'd like to just go over it to make sure you all know. Well, basically in probability, there are two really critical roles. When is the chain rule and what is the law of total probability? And with that, you can do a lot of manipulations for probabilistic expressions, which will be important for machine learning. The chain rule for probabilities as follows. In this class, we have a random variable we usually denote with a capital letter. So let's say I have a random variable, big a. And I want to know the probability that it takes on an event. Event we'll denote with the lowercase letter a. In this class or in other constants. And you may seem this is denoted with a little p. And then the random variable that this little p defines a distribution over is a subscript. And then the event that it takes on is in the parentheses. Then oftentimes the events are matched to the same letter as the random variable big A's. So oftentimes people will just write this as. Appear there, and that's what we're going to do in this class. There's also a probability that big B equals there'll be, this will be denoted in the following ways. And then we can have the probability of 20 events so that a equals little, a, big B equals the Hopi. And this one, we could denote the following ways. But in this class we'll usually just write this as P of a comma b. Alright? So there is a chain rule for probability. When there are two variables. This is Bayes rule. And what it does is it tells me what is the probability of x. Let me write this out with a probability expression first. Let's say I want to know the probability that a equals little a. So the answer is the same thing as this comma and big B equals B. The chain rule says that if I want to know the probability that big a equals little a and big B equals Adobe. I can break this down into two sub-problems. The following. First, I could find the probability that big a equals little a. Then because what I want is big N equals little a and big B equals B. After us continue the probability that a equals a, i need to compute the probability that big B equals b, given that big a equals with all. And what this means is, in notation, the probability of a and B is equal to probability of a. Given, we draw with a vertical bar, the conditioning bar. This will be the probability of B given a. The probability of getting both a and B is the probability of getting little a and then fixing that you got and you got it. All right. And that's the second term right here. Because of this, you could see up to read this in a different way. I could say that this is the probability that I get. That'll be alright. And then conditioned, or given that I have little b, what's the probability that I get a little area? So this would be probability of a given little. Any questions there? Okay, so let me write the chain rule now for three variables. So I might have a P of a comma b comma c. I can write this in many different compositions, right? So I can write the probability that first get little c. And now all the probabilities will be conditioned on getting velocity. So after I get little seat, I can write this as the probability Getting a given I got little c, little c, I have a, I fix those, and then all I have to do is get little b. And so this would be times the probability of getting little b given that I already have little a and little c. So this is a valid decomposition, decomposition of a, B, and C. I can write this in another way. I took the prelim this as the probability of a times the probability of B given a. So I have, AND now, now I have to get a seat. So probability of C, given that I have little a and little b. Alright, there's one more way I could have written this as a probability dot a and C. And then what's left to do is be, given that I got AMC. And so this is going to be probability of B given that I got fancy. Again, these are all valid decomposition of an intuition to have is that basically every single random variable gets to go in front of the conditioning bar once. So here C goes in front of the conditioning Clark, there's a conditioning bar. And after that, in every single term, C is going to be behind the conditioning part. And this term a data governance of the conditioning bar. And so for the remaining term, it goes. Any questions there? The question is, does this require independence between variables? The answer is, this is always true irrespective of indulgence. Russia says that there's independent conditioning. So you may know that if a and B are independent, then I could write that as p of k times P of B. Because a and B are independent, I can remove it from the condition because a, it gives me no information about our apps. So just to make sure everyone's following two example questions. So I'll write this out. Let's say that I have. Billy a, b comma c given d and e. This is equal to something in the numerator that I want you to find, divided by P of D times P of E given D. So take 20 s to figure out what that red box should be for the city correctly. Feel free to talk to your neighbors. I want to raise your hand and tell me what the question mark is. Perfect. Yeah. So this student says that the red question mark is probability of B, C, D, and D. The way that you arrive at this is that you know that p of t times P of E given D equals P of D and E. Then if I take P of D and E and I multiply it by P of B and C given that have no deal middle. Then these two by our chain rule, just equal P of B, C, D, and E, right? Does anyone have questions on that example? Yeah, great. So let me repeat that. P of D times P of E given D is the probability of P of D and E. That's the chain rule for two variables, just like here. Then what I'm saying is to get this red question mark, I'm gonna take this denominator and multiply it by the left-hand side, what that would be as P of D and E. And then times the probability of B and C given a and little a. Because this first term is the probability of D and E occurring. And then the second term is conditioned on that date heard. Then their product is the probability of all thermocline B, C, D, and E. All right, so we'll do one more. And that will be the following. P. I have P of D comma E times a question mark equals P of a, B, C, D and E divided by P of a given B, C, D and E. And I take thirty-seconds topic top of the neighborhoods and try to figure out what the red question mark is. And then after that, we will we will move on. All right. Does someone want to tell me or take a stab at what the exact boxes. Yeah. Yeah, that's exactly right. So the students says At the answer is probability of B and c given d and e. All right, here's the way that we will see them. I'm going to multiply the denominator with the left-hand side. And if the denominator times the left-hand side equals P of a, B, C, D, and E. And I know that this expression, right? This thing has to multiply P of a given B, C, D, and H. It gives me the probability of all of them. So this expression here has to be equal to P of b, c, d, and d, right? Because this thing times this thing gives me the joint probability and everything. Here. I wanna know what multiplies p of D and E to give me a P of E, C, D and E. And the answer for that is what we just did above here. It would be P of Vc getting the right. This is your first time or you're not as comfortable between both voted, please just take some time and we have several questions on homework number one, color go over this. When we come back next, people derive them. 
To get started for today. Our announcements, our one I reminded you that homework number two is due tonight. Upload it to grade scope, and please budget time for submission will be done a lot of sabbatical about problems with submission. Compiling or printing PDFs. Or I'll put into grade scope. So please just pleasant time if there's an issue for the homework, and then be sure to submit your code violence Notebooks in your doc PY files as well. The TAs will be uploading homework number three later after class today. And instead of it being due a week from today on Monday, we decided to give two more days on it, so it'll be due Wednesday, February 2023. Any questions on course logistics? The question is, will this affect the due Jacob future homeworks? The answer is yes. So I believe in conversation with the TAs. Homework number four will also be due the Wednesday after that, and then homework number five will now be two after the midterm. That's how we started clearing. Alright? Any other questions? Alright, we're gonna get back to material. So today's lecture is going to be on backpropagation. And this covers Section 6.5 is 0.6 and the deep learning textbook. And you'll recall that at the end of lecture last Wednesday, we just finished talking about the neural network architecture. We've talked about the activation function f that we would use for it. And we've talked about how for classification we would use the cross entropy loss. So that's great. We have our model architecture, we have our loss function. So we know that at the end of a neural network, we will pass this to some cross entropy loss to do classification. And now we're basically at the last part, being able to actually train and deploy neural networks. Which is we need to learn how to set the weights of the neural network. So remember that in-between every single layer we have, which is there'll be new and biases B. And so in this three layer neural network, we would have three sets of weights and biases. And we need to be able to know how to update these weights and biases. And we'll do that via gradient descent. But to do gradient descent, you know how to update the weights and biases. We need to compute gradients. Dl DW three, DL DW to, and DL DW one. And because this network looks a bit more complicated at least and other things, it's still an open question as to how we compute these gradients. Alright? So this lecture is going to be on backpropagation, which is the algorithm that we're going to use to be able to take the gradient of the loss with respect to W1, W2, and W3, basically any parameters and network, we're going to be able to take the gradient of the loss with respect to a few things that nomenclature. There's something called forward propagation. So forward propagation is just the act of starting at your inputs x and from them doing the forward computation. So linear layers then followed by your ReLu during the forward computation through the network to eventually calculate your loss. So for propagation goes from your inputs to calculate your loss. And this term backpropagation, is essentially the opposite. So at backpropagation, what we're going to do, we'll see is we're going to start off with a gradient of the loss with respect to our output. Then we're going to develop rules to basically calculate all of these gradients by taking our gradient at the end and basically back-propagating it all the way until the end. So that's where the name backprop comes from. Any questions on the motivation of why we need that competition. Alright, and so after we get these gradients, all we have to do is use gradient descent and we'll be able to change these weights to optimize this neural network. Alright, so these slides here, just putting it into text, what we, what we described on the prior slide, as well as the nomenclature before it and backpropogation. There is a question that you might have, which is, why do we need backpropagation? So if you look at the neural network architecture and you'd write out this equations, there is conceivably a way where you can analytically compute the gradients of the loss with respect to the weights, just like you did in homework number two for the softmax and for the hinge loss. Alright, so why did we do back propagation? The first thing is that back-propagation will find out is computationally efficient. Because before every backpropagation stuff, we're going to do a forward propagation step. We calculate all of the values and the number on route from the input to the loss. And if we just pass those values, will find that we can reuse them when we calculate backpropagated gradients. And so you don't have to do extra computation. Let me do back propagation. And then another reason that you might want to do back propagation is basically operationalizes the gradient computation procedure. So while you could write out the equation of the neural network and analytically calculate ingredients. At least for me, it takes quite a bit of time and it takes a bunch of mental effort. It's possible to do. But when you do backpropagation, but we're going to see is that we want to break down this gradient into many sub-problems, each of which are simple to solve. And so just like how decomposing code into functions is a good idea. So two, is decomposing your gradients into these small complications. We find out that when that happens, it also operationalizes gradient calculation, which means that it gives us a very simple algorithm to compute gradients in a neural network. And that is what makes software like PyTorch and TensorFlow is so powerful because there'll be able to compute gradients of any functions that we pass them using this backpropagation algorithm. Alright? Any questions before we get into the mathematical details here? Alright? So we're going to start off with just the simplest example where you can calculate an analytical gradient. But we'll show you, you how backpropagation works in this novel graduate to tougher examples. So we may want to compute the gradient with respect to our inputs x, y, and z, where f implements this function, alright? And this is not difficult to do, right? If I want to compute DS DZ, alright? We know that that's equal to x plus y. And we can also compute ds dx, that equals df dx and df dy y both equal to z. You can just do that by inspection. So this is super simple. We're going to use this example to motivate back propagation. So before backpropagation always comes forward propagation. So what I want to do is I'm just going to assign some dummy values to x, y, and z. There'll be 23.4. And then if we were to do this computation, x plus y times z, what I'm want to do is I'm going to draw what is called a computational graph. Alright? So basically what I do is I take this operation and I break it down where the sub operations are these nodes in the graph. And then our variables are the inputs. So here we have x plus y being implemented by this part of the computational graph. And then the result is multiplied by Z, and that's this multiplication here. And then that gives hopefully straightforward. If you plug in the values 234, you can go through these operations and get that at people's. What happens in backpropagation. Backpropagation always happens when we do it or some settings of x, y, and z. So what we do in backpropagation is we're going to start off with the upstream gradient. What do I mean by upstream gradient? I just need a gradient at the very end of our graphs. So here I'm just going to assume that DL df equals one. And I'm going to draw what the gradient of the loss with respect to each wire and the graph is. The graph as something in red. The wire. Alright, we'll talk about why we do it this way. You may say, where does the LDF come from? So at the output of a neural network, you'll have a softmax classifier. And from homework number two, you already know how to differentiate the loss with respect to the softmax classifier parameters. And so that would be like you're starting gradient at the end of the graph. Alright? And then what we wanna do is we want to take this gradient backpropagated to compute the gradients with respect to every other node in the networks. So we just assume for simplicity that we're going to have our upstream gradient equal to one in this example, but we will always be starting off with some gradients at the output. Any questions so far? All right, let's start to do the operations then to backpropagate. So that propagation really is the chain rule for calculus or the chain rule for derivatives. So let's say that I want to backpropagate the LDF to compute the gradient with respect to DC. If I want to compute the LDC, right, what I would do is I would use my chain rule for calculus, where DLD z would equal D LDF times df. Dz. Remember, DL, DFT is known to us, is the value of our upstream gradient that's just gonna be equal to one in this example. What is the FTC? This is something that we're going to call a local gradient, and I'll have a slide later that describes that more. But DFT is a gradient that we can compute because if we look at F, F is equal to z times the value on this wire over here. Let me just call this wire w. I'm gonna go ahead and call this wire w. Alright? So we know that F equals w times z. That's what this part of the computation graph tells us. If I want to compute d, f, d z, I would differentiate this equation with respect to w, and that will give me that ds z equals w. Remember w is a value of disquiet. If I want to compute DLD, I take my upstream gradient whose value is equal to one. And then I take the value of d f d z is w, the value on this wire, and that's equal to five. So this is equal to five. And therefore DLD z equals Any questions. Alright, and so similarly, I keep backpropagate through this wire by computing DL DW. So if I wanted the gradient at this wire, this would be DL DW. Dl DW would equal D L ds times ds dW, df dW equals z, right? Because remember, you put w times e. So if I differentiate this with respect to w, I just get x0 out. So in this example, the LDF is equal to one and d l, sorry, the FTW equals z and the value of z is equal to four. Alright, so then the gradient here DO w equals same operation as Bob for DLD z, but this is just for the other wire. Finally, backpropagate the gradient to find what DLD x and DLD YR. So if I want to find the L dx and dy l v y, then what I would do is I would write up the computation of this node, which is that w equals x plus y. Therefore, dw dx equals one, and d w d y equals one. Let's say I wanted to get dy dx. So let's say I want to get this gradient here. This is dy, dx. The gradient that I go at. This is DL DW, right? So this here is DLT W sacrilegious DL DW times dw, dx, dw, dx, I know from here, is equal to one. This thing equals one. And then DL DW, this thing equals four. So this equals four. And therefore the gradients, the L dx is equal to four. And for the same reason, DLD y will also equal four. All right, so you see just by using the simple chain rule, but doing this procedure where we just focused on one wire at a time and write the chain rule for that wider. You see that we were able to backpropagate this gradient to compute d l with respect to every single input variable x, y, and z, as well as intermediate. Any questions there? Rupture. And then that button. And so Ross's question is, can rewrite this as DLD F times d FTW. Like to swap these two because these are scalars in this case you can. But later on in this class we're going to talk about the factor multivariate chain rule in which case the order matters. And that's why I'll generally write the chain rule going from right to left. Because you'll see for the nominator layout, our chain rule for vectors and matrices will go from right to left. All right, you guys have had to be followed this example. Alright, so this is the idea of back propagation. We did it for a very shipboard example and now we're going to make it a bit more difficult. But first, some intuitions. The basic idea, as you've seen us play it out, is. We're going to always take whatever computation we're doing and we're going to break it down into a computational graph. That's this drawing over here. The forward pass, we will plug in the values of the inputs to compute our output. And then the backward pass. We'll first take the derivative of the output with respect to our last wire. That'll give us a gradient. In this case, we just said that radius is one. Then basically at every single node, what we can do is we can backpropagate the derivative by using the chain rule associated with the computation at this nerve, which we call a vocal. So let me just tell you more explicitly what I mean by local gradient. So if I start off with an upstream derivative CLTS, so the identity of this wire is, if I want to compute d L dx, so this graph, X and Y are these wires. And so d L, dx would be the gradients. Under this top input. We know that dy, dx is going to equal our upstream gradient. That's the gradient on the wire that's ahead of me. So this D LDS is called My upstream gradient. Then there's going to be a gradient. Dy dx, df, dx, and df dx is going to be called a local gradient. Because df dx is the gradient of this vocal computation or the football function f. So when I refer to a local gradient that refers to df, dx, and that reflects what the gradient is. For this function. In the backward pass, I can always compute the gradient of the wires just ahead at the input. As long as I know what the upstream gradient is and I know what the local gradient is, e, I know what the derivative of this function is with respect to its end points. Any questions there? Alright, so this is a slides just talk to you about what we talked about. We take a derivative. And as long as I know how to differentiate this function with respect to its inputs, I can always backpropagate through that function. This is how it around you'll see pie charts and TensorFlow operationalized gradient calculation. Basically whenever you call an operation like multiply or even something like cross-entropy. Pytorch or TensorFlow will always store what the gradient is of the function that you're calling. It will know how to compute this gradient. So that if you want to backpropagate, it knows to just take your upstream gradient, multiply it by the gradient of d f with respect to its inputs and then you can backpropagate the gradient to it. These foreigners question. Yeah. Yeah, the question is, is the order of these gradients because we used the denominator. So when these are multivariate quantities, yes, it will be because we use the denominator. Other questions, and we'll talk about that in more detail with Tom way says, are there, are there any roles to drawing a computational graph like e.g. x plus y times z. You wrote three independent variables as XYZ and then decompose. All right? This one way, another way. I see, yes. I'm always asking basically when I drew this computational graph, we draw every single operation, the addition and the multiplication has its own nodes. This is the way I think that you should all do it. You could conceivably no, ignored this phosphine and have some other way to draw a computational graph that confusing and it'll probably be two mistakes. And so when you draw these computational data, at least for this class, it will be most helpful for you to really write out every single operation. When you, when you draw these computational gas. That's gonna be a large part of getting, getting things ready. Alright? So just to nail it, the basic intuition of backpropagation, right? Is that we break up the calculation of a gradient into small and simple steps by focusing at one node at a time in the ultimate computation that we're doing. Each of the nodes. And these graphs correspond to a function. As long as I know the derivative of the function with respect to its inputs, then I will always be able to take an upstream gradient backpropagated to the inputs. Alright? And if I just keep composing these operations, I'll be able to take the gradient at the output of a neural network and back propagated to every single layer, the layer that occurs before it. So composing all of these gradients together then returns the overall group. Any questions there? Alright, so let's go ahead and talk about some operations where we're going to see this so often. And there's basically a role for backpropagating that makes it really straightforward. So if you have two inputs, like x and y and they are added together, we know that the local gradients, so in this case, equals x plus y. So the local gradients will be df dx and df dy y. And these things are able to work. So because these things are equal to one, and the gradient here is the upstream gradient times local gradient, which is one. When you see a plus sign and you're doing backpropagation, what that means is that a plus sign just passes through the gradient. So if you see a plus sign, you can always just write that this gradient here, dy dx will just equal d LDF, the upstream gradient, also equal TO. So if we think of these as gates, basically the AG gave the add operation and the neural network will distribute your gradient, the LDF to these two wires. We also will have multiplication gate. So let's say that we have two inputs, x and y. And the output is going to be S equals x times y. And we know the local gradients are df dx equals y and df dy y equals x. Again, since that propagation tells me to take my upstream gradients and multiply it by the local gradient. And the gradient at this wire is going to be the local gradient, y times dy LDF. So this is going to equal y times dy LDF. And then the gradients on this wire is going to be equal to x times DL DS. So when you see a multiplication gate, you can think of it as like a radiant couldn't put switcher where switcher just means I'm going to take my upstream gradient backpropagated to y. Then I multiply by x, by backpropagate to excellent, I have to multiply by y. So whenever you backpropagate through, you're just multiplying the upstream gradient by the value on the other wire. Flushing said, alright, let's continue on then. Um, we're gonna do this, the max operation because we're going to find out that max is going to come up a lot in our neural networks because we are going to be using the ReLu operations, which is max of zero comma x. So in this case, let's say that df dx, sorry, must first day that ash is equal to max x comma y. Then df dx is going to be an important one. If x is bigger than y. Because if x is bigger than y, then f equals x and dx, dx equals one. Then if y is bigger than x, then x equals y. Differentiate with respect to x, I get zero error. So this is going to equal one if x is greater than y and zero. Otherwise. We can write this as the indicator function, that x is bigger than y. So then when I got off it through a max operation, I'm gonna get what is essentially a gradient router. So one of these conditions, x is bigger than Y or Y is bigger than x is going to be true. And so x is bigger than y. Then the gradient comes here, has DLD f, and the gradient on this wire will be zero. If y is bigger than x, then the gradient d LDF comes onto this wire. But over here it's going to be equal to zero. So the max operation will route the LDF to whichever wire as a bigger value. Daniel. Daniel's question is what happens when x equals y? That gets asked every single year. So I always say, practically, first-off, x will not be equal y in our networks because we practically implement them because they'll be like double or single precision. If x equals y, then it depends on how you define your max operation. So if you said S is equal to y, if we return x, then that, then you could assign the gradient to be one when x is bigger than or equal to one. Question. Other questions. Yeah. I'm not sure I understood the question. So you said when we say DY DX before. When I say that, sorry, so I'm not making any assumption about independence of X and Y. Just looking at the function, you differentiate it. You get an x and y depend on each other in some way when we take the partial derivatives of the assumed that the other variables are all constants. The question is, when do we use the max case? When we have ReLu, we know that ReLu of x equals max of x and zero. And so in our computational graph, we're going to have a bunch of bees, or we can have a max where the inputs are x and zero. Yeah, the student's question is, we don't want to backpropagate to zero, right? Yeah, this case it would be useless to backpropagate to zero. But we still need to know how to backpropagate to x. And that depends on the value of X being bigger than or less than zero. We're going to continue on with this. What we're gonna do is we're gonna do a more involved scalar example. The scalar example we're going to be using a bunch of big clinical case we talked about already and multiplication and addition. So I'm going to be using a bunch of other gates as well so that we can make sure that we understand this operation. So what I have here is the sigmoid function. We have 1/1 plus exponential of minus WE 00 plus W1, X1 plus W2. And so I've drawn out the computational graph. So all the way up until this node, this node em, implements this sum here. Then I multiply that by minus one, That's this node. Then I exponentiate the results, you get this. So that's exponential. Then to get the entire denominator, I add one, that's this node right here. And then to get f, I take the inverse of what's in the denominator and I guess the one over the denominator. And so that's this computational graph that we see. And this is where we've drawn out every single operation again, just for the sake of being careful and it'll it'll unsure a reduced headaches and mistakes later on. So what I've done here is I've just arbitrarily chosen some values of w zero, et cetera, et cetera. I've done the forward hopper for propagation through this graph. So if you assign these values and you do all these operations, you will get that f equals zero points sudden the root. Alright? Now we're gonna start with backpropagation. So we're going to assume that we're going to start off with an upstream gradient. Dld. Yes. That is equal to warn. And the first thing that I want to do is a backpropagation through an operation we haven't talked about yet. So what I want to back propagate from one to the gradient over here. I'm just going to call this wire. Okay? So let's say the value of this wire is a and in this particular instantiation, a equals 1.37. Firstly, I want you to think about it for 20 to 30 s. Feel free to talk to your neighbor as to how do I back propagate from the LDF here to the LDA, which is backpropagating through this inverse operation. Alright, if there's some discussion, does someone want to raise her, someone raise their hand and tell me how I back propagate through the inverse operation. So this student told us the answer to exactly. Remember that I break down my computation into my upstream gradient, which is DLT F. And that equals one times my vocal gradient, which is ds, da. Alright? In this case is the inverse operation. So f equals one over a. And therefore, if I want to compute my local gradient, which is the gradient of just the inverse operation. I would get that DF da equals minus one over a squared. Therefore, if I want to compute what the LDA is, all I have to do is I had to do the LDA equals my vocal gradient by local gradient in this case is going to be minus one over the value of a. So a is 1.37 squared times by upstream gradient which is born. And this thing simplifies to be equal to -0.53. So the gradient over here is gonna be -0.5. I guess that's just taking gradient, which is the gradient of f with respect to a and then multiplying it by my gradients of one. Any questions? So does anyone want me to go over this guy? Alright, we'll continue on then. I have, this is the word for the one that we just did. We have the gradient -0.53 here. Next we're backpropagating through a plus sign and no for plus sign, the video just passes through. So if I want the gradient at this node here, it would be just -0.53 times the gradient passes through a plus sign. So we have -0.1 by three here. Now we want to backpropagate through this exponent exponential function. And to do this, we would do exactly what we've been talking about. So if I call the values on this wire, if I call this C and I call this one D wire, which has the value of one, that will also get minus point. Yeah, great. I see a tomboy saying that the gradient at this wire would be -0.53. So if you were to just follow our rules, that would be true. But really those gate rules apply it. This is like some variable because really DEF d1 is equal to zero. So if this were a variable that is gradient would also be -0.5. Alright, so we're going to backpropagate through this exponential. Here. I will write up the computation. So if I call these wires b and c, Then the local computation is that d equals EXP of C. And if I want my local gradient, right, I need to be able to compute DB DC. And the derivative of this function is exponential of C. So if I want the gradient here, I would take my upstream gradient, which is -0.53 and multiply it by the local gradient, which is exponential of c and c is equal to minus one. So the gradient over here as -0.53 times exponential of minus one equals -0.2. Any questions on this local gradient? Backpropagation? Alright, if we've made it this far, the rest is easy. Because the rest are just multiplications and additions, or for those we have our, alright. So if I want to backpropagate to this value over here, we know when you backpropagate through a multiplication, you take the upstream gradient and you multiply it by the value of the other wire. So this would be -0.2 times minus one. That gives me 0.20. And then 0.20 here, I can start to backpropagate through these plus signs, which just pass through the gradient. So the gradient DLD W2, It's going to be 0.20. This is going to be 0.20. These are going to be 0.2, 0.0, 0.20 plus signs are super easy. And then the multiplication for just a tiny bit more complicated. So backpropagating from 0.20 to x one, I would multiply 0.20 by the wire value here, which is two. So this gradient would be 0.40. This would be 0.20 times the value of X1, which is two, that's 0.00 would be 0.20 and DL DW zero would be -0.20. And that's using the rules that we developed further multiplication gates. Here you've seen that by using this backdrop roll, we've taken the LDF. We have compute all the gradients with respect to the inputs W0, W1, X1 and believes you. Alright? Any questions on this example or any step in this example, and then use it as a proposition. So yes, this is scaling up when we're differentiating whether you're doing. Great. Yeah, so his question is about pi torch and how it keeps track of what the gradients are. So if you're multiplying two matrices and Py Torch, right, you wouldn't use Pi torch, his version of matrix multiply. Because pi torches version of matrix multiply, doesn't just multiply two matrices, but it also knows what the local gradient is so that it can backpropagate through that operation. So basically in Py Torch, whenever you make a computational graph, every single node in our computational graph is going to be a defined by torch function. But only that, not only does the operation, but stores or local gradients, and then you backpropagate through it. And then if you want to implement a function that's not implied works, you can define your own function and all you have to do is pass it what the local gradient is. So then pie charts will still know how to backpropagate through it. We'll get more into this in week seven or week eight when we talk about the deep-learning likers. Any other questions? Yeah, question is, can I go over the inverse again? Yes. So for the inverse, remember that we always, when we calculate the gradient at the input of the function here, we take our upstream gradient, g LDF, and we multiply that by the local gradient of this inverse function. The inverse function I'm calling f. So that would be the FDA where f equals one over there. All I have to do to get the local gradient is compute the FDA. And the gradient of y over a is minus one over k squared. So now I know that the LDA will be DLD F, which is one times df dy, which is minus one over a squared. A is the value of this wire. So a here is the value 1.37. And so my overall answer is gonna be one times -1/1, 0.3 s squared. Any other questions here? Yes. Thank you. So the student is asking, when I call somebody the upstream gradient, is it different for every wire? Yes. So when I back propagate e.g. through this exponential function, my oxygen gradient is going to be -0.53 and my local gradient is the gradient of this exponential functions. So basically, I always look at just one operation. The upstream gradient is the gradient at the output. The local gradient is the gradient of that function. And then I want to get the back propagated gradient, which is a gradient as the important. Thanks for letting me clarify Other questions. Alright. Can you raise your hand to to follow? That's awesome. I think that's almost the entire class. So given that, that's how backpropagation works at the scalar level. Now we're going to move on to do back-propagation at the multivariate model. Oh, sorry. Before that, there's one more rule that we have to talk about. So first off, I want to say what backpropagation? As long as you can draw this computational graph where you know the local gradients, you can take the gradient of anything. Alright, so I think on question number two of the homework, which will be a homework number three, which will be optional for C1, 47 students will be mandatory. Proceed to 47 students. He took his paper from Europe's in 2004. And I remember reading this paper. And they just wrote down some gradients of the loss function with respect to weights. But oftentimes in textbooks and in papers, they don't show you all the mathematical steps to get from D L to do just to complete that gradient. And so as a grad student, I was working on this gradient for an entire day and I can never get the analytical solution correct. And then I remembered, oh wait, I could do this with backpropagation. And that's what you're gonna do in homework number three, pushing to wherever you read the computational graph can do with backpropagation, you get the gradient fairly easily, alright? So that's one advantage I talked about where formally for me it's just easier to think of gradients when you break them down into these bite-size chunks. Alright, there's one more thing that we have to talk about, which is what you do when you have two paths converging on a node. So I'm gonna call this wire x. I'm going to call this wire one. I'm going to call this wire Q2. And let's say I have upstream gradients. I know what DL, dq, then I know what DL, dq2. And then in this case, Q1 equals h of x, equals h of x. What I want to know is, if I know my upstream gradients, DLT, dq, q1, and q2. Now there are two of them. I want to compute what dy, dx is. So here's a new situation. We haven't been captured where to gradient paths converge on one question for you all is, what is dy, dx in terms of DLD Q1 and DLP T2. So this is a tricky question. I'll give you 3 s to think about it. Feel free to talk to your neighbors. Does anyone have the answer? I think it happened yesterday. Danielle. That's exactly tracks. Yeah. So Daniel's answer is that this will equal, um, so I'm gonna write this as just a general sum for the general answer, or the system will just be over two terms. So I'm gonna write a sum from I equals one to n. Little n is the number of converging passband. And we're going to have the upstream gradients D L, D Q Pi. And then we have the local gradients dq dy, dx. So you may recall that this is the law of total derivatives. And the reason it makes sense is because x changes through Q1 and Q2 because they are functions of each other. And so if I want the total change of the loss with respect to x, I have to sum up the contributions due to Q1 and Q2. Alright? So if the summation of this with I equals one is how much the loss changes if I wiggle x did acute one. When I equals two is how much the loss changes with respect to x when I recall Q2. And then a total change of the loss with respect to x will be the sum of all of these contributions, alright? Yes, a tomboy saying that in this case h is the same. So d2y, dx is going to be the same for every single eye, and that's correct. In fact, usually, even though I've drawn this example in almost every single case where we're going to see this happen, h of x, this is going to be the identity function. So usually in this class, or we're going to see is that x is going to branch out into multiple different paths that eventually lead to a loss function l. And all you have to remember is that when you backpropagate, x is granting two different paths, when you compute the L dx, you sum the gradients across all of these apps. Alright, so here, D L, dx is going to equal DL, dq q1 plus q2. Alright? And that's the general thing that you have to remember. For neural networks, which is, again, if a wire splits off, when we backpropagate, the x is gonna be the sum of all of the wires, all the gradients on these wires that go back to x. I mean how she's there. Alright, so given that there is one more example here, I'm going to skip it in the interest of time. Yeah. I feel like a battery life of these microphone, usually five or six vectors. Anyways. So in the interest of time, we're going to skip this example. It's just another scalar example that should be valid. Or we're going to be asked for here so that you can follow. Okay? Alright, so now we're going to get into how you do backpropagation for neural networks. And neural networks we're no longer multiplying, are doing operations on scalars. We're doing operations on vectors and matrices. And you know that for backpropagation we are using the chain rule for derivatives. We have vectors and matrices, the order matters and so we need to know what the multivariable chain rule is. For gradients. You just looking at the time crook radical point for a five minute break. So let's use a five-minute break. When we come back, we'll do a gradient. Awesome. I love, like I say, you don't have to follow the way that. I believe that this is a Boston. I want the LDF doing something like a song. So the question is, did my best to share with you some of the pages on their due diligence. Sorry. What did he submitted the works and the graders didn't submit everything. Drawn. Like this is a function. Yes, we could do that. Yeah. It does take some practice. So the answer implementation because the vectorization Professor. Yeah, Could you just say some other resources like that? The question about like someone asked me like that's just a picture like that. And having a more complex. Always break it down to prompt you to find more complex. Basically, if it's going to be pumping, right? Yeah. Yeah. Okay. All right. Everyone. There was a question that I want to clarify, which is that one of the students asked, diol df equals one. Is that generally going to be the case? In general though, gel DF will not equal one. But we assume that this is given because basically when we have a neural network, the output here is going to be the scores that then go into a loss, like the cross-entropy loss. And in homework number two, you all have already learned how to calculate e.g. d. L with respect to the scores DCI. And so because our output layers will be made by cross entropy loss, so we know how to take it. We will always have an option gradients that we can start to backpropagate. Alright, so the examples that we've given, we just said DLP API calls one because we assume that you will eventually know somehow between gradient and I'm just back propagate that gradient. Go back to that last year. When I Is that right? Yeah. Yeah. Yeah. So tomboy is asking, essentially we're going to have DLD Z for one example, right? If I could have done this back propagation where we have particular values of what the wires are and we can back propagate for those particular buyers. So then what do we do when we have many examples, right? How do we sum them together? Well, for that, we will actually use this rule that we derived here, right? So if I have, let's say I have ten examples, x, so I have X1, X2 dot, dot, dot Johnson x ten. Let's say they just go through. A simple architecture is a simple neural network where we have our inputs and they go to a layer, and this will be w1. And then they go to another layer that gives softmax scores. That gives the loss function, right? If I put W1 into this network, sorry, not W1, X1 into this network. I'll calculate the loss for that example. But the weight w1 would have been the same. And if I put x two into this network, it will lead to a loss. But that would have been for the same w1. When it comes to DL DW one, I can think of this graph has happened to replicate in ten different times, where I have ten different losses, but that all share the same w1. And so the gradient would actually be the some gradients across all of my examples. If you didn't follow that, that's okay. We're going to talk about this more explicitly when we do the backpropagation through something called stashed normalization. Alright? I want to get to this multivariate chain rule. So the first thing I've done is I've just put these two slides here that you've seen before. These are just the gradient of a scalar with respect to a vector and a scalar with respect to a matrix. This is stuff from chapter two. Alright? So we know the derivative of a scalar with respect to a vector and a scalar with respect to a matrix. Now we're going to do the derivative of a vector with respect to a vector. And this will lead us to something called the Jacobian. So let's say that we have an operation, Y equals W, X, Y and X are vectors. So let's say that y is n-dimensional and x is n-dimensional. So that w is n by m. Alright? What we can then do is we can look at our vector y. So let's say my vector y has samples or has entries Y1, Y2, down to why. And what I can do then is think about what the gradient of y with respect to x should look like. But first, looking at how wiggling x changes each individual element of y. So if I want to look at the change in y, y1, right? By wiggling x. All I have to do is I have to compute a gradient that we know how to do, which would be the gradient of the scalar y one with respect to x. And that's a factor, right? So if I want to know how changing x changes y one, I will take the gradient of the first element here, y one with respect to x. And I would do the dot product. So I should have a transpose here. So we know that the dot product of the gradient of y with respect to x times x will give me the change in y one from linear approximation. So what do I want to know how all entries Y1 through YN change? Well, I'll just stack these into matrix. So what do I mean by that? If I have this vector here, or sorry, sorry, if I have this matrix here. What I'm going to do is I'm gonna say this matrix is going to multiply some change delta x, right? So this year, Delta y one, sorry, gradient of y with respect to x is gonna be a factor. And if I take this vector and I dotted with delta x, this has got to tell me the change in my first entry of y delta y, y1. This is going to give me a delta y, y1. And then if I take the gradient of y with respect to x and I dotted with delta x. That's going to give me the change in the second entry of y, Y2. And then similarly this is going to give me the change in y n gets smaller. What is this bathroom changes in y at each elements. We'll just call this the overall change in my vector y. What this tells me is that if I create a matrix, were along the rows, I have the gradients of the first row by one with respect to x, right? Going from X1 to XN. Then this thing times delta x is first element will tell me how much one changes. The second element will tell me how much Y2 changes. Alright? So this matrix that tells me how wiggling x changes. Every single element of y is called the Jacobian J. And it is defined to be this matrix over here. Let's just do some dimensional checking. So if x is in R n and y is in R n as we defined above. If I want to know how to change the x, leads to a change in y, then that means that our Jacobian should be R n by n. And we see from this definition or the Jacobian that this thing is indeed n rows by columns. And overall, again, the Jacobian tells me how a small change delta x raised to a small change delta y. Sorry about this monitor flickering. Overhear me just try to turn these connections. Any questions there on the definition of Turkey? Alright? So this is where we need to really careful about our denominator and numerator. So this is a definition of the Jacobian. Let's see how that relates to the gradient of y with respect to x and denominator layout. So in denominator layout, right? If I compute DY DX, DY DX, the gradient of the scalar y one with respect to my vector x. This is going to equal, we know this vector of the same size as x. I take the gradient of y with respect to every single eliminated x. The thing to remember and denominator that is that the thing that we differentiate with respect to. In this case, x, will be the variable whose elements change in a column. So in denominator that the thing we're taking the gradient with respect to x will be the variable that changes along a column. So it'd be X1, X2, XN, the first column. That means that into your layout, this first column is going to be d y one, dx. The second column will be D Y2 dx. And then the column over here will be DY DX, where each of these is an n dimensional vector. So that's just the definition of the gradient of y with respect to x and denominator layout. You can see clearly then that the dimensions of this gradient and denominator there will be m rows by n columns. So it'll be m by n. So another way to remember denominator is in denominator layout. If I have the numerator being n-dimensional and the denominator being m dimensional. The dimensions of the denominator of what we take the gradient with respect to x always go first. So if x is m dimensional and y is n-dimensional, but they'll be m by, I will see more examples of this later on. So by definition, this should be an equals, equals triangle for defined to be and denominator layout. This is the definition of the gradient of y with respect to x. You're going to notice that this matrix is simply the transpose of the Jacobian. So for the Jacobian, the x dimension changes along the columns and the y-dimension changes along with the rows. But Jacobian is n by n, whereas this gradient here is n by n. So in denominator layout, we would say that the Jacobian is the derivative of y with respect to x transpose. Enumerator layout, there is no transpose here. So this is why some people prefer numerator over denominator for various reasons, but one of them being that the Jacobian doesn't require this extra transpose. Just definitions, any questions there? Alright? So given this, let's go ahead and take the derivative of a linear layer. This is going to pop up in our neural networks. So we're going to take, we're going to take y equals w times x. And here we're going to define Y to be an H dimensional vector and x to be an n-dimensional vector. If this is the case, then w is going to be an H by n matrix. And we're going to take the derivative by using what we defined in the last slide. So W times X is a matrix Y. And I'm thinking this gradient with respect to x, sorry, W times X is a vector y and taking the gradient with respect to x. So the first thing that we should do is we can compute w times x. So W times X is a vector y, and that's simply the operation of matrix vector multiplication. So that gives me these entries for this vector y. And so this thing here is equal to y one. This thing here is equal to Y2. All the way down to this quantity here, which is equal to y. Now, after we have Y1, Y2, y h, the gradient or the gradient of y with respect to x will be differentiating y with respect to X1, X2, all the way down to the last element of action. So I would take this Y1 and I differentiate it with respect to x, which would give me an n-dimensional vector. So if I take this E1 and I differentiate with respect to x, this here is going to be DY DX. So what is DY DX? Dy DX is gonna be a vector where for the first element, I differentiate with respect to little x one. When I look at this term, little axon only appears once and it multiplies w1 one, so I get w11 here. The second element is differentiating with respect to little x, for which I get a W12, etc. Similarly, if I differentiate y with respect to x, I will get this column over here. And then if I differentiate y h with respect to x, I will get this column over here. So what you see here is the gradient of W times X, which is a vector with respect to x, is just equal to w transpose. And so this is something that we should remember because we'll see several times in this class, if you have y equals wx and the gradient of y with respect to x is simply equal to w transpose. We can do some dimensional checking just to make sure we've done everything right. So we know that y is an RH, x is in R n. And so DY DX should be an R. And remember the denominator layout, the dimension of the denominator goes first. So should the n by n by w transpose, we can see is the transpose of a matrix W that is h by n. So w transpose would be n by h. And so indeed, this thing dimension dimensionally matches what we expect the gradient. All right. Any questions here? Yes, Tom way, I just wanted to point out that some degree of y is complete. Without doing any definition that should be about new content and match the hunches I'm competitions. Sometimes dimensions that pedigree is wonderful in some ways saying something that we will talk about. The hope everybody on the cost. Which is that the dimensions, the dimensionality checks, oftentimes are really distended the checks to get your transport is correct. So Tom, I was saying if I differentiate y with respect to x, I would think you would might be W. But because I know the gradient should be n by h and h by n, Then ingredients is like w, but I should put a transpose X, make the dimensions work. That's actually an intuition that we're going to use commonly in this class to make sure we get our derivatives right. And what we'll see that more powerfully in our next matrix vector, in our next, in our next pregnant. Any other questions on this example? Alright, there's one more thing called the Hessian. We're not going to be using the hessian very much in this class in terms of it's gonna be something that will come up when we talk about second-order optimization. But you're going to see it more frequently in like an optimization class, but I just wanted to find it. So if we have a scalar f of x, and x is a vector in R, n, and f of x is a scalar in R. Then what's the Hessian is the multivariate generalization of the second derivative. So we know that if I take the gradient of f of x with respect to x, this is going to be a vector that is the same size as x, is going to be our m. And now we know that if we take the gradient of this quantity with respect to x, that's going to be like the Jacobian of this gradient. And the result is going to be a matrix that is n by m matrix is defined here. And again, this is a generalization of the second derivative to higher dimensions. You can imagine interpreting this as telling you the curvature of your function along these different dimensions. You won't be tested on this. This is just something that you will see in later classes. I'm also talked about it briefly when we talk about second-order descent methods in this class. Alright, so now we need to derive the multivariate chain rule. We'll do this in the same way that the scalar chain rule is right. So the scalar chain rule, we have some variable x and we want to know how it affects the z, alright, but accept x, z in the following way. X is y to the function f, and then Y if X is X0 through the function g. Alright, so then we know that d z d x will be just to make things consistent, I'm going to rewrite this as d z d y times DY DX. How do we get this? Well, we can look how wiggling x equals y in the scalar case. We know that a little bigger than x will change y by approximately DY DX. And then if we look at how changing y, z, we know that's true GCD Y. And if for delta y, we plugged in delta y equals d y d x times delta x. Then we arrive at this chain rule. We're going to do for the multivariate case. Let me just go ahead and write this out. The player side, we'll have written this out formally. We're going to have x, that's the vector in R n, y a vector in R n, and V, a vector in R. We'll call this P. And then we're going to have that y equals f of x and that z equals g of y. Alright? So I'm gonna have to relationships for how wiggling x affects z. So in the first one, I could go directly from a wiggle and next to a bagel in z. And in the second one, I'm gonna go wiggle x, see how much that wiggles y. And let's see how much we're going, why we go see, and this will give me phi multivariate chain rule. Let's do the first example. If I want to know how wiggling x, z, we know that this is just the definition of the Jacobian. So I know that if I wiggle x, the amount that is z wiggles is gonna be given by my Jacobian matrix. And my Jacobian matrix is just DZ, DX. And remember, because I've denominator layout, we have this transpose here, but we have to remember, okay. So the Jacobian tells me how much we're going x, because the Jacobian d z, d x transpose. Remember, x is n-dimensional, z is p dimensional, and the Jacobian will then be p by n. Then we're gonna do number two. So we're going to see how delta x first because delta y, and that's going to be through the Jacobian from x to y. So this is going to be delta y. I should have approximately is here. Delta y is approximately the Jacobian DY DX transpose times delta x. Then if I want to see how that wiggles Z, I will see how my wiggling of delta y, because z. And we know that this would be delta z is approximately the Jacobian DZ DY transpose times delta y. And then what I'm going to do is I'm going to plug in my delta y being DY DX transpose delta x into this equation. So this is going to equal DZ DY transfers DY DX transpose x. Any questions? Sorry, I missed. The question is why approximately? That's because it's just a first-order derivative approximation. And so there are second-order, third-order terms that we're going to copy here. But if we just take a linear approximation this question, Are there questions? Alright, so in red, I have a relationship that takes me from delta x two delta z. And blue. I have another relationship that takes me from delta x and delta z. So I'm gonna go ahead and equate these. If I equate these, I get a dc dx transpose equals blue DZ DY transpose times DY DX transfers. And then to solve for dy dx, I'm just going to take the transpose of both sides. And this transpose is going to flip the order of these two, which is why our chamber goes from right to left. So after I take the transpose of both sides, I get that dy dx equals DZ DY times DY DX. And this here is my multivariate chain rule. And then check the mentors really quickly. And then after that, I'll take any questions. So DZ, DX we know to be a p by m matrix. So this is an AR, p by, sorry, d z d x transpose is p by m. So d z d x is n by p. So this is n by p, d z d y will be our denominator layout. Just remember that whatever is in the denominator, the first dimension. So why is n-dimensional and z is p dimensional? So this is going to be R n by p. And then for DY DX is going to be R and then m-dimensional and why it's n dimensional. So it's going to be n by n. And thankfully we see an n by n times an n by p matrix equals an n by p matrix. And silver in good shape here. Okay? Alright, any questions on disqualified area chain rule? Great. Daniel's question is, what happens when we take the derivative of a vector with respect to a matrix, giving us a 3D tensor. We're going to do that actually exactly. Next question is, will this to hold? The answer is yes. So this is always true. Chamberlain, always true irrespective of the dimensions of z. Alright? So the following slides here are just some of the formal knows that go through exactly the same algebra that we just did for this multivariate chain. Alright? I want to pause again and ask any questions on this multivariate chain rule. Because if not, the hardest thing for today's lecture, which is a tensor, derivative. Okay, can you raise your hand if you're following? Awesome. So let's go ahead and do the tensor derivatives. So in a neural network there, right? We're going to have a linear operation, y equals wx plus b. And y will be m-dimensional vector. In this case, x will be an n-dimensional vector, and then w. Going to be an m by n matrix. And we know that in the case of neural networks are parameters or these big. So I need to differentiate vectors with respect to matrices, because ultimately I need to get gradients with respect to big W's so that I can change the weights. And so this means that there is going to necessarily be a derivative of a vector with respect to a matrix, right? So I want to say the following at the offset, this is going to be challenging to many of you. And so this is something that you may just have to review. But at the same time, it's actually not necessary to know to be able to do optimization and neural networks. So after we do this tensor derivative, we're going to see that there's actually a shortcut that you could have used to get to the answer that uses the metric that convoy was mentioning. And actually when I talk to people in deep learning, especially like those in industry, this is how they say the derivative of this comes along. But it actually isn't a rigorous answer. What we want to do here is the rigorous answer so that nothing is magic. But after we do the rigorous answer, so that you know that we've done this rigorously, you can use the shortcut to react. Alright? So let me just say that at the outset and then moved on it. So I haven't do this example. So if I want to compute the gradient d y, d big W, This is going to be the derivative of a factor with respect to a matrix. And denominator lay out the dimensions of the denominator go first. So w is going to be m by n. And then after that the numerator goes, y is going to be m-dimensional. So we're going to expect that d, dW is going to be a 3D tensor. And let me show you intuitively what that looks like. So if y is equal to some vector of values Y1, Y2, YN. And let me just color these different ways so that you see what's going on. So let's say that one is in red, Y2 is in blue, and then y, n is in green. Those quantities are scalars. And so if I were to calculate D1, dW, we know what this gradient is because we define this in backyards here, right? So this dy1dw is going to be a matrix that's the same size is W. And it's going to be the derivative of a scalar y one with respect to every single element in the matrix W. So this is going to be an n by n matrix, m by n matrix. I'm going to draw it as this matrix over here. It's m by n. Then I could look at my second element Y2. And I can compute D Y2 dw. That's also going to be a matrix that is m by n dimensional. And what I produce, I can take that ending the n by n dimensional matrix, stack that right under my red one. And then this keeps going on until I have my last GYN dw. So this vertical stack of matrices of which I have in total m of these matrices is going to be d y d y, d y dw is going to basically look like this 3D cube exactly might not be symmetric and all the natural is just gonna be like this cube matrices where every single slice of the cube is the derivative of one of my elements of y with respect to w. All right, any questions there? The question is, what did I say about this topic before? So what I mentioned that for a cube, the cube isn't fun to have the length, width, and height to be equal to each other. But this 3D tensor of length, width, and height and general rule for specific cases but not in general. So we're gonna go ahead and do this operation. Again. This is primarily so that when we actually get to the answer and then you see the shortcut that we take to get there. It is not magic. And we know exactly where that short clip comes from. We're gonna do this once and you may need to review it because there's a bunch of 3D tensors times one of the vectors and sometimes it takes time to process. But in the end, we'll get to a shortcut, that book that you'll likely just take on and use for the rest of this class. Alright, so we are going to do this in the context of this example, where we're going to look at something that we know the answer to. This is what you did in homework number one, I think was questioned for what the optimal w is in the case where you have x, i, and y being vectors and least-squares problem. So. The way that we're going to motivate this, I'm going to write this out as equal to one-half. And then there's going to be the sum from I equals one to big N examples. And then we'll write y i minus W transpose y minus w xy. And then we're going to use a multivariate chain rule here. So what I'm going to do is I'm going to define GI. Gi is going to equal y minus WX xy. And this means that ultimately the loss is equal to one-half I equals one to big N Z transpose Z. And so you can get the gradient by saying that DL DW is equal to d L, d z times d z, d w, and d LDC is really easy because it's a scalar with respect to a vector. But then this d z, d w is a more complicated thing because z here's a factor and then w is a matrix. So in the context of this example, we're going to learn how to calculate dw, because z is linearly related to W. This will be the same brilliant about actually using our neural network layers. Alright. Hi, my question is, where did this summation go? Oh, you mean like over here? Yeah, so this is a good point. So what I'm going to do is let me actually write this here. I'm going to, for the following work, I'm going to omit the superscript I. So I don't have to rewrite this summation so many times and the superscript I. So basically we're just going to calculate the loss with respect to the weights for one example in this summation. And then the overall gradient will be the sum over all of my examples. Let's do some dimensionality checks to make sure everything works. So in this case, why is m-dimensional? X-i is n-dimensional and w is n by n. So DL DW better be an n-by-n matrix. G LDC is a scalar with respect to a vector z. Z has the same dimension as y. So z is m by one. So z is n by one. And then d z, d w is going to be a 3D tensor, like we talked. But I mentioned the big W go first. So big W is m by n, m by n, and then the dimensions of Z, Z is m-dimensional. So this will be times M over here. And if I take this 3D tensor that's n by n by m, multiply it by this n-dimensional vectors, vector, sorry, these m's cancel out and then the overall result is going to be n by n by one. But m by n by one is just a 2D matrix that's n by n. So indeed, the dimensions work out this example. Great. Daniel's question is do the rules of matrix multiplication applied to tensor products? Yes, so the same rules generalized to three dimensions or higher dimensions. You just have to have these inner dimensions match and then they'll cancel out, right? Yeah, tomboy says, you could just think of a matrix if it to detect, sorry. Alright. Any questions? Alright, let's go ahead and do this. So this is, these are just the formal notes that talk about what we just discussed. And remember, I'm going to drop this superscript. So this is also the stuff we just discussed. Let's go ahead and do the actual gradient. So here we're going to have our z equals y minus w times x. And what I wanna do is I want to compute DZ, dw. And we know that this is going to be m by n by m. The way that we're going to compute this gradient is just like we drew this example here, where we're going to take our vector z and just differentiate each element of z with respect to big W, which will be a matrix. Alright? And then we're going to stack all of these matrices together. So I am going to come back to this slide and we're going to differentiate. I'll call this the k'th element of z with respect to w. And we know that this is going to be some n by n matrix. And because z has n elements, I'm going to have n of these matrices. I simply stack them all together. And that's going to give him got a 3D tensor agreement, right? So the first thing that we have to do then. See what is Z k. For that, I'll just write out this in terms of the sum notation of the matrix vector multiply. So z k here is going to equal the k'th element of Y minus the sum, the sum over j of the kth row of w, dotted with my input vector x. Okay? So that is my scalar, which is the k'th element of my vector. Let's see. Any questions there. Alright, we're gonna go to the next step. Now I'm going to differentiate and try to find d, c k dw. So we're gonna do d z k dw. And what this will be, is this going to be a matrix of the same size as w where I'm going to differentiate my GK. Let me copy that down with respect to every single element. So we've already used the subscripts j, k, and then n. So what I'm gonna do is I'm going to take the derivative of z with respect to every single element of w. I'm going to index W by I and the columns by t. So I want to find the derivative of z with respect to the IP element of w. So WIP is just gonna be a scalar and the I throw in the pth column of the big matrix W. Alright? So let's go ahead and differentiate. If I differentiate this expression with respect to w, middle IP, this is a scalar that doesn't appear in y k. So we're going to have a zero there. And then we're going to have this minus sign, minus sign. And then it's going to be the gradient of this thing with respect to WIP. So this is going to be dy dw IP of the sum over j of w k j times x j. Alright? Now some of you maybe able to see this kind of thing is by inspection. But for me I always finite a mistake, so I like to write these out. So let's go ahead and write this out. I'm going to write minus d, d WIP, and then w k j x j. This summation is going to equal W K1 X1 plus W K2 X2. All the way up to x wasn't RN. So this is going to be w, k, and x. Right? So far just applying again definitions from linear algebra. Any questions so far? Yeah. Were you doing essentially two zooms here, but first zooming in on the kth element of the vector Z, and then we're zooming in on the i-th element of the matrix W. So we zoom in on Z and W zooming on w. So this is actually a scalar derivative over here. Alright, let's take this gradient. So there are two cases to consider. Case one is when I does not equal k. So this I does not equal k. This gradient is zero, right? Because all the w's and this term lead with a subscript k. So when I does not equal k gradient d ZK, DW, IP equals zero. On the other hand, when I does equal k, right? Let's see that. So I is equal to k. So now this is w k p. Let's say that p was equal to four. So this would be dy, dw K4, right? The only term that would have a WK for would be the term that multiplies x four, right? Or if p was equal to two, dy dw K2 would give me x two, right? So when I equals K D ZK, the WIP is going to equal. Remember there's a negative sign here, so it's going to be negative sine and then whatever the value of exit and DXC. Does anyone want me to go over that again? Alright, so I've now differentiated my one scalar ZK with respect to every element in WI and the matrix W. And I can write out what this looks like. So we have that dy, dw is I'm an m by n matrix. And this matrix is largely filled with zeros. So basically this matrix here has all elements equal to zero when I does not equal k. So basically I'm going to draw a bunch of zeros over here. But then when we're looking at the kth row, here, at the kth row, d w, d z k GW, k one will equal minus x one. So the first column and this row will be minus x squared. Dy dw K2 will equal minus x to the second column here will be minus x2, and so on until minus x, d, c k, d w ends up being this matrix where all the rows are zero, except for the kth row. For the kth row is just minus X transpose. Any questions there? Yeah. This is for a single example. That's correct. And then this is also for a single entry of z. None of them will have entries and z. So we're going to stack a bunch of these matrices where at every single step, for step Z1 is gonna be the first entry that's minus k. For the second DCT, DC to dW is gonna be the second row that has gone zero. Right? Okay, so this is from the formal note that go through this more formally, but they're the exact same thing that we just derived in the prior slide. And now we know that we have these DZ case, dw is, and this is what they look like. All right? Now what we want to do is we want to compute d z d w times. So I changed that L to epsilon because I'm just doing this first simple example. So this was previously my Albert. I change it to x one. I want to compute this tensor product, which is my chain rule. So I'm going to take, let me write this out here. I'm going to try to compute d Epsilon TW. Let me just go back to this slide so that everyone's on the same page. It was on this slide where I'm just replacing out with epsilon from my chain rule. I know that D at finding w dw times d Epsilon DC. So I'm going to finally do this tensor product where I have d Epsilon dW is equal to D epsilon. Sorry, that's not right. D epsilon d z times d z d w. And then the generalization of a matrix vector multiply two tensors tells us that this is equal to the sum from k equals one to n, because V is an n-dimensional vector of d z k dw times d epsilon d z k. So this is just like writing a matrix vector multiply, except now it's going to be a 3D tensor vector times a vector. Alright? So this is the same formula for the matrix vector multiply except now instead of this being a vector is a matrix. Then this d epsilon d d epsilon d z. We know that this is going to be the derivative of a scalar with respect to a vector. So d Epsilon z is itself going to be some vector with terms d Epsilon dc1, the epsilon dc2, all the way down to d epsilon d c m. Right? So now we're going to write out what this summation is. So this foundation, sorry, sorry, I didn't say it correctly. This is a matrix and this is a scalar. Because this is the epsilon d z k, which is just one of these elements. Are there questions? The question is, how do I justify this thing here? So this is the generalization. So if I have a times x, right? Like my matrix a times my vector x, if I want, if I want to know e.g. the first element of that, right? I would write this as a sum of a i j times x j. Sorry, this is a one j times x j for j equals one to the dimension of back spam. And so what I'm doing here is I'm just generalizing this to the 3D tensor case. Where this matrix here turns into a vector or dishonest A1 j, because this is now a 3D tensor, it turns into this matrix CCK. Just in the interest of time. I'll, again, this is complicated. I'm more than happy to answer further questions in office hours about this. I just wanted to make sure I get finished it in the next 6 min, as you can all use it for homework number three. So here we are going to have d epsilon d. So I'm going to write out the sum now, going to remove the summation. So I'm gonna write the k equals one term. This is d epsilon d z one. And then it's going to multiply dc1 d w d z one dw is this matrix here where the first row is minus X transpose and everything else is there. So this is going to be minus x transpose. And then everything else who's around. And you'll notice that I took d epsilon d z one and I moved it from the right-to-left. The reason I'm allowed to do that is because the epsilon dc one is the derivative of a scalar with respect to a scalar, a scalar, a scalar on the left or the right. Then this will be plus d epsilon d z2 times dc2 dw, which is this matrix square zero. And then in the second row we have a minus x transpose. And then everything else is Sarah. And then viscous, a plus dot, dot, dot. You have these terms all the way up to the DCM and then DCN, dw. So when I multiply these together, I will in the end get an expression where I have here a matrix where everything is zero. The first row, the value of the first row is minus dF font, easy, one, X transpose. So this is going to be the first row minus d Epsilon dc1 x transpose. This term contributes just a second row to this matrix, which is d epsilon d z two times minus x transpose. So there's going to be d minus d Epsilon dc2 times x transpose. And if you follow the pattern, you're going to get this here, minus d epsilon d z m times x transpose this matrix here, I could have just rewritten as minus d epsilon d z times x transpose. And dimensionally this works because d epsilon d z here is an M by one vector. X transpose is going to be a one by n vector. And in some, that's going to give me an n by n matrix. And this is the derivative of a scalar with respect to an n by n matrix. This gradient should have been also an n-by-n matrix. So the National way, everything works out. All right? Okay, So I just wanted to set you up with a few things for the homework. And I have office hours right after this. I'll be happy to take even more questions there. And we'll also start off next lecture. We're doing this, but in case some of you are starting early on the homework, these will be slides that go through everything that we just derived. And if you go ahead and apply the chain rule once you get the formula for b squares. So I wanted to give a few notes on tensor derivatives. These will be the entity that actually gives you the practical how do you do things? So we know from a prior example, if I take w x, I differentiate with respect to x. This is going to give me a matrix W transpose. We derive that earlier in lecture. So you might have thought that if I take w x and I differentiate it with respect to w, this thing look like x transpose. Alright? In reality, we know that this is a 3D tensor, but just like following the rules of how when you multiply two numbers together, the derivative with respect to one is the other number. This thing is probably something that looks like x transpose. When we did this derivative out, we did see that it was like taking our upstream gradient and multiplying it by x transpose. So how do people generally do this gradient in machine learning? We did the 3D tensor, which is the Windows example. What people do is they say this thing ought to look like x transpose. And then just like tomboy I said, we're just going to play around with the dimensionalities to make things work. So what people do is the following. They'll say, okay, I have a d epsilon d z and this was m-dimensional. And then renew that overall our gradient d Epsilon dw is N by N, Okay? Dx1, d W, we know, should equal some upstream drilling. Dx1 dizzy. And df, dy, dw again is m by n and d epsilon d z is n by one. And then this should be multiplied by dW according to my chain rule, dw over x transpose. So what we do typically in machine learning is we just try to mask the mountain. So X transpose is one by n. And I noticed that if I were to multiply here by x transpose, then I get an m by n matrix. Alright, this is totally not rigorous, but it's what people did. So by matching dimensions, you would come to the conclusion that this might be your derivative. And actually that is the correct answer. But if I were to just tell you this, you shouldn't believe me, right? The way that we get through this is by actually doing the rigorous 3D tensor matrix derivative and showing that indeed equals Jet fun d z times x transpose. Alright? So I've hit the time. I know I went through that quickly will definitely start off by reviewing all of this. 
All right, everyone for can it get started? We have two announcements today. The first is that homework number two is going to be due on January 30th on Monday. I'll put it to grade scope. And this homework has a considerable compliance. So be sure to start the homework early if you haven't already. And then like greenhouse last time, there's going to be a delay and returning to homework number one grades because they're still working out some admin. But just fix the higher the gleaners. Alright. Any questions on any quickly? Correct? Yeah, any other logistic questions? Of course questions. Alright, we'll get some material. So today we're going to cover the neural network architecture, some design choices. And we're also going to get started with backpropagation, which is an algorithm that's used to train neural networks. We talked again elastic there. This will be the reading for the deep learning textbook for these components. We talked about briefly at the end of last lecture of how neural networks are inspired from biological neurons. And last lecture we pointed out three key components of biological neurons. One is that they have inputs which are these arborist like regions of the neuron called dendrites. Dendrites receive inputs from other neurons. There's this thing called the axon hillock. The axon hillock. What it does is it integrates all of those inputs on the dendrites. So the inputs are in red. You can think of the axon hillock as summing up all of those inputs and that, that sum is above a threshold, then the neuron generates what is called a spike or action potential. And that's the signal that's conveyed down the output of the neuron, which is the axon. And eventually this axon will connect to the dendrites of dams thing. Alright. This is a very high level overview of neurons. If you'd like to know more about how neurons work, like the company I teach. Next quarter we'll go into this important detail. Alright? Any questions for this brief recap from last lecture? Alright, so again, the key things are that there are inputs. The inputs are summed and then pass through some nonlinearity, like comparing to a threshold, e.g. at the axon hillock. And then they're propagated along the axon. Neurons come in all diverse shapes and sizes. But when you boil down a neuron into its x and y components, those are the three things are dendrites, the axon, the axon. And if you were curious it too, the output signals of neurons look like if you just focus on this image right here, what we're doing is we're starting an electrode in and measuring from the axon of the neuron. And you can see that basically the signal, if we plot this voltage is going to be at some resting voltage, VR. And then sometimes it'll occasionally spike. And so you can think of these spikes as conveying something happens like the second one. Whereas when it's just sitting at rest, just all zeros. So that's the absolute potential of the neuron fundamentally communicated through this all or nothing. Spike. The spikes from neuroscience are very complex. And so another thing about spikes is that probabilistic. So what you can do is you could record from a single neuron and you can repeat a stimulus. So you should be recording from a neuron showing someone a picture of a cat. And even though you showed them the same exact fat on five different trials. If we were to plot across time when the spikes occurred, which are these red vertical lines, despite train would look different from trial to trial. So usually in neuroscience. So instead of looking at these spike trains, we often look at something called the firing rate of the neuron. The firing rate of the neuron tells you how many spikes you might expect to see given window. And so maybe the firing rate at the start of a trial. So the x-axis here would be time, my start-up around 50 spikes per seconds. So on average, I would expect to see 50 spikes, no 1 s window. And then maybe that rate decreases over the length of the trial. So usually when we think of neural networks, we think of them as their values reflecting what is analogous to the rate at which a neuron is spiking. Alright? Any questions on anything? We've made it to the biophysical neuron. Alright? So this is what the artificial neuron that we're going to use a neural network looks like. So kind of similar to the biological neuron. In fact, I'm actually just gonna go two slides ahead, but we do comparisons for the artificial neuron. What happens is that you're going to have inputs that come from oxygen neurons. And so these are X1, X2, X3, X4. You can, these are the outputs of prior artificial neurons. And then you can think of X1, X2, X3, X4, and then neuroscience analogy as the activity on the axons of the upstream neuron connects to this current neurons dendrites. After that, you take your inputs and you multiply them with base. And this will reflect how different neurons will have different types of effects on the downstream neuron. They could strongly activated or they could even inhibited if the weight is negative. And so, while this process is very complex and biology and the artificial neural network, what we do is we replace those connections with just scalar weights, W1, W2, W3, W4. And then just like the biological neuron has this integrative center for the axon hillock That's Psalms, the inputs and the artificial neuron. We will have this component, which will sum of the inputs. So it's going to keep W1 X1 plus W2, X2 plus W3 X3, etc. And then after that is going to pass it through a non-linear, non-linearity. Just like how biological neuron, I've got thresholding operation. However, when we go through lecture today, we're going to talk about some of the design considerations into how to pick up and what's the best practice. Alright. So that's the artificial neuron and kind of how it was inspired by the biological neuron. Any questions there? All right. So some people, including my own lab and use artificial neural networks to study mechanisms within biological networks. But there should always be caution when comparing these artificial networks to biological neural networks. Because of biological neural networks. Many more complexities as stochasticity, dynamics that are not model that own these artificial neural networks. So the way we think of these artificial neural networks is that their architecture was once inspired by these neurons, a very crude approximation of these biological neurons. But they are far from an adequate and perfect model of these biologics on their arms. So we should always use caution when comparing artificial neurons to biology. Nevertheless, those principles from biology led to these neural networks, which we know performed very empirically now. Okay? So if you're interested in talk to you about any of these differences stomach, which I read it on the slide. Feel free to stop by my office hours and I'll be happy to talk about some of the differences between biological and artificial. Alright? Any questions then before we start to talk about neural networks? Artificial neuron. The question is, when we drew this artificial neuron here, was this output just any real number r? That's correct, yes. Okay, So we're gonna start off with just some naming conventions. So typically a neural network, we'll start off with an input layer that's going to be drawn in blue here. And this input layer we usually denote with the variable x. You can think of this as far, alright? So x corresponds to the actual image that we're going to put into the neural network. The neural network will also have the very last layer called the output layer. And if we're doing classification with C bar Tanya, remember the output layer corresponds to the scores of each class. So this is the output layer that might get you the scores that go to e.g. a softmax classifier like we discussed. And then everything between the input and the output layer are called hidden layers. So this right here is the hidden layer. And you've probably heard of things like the depth of the neural network. A neural network that is deeper is going to have more layers. So this neural network, this would be a two layer neural network. We don't call the input layer since that's like the input. And if you don't call it that a layer, but all the hidden layers, as well as the output layer, are included in the count of how many layers of neural number of gates. When I draw this image, you're going to see connections between each of these nodes. And each node corresponds to, in this case, a scalar number. Each of these will be an artificial neuron. And if I had these connections, e.g. from my input to each one of these are going to have a weight. So we'll call this weight, weight W1 one, W12, and then W1 three, where the first number denotes the neuron identity that's going to H1, so it's index one. And then the second 123 correspond to these three efforts. So H1, just like we've talked about in the prior slide, would you do this computation where it would son my inputs W0 and 11 x1 plus W12 plus W3 X3. And then there's also gonna be a bias, I'll call that b1, and then passes it through some function. And this will result in a scalar, that is the activity of the first hidden unit of that hidden layer. Each one. Any questions here? Alright, and then if you want to know the activity in the entire conveyor, right, we can write this out not just for H1 but for h12 3.4. So let me write that right now. So if you want to write the activity of H1, H2, H3, and H4. What that would be is gonna be a matrix vector. Multiply. This vector here is gonna be the input vector X1, X2, and X3. We know that we get H1 by doing this transformation. And so this would be a w1, 1X1 plus a W12 x2 plus W3 X3. Then there will be a bias term here. Also, I'm running out of space. I'm going to really try to squeeze this in. B1, B2, B3. And then for the second neuron, H2, right, would have connections W2, W2 to W1, and W2 three. So that would be W2, W2 to W3, etc. There'd be a W4, W4, W4 three. And therefore you can see that to compute the value of all the artificial neurons and enrolling a bricklayer. Oh, I have to do is apply a backline transformation and then pass that through a nonlinearity. And so this pain can be written more succinctly as oh yeah, thanks Tom. Like before. This can be written more succinctly as H. This H here would correspond to this specter of the four H's. H equals F. And then we'll call this the vector of w's a big W, the inputs and x and then this bias. So here we see that linear classifier that we've talked about. The components of it shown up again over here. All right? And then one more thing to note, when I write f applied to a vector, right? So this, do you have a plus B is gonna be afforded the vector. When I write a function f applied to it. This means that I apply f to every single element of that vector. So there'll be f applied to the first, the second, the third and fourth elements. Any questions there? For this architecture? Is this not work acyclic? Yes. This network and everything that we do up until the midterm will be strictly before, which means that they're not even any feedback connections and therefore there are no cycles. Later on we will talk about recurrent neural networks, which will have cycles. Any other questions? Alright? So this first layer then, which computes the hidden activations of these units, we write h equals f of w and x plus b1. So these weights here would be my W ones. And then my second layer, which is the output, I would write as W2 times the inputs, which are this vector H plus another set of biases, v2. Okay? And then again, we should think of these. I put, at least in the case of c far as softmax scores. They tell me how likely in class one and class they told me the score of the image, any class one or class two. So these would go into a softmax classifier. And that's softmax classifier would produce some loss function, which is the loss function that we derived. Last lecture. We have Macs and it's the one that you'll implement on homework number two. One thing that you will have noticed and we'll talk about this later, is that generally we don't apply this function f at the output layer. Which means that after you do dependently or transformations, that will be a linear classifier. And we'll return to this in 10 min or so. But it's not a tight with the bebop. Great. Yeah, tomboy. I say softmax quarters. These are the un-normalized scores or the normalized. These are the un-normalized scores. So they can stay there before and go inside there before going to this. So maybe I should actually thanks Tom. Wait, I'll just call these scores, I suppose emphasize that there are normalized. Let's make sure that we have everything dimensionally correct or dimensionally correct in our minds. This w1 is going to take us from our 3D inputs to our 4D hidden units is going to be a four by three matrix. W2 is going to be a matrix that takes us from our four d hidden units to our 2D outputs as two-by-four. And then we'll have also biases, D1, there'll be applies for each hidden unit, so there are four of them and then also be output for each bias. And so that's going to be an art to nomenclature wise. We would say that this network has two layers. And then these two layers, there are six neurons. Neurons are these four and these two. And then we also calculated the number of weights it has. So the number of weights would be the number of weights and W1 and W2. So that's four times three plus four times two. That's 20 total weights. And then there are a total of six biases. And so in total, this neural network with 26 total learnable parameters. Any questions? Yes. The question is, is the softmax function presence in z? Yeah, So these, these, these are the scores that are then transformed into softmax probabilities. So there is not any additional money earlier. This is the linear transformation part that gets you softmax, that gives you the unnormalized scores. And so the Z is what you apply the softmax to. So eventually you'll do g of z. And this will give you softmax probabilities. Where the GSR softmax function, right? Oh great. So the question is, does this comprise another layer? So this softmax operation isn't thought of as another layer. Generally a layer is thought of as things associated with these linear transformations. But even if we had like several linear transformations with no nonlinearity, nonlinearity between layer. That's a great question, thanks. Other questions. Alright. Yeah. Reveal example. Discretion that we do not need. The slope. Actually be able to leave that space Destiny. Tom ways, making a really great point. I'm going to reserve that for just a minute. She's like to combine all I'll be emphasized. Alright, so I'm just showing you here now a three-layer neural network. And so in this case we would have two hidden layers. Our input would be three-dimensional, but then we would have two hidden layers. This one would be four-dimensional, this one four-dimensional. Then again, our 2D output. In this case, the transformations that describe these neural networks are straightforward generalization of a lever to talk. So H1 would be our nonlinear function f. Our function f in general applied to w1 x plus V1. So that if w1 and it'd be one here, and then H2 would be f of W2 times H1, the activity in this way, plus some biases be two and that would give me the activity H2. And then similarly, for the output, there'll be a W3. And if D3, and again, we don't apply that function. And these networks are called fully-connected. We usually abbreviate them with FC neural networks. The name is pretty self-explanatory, but it's because there's a connection from every single neuron in one layer to the neuron and the next layer. So because there are all of these connections that can be learned there called fully-connected. Then another name that you might. These cold is a multilayer, perceptron and LP. So if we say MLP or Fc network in this class, that's referring to these types of neural networks. Alright? So this slide has something works out with the best exercise for yourself. You can make sure that you can compute the number of weights and biases for this three layer neural network. All right, these are very straightforward. Yes, great. So Jake is just confirming that if we were to look at any of these weight matrices, the number of rows will be matched to the number of outputs and the number of columns will be asked the number of inputs. So W2 here would be our four-by-four, where the first floor reflects the number of neurons and H2. And just for reflects H1 and actually vote had been more care should be W1. W1 would be four by three. We haven't slide here to show you that these neural networks, as you see them, they look really sick for two implementing data, but also straight towards implementing code. So this, these four lines of Python code implement this neural network that we see over here. If you're not familiar with this lambda notation in Python, what this lambda notation is saying is that we're going to define some function f. So that's equivalent to saying define S. And then the input is x that corresponds to this x over here. Then it's going to return the value of this. And so this is going to be back times x greater than zero. This function might look kind of mysterious to you right now. Don't worry, we're going to talk about in this lecture. But this will be a common nonlinearity. But we'll talk about it when we would've made the nonlinearities. Alright, So then in Python you would just do each one. The activity of these neurons would be w1 times x plus b b1, and then apply that function f. Same thing for H2. And then lastly, we have our outputs. Alright, hopefully straightforward for the architectures. Any questions? All right, so moving on, let's start to talk about the function. So the first thought is what it is linear. This even simpler example, we'll say at this the identity. So the identity, then these are the equations. So each layer of the neural network. And the point of this slide is to say that is that it is identity or f is linear. That the entire overall neural networks tasting here. Alright, how do we see that? Let's look at H2 right here. What I'm going to do is I'm going to write and I'll rewrite it up here. H2 is equal to W2, H1 plus B1. What I'm going to do is I'm going to plug in the value of h one into this equation. So from layer one, I know that h one is equal to w1 x plus distribute two plus b1 plus b2. Let me now extend my terms. This is going to equal W2, W1, W2, B1 plus B2. Now what I can then do is I could say that W2, W1, right? It's gonna be a new matrix, I'll call it w tilde. And then these two vectors together, I'm going to call a new vector B tilda. What you can see is that H2 is also a linear function of my inputs. If you were to propagate this through all the way to Z. The point of this slide is to say that if f here was identity, or more generally, linear, even though you have many layers in the end, Z is still just an affine transformation of x where WE is gonna be the composition of all of these individual WAS. And there's also a bicep and composition of this extension. So you have a linear classifier. We know that it's limited. Anybody could use newly drawn lines between facts and it can't solve more complicated problems. So if f is linear, then we haven't really gained two bugs or we haven't gained any capacity or watering. Alright, any questions there? Homeless question is, is the dimension of Z always less than x? And many applications that's the case, but it's not always true. I'm actually going to take this as a jumping off point to then say, are there any cases where f might be useful, even though, I'm sorry, what were ef thing? Linear can be useful even though it's linear damper. And the answer is yes. So it could be useful in theoretical settings where you want to simplify the problem and therefore you want to study the linear version of a neural network. Another example which will come up in homework number three, and that's what I'm going to talk about it here, is that you can use a neural network where f is linear to still do some pretty cool things. And one thing that you can do is called dimensionality reduction. I'm going to talk about an architecture that you'll see in homework three is called an autoencoder. And the idea is that you start off with some x. Let's say that X is four dimensional. And what you wanna do is you don't want to work with a four dimensional input because maybe you want to visualize it and it would be really nice if it was, say, two-dimensional instead. Alright. So what you might do is you might design a neural network that looks like the following. I have my four-dimensional x. I'm not going to make my hidden layer just two-dimensional. So she's going to have two units. And then I'm gonna make my output be four-dimensional. And then this will be a fully connected network. So there's gonna be connections between all of these. I won't draw all of them. But hopefully you get the idea. This is a really interesting architecture. Because what we can do is it can make a low-dimensional representation of your x. If you ask the following, you create a loss function L. You take the loss function is z minus x squared. So what you're saying right now is I started off with an x and I want my output z to be as close to X as possible, right? So z is also four-dimensional and hopefully it takes on the same values of x. But to reconstruct z, you have to go through a two-dimensional bottleneck. Alright? So in this architecture, if it were to work, what your neural network has to do is it has to take your inputs x, squeeze all that information into just 22 dimensions here, engage, and then try to read out that information to reconstruct x at the output as the squared loss is going to penalize to make sure that it's being extra close to each other. Alright? So you could do this with non-linear neural networks. You can also do it with linear networks. And that's an example where even if f is linear, so you could do useful things with it in this case, dimensionality reduction. Any questions there? The question is, what is the relationship between or how effective is this method compared to other dimensionality reduction methods like. So. Actually in the case where these are linear, there's a relationship between this very simple than your auto-encoder and PCR. Great. The question is, for an autoencoder, would it be better to use a nonlinearity then to have a linearity in general? Yes, because with the nonlinearity, you would sell debt for power in terms of the nonlinear or the hidden representation, the dimensionality of the dimensionality reduced representation, a non-linear function of your, of your original scapes, Right? Yeah, The question is, is it useful part of this being able to look at the two-dimensional hidden states. And the answer is yes. So this is a form of so-called unsupervised learning where they're trying to find structure in it. And basically these two, these dates are going to be to the states that capture the important information in X and Ben could be e.g. visualized. The question is, when working with neural networks, do we have a choice of the number of neurons and number of layers, or do we not have a choice? We do have a choice. You can choose those as our hyperparameters that you'll find via cross-validation. And I believe it's homework three where we started asking people to certain accuracy classifier or homework floor. And later homeworks you're going to have that freedom to adjust the number of neurons to classify C bar as well as possible. I'm going to take one more question here. The question is, does it help to make the autoencoder symmetric? So actually the students are there constant I mentioned the relationship between autoencoders and TCA is when they are symmetric, but it can be viewed as incremented PCA. In general, you can make that decision if e.g. your data concerning, you don't want to overfit. But if you have enough data, then you will have more modeling power. If you don't constrain these, these what are called encoder and decoder matrices. Alright, let's move on. So it is linear. We haven't increased the power or the capacity of the neural network. So to do so, we should choose f to be non-linear. Alright? And then this gets us to one of the major design choices in Northern Alberta start, which is, how do I choose this? Because I have an infinite number of non-linear apps to choose from. All right, a few notes. We talked about how these are called feedforward neural networks. Fully connected neural networks are multilayer perceptrons. F is usually called the activation function. It's applied element-wise to every element that is applied to the vector. And we talked about this also. There's no activation function on z, but we'll talk about absolute applications after we've talked about this already. So let me get to what Tom was saying, which is Conway was mentioning that we talked about how at the output of a neural network, we're going to have a softmax classifier. Alright? You all know a softmax classifier is linear. So why is it that we're doing this high-powered non-linear, non-linear neural network and then put in a simple linear classifier at the output. This is intended because the way that you can think of neural networks working is it takes data that is not linearly separable. But then through the actions of layers one to n minus one, It's applying all these transformations, f of w plus d. That's one neural network layer supplying all these nonlinear transformations essentially to unravel this non-linear, Non-linear data so that by the time you get to a softmax classifier, they are linearly separable. Alright? So again, you should think of a neural network. We have this example of this could be like polar coordinates that change this non-linear classification problem into a linear one where you could draw a line to separate the points, right? But you can't draw any line that separates the points on the left. So in this very simple example, right, renewal transformation that makes these linearly separable. But then in general, what the neural network is doing is this Getting to look at your data. And then through the machine learning algorithm is learning what are the kinds of features. I want to transform the inputs and two, so that by the last layer. And I can linearly classify them with a softmax output softmax classifier. Then these features don't have to be handcrafted because they're learned entirely through the learning process. In this example, this is a handcrafted feature of saying we're going from Cartesian to polar coordinates. Okay? Any questions? Yeah. The question is, I said that softmax is linear, but isn't the softmax function incorporating the exponential, so it's non-linear. So the softmax function incorporates non-linear functions in terms of the exponentials. But the overall softmax classifier is still a linear classifier. One way to think about this is when we have the softmax classifier, right? The thing with the highest score is going to be chosen as the correct class. We talked about when you're comparing spores through that wx plus b is just drawing lines. Linear hyperplane. In this case, the softmax function has a nonlinearity, but it isn't going to change the order of the scores. The highest scores still wins. It's just turning those words into a probability so that we can optimize. And so in the end, just until a linear classifier, because even after the softmax function, the highest score has the highest probability. Sorry, Your question was, is the nonlinearity to find different than what? Then those linearity in? Are you asking because I'm going to just take video and offline, particularly. Yeah, so this is some of my more vigorous here. So when I say linear classifier, I mean that the boundaries between different classes are linear hyperplanes. And I will sometimes refer to an affine transformation as linear, but you're right that I should be referring to it as math fine transformation. So this is formally an affine transformation, nonlinear transformation. Did that answer your question? When I say that, you mean Like e.g. when I was saying like ethane identity, e.g. here, it still makes it linear. The hyperplanes may be different because if you optimize this with gradient descent, you might arrive at local minima that are different. However, the point is to say that you don't get any increase in capacity or modeling power. Because if you want to separate two classes, that fundamentally cannot be with lines, you will never be able to separate them even if you add more layers. That's great. Yeah, So Tom way was addressing the question about how the soft classes of non-linear function. Tomboy said you can also think of the softmax function as teacher normal, normalizer, but that doesn't change. So ultimately in your classification for the same reason seven. Right? I didn't have an example here of the XOR problem, which I'm going to ask you to review on your own just in the interest of time. If you don't know, the XOR problem is one where you have two classes and they take on these points in a plane. You can't draw any line that separates this. But what this example goes through is if you allow there to be a nonlinearity, then you can perfectly classify them. So this is just one example of one neural network that can do a non-linear classification tasks, the XOR problem, alright, so please just review those on your own. Just really support plug-and-play. Alright, so there are a variety of activation functions and we're going to discuss some of the most commonly encountered ones to address this question. How do I choose? And in doing this, we're going to make good use of this good felon quote, which I will explain a bit in two slides from now. But the closer the following. One recurring theme throughout neural network design is that the gradient of the cost function or loss L must be large and predictable enough to serve as a good guide for the learning algorithm. Alright? That probably doesn't make sense to anybody right now. We'll talk about it and just two slides. So let me first introduce our first nominee, arity, which is the sigmoid. So I've used this notation because it's commented describe the sigmoid activation for neural network. You would say that f of x is sigma. X sigma would take the place of the African the neural network. This is a sigmoid unit. You've all probably seen it before. On the y-axis it goes 0-1. And it's nonlinear because it has these curves in it. And around x equals zero. The slope of this line is close to one. Alright? So she started, the slope of the line is not close to the slope of the line. The, sorry, what I meant to say is that this function is approximately linear. Alright? So let's come back to this quote that says that basically, when you are choosing to design neural networks, you want the gradients of your loss, right? So we're gonna have, our loss is gonna be a function of the weights. We want the gradient of the loss with respect to the weights to be large and predictable enough to serve as a good guide for the learning algorithm. Alright, so let's extend that this names. Let's look at our sigmoid unit. And our sigmoid unit. If I were to apply sigma two w. If I were to have a neural network layer with a sigmoid unit, well, we would have is, we would have our wx plus b. In this case, I'm just going to assume that we have one output unit. So instead of a big wx plus b, I'm just gonna write a w transpose x plus b. So this is the activity of just one neuron. I'm going to pass that through a sigmoid. Alright? So I take my neural network repair, that's w transpose x plus b. I push it through a sigmoid, and that's the output activation for this one neuron. This thing will be a function of the weights. Because remember, when I built the machine learning algorithm, I get to make the weights or do optimization to make the weights as good as possible to minimize the loss. Alright. Alright, so ultimately we want the gradient of the cost function to be large. So I want the gradients of loss with respect to my weights to be large. Why do I want this? Well, later on, we know that we're going to be doing gradient descent. Update the witness, right? And in gradient descent, what we do is w is going to be my old w minus epsilon times this gradient, w dw. So if this gradient is close to zero, then I'm not going to let that, right, because gradient descent will say, okay, this is close to zero, then w is just equal to W. W is, and I stayed at the same value. Let's try it out with DL DW is for the sigmoid. So to do this, I'm going to use my chain rule from calculus. I can write this as DL, DW or d Sigma w, sorry, times d Sigma w, d w. You noticed that I will have the chain rule from right to left, which is probably opposite of what you're used to. There's a purpose for me during this time, we're going to try to keep his confession in class because even though it doesn't matter for this example, because Ellen sigma are scalars, right? So this is just a scalar number and this could have gotten on the left or the right hand side. But we get two matrices and vectors for denominator lay out the chain rule always goes from right to left. So please just remember that for this class and I will always write the chain rule going from right to left, right? Tom was question is, when I write this out, am I assuming that L is right after my nonlinearity sigma? I'm not. I'm just saying that we know some gradients of DL DW with respect to d sigma. And then to get the gradient of the loss with respect to w, I would have to use this chain rule. Okay, here's where I want you to recognize. Something interesting, which is if I take d Sigma w dw, there are regions in sigma where the derivative is zero. So if the input to the sigmoid is very small, then the gradient is equal to zero. And if it's very large, gradient is equal to zero, right? When the input to the sigmoid is close to zero, then the gradient will be relatively large. Alright? And that's good. But if the input to the sigmoid is small, then the gradient will be equal to zero. So one potential con, of the sigmoid is, it could have been the case that the input to the sigmoid, right? This is w transpose x plus b. That's going to be some score, right? There's gonna be some dot product and it can be very negative or very positive. If it is, this will be a very small number. So this in the case where the input is very negative or very positive, will be a very small number. And if it's a very small number, then even if this gradient is large and multiplies a very small number, DL DW will be small. If DL DW is full, I don't take a big gradient or by weights will stay exactly the same and ultimately their stock can be anybody that occurs, right? So we want to avoid these situations where our loss functions, there are places where the gradients are small because those small gradients overall, Kilbourne. Any questions there? Had to be following? All right. Great. So Tom way saying there might be another reason if I can be interpreted as plus and there might be another reason the gradient is small, which this ad you're at a local minima and other local Min at no gradients are small. And that could also be true. The way to practically disentangle this is to be looking at their loss function over iterations. The question, is it Daniel? Janice question is, do I techniques like batch normalization or other regularizations ameliorate this at all. In general, they can help. With sigmoid units are attendant or whatever you get is better validation accuracy. But they won't solve this problem of if you're in a rural part of the gradient here. Alright? So let's get into talking about the sigmoid unit. Prose. Around x equals zero, the unit behaves linearly. And another pro about it, which actually isn't really important, is that it's differentiable everywhere. So some empirical evidence that this differentiation being actually doesn't really matter. But, but people like that, these are differentiable. Alright? But what are the cons of the sigmoid? So the first is that when we talked about at extreme values of the inputs, the units. When I say the unit, that's another way of me saying the activation function. So these are interchangeable terms. The activation function saturates and has zero gradient. And if it has the road gradients, and we've already shown that DL DW will also be zero and therefore no learning occurs. Alright? There's also something about the sigmoid unit that results in so-called zigzag ingredients. So I'm gonna go over the following. The following is really more to make sure that we're understanding gradients and category the center of these activation functions. But it turns out that this is the exact ingredients solving that problem that I'm just about to talk about. It's actually not that concerning because they still do well even if the gradients zigzag. All right, so the sigmoid unit is not zero-centered and actually more importantly, the sigmoid unit is always, is always non-negative. And this is going to lead to a problem called zigzagging gradients. But you can practically it's not that big a problem, but you should understand it. So what's going on here? Let's say that I haven't neural network. The neural network is going to take an input x. And then it's going to apply a weight vector w1 and the sigmoid unit to get half the patients H1. And then there's going to be, and not tell you 1 hz is W and H. Then maybe it goes through another weight matrix and sigmoid to get W2. Let's start H2. And then there's another weight matrix to get some output the city. And then, sorry, I should have, I'm going to update the slides to say these axis are going to become Hs, h1h1 and H one. Alright? So let's consider a scenario where activation function f is the sigmoid. And we're looking at the sigmoid applied to this affine transformation of taking H1, dotting it with the weight vector, adding a bias and then I'll give you a H2. Okay? Actually, sorry, there shouldn't be an issue. They should actually just go to, sorry, there should be an H2. Let me call this output z. I'll put Nazi, but why? Because I do see the next. Okay, so I'm gonna do a few things. I'm going to define z to be w transpose h one plus b. That means that this function over here can be written as f of z, because z is equal to w transpose h one plus b. What I want to do in this example, it's a wanna compute the gradient of the output of the sigmoid activation with respect to these weights w. Alright? The way that I'm going to do that, this is the answer, but we're going to do the work. The way that I'm going to do that is via the chain rule. So the chain rule, I'm going to have a DF Thomas asked me to add another page. I'm going to keep this time way too, so I can reference other things on the slide. But if things get a bit messy, paper be done and I will go to the next page. Let me, let me just erase this always non-negative and put it on top and that all may be hopefully clear up. Some space is non-negative. Okay? So you have TW. I'm going to compute this gradient by doing DFW. I'm going to use my chain rule. So I'm gonna do DZ. I'm going to have a DZ, dw, again, where z is defined to be this w transpose h one plus b. Alright, let's go ahead and compute these gradients. So the first thing that I'm going to do is I'm going to compute the gradient d fw, DZ. Alright? So here, F is equal to the sigmoid function applied to z. So f is equal to sigma of z. And we know from this previous slide, or you can take me at my word here, but you can derive this. The derivative of the sigmoid function is given by Sigma of z times one minus sigma is the derivative of sigma of z with respect to z. Is this, alright? So this derivative here of f with respect to z is going to equal Sigma of z times one minus sigma xy, right? And that's the gradient in. The next one I want to do is I'm going to do this GZ, DW. Let me do this in a different color. I'll do this one in green. Gradient of d z, d w. We know that z is equal to w transpose h one plus d z equals w transpose h one cross B. We know that when you take the gradient of this with respect to w, It's just going to equal h one. And that was from when we did back later. So d z d w equals h one instead of this dG dW term here is equal to h one term over here. So that's the gradient of the output of the sigmoid unit with respect to the weights. Any questions they're using under the following. Okay. So why have I done this? I want you to notice something which is owned by the way, the reason that these things could change orders, like I have green times through here and then this is really tiny screen is because the blue thing here is a scalar, right? So the scalar in front or behind. So sigma of Z is sigmoid applied to something. Sigmoid applied to something is always bigger than zero because sigmoid is non-negative. One minus sigma z is always bigger than zero or bigger than or equal to zero because the maximum value sigma is you can take on is one. So this term here is also greater than or equal to zero. Then what is H1? H1 is the output activations after applying some sigmoid activation in this neural network layer, right? So H1 is the output of sigmoid units, all of which are greater than or equal to zero. So H1 is also non-negative. Okay? What this means is if I want to calculate the overall gradient of the loss with respect to the weights, right? If I want to know in this corner here, the gradient of the loss with respect to the weights. By chain rule. This is going to be the gradient of the loss with respect to DFW. Then we'll have this term here, d f w dw. And we know that df w dw, this expression. Let me use a different color. Now I'll put this in purple. This term in purple is always greater than or equal to zero. Its terms are always greater than or equal to. What is DLD I felt with you. If I look at the LDF, WE DO DFW is gonna be the gradient of some loss with respect to the gradient of the output of a city water. The reason I say that is you should all recall Abbas the scalar and the output of the sigmoid is also a scalar. So this thing is a scalar. And if it's a scalar, it means that it's going to take on some value. It could be bigger than zero or it could be less than zero. But it's going to just be a single number that's bigger than or less than zero, okay? Because dF, dw is always, is a vector which has the oldest element is greater than zero. What that means is that all of the, all of the terms in the gradient are either less than zero or greater than zero. Now what that means is that you imagine having to wait. So I'm gonna draw a space where we have two weights, W1 and W2. My gradient descent step is going to be in the direction of DL DW, right? But if DL DW has all positive values, that means is that if it's all positive, that I can always just change w1. And W2 together to interests together. Or maybe they're both negative. In which case, if it's less than zero, I can change W2, W1, both in the negative direction. But I can ever stepped in these quadrants because they correspond to a W2 being positive and at w1 being negative or vice versa. This is Barney some settings. So maybe our optimum, w is the top right hand quadrant right there, right? If it is, then I can take some gradient descent pathway where I increase both W1 and W2 to go to the optimal w. Like this, like remarks. However, if your optimal W isn't the bottom right quadrant. But I can only take steps in directions where I increase both W1 and W2 or decreased both of them. Then the way that I would get to this optimum would be to zigzag like this. Because remember, I can only ever both increase w2 and w1 or decrease them. And this leads to a zigzagging pathway to get to my office. Tom way of saying, I drew this graph with H2, H2S on, in this problem said, alright, so I didn't need to have this H2 there. All right. Any questions there? Sorry. The question is, is this offer two layer network? So this actually happens whenever there's a sigmoid anywhere. So this would be true for any layer neural network that is easy activation that we would always have zigzagging presents. An optimizer. You said? Yeah, the student pointed out that I have my green and my pink slips because you stopped in the direction of the negative gradient. Statements are correct there. Yeah. Alright. I sort of walk there. I also looked at a time and so I think it's time for a five minute break, but during the break, please think about this and then we'll come back and answer any questions about this exact ingredients. At this point. When you see interpreters Clark, did you like the values of the scores? Once they both put together? Yeah. Yeah. So the scores you could just think of as the higher you are, the more likely it's intact mass. And it's always a relative, James. So the highest score is the best way that the spores are. In terms of like the linear hyperplane. To say the following. If you have one more upstage, 300, et cetera, and then you'll have with your Dr. no other partners have different colors. It's like sounds like it might be like or the composition of those will give you the linear separating places. Yeah, because at this point here, 3.4, 400, yeah. If I hover over my score for 75 for, for class 125, okay? This is not in dispatch. So softmax is this is the linear classifier and nobody can stop softmax. We need a way to find a job. So we need to make this into a loss function. Softmax is a normalization of working for a public document a lot. Like a transformation is solvents. But this makes me feel good job. Yeah. This is a classifier. Renormalization. So surfactant. If we normalize, it doesn't change the order of the sports I want to follow. Whatever the highest score will still be really nice distinction. I can't. Yeah, it's, it's something that you would have to live with and could lead to four. So we will actually not. We yeah. Okay. Yeah. So really it just take a particular researcher will have to do. Okay. Yeah. Alright, everyone will get back to this again because we did a lot of parts here. And for some of you might be the first time that you're playing with radiation thinking about chain rule. So I want to ask if anyone would like me to explain any part of the work here. Again, this might also be something where at least when I took this class, I had to definitely review it if you've taught on my own for everything to click. Those shoes, don't worry about that. That's, that's totally natural. Questions. Great. So Nathan's question is, can I explain this on the left where I started? Dl DW is, has all of these elements, either positive or negative. The way that we get that is we realized that dF, dw is always non-negative. And that came from this step over here. So these terms are always greater than or equal to zero. And then GL, DFW is gonna be a scalar. So because the scalar, it's either a positive number or negative number or cookies. Or if it's a negative number, then it's a negative number times a vector where everything is positive, so every entry will be negative. That's fine too. That's pretty impressive. And I see the student is asking, there could be contexts where f of w could be a vector. So this would change this problem quite a bit because if f of w was adapter, then they W would be a matrix. But then if you worked through all that, you'll still see the same problem. The question is why H1 always greater than or equal to zero? That's a really good question. It's because each one was the output of a neural network layer where the nonlinearity was a sigmoid. So H1 or all values after a sigmoid sinus sigmoid is always bigger than or equal to zero. That's a great question. Yeah. The question is, why did I overwrite x with each one? Is because actually try. I'll put up a neural network bear. So X will have both positive and negative values. And so I wanted to point this out for the layers of the neural network that they will have. The exact ingredients requires it to be the hidden layers. Because actually this student's question, H1 is the output of a sigmoid and that's why it's always positive. Great. Yeah, Tom way such that I had earlier because for CPR is 0-255, it could have been. So C bar, it's also all positive values. However, in general, x could also be negative. So it's just a city this with each one. Great. So this student's question is, for the first layer, you may not zigzag. Is that right? This is actually correct, but none of you should know the reason why yet, because we haven't derived back propagation. So it would be for these to help us later on where it's exact gradients occur. But we'll see that when we derive that. The question is, can I re-explained the connections of the zigzag? I'm from the bottom left here, right? Yeah. So if DL DW is, say, all positive, that means that when I take a gradient descent step, I can only change W1 and W2 to both increase. And actually this should be decreased because of negative signs, but I'm just going to increase clarity. So DL DW has all at least positive. Then it will say if you increase W1 and W2 together. And so that's why you can only take steps that go into a bank might go in the first quadrant where W1 and W2 are both positive or negative, it can only go in the third quadrant. Clients want to be stuck for W1 and W2 both change and put decrease. The question is so it's just the opposite. Yeah, So because the updates are always positive or always negative, that constraints the directions that you can update it. The question is, what is l in this context? Healthcare, so boss function. So it'd be whatever loss function is that the output of your number, e.g. the softmax loss that we derived rocks are all negative. But the question is, when I say all positive or negative, is it referring to the weights in a given layer or the entire neural network in this case and just didn't give them better. Take one more question here. The question is, how is the derivative of the loss function with respect to the sigmoid calculated. Oh, sorry, How's did a scalar 0? Because the output of this sigmoid will be a sigmoid. So in this construction, f of w is sigmoid of a dot product, which is a scalar plus a bias b, which is a scalar. So that's why this DFW. It is a scalar in this example. Great gas. So the student is saying, I shouldn't this be like a big matrix W times a times the vector h one plus eigenvector of these. And so the reason I haven't done it in that case, I just did it for the simple scalar and vector example is because we haven't yet learned how to do the chain rule for derivatives on matrices, the derivatives of scalars, but we will do that next lecture. This term is a scalar and this term is a vector. All right, I'm going to move on for now. Again. You may have to contact the slide, I think is a good example to play around with to be sure that you're understanding chain rule and had it been different gradients, alright? Okay, so not the sigmoid, you, we might often use a candidate activation function, which is just a sigmoid that goes from minus one to one. Alright? And sigmoids are rarely used. If you want to use something that has a sigmoid shape, you can gauge the tan h has the same prose as a sigmoid. Around x equals zero. It's behaves linearly. But the other thing is because it's, the tangent can be positive and negative. It avoids this zigzag ingredients problem, which again, empirically speaking, is not actually a problem, but people always teach it and talk about it and it's good for understanding. So it's really not that big of a deal, but it's good to understand. Great student tomboy set is to ask why is it good for a unit to behave linearly about the origin? For this, I'm going to just give the answer from this slide, which is a line has a relatively high radiant compared to these sections where the gradient is. And so as long as this line of high slope, our gradients don't die off. Going back to tan h. Tan h is just like the sigmoid. It saturates. And so if you give it that is too high or too low, then you're going to have zero gradient and other learning. That's the ten is this. The ReLu unit will be the unit that we will use for this class. And it is by far the most widely used activation function and deep learning. So the Rayleigh unit is this function that I defined before, basically max of zero. And so this is what it looks like at the input to the ReLu is positive, then it just returns that value. And if it's negative, it returns right here stands for rectified linear unit. This is the raven function we talked about. We're going to have to be able to take its derivative. And some of you may notice that ReLu has a place where the derivative is not defined with Sarah. What did we do in these cases? Because I don't know that the derivative at zero for the radius undefined. The following I'm about to say is beyond the scope of the class, is just giving you the term. So if you want to look into this further, but if you were to take a class like convex optimization step in to 36 years, See, that's the class that introduces a concept called subgradients. And subgradients, our gradients that you could still do gradient descent breath. So instead of doing gradient descent with the derivative at x equals zero, which is undefined and we can choose a sub-gradient of it. What that means practically for you all is at x equals zero, we can just define that the gradients is either zero or one. Both of them will work. Alright? So basically derivative of ReLu, of x, it'll be one. So the slope is one when x is bigger than 00, when x is less than zero and zeros, you can choose it to be zero or one. It doesn't matter. Both are valid. For gradient descent. This is another thing which will be in these slides, which is sometimes people do it as a pro that a function is differentiable. But empirically it doesn't make a difference and actually it incorrectly, but non-differentiable ones end up doing better. Okay? Alright, So the retailer units prose. So the first one is that in practice, learning with the railroad, you didn't converge faster than the sigmoid and tanh image. So we're going to talk about this paper in 2012 called the introduces an architecture called AlexNet. This is the first ImageNet competition, whether that's a neural network and it significantly improve the performance of classification. And they reported that paper that ReLu Was six x faster to train with then tan h. So empirically is a lot faster to optimize with David and Tammy. When the unit is active, it behaves as a linear unit and its gradient is equal to one. So over here the gradient is equal to one, right? No, that's good because if we take a GL ReLu, times of d ReLu way, right? Because we know that this gradient is the derivative with respect to x and put it hasn't written equal to one. You'll only die off if the input to the rabies is make it. So when a unit is active, meaning the input is bigger than zero. I wasn't being the derivative at all points except x equals zero is zero or one. And that's very simple to implement. And then there's no saturation as long as x is bigger than Sarah. Columns. Relu, like the sigmoid is non-negative. And so therefore, it also has this exact ingredients problem. And this is where we learned in therapy that it's okay, There's something is exact because effectively does best. Relu is not differentiable at x equals zero. However, like we said, you could just set the derivative at x equals zero to the zero or one and everything works just fine. Then again, the more rigorous explanation for why that is will be in 236 is it BRC Tom, why would they do somethings to 36 ft? We'll talk about. And that's beyond the scope of this course. So you won't be tested on any subgradient from this course. Alright? One con is that if you are in the place where you have zero activation, if your inputs, their baby was negative, then you have zero gradient, gradient and it doesn't occur. So that's one of the questions here. The question is, is there a reason why all of the activation functions are monotonic? That's a good question. The output then you would probably want it to be monotonic because higher score should be higher probabilities. The activation functions do not have to be monotonic. So I'm gonna give an empirical answer here. What's the TAs have a better one for the monophasic knowledge necessary. Do you like? Yeah. Yeah. You could use a non-monotonic function if you wanted to. If I just doesn't perform as well. Alright? Textbook wants the gradient in this case. Why don't the maximum of zero? Well, what property with some older floor. Oh, so tomboy saying, why don't we put an Alpha here to scale the gradient where outfit object. You could do that as well. But it's just a scaling factor. So yeah. Yeah. Yeah, so tomboy is asking, why can't we make this alpha x, in which case the gradient would be alpha and you can make alpha larger because good fellas, I said you want ingredients to be large. So good. Hello statement is a relative one. You wanted to, like in the tan h you wished it was always Sergeant to do sometimes decrease. But then doing this by alpha would have the same effect. Essentially, it's like scaling a weight by alpha. So what matters here is the relative. Yeah. So the TAs are just basically saying there are a bunch of dogs that can control the scale of these gradients, e.g. also the learning rate. Okay. So with Raymond coming out, shortly after came out, people were like, Oh, well, instead of ReLu, let's use the soft clustering it. The soft clustering it kinda looks like a ReLu, but it's an actual function that doesn't have a discontinuity. The gradient is everywhere defined. And therefore it's a version of the railroad that is differentiable everywhere. But when you run the actual experiments. This is a paper by Zagier, Laura. And I want you to focus on a few columns, a few rows, sorry, this row here. So performance of ReLu. And then this here is the performance of soft costs which is everywhere differentiable. And what you can see is that when you look at error rates, so the lower you are, the better ReLu, outperform stock plus everywhere. So even though soft costs. It's differentiable everywhere. That doesn't really help for you. I really was still better. Question. The question is, what about the performance difference between ReLu and thank you Bailey? So let me first tell the rest of class we're thinking it is a leaky ReLu is this function where instead of when x is less than zero, everything being Sarah allows it to be a slightly sloping negative line. So in this example, I wrote at this line here is 0.1 times x. This is the alpha here. Even though I wrote this is 0.1 times x. That's just a visual thing. If I made this 0.01, you wouldn't be able to see it. But usually alpha is on the order of 0.01 for the leaky. And so in this case, ReLu is not saturating on the left and that doesn't go to zero. There's still gonna be some small gradients there. And then there's also something called prelude. Prelude is a parameterized leaky ReLu, where this alpha, instead of being set by us, can be learned through optimization as well. Okay? So that's what it is. Alright? Any questions on ReLu, soft costs? You maybe prelude. Right? Great. If I could put your question into my own words. I, because I think I had the exact same question when I first learned this stuff. It looks to me like tan h is quote unquote more non-linear than ReLu, right? Relu is like it's linear or zero. Whereas Canada has all of these curves. And one might think that because tan h seems quote unquote, more non-linear than ReLu, it should work better. Is that basically what your question is? Yeah, So that's a good question. I would say that our intuitions about like what is more non-linear breakdown when we are composing and many of these within their own effort. There's something that we won't talk about it as classical theory of deep-learning class, which is that important neural networks have something called the Universal Approximation Theorem. You can actually show for an infinite width neural network using even just a ReLu, it could, it could implement any functions. So the mere act of adding a nonlinearity, even if intuitive, it doesn't put quote unquote that common here gives us the capacity of the model, any function back there? Sorry, can you repeat the first part of the question? Yeah, Great. Question is, I've been saying performance. Does this mean classification error or does it mean the amount of time it takes to train it? I've been using performance to mean classification error. So the ultimate train network will reproduce, will have better class, will have lower classification, classification error. But it also happens to be that babies also the fastest. I think one more question. Yes. So Tom Weiss, that is one pro of ReLu is that it has this on-off property that either active or snack. Alright. So I'm going to show there's a lot of people have made their move units. This is the exponential linear unit. You can look at what the function is. I have this result from this 2015 paper that compares ReLu, leaky ReLu. Relu is a shifted ReLu, a mood is that exponential exponential rule. I mean, the point of this is to say, you can probably get some increase in performance by using a leaky ReLu or exponential linear unit. But oftentimes those differences are marginal. And so typically in practice, there's also a Mac side units that they did fellow invented, it says in the textbook, but we won't test you on that. And practice. The Rayleigh unit is going to be the thing that you could start off with. You should probably not use sigmoid. And if you're going to use sigmoid, you should use tan h instead unless you have a reason for activations to be non-negative. If your thing is, if your network is doing well, it's going to be worth trying out. Vicky ReLu, preview. One of these more, one of these hire, one of these things, but additional hyperparameters are, alright. Any questions on activation functions? That's leaky ReLu avoid is exactly yes because it can be negative. But needed a click. Event. We can have linear approximation. Okay? Yeah, yeah, So a rupture the same non-linear decision boundaries can be approximated by piecewise linear decision boundaries. Which is also a really good point. We're going to see that when we talk about adversarial examples. But you can pick up the railroads giving you interrogate this piecewise linear boundaries which are nonlinear. You didn't follow that, no worries. Okay. That's the neural network architecture we've talked about. Now there's one more thing which is an output activation. When I say the output activation, I mean that we're going to take our scores Z and then we're gonna do something to them. So in one case we might take GFC where g is the softmax function and then do a softmax classifier. Or do you might take Z and you might compute the mean square error. Whatever this is, I'm going to call this the output activation. And the question becomes, what output activation do we use? I want to talk about this because I think it's often overlooked thing that it really is important to choose functions at the output wisely. So we're gonna just do an example that I hope convinced you of that case. We're going to consider a neural network. The neural network outputs one score at the output Z for binary classification. So it's going to output a single z. If z is large. We're going to say something is in class one. And if z is actually, let's say negative, negative Z. The more negative it is. We're going to put that in class, Sarah. This is large positive c. And this would be large negative z is in class there. So there's just one score. If it's positive, classical I knew it was negative class. So this is different than the softmax or we would have a score for each class, compare them. But can actually be shown that softmax for two classes reduces in this class one, we're going to say that our output, I'll call it y e.g. I. Would be equal to one. And if this in class, the road and wife, e.g. I. Will be equal to zero. So one way to build a binary classifier is to get the score. And then just like the softmax, turn it into a probability, but passing it through a sigmoid. So we're gonna say sigmoid of c is equal to the probability that X is in class one. Because this is binary for the probability that excising classes are only just see one minus sigmoid. So this is a binary classification problem. And then the softbox is the multivariate generalization of this binary classifier. Alright? We're going to consider two loss functions. So the first loss function is mean-square error. What I do here is I take my Y-i. So let's say that the correct class was one. We put a one here for y. And if my classifier was really good, and z I should be a large positive number. So then sigma z I would be close to one, all right, and then vice versa. If y was equal to zero, hopefully Z i's are large negative number. And so then as sigma of z i would be plus. Alright, so this seems like a really reasonable loss-function. In fact, a few lectures ago when I was talking about what loss function to use for the softmax classifier. One of the students raised that they can take a mean square error of one-hot representations. And this is what that case. One byte in a few seconds asking you said there'll be a one by n Here. Yeah, we can put an n here also as a normalizing factor. There's also the other loss function which is cross-entropy loss. And this is the loss, the same max likelihood loss. We derived for soft max, just simplified for two classes, we derived for softmax. What I hope to show you through this example is that whenever you have a sigmoid or a softmax activation of the output, you always want to use cross entropy and you never want to use mean-square error. Alright? Why is this the case? So let's draw a picture for intuition. We're going to have on the x-axis, our score, e.g. I. Saw call that z on the y-axis will be the output of the sigmoid. E.g. I. Will call this sigma xy. This is what our sigmoid looks like. And here is one, and here is what the sigmoid looks like. If I want to compute the mean square error, just e.g. I. So the mean square error, e.g. I. Am going to call this MSC superscript I. So it's just for one example, this is gonna be the value of y of Pi minus sigma T of I. This whole thing squared. That's the definition of mean square error. And then just like we've been doing before with gradients, I can write what the derivative of the loss, which is mean square error, is with respect to d z by using my chain rule. So I would write this as d mean squared error, e.g. I, d sigma z, and then d sigma z, d z. Alright? What I want you to notice is the following. Remember that sigmoid has these points where the gradient is zero. So this term here, it equals zero when z is very negative or very positive. But let's say that Z i is some value like negative 50. So DI here is equal to -50. And let's say that for this example, it happens to be the case that the true label y is equal to one. So y is equal to one. I want my network to change by the base to make ZI as big as possible. But because of our initialization, we started off with z I minus. What you should see here is that this network will never learn to make this bigger. Because my gradient is going to be zero. Because the gradient is over here, is there. Alright? So because d sigma d z is equal to zero when z is -15, even though I want the I to increase to a positive number because y equals one. Alright? Why is this in terms of an intuition? Let's say that I were to try to make it change my network ports to say, Okay, I'm going to try to make the eye from -50 to -40. Alright? This case, let's calculate the mean squared errors. So if I compute YI minus Sigma of -50, here, y is equal to one. This is approximately equal to if I try to change z to be -40, right? Well, the sigmoid had zero spoken, so still sigma of -40 is still going to be zero. And so one minus zero squared is still also going to be approximately equal to one, right? That's just another intuition to see that we're in bad shape here because the network has no incentive to change the parameters. Because when it starts thinking the parameters in this area, my loss remains the same. So if we were in this situation where I started off at a bad CGI and I want to increase the eye because I have zero slope. I would never burn any questions there. Okay, So I've drawn this picture or I, I've written this more formally by taking the derivative of the mean square error, e.g. I. With respect to DCI. And this is what the derivative looks like and so on the function, sorry, I'm just pardons. The x axis is the value of C and the y-axis is the gradient. This is what the gradient looks like, which is this function. If I plug in y equals one, alright? So if I look at this plot, I can see that there are a few regimes. If I'm in this regime, things are splendid because when y equals one, I want c to be large. And if z is large, like gradient is close to zero. And so there's no learning because I already have. Even if I'm in this range, things are fine because here my gradients are not zero. So the gradients are not zero. And I'll go end up changing the weights so that eventually z increases. Alright? It's in this regime over here that things are really bad. Alright, dad? Because here is very negative. I want him to be positive. But here the gradients are all close to zero. So no learning. On the contrary, if you were to differentiate the cross entropy, e.g. I. Will leave this as an exercise for you all on your own. With respect to z, this is what the gradient looks like as a function of z. And this is really good because when we are close to the correct answer, we have less learning the gradients go closer to zero. But the further away we are from the correct answer. The more learning happens. In fact, as G becomes more negative, I'm going to make larger changes to my network to try to make it more possible, right? So if you have at the output of your network a sigmoid activation or a softmax. You definitely don't want to use these square error. You want to always be using a cross entropy loss or the max likelihood loss, because it has the desired behavior that when you're far away from the correct answer is gonna give you big gradients to change your. Alright. You raised your hand if you follow that. Awesome, That's most of the class. Any questions on this? Great. So the students question is, could you address this? Let's say that you're in some situation where someone's like no, you have to use the square error loss and they don't want to listen to reason. What's something that you could do? You could change the initialization to be in a regime where hopefully all the starting to, the eyes are small and you would, you would hopefully have your age. They're going to later find out that initialization will matter a lot for these neural networks. Great question. It's better. Z always is bigger, is always better here. Only because I defined that Y equals one. So in the case for y equals zero, we would want smaller, say, Oh great, nasa mission is asking, Would there be a problem if y equals zero being in the green region, the answer is yes. So this polymer also works in the reverse direction. If y equals zero and you start off with a high Z, you won't have anybody because of Surette. Yeah, that's a great question. Right? The question is, is this the case? In general when you use softmax or sigmoid with mean squared error and the answer is yes. So basically like the, the key thing about cross-entropy loss is that it'll have this log that and does that exponent, which then leads you to not have this saturation. Alright? So other types of output activations be on the sigmoid. You can just have a linear or identity activation. I wrote something here which is related to statistics, but you won't be tested on it, so don't worry about that. Most commonly our output activations are going to be the softmax outputs. We're gonna be using that for the rest of this class. And we're going to use that in tandem with the cross-entropy loss. I have a question here which I'll, I'll let you all dry it on your own, but you should see that if you were to have, let me just draw it out really quickly. This function here is an output that looks like maximum Sarah. And then Min of one z. So it looks like this. And this would not be a good output to use. Again for the same reason as a sigmoid, that if you're caught up in these regions where there's not going to add. Okay, So again, in this class we're going to be using the softmax output activation in tandem with the cross-entropy loss. Cross-entropy loss is the same thing as a max likelihood lost that we derived last lecture. Alright, in the final two slides here, I just want to motivate, but we'll talk about next time through it's the backpropagation algorithm. So basically, we have our neural network architecture. Now we have chosen what the activation function is f will be and what the output activation function g will be, which goes to Softmax. Softmax uses our cross-entropy loss of power. After we've defined this entire architecture, we know all of the functions being used. We know the weight matrices, W1, W2, W3. And we have our loss function that tells us how good our model is. In our machine learning problem. We know that we're just missing one more thing, which is, how do I change W1, W2, and W3 to make L as smallest possible. For that, we know that we needed to compute things like DL, DW. Right? Now the problem is that this function for the neural network when you impose everything becomes pretty complicated. And w1 is pretty far away from Tau. And we need to be able to get this gradient to know how to change W1 to make Alice small as possible. So the algorithm that we're going to begin, then next lecture will be called back propagation and it tells us how to exactly compute these gradients so that I can change the w's to optimize. So you get that on Monday. Individual function when x is negative. 
Alright, let's get started. You only have one announcement before we begin. And that is that homework number three is due a week from today. And please be sure to print out your Jupyter notebooks as well as your code for that assignment. All right, any questions on NH4 statistics? Okay. I went through this tensor derivative last factor and it was a bit fast at the end and we didn't get a chance to ask too many questions. So I wanted to come back and recap this and I'm happy to take any questions on this tensor derivatives. So we were in this study where we have y equals wx, That's right here. Here, y and x are both vectors and w as a matrix. So if we have d y d w, d y d w is a derivative of a vector with respect to a matrix. And we talked about how you can think of this as a 3D block. So what we can do is for each element of y, we can take the derivative of that scalar element with respect to a matrix, which we know is a matrix of the same shape as W. And that's this red matrix here will, would therefore be the derivative of the first stellar element of y with respect to the matrix. Now we can do this for every single scalar element of y and that all these matrices together we get a 3D block. And that is what the gradients dw will look like. It's gonna be a 3D tensor that looks like this block right here. Alright, Were there any questions about that? Russia. Music, pop music. Oh, I see. Okay. I see what you're saying about it. Yeah, so we talked about how in denominator we add the dimensions are m by the thing in the denominator. The W are the leading dimensions of the resulting tensor. So it's going to be n by n, the dimension of w, by m, The dimensions of y. And so if we're thinking about this block is having the height in the first dimension, the second dimension, and the depth d The third, then you could rotate this 90 degrees if that helps you to visualize what that cancer is. Thank you. Other questions? Okay. So then after that, we did this problem where we have z equals y minus WX. And we ultimately wanted to first take the, sorry, I wanted to first take the gradient of z with respect to w. And so we did the operations for that. Following the exact same logic over here. What we did is we broke down into its scalar components, ZK, and we wrote what each of those dk are. And then we differentiated those ZK with respect to w. And that gives me one matrix. Alright? And then I stack all of these matrices for k equals one to the dimension of Z, which is N. And that will give me a 3D tensor corresponding to the gradient of z with respect to w. So this was the derived matrix, which is the gradient of a scalar element of Z, Z k with respect to w. And we saw that it was a matrix that's everywhere zeroes except for the kth row by x minus x. And what we then saw is that by just changing Z1, Z2, Z3, all the way up to Z, right? If I just stop all of those and I have the 3D gradients, but 3D tensor gradient d z, d w. Were there any questions from this slide? Alright, so here's the last part of it. In our question that you're trying to solve for, we had a scalar epsilon. Epsilon, I believe was equal to something like one-half Z transpose Z. And I want to compute d Epsilon dW, where then I would use my chain rule, which we derived last lecture goes from right to left. So I would take d Epsilon DC and then multiplied by d z d w. This is going to be that 3D tensor that we just derived. Times a vector. And we saw that the operation of this 3D tensor times the vector will simplify to a matrix. And if we went ahead and wrote out this operation, we said, we see that this thing here, d z d w times d epsilon dc, mathematically simplifies to d Epsilon DZ. That's this term right here. Then the effect of multiplying on the left by this 3D tensor d z d w. Simplified to take the d epsilon d z and actually write multiplying it by this one detector. So the Chain Rule runs right to left. It's a, formally to compute this, we have a 3D tensor times a 1D vector, but when we simplify all the math, it turns into a 1D vector times another 1D vector transpose. So this is the result that we get from the Chain Rule by following these steps precariously. Alright? Are there any questions over here? The question is, is the same as well? Yes. So in this example, if you look at the slide from last lecture, the loss of epsilon was just one term of the loss for one example. But the loss is the sum of all of these epsilons. Other questions. Okay? So I also talked about how the last lecture, the 3D tensor that we went through is really to show you all that everything works rigorously and nothing is magical. But really there is a much simpler way to arrive at this answer. Kind of just using intuitions about what gradients to reflect and also naturally dimensions. So I'm going to redo this because this is quite important and it'll be the way that you usually end up doing these ingredients for backprop your assignments. So we had derived this result earlier last lecture, where if I take w x, which is a vector, the product of these is a vector. I differentiate with respect to x, I get this matrix W transpose. Alright? So even though I know that this gradient of a vector with respect to a matrix is a 3D tensor. If I kinda follow the rules of how when you differentiate with respect to a variable, you get the other variable, right? So if I differentiate w x with respect to w, actually get something that looks like x transpose. We can say that the gradient of w with respect to w looked like x transpose. Because it's not rigorous because this gradient is a 3D tensor. X transpose is a row vector, but it should look like that because of this trend that we see in the gradients. And so what people usually do is they say, Okay, if I know d Epsilon dw is an m by n matrix, I know that the Epsilon DW, right from chain rule is going to look like a swan. Dz times a DZ detail for you. What I can say is I know that this dw is going to end up being the epsilon d z times the gradient dv, dw look like. So this d epsilon d is the I copy down here and that's an n by one matrix. Sorry, I meant by one vector. And then I say this gradient of w x with respect to W. So let me change all of these years to i w x's bit more clear. Wx, wx, this gradient dw, dw soup look like x transpose. And so if I fiddle around with dimensions, X transpose is one by n. If I take this m by one and multiply by one by n, I get an n by n. Alright? And by doing that dimensional matching, I can come up with this as an argument for what this gradient should be. It's not rigorous, but you can use this in the future. We have already shown it is rigorously true by doing this 3D tensor. But now moving on to the future, you can just use this results and just try to match the dimensions to get your gradients. Alright? Again, this is not rigorous, but it's a really handy trick moving forward. Any questions here? Yes. We verify this by using the numerical gradient. Yes. So in homework number three, we will have to implement backpropagation and there'll be a numerical other questions. The question is, does this gradient here denominator layout? It is. And I think I know where the clustering comes from because in denominator layout, we need to multiply. We need to go from right to left. But here in the final answer looks like we're going from right, from left to right, right. So how is this denominator layout? It's because this expression is a simplification of having done correct denominator layout where DC dw is indeed on the left. But this is a 3D tensor. And when you do this, calculation out, simplifies to multiplying by X transpose on the right. So we did things rigorously denominator layout with the chain rule going right to left. It's just that when you happen to simplify it, it ends up as the same effect of multiplying X transpose on the right. That's really important to remark. Any other questions here? Great homework. Question is, does this simplification only hold for 3D sensors or does it go for 4D tensor or higher dimensional vectors? And it does hold for higher detectors. So given that, I'm going to go ahead and the next slide and just summarize everything that we need to do back propagation for all of your homework questions as well as for the coding. Alright? So the first thing is you will have a neural network layer. Neural network layer. We're going to multiply a matrix W and some input x. And that's going to give me some vector hidden activations. I'm just going to call this y for simplicity, since we've been using Y, X, and W and these examples. So we're going to have y equals w times x. And in backpropagation, I'm going to have some upstream gradient, DLD. Why? This is gonna be a vector derivative of a scalar loss with respect to the vector y. And I want to know how to backpropagate to both w and tax. So here are the rules. If I want to backpropagate to do to x first, I want to compute dy dx. Then we derived this is just gonna be del dx equals DLD y on the right. And then there's going to be a DY DX over here, which we derived already is w transpose. So backpropagating to D L dx will be w transpose y. The other more challenging one that requires this tensor derivative. But from here on out, you can just solve by saying the gradient should look like a transpose and then match dimensions. We would have that DL DW, He's going to be DLD Y times X transpose. All right? And again, you can kinda see the symmetry and b's when we backpropagate through this multiplication, right? We're multiplying by the thing on the other wire transpose. So backpropagating to DLD x, I multiply it by the thing on the other wire, which is w transpose. To get some DL DW, I multiply DLD, why my upstream gradient by what's on the other wire transfers, right? And this is the thing that we've rigorously derived. Capture any questions there. Okay, so then we're not going to show the following in class because it would involve doing 40 tensors. But this pattern that we see the generalizes ever want to use that dimensional matching tricks that I talked about calcium back there. The question is, why is X transpose on the right? It's because when you go ahead and you do the gradient using the chain rule, when a 3D tensor is on the left and you simplify through all the math, it ends up being multiplication by x transpose on the right. So we did do the chain rule correctly, where this 3D tensor is on the left. It's just that when you mathematically simplify everything, as it turns out to multiplying by a row vector on the right question. And that's something again where I encourage you to just tilting to you. Please just go back and watch the lecture or watch the segment on this 3D tensor derivative. And I'm also happy to take any questions here. The question is, why is this w t, w transpose on the left? This is chain rule. So the chain rule is ideal dx equals DLD y times DY DX and DY DX, we derived last factor is equal to w transpose. So this is just straightforward chain rule where this simplifies to w transpose. This is also the chain rule, but it's sympathizer, right? Multiplying by X transpose. With this, you can back propagate through a neural network layer. I was just next thing also, we're going to write down the backprop rules for if you have a maintenance funds, a matrix. So let's say that we're in the setting where we have a big matrix, Y equals W times a big matrix x. And here I'm going to say w is, sorry, I'm going to say x is n by p, w is n by m, and then y is therefore n by p. So if you tried to take the 4D tensor derivative of a matrix Y with respect to the matrix W. That gets carried very, but the same kind of intuition that I mentioned before, it happens in that case. And following these rules can be used here and that's all you'll be responsible for for this class. We're just going to write out what these gradients are. If you want to backpropagate to D L, dx, and you have some upstream gradient DLD. Why? Backpropagating through this will actually be the same one as this one. It's going to be multiplying by W transpose on the left. And then if you want to backpropagate DL to get DL DW, you would take your upstream gradient DLD y, sorry, I shouldn't have renewed far away. You're gonna take your upstream gradient d L, d y and multiply it on the right by X transpose. And so you can see these two look eerily similar to this, where instead of x and y vectors here, they had some big wire matrices. And dimensional way everything works out. Any questions there? Alright, so that's all you need for homework number three, these backprop rules should allow you to solve all the three nm paper cautions that we gave you for solving backprop. And then these are also the Bangkok rules for neural networks. So I'm also just going to take this opportunity to buy what backpropagation neural network there looks like. So let's say that this is our neural layer. The neural network layer we know comprises first a linear operation, w times the activations are the input, right? That layer, I'm going to call this thing H1. So it's clear that this could have been e.g. the output of a neural network layer. We add a bias to it. And then after that we pass them through a non-linear activation, Wally. And that gives us, we'll call this thing actually, I'm here NH2. So this is your next layer. And eventually this thing just gonna go to a softmax classifier. And that's going to give you some loss. Okay? So in homework number three, you're going to write the backpropagation for the neural network layer. It's going to start off by getting some gradients with respect to your softmax. And so this is exactly what you implemented in homework number two, a softmax, loss and gradient. So this is going to give you the derivative of the loss with respect to your Softmax parameters. And then we're going to assume that we backpropagated this. So we have some upstream, upstream gradient, DL, DHA. And now what I wanna do is I want to back propagate DLD A12 through all these operations so that I can get the gradients with respect to weights and oxygen bears and neck and depleted. So the first thing that we're going to do is we're going to backpropagate DLD CH2 over here to the LDA. So if I compute the LDA, that is going to equal my upstream gradient, DL dA2 times but local gradients of the ReLu operation. Remember the operation is a map of my input a when the zeros. And so know that the maximum function routes the gradient. So whichever item is bigger. And so what I can do is I can represent this routing via indicator that a is bigger than zero. So this is going to be here are the same size as a, where a is bigger than zero, that element, sorry, that one is bigger than zero, then the first element is one. If A2 is less than zero, the second element is zero. So the scar to be the same size vector. And if a is bigger than that element of a is bigger than zero, then the gradient should pass through. So I'm going to just multiply these element-wise. Alright, so I do that with this notation here. This is called the Hadamard product. So DLT H2 is an n dimensional vector. Then this indicator is also going to be an m-dimensional vector. This Hadamard product just means take the first entry of this vector and the first entry of this vector and just multiply them together. Alright? And that's going to give me the LDA. So that's backpropagating through the reboot. Any questions about that? Great, Yeah, because every element of this indicator vector will have, will be either one or zero. It's not gonna be that all of them are ones are oligomers. Yeah, So in general, it'll depend on the value of the a in that element. And as long as some are positive and some are negative, you'll have ones and zeros in this factor. And basically what this is saying is that the LTA is going to be equal to DLT H2 whenever a was bigger than zero, right? Right. The question is, where does the chain rule come in here? So in this case, we didn't have to use the chain rule. This is because let's say that let me actually just write this out. There'll be better. Let's say I had my vector a and I just have three artificial neuron. So if I say the values for like 52, negative one, and then three right here then goes through a ReLu. So after it goes through a ReLu, which is comparing these to zero, I would get 5203 because the ReLu is applied element-wise to each of these components, right? And this ReLu was doing max of zero. And whatever element in a row. When we backpropagate through this, we have from last lecture this gradient where if we're backpropagating through a maximum, this gradient d LDF is just going to go to whichever wire with. So in this case, for the first element of a 52 is bigger than zero. So the gradient DLD H2 in that first hour in the first dimension would drop back to the first element of the LDA. But the second element of a was negative. And so the gradient here would be zero for the second element because, because it was not bigger than Sir, our intuition is if I take negative one and I would go a little bit, not much change my output at all because my optimal voice stays values and you can write, Tom way is giving a better answer to your question, which is that the chain rule does apply our next sorry, I should have said that also worked fairly. The chain rule applies because that's how we ever backpropagate through anything. I was doing it element-wise, but tomboy was saying and other ways that you can view this is the chain rule applies. But TH2 da is a matrix that is diagonal and the diagonal elements are one or zero based off of that element, if a is bigger than zero. For all of these, if you're having trouble following this, don't worry about it, but there's something where again, just try it out on paper, looking at each dimension individually and hopefully everything will make sense. The question is in lecture and discussion, Yes. You did find this indicator function as returning all zeros or all ones. So I'm not sure what that okay. Yeah, The TSA, they didn't define it that way. Let me tell you how the indicators used here. So let's say that. Let's say that this is our vector a here. If I do indicator of a greater than zero, this equals checking each element and comparing it to the first element is greater than zero. Okay? So the TAs are saying that in discussion, the argument to the indicator with a scalar, in this case, our a is a factor. So when I say Indicator a greater than zero, what I'm saying is we're going to look at all the elements of a 52 -1.3, compared them to zero. And for each one we're going to say whether it's true or false. So 52 is greater than zero, that's one minus one, not zero. That's alright. Any other questions on this? Okay, let's continue on to do the backprop. So now I'm going to back up to my biases as well as the squire. So this is just backpropagating through a plus sign. So if I backpropagate through a plus sign, the gradient just passes through. So we can say that DHL DB gradient of my boss with respect to my biases also equals DL DC. That's the gradient at this wire for C corresponds to the value of this wire. And this is just equal to the LDA, where D LDA is this quantity. Alright? Back propagated to this plus sign. The last step is to backpropagate through this multiplication, which we now know the answer to, because we did both of them. So if I back propagate to the LDH one, then Dio di H1, we know is going to equal w transpose times d L d, c. Alright? And the LDC put the LDA, which is equal to this point. I can plug everything. And then similarly we will have to backpropagate to here. And so DL, DW, we know, will equal LDC, the upstream gradient. And then it becomes a, right multiplied by x transposed by H1 transpose. So that's backpropagation neural network. With this, you can compute the gradients of the loss with respect to all the weight matrices in your neural network. And if you have all those gradients and you can optimize via gradient descent. Okay? Any questions here? Customers, is there an easy way to remember what does all that work is on the right? So I'm gonna give an answer, which is, I don't remember. The reason is, oftentimes, when you look at code, it depends on how big matrix of examples. Exclusivity is N by the future, because this is what we do in the homework. But sometimes it's features by n. So in the end, all I remember is that if I go back to hidden activations, I'll have to multiply by something like w. If I'm going back to DL DW often multiply by the hidden activations. And that's where then I put a break point and my kid, I do x dot size or gradient dot size. Choose the transpose of the orders so that the dimensionalities correct. Alright, that's how, that's how it's being done. And of course for the gradient checking to make sure that that's why I mentioned in that strip is a very frequently that said on the exam, if they asked you to do a backpropagation question, you will expect you to get these dimensions correct. So make sure that for the exam room will give you a cheat sheet. So you want to write down how to backpropagate through these matrix vector or a matrix matrix multiplies. Or you can also take the dimensionalities of things. Okay? Other questions. Okay. Yeah, so sometimes they're giving some tips to remember things. I won't repeat that one because I guess like students may think of different tricks for how they remember things. So yes, this is the answer. And one way again to always check it is to match or dimensions. Other questions, Yeah. Alright. Can you raise your hand if you feel that you generally understand how to back off through the neural network. Alright, I see like maybe that was 75% of the class. I want to ask if there are any other questions on the back propagation steps. I'm happy to answer them. Yes. So the question is, what are the dimensions of DLD X1? So let's say that in this example because I didn't give dimensions, we'll see, we'll see H1, H2, m-dimensional. We'll call H1 n-dimensional. And then if H2 is m-dimensional, that means that w has to be N by N D 0 T H one will be m-dimensional. Tld top view will be n by n. The LDA will be, I'm sorry. Each one is n-dimensional. So this is the LDA will be m-dimensional. Db will also be n dimensional. You can go ahead and verify all of this and introspect yourself well, so, all right. Any other questions on this path problem? Alright, cool. This is something where, you know, I also had an electrode like this and it's for me solidifies when you actually put it up in the homework or do a trick question. So please come to office hours if you're still struggling with anything there. This is our last slide on that truck. So now that the gradients DL DW, that we can now compute the neural network. We can go ahead and apply our learning algorithm or gradient descent. And gradient descent will tell us how to update the weights W to make our loss, cross-entropy loss or smallest possible. On homework number three, you're going to find out that when we do this naive Lee, their performance is still not going to be great. That's because in addition to just the loss function, how to calculate the gradients, which is backprop. And then gradient descent, which is our naive learning algorithm. We're gonna find out that actually there are a lot of specific trips for neural networks to get them to train well. So this is going to incorporate several regularizations as well as initializations and then also use optimization techniques. And that'll be the topic of the next two to three lectures. We're gonna talk about some of these specific tricks for how to train neural networks file. So in this lecture, we will probably get today through two things. One is, how do I initialize the weights in a neural network file? And we're going to find that, that absolute intensity difference. And then the second thing we're going to talk about is something called batch normalization in this lecture. Then next lecture we're going to talk about other types of regularization is including something called stomped out that you may have heard about. And then the lecture after that, we're going to talk about optimizers like using momentum, RMS prop, and Adam, who here has heard of being Adam optimizer despite the appearance. Yeah, so several of you may know the Adam optimizer and know that it's much better than just vanilla. Gradient descent will go over the details of why that actually helps to train these neural networks. Right? So regularizations and specific train for neural networks. These are the chapters to look at in the Goodfellow textbook. We're going to start off by talking about initializations, which is something that we may not usually give much thought to. This is the initialization of the weight matrices. W1 is worth the price is P1, W2, and W3, V3. So the first thing I want to get in your head is that initializations matter. And what that will do for this is to say, maybe you might start off and training neural network by just be something surreal but something close to zero. And I say, okay, maybe the neural network will learn to like bigger. So what I've shown here is Python code, or a ten layer neural network. And each layer will have ten layers. Each layer is going to have 100 hidden units. Alright? And then I'm going to write a for-loop over my ten layers. I'm going to do something simple. I'm just going to write the neural network, which is going to be my waist times my hidden activations. And then after that it's going to pass through a ReLu. But when I first do my neural network initialization, I'm going to initialize each weight matrix so that its elements WIJ in a weight matrix W come from a normal distribution with zero mean and a variance equal to 0.01. So this is a really small distribution, or the distribution that will give you a relatively small values. All the values will be around, centered around zero and their variances is gonna be zero. What happens if we go ahead and do the neural network and run it forward? So just the first iteration of a forward pass with these small weights. What I'm going to do is I'm going to plot the following. I'm going to look at every single layer. So you remember that there are ten layers. So this is, Hey everyone. This is layer two. All the way to layer ten. Within each layer, there are 100 artificial neurons. I'm going to do is I'm going to take the mean of all of these artificial neurons. So if I take all 100 neurons in layer one and I take their mean, I get 0.04. And then I'm also going to take the standard deviation. So this plot on the left is the mean in each layer, and on the right is the standard deviation in each layer. So what you'll see is that as I go to layer two and I get a smaller number, and layer three. And beyond it eventually approaches zero. This is not that surprising because the weights are small numbers. It's a small number of times activations to bake you smaller and smaller, and that's why you go torques around. The standard deviation also goes towards zero. And what that means is, if we were to show you the actual distribution of the unit activations in each layer. So this is layer one. Layer two. The x-axis is the value of the activation, the value of artificial neurons. And the y-axis just counts. And so this is giving a distribution, a histogram of my activations. We'll see in layer one several or non-zero. But as you go deeper and deeper layers, all of the activations are just zeros with neural networks. Alright? This initialization leading to later layers having several activations. That was there a question? The question is what is on the y-axis? The y-axis will be counts. So the more, the better way to have written this would be to divide by the total number on the y-axis. And it will look like a distribution. But basically, if the y-axis is larger, it means more units had that value. So this is run across many different epoxy. So after how many different examples? So after having run our examples, I look at units that had a value between zero and let's call it 0.1. There were 67,000 of them. There are 50,000 with a value between 01100. Again. But just to show that in, as you go to a deeper and deeper layer, you have to pay six becomes there. I'm going to tell you, it's really bad for learning if the output activations or posters, or you might think, this is not our problem because when I do learning, the learning is going to make those weights bigger and bigger. But intelligence and someone tell me why? Yeah. Because it kinda just go so heart rate. So once it says kinda like the sigmoid will be in areas where the gradient is equal to zero. So there won't be any learning in the first place. That is correct at the high level. And then I want a bit more detail on why the gradients are zero in this case. Yeah. Perfect. Yeah, so let me just write what the students said. We know that. Let's say that we're at the output layer and we're computing our scores Z. So the Z is going to be our last layer, weights w ten times the activations from the pirate layer H9. Alright, so this is the computational graph. We're going to have a weight w ten inactivation H9. I'm just gonna ignore the biases for now and it gives me some z. All right? And then we may have an upstream gradients DL, DZ. And now I want to do WE tag. So I'm going to backpropagate to DL, DW ten. And we know that when I back propagate, the rule is I had the LDC times what's on the other wire, H9 transfers. Here's the problem, which is that H9 are all zeros. So if H9 are all zeros, DL DW ten is all zeros, nobody is going to happen for DL DW, for wait time for w. And so by virtue of having all of our activations equal to zero, we're actually not going to have any learning occurred in the number. The same applies if the same applies for all the other weights earlier that purpose. Okay. Any questions here? The question is, if we have a non-zero bias, does that mitigate this? It does not. So if you recall, for the bias, what happens is that the upstream gradient will just pass through a plus sign. And so if you look at the equation for DL DW, nothing about the bias appears. It's just an upstream gradient times the oh, I see Gesso, I'm saying. But then in this case, if there was a bias here, like a benign, than H1 will not be equal to zero. That is correct. So in that case, there could be some gradients on the P1s are the biases are also crystals. They're out if you initialize emphasis around, they'll also be yes. So, yeah, I'm always asking when I initialized to zero, are they all positive or are they Gaussian? And some are negative and they're Gaussian and some are negative in this example. Alright, so it's bad if the weights are initialized to zero. And here I've just written the code to do the backpropagation is look at the gradients. And what you'll find is that if I plot the distribution of the gradients look like. This is what they look like. And the key thing is that all of these are numbers times one minus seven. So all of these gradients are eclipsed is thereof that the gradients are close to zero than the goals aren't in. All right? So small gradients aren't the answer. The other thing is if we consider a very large weight initialization. So if you had done WIJ comes from a normal distribution with mean zero and variance one. If we were to go ahead and look at these networks using the same thoughts and said Here I'm showing the mean of the artificial neurons in each layer going from layer one to layer ten, as well as their standard deviations. You can see that in this case, the wafer to large, all the activations are going to explode. And if they explode, we know that this is bad for the gradients because we know that the DL DW is, I'm going to multiply the activations. At the activations are hundreds of millions or tens of millions. Those gradients are going to be really big. Big gradients are not good learning. So another, but you might have is, well, I was showing you this for Rayleigh's, but maybe you can mitigate how much the gradients explode by using cat age. However, if you use tan h, Alright, this is just asking you about tonight to general these candidates for the small initialization. The small initialization will still send all of your weights to zero. And then if you use Kenny to the large initialization, the activations for exploded ten age is bounded by one and minus one. So if you actually look at the distribution of the activations for a tan h network with large initialization, you will see that most of them are between -1.1. We also know that when the input to the tan h is very large or very negative, right? The contaminates, we're going to be in a region of zero, the gradients. So those learning is going to happen. Alright? So basically all of these scenarios, so not all the scenarios of a small initialization and a largeness realization both lead to learning difficulties. The question becomes, well, if not small, not large activity is gonna be intermediate, but what the intermediate value do we choose? So we're going to derive an initialization or an intermediate value of the weights that will lead to stable training. And this is quantity called the Xavier initialization. And we're going to have the following setting. We're going to have a neural network. And this is going to be the first layer. And then there's gonna be a weight matrix, W2, W1 here from inputs X to our second layer. And just for notation, I'm gonna use something that we had introduced when we first did neural networks. I'm going to call all of these units h one comma 11 comma two, h one comma three. Let's say that there are 100 units in these, so each one comma 100. So the first number tells me what later on in the second number tells me which student in that layer. So these would be H2, H1, H2 to Dan, to H2 comma 100. For the Xavier initialization, derive it. You are going to make an assumption which is first that all of the units within a single layer are going to have the exact same statistics. And so what I'm going to do is I'm going to simplify the notation even further. I'm going to say H1 here is a scalar. So this is a scalar that refers to any unit within the first layer. They all have the same statistics. And so if I see that the variance of H1, the variance of each one, this is going to be a scalar number that reflects the spread of one of these units in this layer. And that spread is going to be the same for all. Good I think. Alright? So I write here variance of H2. That's going to be the variance of just one of the units in this layer. But all of the layers have the same parents because they have the same statistics. So what does obvious idea was, was when I showed you the e.g. the small initialization, right? We saw that the standard deviations, which are the variance, the variance is the standard deviation squared at the initialization is too small. The variance goes to zero. And if the initialization is too large, the variance does explode. Simply. Xavier's idea is to say, we're reporting to take the variances of units in a given layer. And we're going to say that they're going to be approximately equal to each other. So the variance of neurons in each one will approximately be the variance of neurons and H2. And this is all the way up to the variance of neurons in our, let's say we have L layers to be bearing HLR. Then. Now we're going to make another assumption, which is not only are the variances of units and beta one plus unison layer two all the way to layer l. But if you backpropagate gradients with respect to loss, derivatives or gradients of the loss with respect to these waves are also going to be similar across all the layers. So he wants that the variance of the gradient for the weights and layer one are going to be approximately the same as the variance of the gradient of the loss with respect to the weights in layer two, all the way up to Layer. So now we're going to make an assumption that we want this to be true. And given this is true, we're going to try to derive what the optimal setting for the initialization is. Sitting on a questionnaire. Identical distribution or their means are. Great. Tom voice says, we're asked for clarification. When I say that H11 has the same statistics as H12, what are we assuming here? Are we assuming that the distributions are exactly the same? All we're going to assume in this case is that the expected value of H11 equals the expected value of h B12 all the way up to 81100, as well as the variance. Variance of H11 equals variance. Each one comma two equals dot dot. And that's why I just simplify it to the variance of each one. So if you have all the sensitivity, so remember this H one could represent any single unit and just maybe not for this derivation that need not be correct. Alright, let's take a five-minute break. When we come back, we'll show greatest masterpieces to in terms of initialization. So, okay, so the mathematical reason, second sentence to zero, we're trying to understand that all the weights are set to zero. Then the argument is that since the DL DW or whatever w, which would be DL with hh3h is just It's useful. This would be very small. Well, yeah. But then the thing is that W2, this is a thing that simplifies to multiply the other side by h. So the rule where, where when I have, when I had this multiplication, so this would be like a W3 and this would be an issue for you. When I back propagate in orange to w, I'm going to get multiplied on the right side by H2, right? And in this case stage two. Right? Okay. That makes sense. Yeah, In the same question, would he only had really when bitmap we can get them and advocate Florida's new helper function that we can derive an analytical form and can be ambiguous and very easily find out that matrix. Yeah, so if you can't get a derivative and you can't backpropagation. But as long as we can write down the function. Whatever this nominee or fingers hoping able to calculate the gradients or else something called a subgradients. In those cases, we need to have defined radius the backprop. But in those cases we can backprop with someone I say, like really doesn't have a derivative because it has a peak at zero. Bogey could just define it to be zero or one. That's a subject in and establish, oh, I didn't mean that the derivative doesn't exist. What is the current format analytical columns so as to begin this night in the simulation so that I can just get the points or the matrix. I don't know what the setting would be though, where you wouldn't be able to write the analytic put the record is because this will just be like a scalar function. You'll always be able to write the analytical gradient or subgradients at that expression. But if you think of an example that I can pick up just talking about again. Let me just get back to your timer. At the beginning when we did it in their shoulder, like a specific firm. So that's a negative term in front of them. Disappeared and that's because I should have been there about this also. In this example, we were doing z equals y minus w. Okay, so that minus sign comes to this morning, right? Yeah. Great. When you pull the cord, we have done with small initialization. Yeah. How do they began to speak in respect to this? Yeah, so when you have a small weight initialization, the variances, we'll quickly decay to zero. So this part here, the very specific conditions where you have hedge one-and-a-half just derived from, I mean, you just randomly. But how did you get the standard deviation for HTTP? Calculate it from his jacket. In this case, I just did it incorrectly. So I just looked at all the activations. I actually took their ********. So to actually compute it, will need to make some assumptions and we'll show that in our software initializations derivation. You move to the next slide. The last slide. That's fine. Yeah. So when you talk about the statistics of these isn't like Don name, Let's have to stick. So this involved even the statistics of detail tonight. Yes, they will be a function of the data because ultimately they're all, yeah, one is a function of W in order to get the proper initialization of the dates. Yeah, they should have some information about data, isn't what that means. In this case, we will be able to derive something you're definitive data. If the data statistics were scaled and W1 is independent, then the variance of these. So if the variance of x was like say 100 or two things, you say firstly, z-score. So the variance of this would be one. But even if you weren't then variance 100, then that will scale up all of these variances, but they would still be approximately equal to. Okay? Just because of time syntactic, pass a five-minute break. So can we talk about that now? Great. Thank you. All right. I will get back to it. You're talking about how we are going to want the variance of the units in every single layer. The approximately, another case that the units won't explode or vanish. And then that will allow us to do training. And so this leads to the Soviet Union Square edition. This is just a slide that formerly says what the book we just discussed there. So what I'm going to do is with the Zagier initialization to do any math to kind of increase your risk for what industry. You are going to make a bunch of simplifying assumptions that are not true. But the assumptions will get us to an answer. And if that answer allows us to save a general about books, that's great stock and it failed because our assumptions are there. But we're going to make these assumptions to just try to get at least to an answer. And we'll see that even with these assumptions, things will work out. So we're going to be in this setting, we're gonna be using the same notation as on the prior slide. And in this case we're gonna be looking at layer I minus one. Going to layer. Remember we're going to have this notation that this unit is going to be H and layer I minus one comma one. This will be h in there, I minus one comma two, et cetera. Then there's gonna be a total of n n units. So this is going to be h of I minus one comma n. So n is going to be the number of units in this layer, i minus one, we call it in because you can pick up where I minus one as the input layer I just bought me care about analyzing. And so what I'm going to do is I'm going to look at some dinner and where I, this is still a scalar. This is just any unit. And later on, I'm using this notation. Little HIT to reflect the fact that all these units have the same statistics. So I'm just going to call this HR here. If I want to compute the value of HI, right? We know that all of these units have some weights in my neural network to HI, so it's gonna be a dot product with that weight and all the units in the preceding layers. And so that's the summation over here. All right, What I wanna do is I want to try to get salvia condition. I want to assume it to be true and let's see what that means for my waves. So remember the variance of HI will equal the variance of h of n minus one under this IV or initialization. So what we're going to do is I'm going to simplify this expression. I'm not going to simplify. I'm going to take the variance of both sides. I'm going to take the left-hand side, that's gonna be bare. Hi, and then I want to take the variance of this summation. Alright? So if I take the variance of summation, I will need some rules to simplify it. So in general, if you take two scalars, W and H, W corresponds to w I j, and this H corresponds to h i minus one j. If you take the variance of their product, then there's this variance expansion formula. And then we're going to make a few assumptions here. We're going to assume, again, these may not be true, but they're necessary for us. So rather than Massive, we're going to assume that the weights w, as well as the activations in the prior layer h i minus one have zero mean. Some of these students are going to notice that this assumption that h minus one has zero mean is really, really not true because if the output is right, then the minimum value in the HIV can take on its surroundings. But then remember in this example, we're assuming the units are going here, so there can be positive and negative in this case. Alright, so if the W's and the H Price have minus ones have zero mean, then this term equals zero. This term equals zero. And we're just left with just the variance of W times the variance of h. Alright? So now if I take the variance of both sides, I'm going to get an equation where I have on the left-hand side the variance of HI. And then on the right-hand side, I will have a son of the variance of the W's times the variance of these Hs. And we assume that all of the units in layer i minus one had the same statistics. So I'm going to take that variance of h minus one and take it outside of the summation. Alright? Any questions on the again, this is just like a rough hard to give us an answer. That remember that Xavier initialization says the variance of all the layers is approximately equal to each other. So the variance of HBr and HI minus one are equal to each other. I can divide both sides by doing so at h i minus one. And this is going to lead to an equation that we're going to have the sum from j equals one to n of the variance of w I j is equal to one. And then further, I'm going to assume that all of the weights themselves have the same statistics. So that is the case. So this is going to assume which have same statistics. That means the variance of w I, j is the same for every single value of j. And so I'm just going to have n of them. So this is going to lead to n, n times the variance of W, or just call this i. Now, the weights going from minus one to i equals one. Then this tells me that the variance of the weights should be one over. And any questions there? Yeah. The WIJ in this case are the weights that go from units in there, I minus one until later on. So I had the WIJ, this would be wi, wi, wi Wait for one particular unit later on. Great. The question is, is this always to me that we have a multilayer perceptron? Yes. So it's assuming that the form of how the artificial neurons and where I might want to go to layer i is through this linear conformation. The question is, is there an analog for other arbitrary architectures? Yes, So there is also an initialization rule for recurrent neural networks. We will talk about that when we get to the RNN after his death. What is the n? N? N, N is the number of artificial neurons in layer i minus one. So there were 100 years than n, n equals 100. Tomboy. We're assuming that the expected value of the weights is still zero. Yes. There are going to run other functions here that may not be true, but it gives us the service to get the linear case. And we'll see if it works practically in the nonlinear case. Recognition that's going to make that distribution. Nasa Tom way is asking if n is large, isn't just a small weight initialization? That the variances are small, the answer is yes. So it's all relative to the size of the networks. So for whatever network I showed, whatever industrialization I chose was smaller than one over n. Right? Perfect. Yeah, so this student is asking the question that gets me to my next slide, which is, if the waves are identically distributed, the variance of the w's is one over n n. But then Xavier initialization had this other condition that the backpropagated gradients to pop the same barriers. And so we follow the exact same type of argument. You'll find that the variance of the weights, if you look at the backpropagation condition, should be one over and out, where n is the number of neurons in layer. So this is number of neurons in layer. I'm just going to send n. Number of neurons here is an R. So what do we do? What we do is we just take the average of the n average, right? Will be the average of n in and out. So it's just an n plus and add over two. And then usually what we do is we initialize the variance to be one over an average. So it'd be two over n plus one thing that you can do. Oftentimes, people don't even consider this and they'll just set it to one over n. And that's generally fine as well. But if you want to take into account the backward condition than you would set it to two over n plus. Alright, let's go ahead and just see if it's empirically works. So I'm going to run the exact same experiments where now the weights here are gonna be drawn from a distribution whose variance is going to be two divided by the variance of an plus an hour. Sorry, the variance is two over n plus n. Now I see this bird here. So that means that this strand and function takes the standard deviation or variance. So this is the Xavier initialization. And if we look at 108 numbers and we looked at this same plots, where we look at the mean activation in each layer, the standard deviation, and then the histogram. You can see that the units are starting to go towards zero, but they last for much longer time. So all the way up until Mayor, Can you still have a reasonable distribution of non-zero activations, which means that it hadn't exploded or banished to zero, and therefore it will have reasonable gradients. So we can do, alright. Any questions here. So the expression that we used in the answer, actually use that expression with all. Great, yeah, so Abishek is saying, when we derive things, we derived this initialization. It was with certain assumptions like no nonlinearity. And all of these equals statistics between weights and activations. These are not true in real life and real life. We also have nonlinearity. Even so. It's good initialization. And the fact that in Cherokee still works, even though these assumptions are not true in this thing work. What each layer, we have particular values for the weight distribution, as opposed to the previous example. All three plus in this case also be scandalous smallpox. And he jumped, they're wiping the example that we showed earlier when we were all going live. So as you're going deeper than you are composing functions, monkey password, it's going over there, specific. Maybe that's right. Yeah, so if the layers have different number of units input and at the output, then the variance will be different. Because the variance will be a function of the number of inputs and outputs. So one point of time I made him a fish layer could have, will have a different initialization. And that's also true. That makes sense like if you have few units, e.g. you'll want your weights to be initialized to be larger than if you have many units. If you have 1,000 units, are doing a sum of 1,000 inputs versus if you have two units is just the sum of two of them, right? So to make them have approximately the same scale at the output, you'll want the weights to be initialized differently. You should have made the number of units, correct. Yeah, yeah. So Tom ways mentioning this is especially relevant because in neural networks oftentimes we will reduce the number of units as we go deeper layer. So these would be initialized with different theories. Any other questions here? The question, is it a necessary condition? For, is this a necessary condition? This condition you're talking about for the neural networks to train. For it to be high performance starts. There are other initializations that could work, but this is a reasonable initialization that keeps her activations alive by the end. Excellent. Right? Yes, It's always saying, and by the way, I want to emphasize is the spread of industrialization. It's not for training the vagus initialization at the start so that our gradients won't die or explode. But as training occurs, the training can alter the wage, so they have very different variances amongst players. What we wanted with initialization is just to get training kicked off. But then after training, the variances in each layer will definitely be different. Question is what is the difference between ending in and out? So in this construction, and n is the number of units in layer i minus one, and n is the number of layers, number of units and b are. Any other questions? We'll take these last two normal by, Sorry. Oh, you're saying, is there a particular value for these studies? Emphasis should be equal to 0. Like would it be better if this thing was constant? Yeah, it would be, it would be better if these all look like this. But it's hard to like very quietly to the initial weights so that everything set up a standard deviation of excuses not to pursue. But still after ten layers, it hasn't gone through. Said that last step you start training. Now, what's the homework? Yes. So the students verifying, we haven't adjusted by that these assumptions are true. We're just using them. And then we rely on the empirical results to see a reasonable kind of like the ends justify the assumptions. That that's true. For a more complicated setting where you include the nominee area, e.g. we won't be able to come up with simple derivations like this. And so we make simplifying assumptions. Getting the answer, we try it. And there's profit of works and it actually personally, alright, it works for tan h. But then this is a nice follow-up to that student's question. If you use a Salvia initialization with bamboo, it fails. So if you use this audio initialization with rabies, we see that by the end, we also had traditionally assigned to zero. And because most units are most neural networks for using the baby activation, this is important to address. And colleagues in 2015 suggested a bridging this one over n, n that we derived to two over. And then using just the heuristic intuition that many units will be close to zero. And so because ReLu might kill on average half the units and this is not true. It's like usually no text 20% of the units that are zero. But they said because rarely kills off the units less increases by a factor of two, right? And then there's also this other initializer uniform also by Zagier. And this is often used in neural networks. But returning back to her, the Xavier initialization doesn't work. Probably because the ReLu has several units that are going to zero that reduce the variance. So her size to multiply that by two. And if you go ahead and you multiply that varies by two, all of a sudden things work. Alright? So the initialization is the Xavier initialization with an extra factor of two. However, sometimes something as simple as a factor of two mixes, very big different. Initialization is a very important aspect of training neural networks and it remains an active area of research. This is the plot where I believe in blue is ReLu, and in red is tan h. And if you use this audio initialization, they said that you don't ever get better error, but if you use that Dr. to initialization, then all of a sudden you take care. Alright? So initialization matters in this class moving forward, this always confuses off your initialization. And this is just a table from a paper in 2015. The papers called all you need is a good admit. That's not true anymore than just good at it. But this is showing different initialization methods like Zagier and SRA. Here is the initialization and it shows that with different initializations, you can get as much as what's like 3% improvements in some of these particles is Claudia. Any questions on initialization? I got the question is can I clarify the two graphs? So I believe that sorry, I think I misspoke. I'm 90% sure of this. Tas check this for me because I haven't looked at this paper in a while. I believe that this is for tan h networks and this is for ReLu networks and tennis networks. If you use the initialization which differs from Xavier by a factor of two, they both train, but in ReLu, zombie or doesn't train and Rabia traps. So thanks for that. Archean rocks and facts on the x-axis, what is the epoch? These are training to be false. So this is a gradient descent. Oh, I see. Yeah, so in gradient descent, you know that we have mini-batch gradient descent. Let's say we have 1,000 examples named mini batch over say, 50. So 20 iterations of gradient descent will be one path for our dataset and that's called one. So one epoch is one entire path of the receptor. That's great. So the students asking in the firefight, I also wrote this initialization. So in this paper by his Amiri Baraka in 2015, they discussed the math WE went over for giving the one over n in initializer. They also motivate a different initialization where you draw from a uniform distribution. And then this is the range of the uniform distribution. So sometimes when people say solve your initialization, they need this uniform distribution. But in this class we'll refer to the one over n, n. And I'll refer you to this paper if you want to learn more about what that initialization is. Alright, so that's the initialization. The next thing that we're going to talk about is batch normalization. Batch normalization is a really important technique to help to make the neural networks practically be less sensitive. So some hyperparameters like learning rate, you'll find that training is a lot easier with batch norm. So in homework number three, you're just implementing neural networks. But in homework number four, we'll ask you to implement batch normalization as well as from it, which we will begin the derivation of today and we'll finish the derivation of backprop through batch normalization. And Monday's lecture of next week. You'll see that once you implement batch normalization training and choosing hyperparameters and the neural network will be a lot easier. Alright, so then another thing here, which is that if you look at batch normalization was first introduced in a 2015 paper by I'm not sure I'm pronouncing these race, but IOC and security. In 2015. And they motivated the need for batch normalization through something called internal covariate shift. And that's also Bella uses in the textbooks. So we're going to teach batch normalization from this motivation of solving something that we'll call internal covariate shift. But there's been some more recent research which is asking why does bashed arm really worked? And then maybe for other reasons. Alright, so this is also an active area of research. Okay, So let's first motivate batch norm through internal covariate shift. So we're going to imagine we have some neural network. We have our inputs x. And then we're gonna go through some weights w1 and B1. I'm not going to write the d is just for convenience. We're going to have activations H1, H2. I'll draw one more, H three. And then we'll have wage W2, W3. Eventually these go to some loss function L, right? And we know how to backpropagate to get all of the gradients. So we're going to have gradients by DL, DH three. Backpropagating from the loss will have a d L, d, sorry, not H3, and then W3, W3, PL d W2 to W1. So these are all gradients. Tell me how to change W1 and W2 and W3 to make my law smaller. Does anyone see a problem with the following, which is when you're going to implement, I'm gonna compute all these gradients, W1, W2, and W3. And then I'm going to exchange all of them together by doing my gradient descent steps. I'm going to do W1 is my old w1 minus epsilon DL DW one. I'm going to do this also for W2 and W3 all in the same batch of data. If someone told me a potential problem with that. Yeah, Perfect. Yeah. The students said let me just reiterate what he said. So let's see that we're looking at the weight W3, right? Dl DW Three is a gradient that tells me how do I change W3 to make my boss molar, right? Mr. Changing only W3 in a gradient descent step, Change W1, W2, and W3 together. I may be doing something unwise because calculate delta w three, this tells me how to change W3 and the absence of anything else changed. But now W1 has changed. Now W2 has also changed. And so now my H2 might be wildly different than what this layer was expecting. So how do I change W3 might be totally different now because H2 is different since W1 and W2 also changed. All right. Does anyone want me to repeat that? People do. Because I like my dad said, most clearly, DL DW three is telling me, how do I change W3, assuming everything else stays the same to make my losses smallest possible. But when we do gradient ascent, we change W1, W2, W3, all at the same time. Alright? So now that I'm changing W1 and W2, when I use DLT W3, which assumed that W1 and W2 are constant. This change to W3 may be nonsensical because now H2 has entirely different statistics than the A12 when I initially took my DL DW. So basically said even simpler. Dl DW three tells me how to change the W3 with everything else stayed the same, but I'm changing everything else. So how do I know that the only WPA3 is actually the question, does this problem? So the question is, does this problem still arise? If I'm just going to say more generally, the second thing you said, you take the gradient of L with respect to older rates at once. This problem does not arise. Mathematical relationship between each one is a function, right? So tomboy is saying, essentially all these things are linked. White. H3 is a function of W three, H2, and H3 itself is a function of W2. So H3 is going to depend on both the values of W2 and W3. And H3 will also and affect the loss. So I take DL DW Three, Let's give a concrete example. Let's say that H2 has values that are 1-2. All right, so those are the statistics of H2. Abs mean is 1.5 and experience is like one, right? So W3 is doing an update assuming that my H2 is going to be 1.5 and variance one. But now DLT W2, W2 much, much larger. So now H2 has a mean of 20 and a variance of tech, right? Then the update that I do to W3, which assumed that H2 would have much smaller mean and variance is less. So that's the internal covariate shift problem that we're going to fix with batch normalization. Batch normalization is that you're going to take every single layer, H1, H2. And we're going to normalize them to have unit statistics. What's this unit statistics mean? So if my layer I is a ReLu of excide, we're going to assume the unit since it's 600, ended up before the activation function, the axis. So it's going to mean that the expected value of x dy is equal to zero and the variance of x psi is equal to warn. All right, so intuitively what is that saying? It's saying after every single layer, we're going to put something that normalizes back to lease their own variance one. So in that example that I gave, where W2 mean H2 has a much higher and much higher variance. And W3 was changed expecting issue to have much smaller venomous barbarians. Fashion on layer comes in-between here and says, your statistics are going to be normalized back down to mean zero and variance one. So hopefully my updating W3 would have made more sense, right? So basically, after every single layer, normalize the mean and the variance is 0.1. And then hopefully mitigate this column of internal covariate shift. Any questions there. Alright? So you'll notice I talked about applying the mean zero and variance one on the thing before the railroad. Sorry, you said at the beginning, HA, HA, minus one times wi minus. Oh, yeah. Just to be clear, I'm just going to say that at psi is the product of a wi times and HI minus one. These are multiplied together to give an XI, which is then given to to give HR. In this class, we're going to apply batch norm on the axes. And it's gonna be in the following way. In our vanilla neural network without a batch norm layer. What you'll see in homework number three is that we're going to define this w times h to be an affine layer that I find where is gonna go down into a ReLu Bayard. And then we'll have multiple layers of a neural network. So then that would go into an affine and we'll go into a raving. When IOC and Saturday first proposed batch norm would apply the batch normalization before the railroad on these oxides. And so in the homework, we're going to have an architecture that looks like that fine. This will be homework number four. We're going to use a batch norm layer. And then we're going to use a re route. And then we'll repeat this offline. Batch norm, re-root, et cetera. Since this paper in 2015, people have done more extensive experiments where they do the batch norm on H's instead of the axis. And this actually helps performance. So today you would do affine, ReLu, been, bashed, norm, et cetera. Alright, but our assignments will go off of the original IOPS. I didn't taper. When we derive the backpropagation for batch norm, it will be for this architecture. So in this class we're going to use this. Let me make that clear. Their star we will use we will apply batch norm before rabid. Yeah, right? So telomerase is saying, when I'm motivated it, I said I want H1, H2, H3 that have statistics. But if I were to actually apply this bashed on before the railroad and the output of the Ravens does not have enough statistics. And that is correct. And what is statistics? Well, it's not easy to write down because this will be a nonlinearity. And that's why it probably makes sense to apply batch norm after review and w's to higher performance. But again, I'm just going to go off the original paper. The question is, what is the affine? Yeah, fine. Is w times h minus one plus BI. Question. The question is, did the original paper have a rationale for applying dashboard before baby daddy didn't get any rationale. The question is, how is this different from feature normalization? Is it different from feature normalization? Like z-score in e.g. so the scoring is this idea of normalizing your input. You didn't have the zero and unit variance of people didn't know what the scoring was. Batch norm is essentially doing the same thing, except with a few key differences. One, we're going to see that bachelor actually allows you to, to, to, to learn parameters that allow your needs. Not these are undergoing asked to not be one. So we'll talk about that. The second thing is that feature normalization is typically done offline, but officer data and then you use that in that form. This is part of the computational graph and we'll have to backpropagate through. And this is actually a very critical prior to this paper, people had tried to do feature normalization without backpropagating through it. And if you don't drink and the bartender, that picture that normalized. And so it actually didn't know how to essentially his book where he talks about this, it's like use with effort. So it's important that we backpropagate through this normalization, also. Normalized and ask this question is because we are normalizing the activation. Does that make? Well, we talked about the weight initialization a bit redundant and the answer is yes. So when we applied that foundation will be far more improved robustness to weight initializations. You also see that in, I think will be accentuated to comment number four right there. Well, even if he doesn't do the contestant yourself and you'll have more reluctance. Has the question is, is batch norm and affine transformation or does it introduce non-linearity? So let me just go to the slide where we talked about with dash. Dash normal look like the Z-score and operation. We're going to take our activation, right? And we're going to subtract off. So actually let me, let me first draw what I mean by XOR and whatnot. Since these are always things to keep track of, we're gonna be looking at just one layer. That layer, we're going to have activations. Artificial neurons are so x i refers to just one of the artificial neurons in this layer. So batch norm is applied element-wise to every single artificial neuron. We have 100 units than we would have I going 1-100. And what we do is for that. Artificial neuron. The artificial neuron. We subtract off it's mean, and then we divide by standard deviation. Now, instead here do square root of the variance plus Epsilon. Epsilon is just a very small number to make sure that you don't divide by zero. But if your, if your variant is not close as irrelevant, this is just dividing by the standard deviation. Alright? What are mu and sigma squared? Ys are the means and the variances of the unit XI and the computer the cross-examined. So what we do is we look at x-i a cross N training samples. So this M here is m training examples. So far ten, I will show my networks 50,000 images, e.g. so in this 50,000 images, I will have 50,000 values of X1. And if I just average them together, then I'll have the average activity of this first neuron. So this is m training examples and this is for the artificial and unit. Great, yeah, so tomboy says, and this is a batch size, not the training set size and that's correct. So whatever your batch size is, maybe you have a vacuum 200 examples and your m would be 200. So let me say I'm the training batch examples from what you compute your mean and your standard and your variance. Any questions on the first bullet point there? The question is, we've been using H for the oxidation. So should this be HI, because we're going to follow this convention, but we're going to do it before the ray tube will call the thing before the ray Bu Zai. And that's just following the notation and the 2015 paper. So they're basically a longer fast and it's not a long decimal. Then another method that they use, but it's great. So remaining is pointing out that the computation of the statistics is along the examples in the batch, the m examples in the batch. That's why it's called batch normalization. We're being as saying That's also something called mayor normalization, where you average across these neurons. You're not going to talk about that at all in this class, but that also does exist. Alright? So that's the normalization procedure. After that, batch norm has two other important parameters. And I know that you will all have questions about this. The parameters are Gamma, I and Beta. And these are learnable parameters. And we know that if we were to take Psi scale it by Gamma I and then shifted by Beta I. The result which we'll call y I will have the following statistics. So we know that x, sorry, has mean zero and variance one. However, if I multiply excited by gamma and then add beta, and I call the output y. We know that the expected value of Y equals beta i and the variance of y i equals gamma i squared. And this is a part of the batch norm beta. And Gamma I is not equal to one and beta I is not equal to zero, then the output of the batch norm no longer has units statistics. It's no longer means they're out there, it's one. Alright? So this is a critical part of batch normalization. I know the question that you have in your mind is, why did we do all of this normalization only to undo it by this beta i and gamma i term. So what I would think of this, think about this in the following way. What we're doing with batch normalization is that we are normalizing the statistics to have zero mean and unit variance to address that internal covariate. However, you are significantly limiting the capacity of your network. Every single layer has to have insurance with the same statistics. We know from looking at neural networks. The features computed in each layer, well, in general, will generally be dramatically different and maybe they need to have different statistics. So batch normalization then gives you these two knobs to say, okay, if you need to have a mean that's different than zero or very specific, the button will allow me to do this, but this kind of biases the network to try to find solutions with mean zero and variance one statistics. And if they need to be different, the network has the capacity to change those things and those prices too. Yes. So Julie's question is, I'm sorry, j, is this the activation of the artificial neuron e.g. the second one, when fed the J image, that is correct. The question is, what happens with different layers? So remember that there's just going to be a bachelor applied at every layer. So that's why we don't have like another subscript white guy I for the layer. This will be an operation that's what applied just on a single layer. Since the question is computed over the entire set of training examples. So it depends. So I believe in the training phase in homework number four, we'll ask you to calculate it over all of the examples from the training set. But when you run it in real time, you will have to compute a running, the inner running variance. I'm not 100% sure that shoe, so it's either going to be over the entire 20 centered over the batches in your training set. You know, TAs, if it's batches, our entire training set. Well, it's going to be one of the two. If he's entitled instead, you'll have a better estimate of the mean and variance. This whole thing, right? Yeah, so this student is saying when we run it in real time or when we run it in France and testing. And we're only class, you have one image. What do we do? Because we're not going to get a good mean and standard deviation from this one and h. So usually we will initialize the mean and the variance to be what was found in training. But then we will keep passing it in many examples for inference. We're gonna keep a running tab on what the mean is of the few examples coming in and what their parents, it's not perfect. We call the running and it's running periods and the only implement that in homework number four. This right does not remember, right? The student is asking, if you put it after an activation and deactivation has a bias. E.g. there's something called a shifted array route where ship the ReLu, up or down. Oh, great. Yeah, so the student is saying right after an affine transformation where we have a WHI minus one plus BI, BI, and this data I essentially be done didnt the answer is yes. Any other questions? Alright, so that is the batch normalization layer for the forward pass. We insert this into our neural network. We definitely have to backpropagate through one word before we, before we show the computational graph, which is, you'll notice one thing. In batch normalization, we are Applying the mean and the standard deviation or the variance on to each unit in isolation. Alright? But you may know from your statistics class these units may also have a covariance. And so you can clinical better normalized or whitening your data by normalizing by a covariance matrix. And it is possible to do this. But we practically don't do it because this computation is much more expensive than doing it. Doing it unit by unit, as we've drawn here. For computational reasons. Efficiency, we won't do this normalization by covariance. Alright? And then I just want to reiterate that we do have these Gamma I and Beta parameters, which could make the statistics not mean zero variance one. And that will actually be important for the network and his wife. Several people also believe that doesn't really help with internal covariate shift, but it is the motivating reason, but it's usually taught for batch norm. Alright? So what we will do is we'll call it a lecture actually for today. When we come back on Monday next week, we'll try the computational graph with batch norm, and then we'll start to do back-propagation and derive the steps of how to back propagate to all of these operations. 
Good morning. Can you guys deal with what we talked about in prior lectures from last quarter? Alright? Yes, Daniel. Daniel's question is he's been copying Dr. White House their cells instead. Okay. Fantastic. Yes, they're vomited TAs or equal to its documentation. Able to read that before. Okay. So last lecture, you're talking about batch normalization distraction. I'm going to have to convince me, ask you, so please don't distract other students. We're talking about batch normalization factor. And we talked about batch normalization is going to help with this problem called internal covariate shift. Where the issue is that we're going to be calculating gradients of the loss with respect to all of the weights. And for each of those weight updates, what we do in the training that we take. Sorry, I see a video there also. Please guide students here are paying tuition to take these classes. I know that this might be for some YouTube video or something, but please for the students I'm going to ask you. I'm working. I'm sorry. Can you please all right. I'm just asking why we didn't use it. Like Wow. All right. Sorry about that. Everyone wanted DMCA, YouTube. They do post it and it's not respectful. I appreciate it. Alright. Because he was asking you a question about not being a full tell you right now. If you took the fact that the TAs are very helpful. Alright, that was 10 min material. So we're talking about batch normalization and batch normalization. What happens is we're trying to help this problem with internal covariate shift where we update all of the weights and redo it, unfortunate or we do it at the same time. And what that means is that I may be making a change to W3 to try to make my loss folder. W2 might also be changed. And maybe when I compute this gradient, DLD or W3, all the values H2 or 0-1. It's on adjusting to try to handle this and the scenario where each switch 0-1. If I go ahead and I also changed W2, maybe W2 is such a large change and other values of H2 are determined by ten. And now my W3 doesn't matter that much. So what we wanna do is we want to reduce this problem of an internal covariate shift by normalizing the statistics. These hidden activations to be in the range of having zero mean and variance. And therefore, even though we're updating all these bits together, even though we're updating all these waves together, hopefully those changes that we did. So basically the more relevance to decreasing force. All right? Any questions there? Alright? Okay, so this moves the conversation. What we do is we take one unit at psi. So there's going to be both a subscript and superscript in these examples. Xii, try here is the neuron upper layer. So if we take a hidden layer and there are 100 units, that's ten, or x2 would be the second unit in this layer. And then the superscript j, x i superscript j is going to, the j is gonna go over exactly. Alright? So you can think of looking at this layer in the neural network. And we're going to have a mini batch of examples. They're going to be images from C4 prime images. And let's say that there are 100 examples, right? So we pass all those images. And for each example, we're going to have a corresponding activation of what this artificial neuron is during that exam. So that you can compute the mean and the variance. We can go ahead and across all of those examples. That's why there's the one over n sub j equals one to m average the activation and inactivation. And then the standard deviation or the variance is going to be the sample variances and across all of those examples. All right, any questions on that operation? And then lastly, even though this will normalize the means and variances to zero mean and variance one. We are also going to scale and shifts the normalized activations. This allows fashion occupies an additional degree of freedom where it says, you know what? My copy. A good idea to make all the activations have zero mean and unit variance, have different needs or different variants to capture different features of the image. And so it is better for the activations that have a different bean or different variants. We allow there to be two of them learnable parameters. So for every single unit, alright, we give it a gamma and beta are. The Beta is going to be how much we shift the mean by Gamma I is going to be how much we scale it by which we'll scale the variance by gamma xy squared. All right? So we're going to have a high level of batch normalization is going to first make the mean zero and variance one. And then after that is going to have to learnable parameters that could make them being different than zero and the variance difference of one. So desire. Any questions here? Great. So the question is during training, do we need to compute a new view of sigma i for every four tasks? And I forgot to look at the homework to see what we expected there, but we do make the trait and fast one UI and sit nice spread across the entire dataset. Or is it for each batch? Okay, so tomboy says for each batch, so for each back to the computer do view and a Sigma I squared. Any other questions? The question is why the label here, know why? That's a great question because I've used that before. Why did I here is the scale that the patients. So you can imagine that if I take this layer and I pass it through a batch norm, what is going to return is the same sized vector. But these are now all the Y-i. So this is y one, this is y two down to 100. So this will be the re-scaled batch norm. Pitchers are the more ions. Question is, how do you know what value to shifting scale it to? We don't know if that was going to be burned by the algorithm. So these will be displayed Iceland Gamma i's are going to be fit during stochastic gradient descent. The algorithm will in a data-driven way to get your losses gets possible. The question is, can I talk about why we then normalize and scale of the data? So you might ask this question because you think that is something that peaceful purposes having done cache warm. Because now the teachers can have different meanings in different areas. And so the reason or one way I think about this is what this operation does, is it biases and that works towards 01 activation saying, can you solve this problem with a nice or ovarian just want activation. And then if you can't, if it really helps to make the bearings different than what it can mean, different things in one than zero. But if the question is, do we initialize the beta and the gamma eyes? Yes, I believe that the TAs can check me on this, but we should initialize them to Beta I equals zero and gamma equals one, which means that it would be mean zero, variance one. The students question is, can we think of this as a two-step process which is first to normalize and then scale and shift. Yes. Alright, so with that last lecture, we left off with drawing out this computational graph of the batch operation. So here is the denominator of the Z-score, which is square root of sigma phi squared plus epsilon is done by this here. So the Sigma i squared plus epsilon I square root, I square root it and then I put it in the denominator, that's the inverse x I minus mu comes from here. And multiply these together and that gives me x psi hat. So that's this. And then I multiply that by gamma i. That's this node right here. Beta i, which is this node right here. Alright? Remember, in this computational graph that I've drawn here, there are. So remember that this is for just a computational graph for one unit. You didn't ride, right? Is just one of these artificial neurons in layer. And then just for one example j. So one example j, and that example will be one. This is a computational graph for one unit and one example j. What you'll notice in this graph is that everything has a subscript pi. So this graph will look the same for every single other unit as well. So on the next slide, I'm going to drop the subscript i for convenience because the subject I will always do that. Yeah. I'm looking at something up front, right? Yes. That's correct. And the axon is just a small number that prevents that the variances are owed. You don't want this to be infinity. So the epsilon is a small number that prevents that. Other questions, right? Yes. So Daniel asked when we talk about X-i, is that input or is that an actual neuron doesn't actual neuron. So let me ask you this, have this be for next year. That notation, we're gonna be using the exact same notation as in the IOC Saturday paper. So usually Zai was reserved for inputs and HI, in our class has been the given units. But here x, y will be the artificial neuron activity. And then y, instead of being labels are going to just be the output of the DashCon later. Alright. Any other questions? Okay, Let's go ahead and do this back propagation. So you're going to assume that we have some hops, Srini gradients, this upstream gradients, I'm going to call d L, d y. And then I'm talking to the subscript, I'll just keep the superscript case. So DL DYJ, right? And when we do backpropagation, we need to know how to backup to the parameter, my parameter to the beta and a gamma. And then we also need to know how to back propagate all the way to begin, right? Because these inputs, the x i's, are the actual values of the artificial neurons. In layer. I need to know their greatest so that get back up to earlier layers. So we're going to start off with are up-to-date. And the first thing that we're going to do is backpropagate one step. So we have a plus sign here, which means that gradient just passes through, right? So the gradient at this wire or at this parameter beta i, it's just going to equal d L d theta. Sorry. There's gonna be DLD y j. That's the backpropagate. Then you're going to notice something, which is, if I read the gradient DLD Beta, I get, I've just dropped the eyes for convenience. I have a sum for j equals one to n over all of the DLD Wij's. Whereas if I'm doing it this graph for just one example, I just have a DLT YJ. And someone tell me why the total gradient for DLD beta here is gonna be the sum across all my examples. This is not an easy question, so please take it to the comparator. Object does have to do with Watson of derivatives. It does. So remember that what I'm doing is I'm drawing this graph for one unit and one example, cherry, let's focus on the fact that I'm drawing this for one example. So let's say that my dad had 100 examples. I'm going to be running this computational graph, these computations for j equals one to 100. So because this is a graph of just one example j, but I have 100 examples. I'm going to have 100 replicas of this graph. So let's say I'm going to draw another one. Let's say I'm going to just replicate this graph. So I'm going to draw exactly how it looks, but not label everything. So what I've done here is I've just drawn a replica of this graph. This graph is going to also exist for unit I, j plus one example. And this is the output for the j plus one example. This graphic, this also for the j plus one example. However, when we look at this Beta I, write this Theta I is just one parameter that is the same across all examples. For all examples, I'm going to multiply by gamma i and then add beta, which means that this beta i, which is added at this node, It's also going to be added this node. So this is going to be the same exact data that comes from I, j plus one example to shift the activations. So there's also an upstream gradient, d L d y, j plus one that brought back propagates here to be beta i. And then remember we talked about how when you have a converge onto one parameter, other gradients, sad. So that's why DLD beta is going to be the sum of the losses across all of my example. These are, sorry, the sum across all of my examples of these gradients, DL, DYJ. Quick reminder letter I and j represents is going to be one unit in the network, meaning one unit in a layer. Meaning I is going to reference e.g. if I equals two is going to be the second artificial neuron. And then J or a trust my m training images. So if I have 100 C part-time images, j goes 1-100. Indexing does images. Yes. So Daniel is saying across the batch, is it the case that the same upstream gradient is obligated to pay the high? That is correct because that's going to just be the sum across all of my examples. Whereas when I back propagate, we're going to tell us to derive what the backdrop for the x j's the plank wholesome. And then just to be clear, moving forward, we don't have to worry about the price because there's a subscript eyes, because there's going to be the exact same graph for everything. But I'll defer it on this one slide. I said we're going to drop this off to apply for convenience. But indeed there is a Beta, a Gamma, there's an IBC be on everything. That's fine. Let's keep going on oh, actually, before I do that, any questions on this. So I'm told me to make sure that you all understand these because this has been, uh, there's been a barrier. If somebody asked you about this. Yeah, The question is, can I explain what task converge again? So basically, let me draw it in the following way. There's going to be a beta i parameter. Beta i parameter is always going to be added to gamma xy prime irrespective of which example RON, there's no j index. So this beta i is basically a parameter that sits here and it's the same data I added here to every single activation across my examples. And so the conversion paths are that beta i, which if there were 100 examples with branch out to all hundred of these graphs, is going to sum up the gradients DL DYJ from j equals one to j indexes the data instances with this index. Indexes which artificial neuron in our layer of yard. So we can just go ahead and set it for the rest of this class. For the rest of this lecture, I is going to equal to, so we're going to be looking at batch criminalizing. Just a second neuron in this layer. Split it up. All of our data is that we started off all of our data to every single word. So the question is, what is the difference then between the neurons? Because neurons in general have different weights, the upstream to the earlier observations will be different than one another. So because of the weight matrix has different weights for every different you're on the right. My question here. Here's the individual neuron with an array or just to, just to make sure that's clear. What is the batches of students question? So the Batch refers to if m2 equals 100, then I have a batch of 100 images. That means that I have 100 images of cats, dogs, airplanes, whatever. And for j equals one to 100 for each image, I'm going to pass that into the network, and that's going to cause x2 to have a different value. So u to the main activity of this second neuron is going to be the bean across my 100 examples, and that's the variance is gonna be the sample variance across those 100 examples are saying, then that means that each different batch will have a different view and a different Sigma squared? That's correct. Yes. Yes. Oh, sorry, I'm sorry, I misspoke there. So batch norm doesn't question, does dashboard happened after the non-linear activation? Nowhere. So today the answer is yes. People have done experience and experiments and they see that it's better to put back on after the acquisition box. In the homework, we're going to follow what IOP and said that he did in 2015, which pushes, which puts batch norm right before the non-linear activation function. Great. So Tim's question is, well, the weights and the biases will also change the mean and the variance of the data. So isn't this kind of redundant? The answer is that while in theory it is possible for the weights and the biases to have it set so that it learned mean zero variance one features. In practice, we know that the weights and the biases or just be updated by a gradient descent. So when do you can think of this? As? You can think of this is, you can think of this as a technique that biases or network towards variance 01. Because it's highly improbable that stochastic gradient descent would find that solution. We're going to see this comes up later on in an architecture to call residual network the price net. And that's another thing where even though in theory a neural network because of residual layer by adding it really helps to train the wall. Alright, so that's the backdrop to DLD data. The question is, why don't we learn the betas and gammas and then fix them as hyper parameters for future training. You couldn't do that. But it seems to be more constraint than if you just let the network to learn two betas and gammas. And so I would expect you to achieve much better performance if you let it in Canopy or the sound optimal, sub-optimal. Okay, I'm gonna move on for now. But if you have a question about this, please feel free to come see me during the bridge. So DYJ backpropagation these theta i's. And then because again, this beta I is the same beta i for all of my examples, then the gradient is gonna be the sum across all my examples of the beast DL DYJ is the gradient over here is also going to be a DL DYJ backpropagating through a plus sign and then we can back propagate out to gamma i. So let's do this in a different color. I'm back propagating out to TLD Gamma. And we know that when we back propagate through multiplication, you take the oxygen gradient and multiplied by the value on the other wire. The gradient here is gonna be my upstream gradient, which is DL DYJ. And the value on the opposing wider dispatch had j, right? So it's going to be DL DYJ times x hat j. That's where we get this term. And then again, because of Gamma I is the same gamma i across all my examples are going to be summed from j equals one to 100. These are all scalars. Yeah. So, yeah. You may have also wondered what the outside j on the left or the right, because they're scalars. It doesn't matter if you go on the left progress. Alright? That gives me this gradient over here. So this is the purple gradient, my gradient DLD beta. Now I'm going to backpropagate to this wire right here. I'll draw this one in bike loop. I'm going to put the gradient over here just so that this case is encoded. And this is going to be DL DYJ times what's on the other wire, which is gamma i. So this gradient here is DYJ gamma hard. I'm sorry, I'm going to drop the subscript sides, like I said. So the gradient of L with respect to d x hat j is going to be DL DYJ times Gamma. So this is my blue gradient. Any questions about the purple? Alright? And then after that, we're going to do one more back propagation step. So we're going to backpropagate and I'll do this in green to this right here. If I want to compute DL DHA, alright, I'm gonna take my upstream gradient, which is this blue one. I'm going to call this D L dx j. So this is going to be PL d x hat j times the value on the other wire, which is b to the k. So this is b j. So when you see me write it out right here, I have the x hat j, that's this term. Then b j here is going to be the value of this wire, which is gonna be one over the square root of sigma squared plus, alright, and so that's why this term here is equal to DJ. And I've just expanded it out. Any questions on those pretty Let's do one more actually here, I'm going to back propagate to DLD BJ also. So my two D, L, D DJ, I'm going to take my option gradient, which is dy dx that J, it's going to be DLT x hat j value on the other wire, which is here a j. Right? So those are radiuses that propagated all the way to these workers. Any questions on any of those gradients? The question is, do we care about the wire that's coming out from the J? Which wires that yes, we do care about these. We're gonna do them on the next slides. Are there questions on dissenting of these gradients? Next five years, next time. That is correct, yes. So we will get to that in later slides. So we will have to write a DUDX I turn. Okay. Other questions like that. Right? So if I understand rocks this question correctly, it says that theta i and gamma i are shared for all the J computational graphs. Xy is not exciting as a superscript j plus one and epsilon I j. So those are not shared. The question is, what is the proper order of are you talking about the terms that multiply? Right? Great. So the question is, here I put BJ on the right-hand side, but here VJ is on the left-hand side. And the answer is because they're scalars, you can put them on the left or the right equipment. Yeah, so the question is what if you were doing, was doing this for the matrix that you would have to get the ordering correct. I'm going to follow the denominator layout. But we won't do it with a matrix because it turns out that if you try to do this normalization with the matrix, you need to compute a matrix inverse across or parameters. That's very expensive computationally. So that's why we always break it down into just a scalar computations. All right, we're going to move on then. So we're going to start off then on this next slide with DLD BJ. So that was this gradient in green that we calculated on the previous slide, DLD BJ, That's right here. And this thing was the L dx j times a j. And a j is just x j minus mu. So that's why I write that. Djs x or x j minus mu times d L dx factor. Okay? Now we're going to finish the back propagation along this part of the graph. So the first thing that I'm going to do is I am going to backpropagate from here to here. Can someone tell me what that backpropagating gradient will be? Someone else other than Daniel. Welcome back today. I know that's perfect. Yeah. That's correct. So remember that when we backpropagate, right, to take our opportunity gradient times local gradient. So this operation here is saying that d j is equal to the local operations inverse of d j is equal to one over CJ, right? And therefore, if I take the vocal brilliant, I'm computing d b j C j. And this is equal to minus one over c j squared. What that means is by backpropagate here, it is going to eat both. The gradient is gonna be d L d b j times minus one Over the value of C j, this whole quantity squared. So I've written that expression out here. What is CJ equal to C j is equal to the square root of sigma squared plus epsilon. And if I go ahead and square that, I get a one over sigma squared plus epsilon, and then there's a minus sign here. Alright? So this minus sine one over sigma squared plus epsilon is my minus one over c j squared. Then this is DLD, DJ is this term. So this is my upstream gradient. This component here is my local gradient minus one over some cases there. Any questions? Alright, now we're going to backpropagate through the square root symbol. So, well the square root operations. So if I back propagate here, what do the exact same computation of a local gradient. So here the operation is C j equals square root of EJ. Alright? Therefore, D C j d e j equals 1/2 times the square root of this gradient here is gonna be my upstream gradient, which is my purple thing. Minus one over sigma square plus epsilon times this. And I'm going to further multiply that by 1/2 square root of e j 1/2 is over here. The square root of e j is equal to square root of sigma squared plus epsilon. And so that's why then she Min squared plus epsilon goes to sigma squared plus epsilon to the three-halves because I'm adding here a square root. That will be my gradient. Backpropagated to d L, d e, j, k. We're doing a bunch of stuff here. Any questions or anyone want me to explain any of these treatments? Can you raise your hand if you're following so far? Awesome, that's good. So let's go ahead and do one more thing. Which comes back to the student's question over here about backpropagating through. So they also knew the gradient here, DL DHA. Dha is what we computed over here. And it is equal to dy dx times dy j. So we know that this gradient DL DHA. And what I wanna do is I want to backpropagate to D L, D view. Alright. Do DHA. Let me write it out here. The LGA j was equal to d L d x hat j times DJ. I remember j is equal to 1/1 over the square root of sigma squared plus epsilon. All right? So DLD a J equals one over sigma squared over the square root of sigma squared plus not fun. That's this term here. Times dy over dx j. So the first thing that we're going to do is we're gonna take this gradient and back, propagate it to you. Let me do this in a color red. When it back propagates to view you as being subtracted from x j. You haven't minus sign. And so this expression here, which is equal to this gradient is also going to get a minus sign. Alright? So backpropagating DL DHA here gives this expression here. But this is not the complete derivative. Because new comes in somewhere else in this graph. And this is therefore incomplete. I see red. I need to read without the summation. Yes. So this ambition here though, will be the same argument as before. And so that's why I'm just going to be summation there. Right? Yeah. The question is sigma squared also will apply to every example. Jason will also have a sum j equals one to m. Yes. Yes. Okay, So there's one more thing that we're missing here, which is that when you look at the operation of batch norm, Sigma I squared is also a function of you are. Alright. So your eye is going to go through this function. I'm going to call it the sample variance function to tell me what the value of Sigma I squared is. So there's also going to be a function called sample variance. Sample variance. Compute Sigma I squared. And it has inputs x, j. As well as to do so because this view is the same, right? To get the total gradient DLD in you, I also have to backpropagate to this new and then sum those gradients together. So I need to also compute a plus d L d sigma squared and then a d Sigma squared d nu by my chain rule. Again, that's because sigma squared is also a function of. Alright, so to do that, I'm just going to write up the equation here. So the sample variance equation tells me that Sigma squared is equal to one over n. And we have a sum from j equals one to n. And we have an x, j minus mu squared. If we take d Sigma squared DMU, what we're doing is we're doing d Sigma squared DV view. And this is going to be taking the derivative of this expression with respect to mu. So I'm going to get the two to come down. So I'm going to have a two over m. And then sum from j equals one to n of x, j minus mu. And then I have to differentiate with respect to D, which is gonna give me just a minus one-half. So this is going to give me a minus sign over here. What that tells me then is that this gradient here is going to equal DLD sigma squared. And then for d sigma squared d new, I'm going to plug in this expression right here. So let me erase this. And I plug in d Sigma squared DU, which is going to be this times two over n. Sum from j equals one to n x j minus. And then that is my gradient. Questions here. Daniel's question is, isn't new, something computed as a result of the data? Meaning it is calculated from the data. It's not really a parameter. I'm not going to do gradient descent on it. So what do I have to calculate DLJ? And you constructed the student's point earlier, which is that in the computational graph, you depends on x j. We need to compute a DLT view so that we can back propagate it to x j, x j. We need the gradient for because that's going to give you the greatest upstream. Other questions here. Alright, so that's actually brings up a bit to get to more babies here. So we have all these gradients. Now, what we wanna do is we want to ultimately backpropagate to DLD acts. Like you're just answering for Daniel's question, influences u and Sigma. So I also have to know how to backpropagate the gradient DLD due to D L dx and then DLD Sigma squared dy over dx, which means I have to compute what DLD Sigma squared. So these are gradients, DLD a and DLD that we already know the gradient. There's a question here. For the DOD view. The customer, is there a reason that what the question is, is the reason for this summation the same as for the beta i's summation? Yes, it is, yeah. Because u is going to be the same view for every example. Alright? So now we need a backpropagate to d sigma squared. We know DLD EJ, we did that here, gradients. So let me write that in blue. We have DLD j, that's the gradient right here. And what we wanna do now is one of that property, that is sigma i squared. So this is just a plus sign. So the gradient passes through. And then for the same reason we typically keep talking about the Sigma iceberg serves every example j. So it's gonna be the sum of the DL ij's. And therefore, this is a backpropagated gradients for DLP sigma squared. This expression is exactly what we calculated on the prior slide in blue. And now overdoing here is something that across all of my examples, j equals one. All right, so DOD sigma squared is basically for free because we already knew this gradient calculated through a plus sign. Alright, now we come to our last thing, which is we need to get dx j. We need this gradient right here. So for this gradient, recall we had a DL, DHA that's in green over here. We're going to have a blue gradient, del d Sigma squared. We'll have a purple gradient, DL d u. And remember that all of these are function of x. So u is computed from X bar. The sample mean, sigma is computed from X by the sample covariance, or sorry, the sample periods. And so what we do here is we're going to write the chain rule. So first off, D L dx j is going to be a component from the green backpropagating gradient that's propagating through a summation. So it just comes through, That's the steel DHA. Then for the Sigma squared term, remember that there is the sample variance function that is affected by x JMU. So to backpropagate here, I needed to do DLD sigma squared times the local gradient or the sample variance is sigma squared x j. That because that's going to give you this term. And the last thing is the backpropagating front of you. So mu goes back through a sample mean function for which x is also an input. And so to backpropagate through this view, I'm going to have to calculate DLT view of times d Mu dx j. Alright? All of these are gradients that we've already computed or ones that you can see. So I'm going to just, I'm not going to compute d Sigma squared dx j, but please do that as practice on your own. That would be taking this expression right here and differentiate it with respect to x j. And then when you add all of these together, because these are all convergent pass on the state x j. So the law of total derivatives apply. Mp get the total gradient d L, dx j. Yeah, So you're saying you appears here and remember, you also has XJ affects this new shouldn't there be another cluster, the new term there isn't in this case because this GLP view term came from this slide and this slide DLT knew already took into account the effect of both of these pathways can do. So. We've already talked about gradients. So great, yeah, so Anna's question is, when I propagate, my back propagated through this view, I've got a minus sign. When I talked propagated through, backpropagated to the x j, I didn't have it. And that would just be because right here will have a j equals x, j minus mu. So because the minus sign is on the new, then that's why that minus sign come from the view, but not for the extra question. Any other questions on that? We have seen that should be right. Back with some great boxer. This pointing out that this deal dx j is for one example of j, right? And so what's going to happen when I back propagate to train the parameters of my networks. Well, in that back propagation code, you're going to have a dy, dx j. Let's say this is let's say that there are, I'm sorry. This is, this is a scalar, so this is an R. But let's say that we had 100 units. So we would have a big X being a, a matrix, which is the number of examples, which was little n by the number of artificial neurons in that layer. Let's call that 100. And then this would multiply a big dealt with you. So you would not sum them together. It would be concatenated as a batch, just like in homework number three. And you've got complicated. This is great. The question is, is this just be seen as an example of continued back propagation? Or do you need to know this for the exam ladder? So you need to be able to backpropagate through these types of operations as well. And there's good chance that there might be an exam question on that. Alright. Thank you, Tom. I said Tom, I said that they're going to do this example in discussion as well. So this is something where it went a bit fast for, you know, already the TAs are gonna go over it again and discussion. And you can ask some more questions here. Alright, let's go ahead and take a five-minute break and then we'll come back to continue on. All right. Yes, correct. All right. Sure. All right. Hi. We'll get back to it. So we're gonna follow the convention used by, oh sorry. Actually before we do that, last questions on any of the gradients computed or normalization. Alright? So the batch normalization layer, we are going to follow the convention of IOPS IgD, and place it before the non-linear activation functions. So we're going to have it w times h plus b. That's my linear layer in a neural network. And instead of passing back directly to ReLu, I'm going to first passage the batch norm and then Turabian. Alright, So in homework number three, you implemented these new layers. So in homework number four, you're also going to write a batch norm later. And that DashCon layer, you're going to write the forward and the backward pass. Those fees instead of just asked find Bailey, I find really find batch norm, ReLu atropine batch norm. Alright. Any questions? Alright? And so there's gonna be one more thing for homework number 42, more things that we talked about, which are gonna be dropped out and difficult to misers. And hopefully we may get to drop out today and then we'll do optimizers on Wednesday. All right? So this gets us into the topic of regularization. Regularization is, are things that all of you have heard of before and a fire machine learning class almost surely. So the first thing is that we're going to talk about are going to be a recap for many of you ever just going to go to them if they parked at the end, you also implemented some regularization. So like, like L2 regularization on your homeworks, right? Before even getting to that, if I asked you to define regularization, think about the definition without reading the slide here, I didn't ask permission. It's interesting to think how you would define it because this is a term that we throw around a lot. Regularization incorporates many penalties. Dataset augmentation. So the first thing that we're going to do is we're going to make a working definition of regularization for this class, which is set regularization is any modification we made to the learning algorithm that is intended to reduce this generalization error, but not its training error. In other words, if we have a validation and testing error, regularization is something that's going to make those things go. Alright. This is a definition taken from the deep-learning textbook, like the fellow which, which I wrap it in orange. Definition is pretty wide and it may incorporate things that you hadn't thought of a regularization. So one thing that incorporates is early stopping. Imagine we had a situation where on the x-axis we have number of training epochs and on the y-axis we have our loss function hour. We know that if you train with stochastic gradient descent, you're training loss in general will go down and down. But your validation loss might go down and then start to increase as you overfit. We talked about this before, right? So this will be our validation loss. The act of stopping your training at this point. And using the bottle at this epoch is a form of regularization because it's an intervention that makes her generalization error, your validation error better. Alright? So if it's talking early under this definition is an example of regularization. And then there's this really interesting discussion. You won't be tested on it. But in the deep learning book on pages to 42% to 45, Goodfellow actually gives a really cool insight that early stopping could be viewed as a form of weight regularization with you hold on the homeworks. Yes. I was saying in this setup, you would need to evaluate your validation loss on every inbox. And that's correct, but you could also evaluate it up for you, you know, ten bucks. But then you would enter your resolution would be stopping at the regions that are multiples of it. The question is, will the validation set? The different for all the epoch? Depends on how you set up your batches. If you're randomly sampling at every possible different batches, then it could be different. Actually started. Let me walk. In general. Yeah, we will just reserve a separate validation set which will be the same. That will be like my one fold, that's for validation and that'll be the same across all of the same validation set has to be used for that folded cross-validation. Alright, I'm gonna go quickly through the next few questions are several parameters because all of these already, I see them. So a parameter, norm penalties. And for the next few lectures, I'm going to use the letter J to represent loss instead of l. Just to be more consistent with the textbook, people will use J and L together. But for these phi j, remember that j is lost. What do you do when you have a parameter norm penalty is you have your original loss. And then to that you add some function of your parameters. Here we denoted this by omega. And one thing that you've implemented under homeworks is L2 regularization, right? So if w is a vector, then omega, which simply be the two norm of that. Vector squared. And on the homework, right, the w's are not vectors, but they're matrices and so on the homework, you all have done that this regularization term is one-half. And then we take the Frobenius norm of the matrix. And then you also know from the home birth and from how we define gradient so that you do D L, w or not, the L is going to be d sigma d omega. This is just going to be four. Because this thing is added to the loss function. This thing is absolute loss function and we want to make the loss function as far as possible. The effect this will have is to make our weights smaller. One interpretation of L2 regularization is what is called weight decay experiment that comes from. So let's say that I were to do L2 regularization. So I have my original loss J and I have added to it. We're just going to work in the case where w is a vector instead of a matrix. W is a vector. So I'm going to kind of analyze the norm of the vector squared, which is w transpose w. And I'm going to call this my new loss function, which is J Tilden. So if this term is in here, right? If there is no regularization, we know that when we update our parameters, we will do w is going to be my original w minus epsilon times the gradient of J with respect to. Remember that j is the original loss. J tilde is the one that adds parameter norm penalties. So what happens when we actually have regurgitation? The case where there is regularization. Now my update is going to be w. And my new loss function is g Tilda with this parameter norm penalties. This is W minus epsilon gradient with respect to w. J will be to do is we can go ahead and expand out this gradient with respect to J tilde. So if I take the gradients of both sides of this equation, that gives me the gradient with respect to w. That's going to equal the gradient of J with respect to w term over here. But then I also differentiate this term with respect to w and I guess being out there. So this equals an update row where it had w minus epsilon. And then we have w plus gradient of J with respect to w. I'm going from here to here, I've just substituted what I do simplification on this. This leads me to a new update rule. W is one minus x one alpha w minus epsilon gradient of J with respect to w. Now if you compare this weight update rule, which comes from parameter norm L2 regularization with the case where there's no regularization. The only difference is that this W changes to one minus epsilon alpha times. And that's why this is sometimes called interpretation of this L2 regularization is I were taking the gradient steps, but every iteration we're also decaying w by one minus epsilon. Yeah, So Daniel's question is our understandings of epsilon alpha where this could go the wrong way. And that's the case, then that's fine. I'll better set so large that the waste actually scale up. Almost surely your gradient descent isn't going to converge because you're always full. I will explain it. Instead, you would want to pick up, alright. I hope this slide, I wrote not tested. This is just if you've seen different types of regularization for Gaussian distributions and other classes. There are connections to this L2 weight regularization on the slide for those of you who want to participate, but you won't be tested on this. Alright? While we usually just regularize the norm of a squared are there for PBS norm. I want to point out that there are other things that you could do with this L2 regularization. So when I penalize just the norm of the weights squared, right? I want the weight to be smaller and smaller. What I can do is I can penalize. Norm of the weights minus some vector b. And what this does is instead of taking the waste was smaller and smaller, It's going to make the weights closer to this vector v. So if you have prior knowledge that the weights should have some value close to something that you could use L2 regularization to try to push it to that value. Then maybe in another setting, you might have some prior knowledge that two sets of weights should be close to each other and valleys, you can go ahead and just kinda lights are different. And L2 regularization will cause the W1 and W2 to be closer to each other. Alright. One more thing that you have likely seen your fire. Yeah. That's wonderful. Yes, a tomboy saying, you may have taken an optimization class before, but maybe convex optimization, but he didn't say optimization. And in constraint optimization and he would take your constraints if I didn't as an additional withdrawn came factor and that's exactly what this is as well. Did you haven't taken an optimization class and what I just said doesn't make any sense, you know? Okay? So there's one more form of regularization called L1 regularization, you've likely seen before. And all the changes with L1 regularization is instead of using the 2-norm, the square root of the sum of the value squared. We're using the one norm, which is the sum of the absolute value. And this has a really interesting empirical property, which is that if you apply the L1 norm to a set of weights w, a lot of them end up being zero. So this leads to so-called sparse weights. And I'll just give you a rough intuition for why this is, if we have a 2-norm, write a two norm is a quadratic. This is our 2-norm. And a one norm is the absolute value function. So the one norm here is our absolute value function. If I am changing my base, my 2-norm smaller, right? What that means is that I want to make the two norm as small as possible. You'll notice that as I get closer and closer to zero, the gradients become smaller and smaller because the parabola curves in. And so as it curves in these gradients here getting closer to zero. Whereas if I take the gradient of the one norm, they're always large and the same irrespective of if by waves are large or small. The 2-norm, as you get closer to zero. Gradients or rapid change less with the one norm. When you're away from zero, the gradient is always the same. And so empirically the speech to the one norm resulting in both sexes. Alright? So sometimes instead of the waste being sexist or you might want to parse representation, you might want your artificial neurons themselves to be sparse, has many of the occupations that are over. And so it's interesting to regularizing the weights. You could go ahead and just regularize the one norm of the activations. And this will lead to many of the occupations. Alright? So those are general things that you have seen. Some neural network specific regularization. And the first thing that we're going to talk about is dataset augmentation. It's really straightforward, really intuitive, but actually thinking about the performance. So let's say that you want to classify that this cat is a cabinet. Cat is still attack if I flip the image, if I crop the image, if I do some type of ripeness or distortion on the image, right? I could do a lens correction. I can even rotate the image if I wanted. All of these are images of cats. You'll notice that these images, right, the pixel values could be very different because if I cropped, this pixel, which was originally white in the original image, is now a dark color. So the pixel values can be very different because all of these need to be classified as cats. I can, in effect, make BI dataset on larger by taking different flips and Fox advantage. And using those additional data. More data helps to avoid over fitting. And therefore it can have a regularizing effect. Any questions there. Alright, so this is a paper from an architecture called Google neck. We're going to talk about what you will get is exactly, but in 2014, it wants the ImageNet competition, this one where you have to classify the images that we've talked about. And what I want you to do is just focus on this part of the table. And if you read the description here, what you'll see is that for Google Maps, they made up to 144 different replicas of the exact same cat and your input image by taking many different crops. And what I want you to notice is if you just don't do any dataset augmentation, that's one crop does a pretty good job. The error rate is 10%. But if you take these 144 columns, again, just taking the same image and copy it in different ways and they're flipping it, the error rate decreases to 7.89%. This is a 20% reduction on the base error and that is a significant performance improvement for all you did was take the image and crop it in different ways to get many more examples. So this has a really strong and positive affect. The ultimate question, right? Yeah, so the question is or statement rather is we have to be careful about the types of augmentations that we do. They have to be reasonable ones that lead to that first one to Bowlby would want the algorithms to be able to do so like e.g. if I didn't augmentation where like I blacked out a part of the image outright copy what I want to do it because I wanted to say that these are cats, but trumping is reasonable because each of these crops is still a specific order. I can just prompt that. Right. Yeah. Great. Question is do they do different prompts depending on what images quite maybe you want to call caps and cars in different ways. So this paper did not. They applied the same exact copying out with them. So all different types of images there. And that led to a great performance. You could conceivably think of a specific Crawford and classes. And I'm not sure if that would help or not. Student asks, what is the number of bottles in this case? We're talking about that in just a few slides, but that is the number of ensemble model. Alright, so this is for integers. We took this idea and we'll try to finish interfaces as well. So I'm going to show you a video from my grad school days where what we're doing, if you're trying to build robust brain machine interfaces. And we perform these experiments where there's a monkey who learns to control this cursor just by thinking about it. And the left side, it's same day is the state of New York decoder. Up until this point. What we realized is that over the course of time, so let me pause this video. Over the course of time. Neural data also fluctuate. Over the course of an experimental structure. Sometimes neurons you're recording from just disappear. Sometimes as a reward for doing this child to one of these get juice and the juice has sugar in it. So sometimes they become anchored by experiments is that activation of a neuron toiletries. And then sometimes the monkeys will become more drowsy and there'll be a decrease in global decrease in the neural activity that we perform dataset augmentation is that modeled these increases or decreases and also loss of neurons. And you also trained a neural network on the right that can take advantage of all of these augmentations. And you can see that even when the state of the art on the left, the RNN, which the neural network that has these augmentations, does our data set augmentations, oftentimes make sense. Many more settings and this is a figure from a paper. I'm going to skip it for now. Talking about his work, feel free to drop by my office hours. Type of augmentation is called labels to the game. So in the very first lectures of this class, at least one student who asked this question when we showed the ImageNet dataset. We have an image and we assign the label. Those tables are given by humans and sometimes with labels are not correct. And so what we could do is something called labels moving. So. In your homework right now for C4 ten, you develop what are so-called one-hot representations of the data. Which is a factor that is zeros for all the incorrect classes and one for the product, one where this is, you know, Airplane, car, cat, etc. The idea is you say, Okay, Are, they will be, aren't perfect. So instead of using this as my true label, instead what I'm going to do is I'm going to make my true Example. If I need to guarantee the following, I'm going to say instead of one for the correct class, that's going to be 0.9. Then I still have ten per cent left them. I probably distribution, I'm going to distribute that amongst all the other classes. So all the other classes will not take on a value like 0.01, 10.01, 10.011, all the way down to 0.011. So this is the distribution that sums up to one. But if we now use this in our cross-entropy loss, this is saying the correct classes cat with a probability of 0.9. It turns out that using these labels also has a regularizing effect. Alright, so within this table from a different paper, I want you to compare these two numbers. This is the error rate for a network called Inception V2. By the way, within two or three lectures, you'll understand what inception and Google that the end here is batch norm, right? So you understand what all of these terms mean in a few lectures. But if you just focus on inception B2 versus inception v2 with labels smoothing, you can see that there is a small bumper of 0.6%. Performance improvement. Regions depending on the particular class, give me one region versus another. So this time but maples ArcMap does affect the bathroom supplements. Thomas question is, how does this affect the back propagation through the softmax? Depending on why did you want to look at this image? Great. Yeah. So I'm always pointing out that when we did the softmax classifier, remember the numerator of the likelihood? It was just the exponentiated score for the correct class. So that wouldn't apply here because we no longer have a correct class. And so that's a softmax but has to be updated. Alright? You other things that are helpful. One is called multitask learning. It's because they did the following. Maybe if we were training at work to do one task for me, it would be really fun to, to that past. So instead, why don't we have a neural network, many related tasks, and I hope we identify features that are helping all of those tasks. So let me write that up here. Let's say that we have three different tasks. In this task, the goal of the neural network is to label, label the class. Every, right, so all of the purple ones would be labels for street. All of the red ones are labeled for people into the table. Alright? Another task that you could do is called instance decoding. And so this is to label all instances of a class. So in this case, we told the network for this example to label all instances of people. And so it goes and picks up all the people and in colors, but maybe it makes them. It also enables this bicycle as a person, but it's not. Alright. And then maybe another task that you may want to do. A task that we do frequently is a definite decoding. So tell me how far in the z-plane each pixel is. These are all things that we as humans are able to do, right? So it stands to reason that in our brain are developing a core set of features that allow me to do all of these tasks together. And so the idea of multitask learning is to say, let me take many tasks at bottleneck by saying I need to use a set of features given by a neural network that work. To do all three of these tasks at the same time. The way that I would do that is that each task will give me a loss. So maybe the first task could be L1. Second task is the L2, this task is L3. And then I was something together, L1 plus L2 plus the L3. And I can back propagate all the way to the start of the network. And that would then transfer features don't work on all three tasks and those features may be better explained on one task. For me. The question I believe the question was assign different weights to these different losses, right? Yes, you can. If some tests are more important than the others, maybe, I'll, maybe the first task is more important. You can give it a weight of 0.5 and maybe these are 0.25. So you have freedom as the experimenter to change. The question is, is data augmentation as subset of regularization techniques? Yes. Because remember here we're saying regularizations and anything that increases that makes my generalization. Question is, are there four numbers here? It depends on how you define a network, but there'll be neural network layers for the encoder. And then actually I have it drawn out here. So you might have three different tasks. And here this could be a neural network with many different layers. But then yeah, you would have separate neural networks with its own parameters that then branch out for each task. The question is, that's when the intuition of why did we sum these boxes together? Because if I want, if I didn't have L2 and L3 here, then my backup to get it lost with only changed the encoder or the earlier layers to find features that are good for just the first task I wanted. If I wanted to also learn features that are good for tasks 2.3, they have to be in the loss. For the other part of question, I didn't follow it entirely. Abandoned office hours. Alright, so another really important thing is called transfer learning. This is really useful in the setting where you don't have that much data. On a bunch of PhD thesis committees where students are trying to build neural networks that classify medical scans like MRI images, PET scans, CT scans to try to be coded into another prokaryote. Then here you're working with doctors who had very limited number of stems, but they don't have as many scandalously have images in ImageNet. And you know that if we don't have that much data, then numbers are very prone to overfitting. So how can you still build neural networks and the contexts where we have the middle? So the idea is there's something called transfer learning. Let me draw what this is like. Actually, let me first explain the intuition. The intuition is that we're trying to classify these images. When a Dr. looks at the medical image, right? That Dr. is using the same brain that he uses to look at a picture of a cat. So prior to assist would be that the images, Sorry, features in a network that are good for classifying that a cat, a cat or a plaintiff must also be useful for identifying features. An image that says, okay, because there's a malignant tumor, tumor. And this is, so the idea is that the first train a neural network on a large dataset. But the initiative, and then we fine tune it for this MRI or CT scan data set. So the idea is the following. We have the neural network and it has some number of layers. Let's say it has 157 layers. We're going to talk about this neural network in a few lectures called the ResNet with 157 layers. And then after that, remember at the end of every neural network is a softmax layer which comprises write a linear operation. And then that goes into my softmax function. That gives me a lawsuit. Softmax does get me a loss L. So what I can do is I can train this entire network on Commissioner for a lot of data. And then transfer learning is the following. To fine tune this network to be able to classify MRI images. What I'm going to do is I'm going to take that network trained on ImageNet. And I'm going to fix all of the parameters in my 157, right? And then what I'm going to do is for this new dataset where I have, let's call it MRI images and corresponding labels like a medical diagnosis. I'm only going to use those examples to retrain or fine-tune. The softmax classifier at. What this says is that each one to 8157, this is a neural network components that is going to identify the features that are good for classifying images. And I'm going to use those features which I can now get from MRI images, but MRI images through this exact same neural network. And then after that, I'm just going to tune the linear classifier at the output to try to reduce the loss like accuracy of diagnosis. And turns out that this works really well in many settings when you don't have that much data. And this allows you to translate these large neural networks to small data centers. Any questions they're saving of the features in that you mentioned that they just don't eat much different than the screaming of the features. And affects NRC, right? It says, are these networks learning is scale invariant features. Likely they are because the ImageNet dataset would be able to classify objects even if they are of different sizes. And in some ways point is, maybe that might not be the case for scans or CT scans. So if there are features in these Petr CT scans are different than the ones that Verde's for classification that in those cases in my transfer as possibly. Other questions. Okay, with that, we're going to get small sample methods and methods will lead us to truck out. So ensemble methods are another significant boost in performance without very much cognitive effort. And here's the intuition of ensemble methods. I could trim one neural network to do CFR ten classification like you've done in homework number three or will do in homework number three. But what if I have the following instead of one network, I have ten numbers. These ten networks make independent errors, right? If you have one network, if the error, you're dead in the water. But if you have ten networks and you show an image, maybe two of the networks make an error, but the other apes get it right. So what I can do is I can pull the knowledge of these ten networks. And because there may be a dependent errors, even though some networks may get some examples wrong, the majority will get them right. And that should boost performance. And that idea works. And so the idea of ensembling, It's for the same dataset to train multiple different models. Then you average the results together when the testing here. And this almost always increases the performance by a substantial amount. Intuition for how this work is. Work is probably something that you've seen in your very first statistics class. If we have k independent models, right? Then if I want to know the average error across those k models, so epsilon I is the error for one model. If they're independent, we know from statistics that if I were to compute the expected value of the error on model i and the error model j, because they're making independent errors, epsilon, epsilon j are independent and therefore, this simplifies the expectation of epsilon i times the expectation of Epsilon. If we go ahead and we carry forward this expansion, I'll get one over k squared and then an expectation of epsilon, I square it. That's applying. If you do out this, this quadratic, this is what it simplifies to. And therefore you see that epsilon I is the error, sorry, expected value of epsilon I square is the average error for one model. The average error when you average the ten models together will be decreased by 1/10. In this case it's the variance decreases by one over tangent to the average error with decreased by one over square root of two. So this is why it's good to average the outputs of a different models together. Now you may say, well, you know, these models are not ever fully independent. So if they're not independent, this is what the error is. And the worst-case scenario. Is that your models are totally correlated. Epsilon i equals epsilon j. In this case, ensembling doesn't help me because if you plug in epsilon I equals epsilon j equation, it just simplifies to expected value of epsilon I square. But as long as they are not perfectly correlated, It's obviously have some independence about guns, some degree of independence. You will reduce the error by averaging the output. I'm sorry, the question is, here we are multiplying the errors together. In the original expression. We're taking the average of the errors. And when you do this amplifying that, you get some of these terms here. Because she just has this first equality holds. I'll leave that as an exercise to you all, but it holds from, from simply this expansion here. And also in this proof, we assume that expected value of epsilon i equal zero. So all of these cross terms are going to equal zero. So when you, even though this is like an epsilon one plus epsilon, epsilon, epsilon 100. Yeah, you square it before taking the expectation. All right? Okay, So hopefully that tells us that it's a good idea to build any model outputs together. There are multiple ways that one could do ensembling. Outside of deep learning. One common way is called bagging. Bagging stands for bootstrap aggregation, and the idea is really simple. We start off with a dataset with an example. Let's say that we want to make k models. So from this example, from these examples, I'm going to make k different datasets. Each of these datasets, I'm going to draw N examples. With replacement. This is the context where I want k models. So they get up k models, I'm gonna make k different datasets will have lot of overlap. But because I'm drawing with replacement, these data sets are going to be different from each other. I can then train one model on each of these K datasets. They'll give me k different models and then I can average their outputs together. I can ensemble them together. And that also gives me hopefully the benefits of ensembling. Alright. This hasn't really done for neural networks because we're going to learn that some of these neural networks take a really long time to train. But just for now we're going to talk about a number called VGG net, and that took three weeks to train. So you don't want to train. Don't want to have to make leaps for every single model. Even if you run them in parallel, that would require a lot of computational effort. Another thing about neural networks is because they are generally higher capacity, more complex models that some simpler things that you might have learned in applied machine learning class. It turns out that even with the same exact data center and examples, if you just initialize neural networks differently because of loss of surface is. So, so I guess Com class. It turns out that these different initializations tend to lead to partially independent particles. So to get your k models or K neural networks from any examples, you could really just do k different initialization. And those models are often sufficient. We are different, right? But then again, neural networks take a long time to train. So usually you don't want to train different models. You want to do this some other way. One way that this paper from ice CLR 2017's decided to do it was to say, on my x-axis I have the training epochs and on my y-axis I have lost. And what I can do is I could just train this standard way. That's this blue curve right here. And I can wait many epochs to get a best model, right? That's one way that I could train. This paper said, well, we know that ensembling is really good. What if I just do this really wonky training where I take large steps and I get the error north to the minimum but to some respectable level. And then I basically turned back up the learning rate. So I have my local minima and I go to another local minima. Turn out by learning rate, gets to another local minima. This will give me six models where each of these six models on their own is probably worse than just one blue model at the bottom. But if I take the six models and average ensemble, their outfits together actually do better than this one model. Again because ensembling. Yes. So Tom weighs point is It's stopping early for each of these epochs. And then using this as one of my six models. The more standard way to do this type of ensembling or this bootstrap aggregation and neural networks with just one training class is something called dropout rest. 03:50 P.M. so on Wednesday we're going to start off by explaining exactly how dropped out for externalities. 
So a few things before we begin. First is a reminder, homework number three is due tonight. And please be sure to upload the code with the assignment as well. We've been receiving a few e-mails about late. We've been receiving a few e-mails about ME days and I just want to remind you, oh, you're welcome to use your late days but got to the foreign cars and you have three free rate. Alright, so there's no penalty for guessing. Here. We're going to upload homework number four today. And so that's an assignment that the TAs asked me to tell you all to start early. Because in the assignments, we will also ask you to do some hyperparameter optimization to achieve at least 60% accuracy, punk boy, yeah, 16% accuracy. So that optimization will pick something. So please be sure to start early on homework number four. In homework number four is going to be due Friday. Not this Friday, but the Friday after that. He taught boy, that's 17. Regarding the midterm exam, all of the past exams that we have ever given for this course, our ability to grow and learn already under modules. And for this year, the exam is back in person in prior years, but the pandemic, it was remote. So for the in-person exam, the exam is closed book and closed notes, but we will allow you to bring forward. She cheats shipping one standard 8.5 by 11 inch paper. You can write on both sides. So you'd have to pay tools sides, and you are allowed to put whatever you wanted to see shapes. So I have friends who would, you know, they would write cheat sheets and they would also like to put my lecture slides on there. And then after they made a cheat sheet, they would scan it at like 60% size and put it into a corner. And then this led them to tissues that are extremely small materials. You can pick whatever you want to print out, retain any questions about any of that until after. So this question is, what will the midterm cover up to? It'll cover up two covers up to material a week from nasa covers up to, and including Wednesday's lecture on February 15th. And therefore the midterm, you'll have a week after that to study further material quite. The question is, will the midterm review session have a Zoom? Often there'll be a Zoom Room and the TAs will record on Zoom how clever they won't be monitoring the chat so they won't answer your question builder. And then the TAs will put effect session. The question is, you need to write matrix could put the brakes on a PC. Now, there is a gradient that would come out of the matrix cookbook. We'll provide that gradient. Other questions. Yes, it's almost that'd be what remained. I'll provide simple, in fact, the gradient of its two norm or the ones that we derived in class by the backprop gradients. Stop pumping through a matrix vector multiply or a matrix matrix multiply. You see if I've got under teaches other questions on admin. The question is, will the exam cover homeworks for Biden? Will cover homework four. We're going to finish the material for homework number four today. Homework number five is on convolutional neural networks, and we have pushed the due date of homework number five past the midterm. So the lecture material up to an including next Wednesday, we'll have some convolutional neural networks, but it won't have all of it. That's pretty hard to do homework, number five, so it'll just be whatever we get up to understand and swipe this one. Are there any questions? Sorry, can you repeat? Your question is, what do you need to be prepared to use knowledge from the homework to solve questions on the midterm? Yes. Will there be any way to tell you? Yes. We may ask you to write code on the exam. It will be handwritten. Yeah. I got it. So Tom ways terrifies. We're not going to be testing on it'll just be on more likely concepts around how you'd like to add something. Other questions. Are you allowed calculators? You're allowed to bring calculators, but we typically design the exam so you never have to use a calculator. Other questions. Alright, I just have one last bullet point, which is a word of thanks. So we all know last factor didn't start off in ordinary way. And while I was up there, I can confess to you that my mind was totally flooded and I wasn't sure exactly what was going on and how to respond. And therefore, I wasn't even process what was said in the audience. And when I went back to look at the video, your support for me. And that meant a lot to me because I think that honestly that's probably what's most effective at helping the pranksters to eventually leave. So I just want to say, Well, where did they so all of you for your support. And it's just really appreciate that. And I think that that was what's effective to meet you all. Alright, we're gonna get back into material. So last lecture, we were continuing to talk about things that are tricks, they should be regularizations, I think helped to increase neural network performance. We talked about this concept of ensembling, which at a high level is to say, instead of training one model to do a task, bot, training them, or some number and average the results together. And the basic intuition is, for a given example, if those ten models make independent errors, if one model gets it wrong, then it's wrong. But if you have ten models and only two or three of them and get them wrong, wrong, but the other seven get it right and you average the results are probably gonna get it right. So as long as the models are making independent errors, if you ensemble them together, you will get better performance. And we should last lecture some tables that talked about it showed that compared tree. So this actually motivates a technique that is very frequently used in deep learning to regularizing network called troponin. So why dropped out? Well, we also talked last time how we could build them ensemble by essentially training ten separate neural networks. But train a neural network to be extraordinarily expensive so you don't want to devote the computational time discontent networks. Dropout is something that will basically allow us to get some of that regularization effects of ensembling. And while we won't cover it as much in this lecture, Ian Goodfellow in the deep learning textbook. Significant space to, to talk to you about how dropped out is approximating, ensembling through bagging. Let's talk about them with a diesel to drop that arm. So what happens in dropout is it starts off with a hyperparameter p. Alright. Let me first give the starting, let's say that we have 100 artificial neurons. What we do in dropped out is you randomly a given neuron and we set it equal to zero, meaning that we chop it out of the network. So if I had 100 neurons are, what I would do is I would draw 100 Bernoulli random variables with probability p. So this probability is essentially going to tell me how severe by drop-down. He told me the probability that I keep a neuron. So what I would do is I would draw 100 of these random variables. And so because Louis, they're going to be zero or one, it's going to be one with probability p, and it's going to be zero. With probability one minus p. I'm going to take, what I'm going to do is I'm going to take this mass m. I'm simply going to multiply the activations H of my 100 neurons by this mask pattern. So that goes around the activity where we drew a zero for that Bernoulli random variable. All right. Typical values of p are 0.8 for input units of the network. And then at t equals 0.5 might be a bit severe. I think in the homework we give you, P equals 0.6 for the hidden units would be that you would keep 60% of your units. And then on a given iteration of training, throw away or set to 0% confirm. The picture that you have in mind is that this is your neural network, shown here. This neural network has 13 artificial units. And what we would do then is for each of these artificial units, we would make a mask. The mask is also 13 dimensional. The mass is going to contain zeros and ones. And so I multiply this mask by my artificial units. And the ones that multiply is zero or effectively dropped out of the network. And so for a particular mask, this might be the network on some training iteration I is instantiated since all of these other units hoping trump card. Questions on the mechanism of what's happening here. The question is, does a massive change with every iteration? Yes. Over the course of training? You'll notice. So first off, if we have capital N units, you'll notice that there are two to the n possible configurations. Since every single unit can be in one of two states, there are dropped down. And so we're going to talk a bit more about why dropout it is a good idea. But at least the first idea that you can see is that if we're changing this mask, every iteration of training, what we want is that this neural network robust and works well in many different configurations to work out well when despite units or dropped out, it has to work out well if you go another subset of units or talk down on the bench for a given me. Any division can be wonderful. That's a good question. So what do we do in homework for author? Always putting out, I'm using the word iteration here and that could be vague because it took me back. They're doing a different mask on every mini-batch or you're doing a different task on everything. I'm going to guess that in homework number four, we tell you to do at every thought, but I'm not sure. Yeah. Yeah. So it's almost surely will say it will typically use the same probability for each layer. So usually you would want the probability to be quite high for the input units, meaning the first layer, because these are the ones that are capturing information about your images. So usually we might the first P for the first side, The Chief of the first layer, something like 0.8. And then these ones, you can be potentially more aggressive. People do hyperparameter. Values between 0.6 is 0.9 are typical. In the teal shirt. First, we define talk in general or the whole network with one exception. You might define it for the implementer separately, but generally it's to the floor. Yeah. Yeah, missing in this example, p is the probability that we keep a neuron. So here, P is the probability we keep a neuron. And we're gonna go with that convention for homework number four. But then the student mentioned that in the applied torques dropout layer can use a public IP dropping neurons. So instead of passing and p, you would ask them one minus p. So, but for the implementation, but we're going to do with p be the probability of getting it wrong. Does this also work? We basically have different mass for every single one. That's right. Yeah. It's asking if instead of doing dropout on the pocket into it on a batch, does that lead to better performance? I'm not sure if the answer doesn't, TAs or anyone else. It's something that we can talk about homework number according to my lab experiments, but I don't know if there's a consensus. One can be approximated by making the line. I see lots of people. Yeah, that's a rocket ship. And Tom y are both essentially stating that it may be too aggressive to change the dropout batch. Let's take these three participant aboard. Great. The question is dropping values, especially normalization. You've done it before or after batch normalization should be done after that. So the question is, when we multiply the activations by this mask, is it before or after? Oh, yeah, just send me the testing phase. You will get to that. You do something else in the testing phase. So we'll have that repeat just like wonderful. The question is, how does this affect the backpropagation? So when you do the backprop, you will need to backprop through multiplication by t. That talking through a simple application should be something you are all comfortable with. So you're not going to talk about the backprop lecture, but you'll implemented on homework number four. Alright, so that's trumped up. This is what dropout, so ******* good. So here I have my affine layer, wx plus b. It goes through Rava and that gives me they didn't do this and they are one. Each one. What I do is I just draw this mask of the same size as H1, where it's going to be one or zero with, it's gonna be one with probability p and zero with probability one minus. Alright, and then after that I just multiply each one by M1, and that will give me the drunk units in there. Alright, so this is a super simple operation to implementing code. It's just an additional two lines of code for each layer. Alright? So that's how the forward pass changes when we implement Dropout in one message. Remember that for each net, right? Yes. Oh, yeah, so and this other student questions, tomboy, staying here, I defined one mask for the entire network and then multiply this mass by all the units. And here I am doing it per layer. They're equivalent as long as the parameter p is the same. But this is how we wrote it out for layer in this example. Alright, so that's struggling code. Now we're gonna get back to this question, which is, What do we do during testing time, right? Because and training time we just kept generating all these random. Nasa gave us different configuration to the network. So what configuration do we use intestine. We're gonna give an intuitive argument for what they should be. And then the argument is that let's say that we had four units, H1, H2, H3, H4. And these are their weights and the rest are just gonna be W1, W2, W3, W4. And this leads to HR. So let's say that for this example, we're going to have a dropout parameter, p equals 0.5, right? P equals 0.5. Then for these four units, Let's say I do training iteration, one. Iteration what I dropped that mask on these four units. So the mask will be for the first four units, H1, H2, H3, H4, and it's gonna be 1010, right? Sorry. This means that HCl is going to equal ReLu of W1 x1 plus W3 X3, because those are the only units true mammal. It's the same for every example that alright, and then let's see, I'm training iteration to redraw a new mass m. And now this mask here is 0101. And so for training iteration to the network was using the second and the fourth unit. So the ReLu of W2 H2 plus W4 H4. Alright? Now let's say that in the test phase, we decided, let's not use any configure any particular configuration. Let's just use all the units. So let's say that in the test phase, or at least to start, we're going to abbreviate this. The test phase. Let's say that we just did h equals ReLu of W1, W2, H2 plus W3 X3. And then WE, for each four. If I were to do this, how would the statistics of H out in the test phase compared to the statistics of HR in different periods. Right? The skill will be different because in the training phase, we throw away half arguments. And so on average, this h out and the training phase will be two times less than if I were to use all of these units together. Alright? So what we then do is for the test phase, equalize the scale of h out. We're going to take all the units together and simply multiply it by the value p. And therefore, the scale of HM, the test phase where I use all of my units will be similar to the scale of HR in my training days. Another way of doing this is that over many iterations or training, the contribution of wi, HI. So the contribution of unit one wasn't always just like HI. W1, x1 only existed in the training iterations. So the contribution that WISMDI to each out was P times wi times HI. Alright. So this rule of scaling these values by P captures the fact that these units only contributed P, percentage, or proportion of the test phase. What we do is we don't use a particular configuration. We use all of the units, but then we scale down how much they contribute to factor p. Any questions? Yes, here is the p is the probability of other questions. Alright? So what that then means is that at least in this current iteration is written. Sorry. There's one more thing to say here, which is this row here of scaling the piece. You can think of as scaling these weights, W1, W2, W3, W4 by the value p. This is something called the wage scale inference rule. And although we haven't very intuitive argument for why it's reasonable, like we just discussed. This is from the good Philip book. There's not get any theoretical argument for the accuracy of this approximate inference rule in deep non-linear neural networks. But this is another one of those things. It seems to be the captain, but we don't have a theoretical reasoning. And then instead of scaling the weights in this class, we're going to scale the activation. That's typically how that dropout rates and productive. So instead of multiplying this P within the ReLu with these grapes, what we actually do in this class is we can feed the baby. When we put the p outside, we multiply the activations h out by a homeless person is it will be after the batch norm? That's correct. The question is, why is it putting it? Why is it that putting it outside the ray moves makes a difference? Because if this number was greater than zero, then the p would come through anyways. Yeah, that's, that's correct for the rating occupation or other activation. Alright. Okay, so because in the test phase, we take RHEL, we multiply it by P. This is what the testing phase looks like. So we have the testing phase where we're going to use all of our units. Do I find later than our ReLu? And then we multiply by p. We do the same thing for layer two, and then we have to type it here. This is W2. So that's this inference rule. Or from now, there's something that we don't like about this, which is we generally don't want to change the text of our neural network class. It would be great if we could just add dropped out. The training and not have to have like a different test versus test underscore dropped out that has this P here. So the way that we can get around this is we use something called inverted dropout, where we kick this p factor that would normally be in the test phase. And we move that key factor into the training phase by dividing each one by p. So p equals 0.5. If I take each one and I divide it by P, Then when p equals 0.5, that would be like multiplying these things by two, right? So now if I multiply these things by two, the scale-up H AB is the same as this h Tau. So inverted dropout says to get rid of the P and the test phase so that I don't have to modify the testing code. I'm going to in the forecast, simply divide my mask or my activation to call it b by P. Then my testing base, I don't have to have an HP and the test case. Any questions there? All right. So that is trumped up. The question is, is dropped out to the inputs the same as profit? It could be, but if you crop an image or do you like black and, or abuse or allow parts of the image? Then for fully connected network, it could be, I think that's an equivalence, but like for a convolutional neural network, that will be too many units in the next layer having a spatial zeros, which wouldn't be the case if you just draw on a random mask so that there are differences? I would talk ever at Foundation. Do you mean does our population change when we incorporated dropout layer? So because basically non computational mask, right, we're going to have an H1 and now it's going to multiply a mass m. And that's gonna give me, I'll call it H1, capital superscript D for dropout. So there's an additional multiplication operation here. And you're going to have to backpropagate through this multiplication operations. Question is yes, that's correct. Other questions? Questions. And so that's the verdict drop that. And this is how you're going to implement drop that on the homework. Alright? So you might have been the question, how is this a good idea? It's really funny because I'm Jeff and his group are the ones who discovered dropped out. And they were presented once in a talk because they said we did this pain. They have been by accident. I did this thing called dropout and they found that lead to increased to performance and 2%. And the audience are like, That doesn't sound right, but it's often a simple enough that the audience, some people tell anecdotes of how they were coding it on the fly. By the end of the talk, they saw a 2% increase also in their data. It's a very simple and quick way to get performance. But the fact that they tested it brings up this question that we all have, which is dropping out. So again, I'll refer you to the Goodfellow textbook for more details. But Goodfellow talks about how approximates bagging, where when we have dropped out, overdoing it. And every single key part, we are using a different network configuration. Alright? Our total task space, use all of the units. It's kinda like adding all those numbers together. And so, because you have many different configurations that must be good at predicting the output and can predict the output when you combine them all together and you're testing phase, you have an effect of ensemble as a result of dropout. Other reasons why he dropped out might be a good idea. You can think of dropout as regularizing each hidden unit to work well in many different contexts. The context being different masks. So a unit has to work well. If it's an active in two consecutive epochs, that would be two consecutive, two different architectures, effective architectures. I'm asking that you didn't have to worry about in both of those contexts. That's what number two. Number three is trumped up may cause units to encode redundant features. So maybe if you want to detect the cat, you need to get features like that. It has 20 years and the network has to learn that there's only one artificial neuron remembers that learned it might not be robust if the activations on that neuron are in any way altered as a result of some other factor in the input. And so when you have dropped out, many neurons have to encode these features, especially if their vital. And so these two redundancy in the network. One more thing to note is that when you run dropped out, typically you have to make them larger. So if you have a network with 100 neurons and it does well with, without Trump got to have good improvement with dropout. And we'd have to increase that to 100 units to something darker. Which also makes intuitive sense because when we dropped out or we're going to be throwing away a bunch of pounds. Any questions on any of these intuitions? Copying off some units. Dropout rates are set to zero. Tom was asking when you do drop that drops out. We all the ways that companies that needed or sexes there are. Tom was asking, does that have an effect of something similar to that L1 regularization? Which bases are always read. The answer is no because if it was always trumped up in gas, but because on some interval it dropped up, those will lead to great effect change but switched to be nonzero. We apply. I really only some delays. You can trump. I've just talked about layers if you want. You certainly see in the homework, we will put Trump on every layer. There are some layers, but he wanted to, particularly for companies and big fire information, regularized some boring display the chalk out. The question is, how would you determine which layer you should not do the drop that arm. Yeah, that's not something easy to know, a priori, which is why people just generally quite tired. Hi, The question is, are redundant features a good or a bad day? Redundant features can have a quote unquote con in that invaded more than expended to do the same computation. But if you can tolerate that, but it's generally a good thing because then you aren't reliant on just a few numbers. It's kind of like the idea about sampling, if you have any that perks that will look for this future appointment. Yeah, so even if one neuron, as is often the others, are there. Is it generally the case that we had better testing versus training accuracy? Let me do drop that. You mean like an absolute value of testing higher than the training node. Generally, the testing error will still be, the testing accuracy will generally be worse than the training still. But it'll reduce the gap. The training error will, the training accuracy will go down and testing accuracy both. So that's the regularizing. The question is is the drop-off I'm asked if he didn't each batch or is it saved over the entire talk? Actually, I'm going to just ask the TAs to look at homework number four. We were saying you bought because we think it might be a bit excessive for each batch. I haven't I haven't written dropout code in awhile. So the TAs will give us a firm answer in a minute. But the question is, if we do it for a whole, you talk with at the poem doesn't kind of definition for it. There'll always be epoch, which is that you pump is just one facet of your chain. I see. Right. Okay. So this to me is asking a question which is beyond the scope of this class. But if you're a deep RL context when you're turning from a replay buffer and you're not going to go over all the samples from the replay buffer, or you're going to sample for every iteration. I don't know what the best heuristic is, but you may want to keep a mask or a few, a few videos to saplings from the takeoff it, that would be my guess. Music, any last questions here on top up? Alright, so that concludes this lecture on importance of various regularizations, augmentations and industrialization is that improve the performance of neural networks. So remember initializations thought here is unfortunate. Regularizations. We have things like dropout, batch norm, and then did augmentations. We've talked about how we can just take different costs of images. And that's a way to get a 2% of Africa is considerable. Okay? So the TAs are correcting that. Crafting what I've been saying incorrectly, this vector, which is that the masks are drawn for bash, not premium fonts. So they are changed every single batch. All right? Okay, so in addition to these tricks for improving the performance of neural networks, you also know that there's one more component of our machine learning problem, which is the optimizer, and that's the cast of gradient descent. And are there particular ways we can improve this for deep learning? I'm going to say one thing at the top also, which is that even though all of these optimizers were developed to deep neural networks, they're also available outside of deep neural networks have just general machine learning algorithms. So that's gonna be the topic of the next set of slides. This is gonna be optimization for neural networks. And we're going to talk about ways that we modify stochastic gradient descent with momentum adapted, brilliant and adaptive moments. And this is something that probably a lot of you have heard of called the Adam optimizer. And so we'll talk about exactly what. We're also going to add these at a high level, introduce you to the concept of what's in order. Gradient descent methods are. Although they are rarely seen in neural networks for reasons that we're going to talk about later on. But if you would like to learn more about these than the optimization classes to 36, BMC will cover them. The associated reading with this topic are components of chapter eight. So what we know, going back a few weeks, we now know how to design neural network architectures for loss functions and the activation functions to choose. We know what the hyperparameters and cost or loss functions for these numbers should be right, for classification would be, we should be using the cross-entropy loss, which is the negative log likelihood of the softmax classifier. We know how to calculate gradients of this loss with respect to all the parameters in the neural network. That was the backpropagation of lecture. And then we talked about how to initialize the weights and regularized and networks and ways to improve the tray. That was the beginning of this lecture. And just like we were saying, now, we know how to optimize these networks with stochastic gradient descent. But can sarcastic gradient descent the improved and because the sector, the answer is yes. So let's get started. So this is the destination will be using for this factor, the cost function. This is just the loss, is what I'd usually written as L, but we will use the variable j as well in this vector. And then our parameters are theta. We know that then stochastic gradient descent or Mini-batch gradient descent, I'm using them interchangeably, is that we update theta by stepping in minus epsilon times the gradient direction. This is a slide just from a prior lecture, which is just recapping the differences in batch, mini-batch and Catholic algorithms. But remember in practice we always use the dash. We call that mini-batch gradient descent, stochastic gradient descent. Alright? So this again is what we know. We'll just do this because when we introduced the other ones, we're going to follow the same structure. So let's say that we have an initial parameter setting theta. We have the minibatch with ME examples, which means that we have, this is C4, ten images, x and m labels. Why do gradient descent? We compute the gradient of the loss function with respect to those parameters. Remember that's the softmax loss and grads function. You didn't hover number two as well as the backpropagation homework number three. And then after we get this gradient g, we update theta by just typing in minus epsilon or gradient direction. We know that the way this works is that we have some function. This function is like softmax, loss and grad, which you implemented in the code that I'm going to show here. These axes are the weights. Softmax, loss and grad is going to return to you the loss function and the gradients and then update the width. I'm just going to subtract off epsilon times, right? We're going to return to this function called Beall's function that we talked about in which the capsule gradient descent lecture. And then we're going to optimize it or we're going to update it with our new optimizers. So here's just a reminder. This is the video of what stochastic gradient descent looks like on this fault surface. So starting from initialization, this bottom right-hand corner, stochastic gradient descent will follow the gradient which is perpendicular to the contour lines and go to the optimum. And here is a video of that and Cassian gradient descent, but starting from another location. And you'll see that it's moving slowly. Let's talk about the first thing that we're going to do, which is momentum. So we talked about how the cost of gradient descent, we have to set the learning rate epsilon. And sometimes when epsilon is a bit on the larger side, we'll run into problems like this green descent curve where we get into a valley, we start to zigzag back and forth, right? So this zigzagging back and forth along this valley that researchers to think of an idea called momentum. And momentum works in the following way. Let's say that we weren't exactly. So we start out at a location Studying of our legs and our first gradient points in this direction, I'll call that G1. Then we get another gradient and that points in this direction, G2. Do a third gradient, G3 like this. And then maybe a G54 like this, right? So our average class grade is to get the overall direction that we're going in and cancel out these zigzags. Alright? How does it do that? It sets a new variable d. This is our momentum. It's initialized to zero. And then we have a hyperparameter called outlet. And it also is essentially going to be how much of a running average to do. This will become more clear to write it out. So let's go ahead and write out this momentum update step. So first we compute the gradient of g, and then what we do is we update this momentum vector. So let's start off at the initialization. We have that's at the very first time step to the substructure will be initialized to zero. Alright? At the second time step, we compute a gradient. And that gradient will be equal to g1. Alright? So that's my gradient at the first time step by dy is going to be alpha times the prior B, which is zero minus epsilon times by gradient, which is G1. So b1 is just going to equal minus epsilon G1. Then after that, we're going to calculate a V2. V2 is going to equal alpha and alpha. Let's say it's gonna be like 0.9 e.g. it's going to be equal to alpha d1 minus epsilon times the gradient at time step two, which is G2. And if I go ahead and expand what D1 is here, I'm gonna get that this is equal to minus alpha, epsilon one minus epsilon g2. Let's just do one more. We'll do v3. V3. I'm gonna get that this is equal to alpha v2 minus epsilon G3. This is equal to minus Epsilon. And I'm gonna do some simplification. This is minus epsilon times alpha squared plus alpha G2 plus g3. So basically what you see is that our momentum is going to be this effective running average of the past gradients. Gradients further in the past are weighted vest because alpha is a number less than one, so alpha squared will be smaller than our drew. The picture to have in mind then is that in our momentum step, what we will have this we're going to have, let's say that we're trying to step into direction before. So we're trying to compute the momentum for V4. V4 is going to have on D4 is going to be equal to minus epsilon alpha cubed plus alpha squared G2 plus alpha G3 plus t4. And what that means is that we're going to have a vector like this. This is alpha cubed G1. Then we're going to have a blue vector. This is going to be alpha squared G2 bit longer. And then we're going to have a third vector, this one here, this is alpha G3. Our fourth vector here, this is G4. And if I add them all together, the overall sum is going to be equal to this vector. Respect to your here is before. Alright? So that's how it works. Basically, we're averaging or has gradients and it gives me the direction in which the gradients don't cancel out. And that's why this is called momentum, because essentially we're capturing the momentum of us pulling down ago. So another picture to have in mind is that if we're zigzagging and gradients in this valley of loss function, the momentum is going to compute this red arrow over here. Alright? Any questions, sir? Alright, let me show you one more quantum number. This is a picture to have in mind with momentum. So momentum we introduced this new variable v. Remember v is the momentum vector and at every single point in time. So if we are at this point in parameter space, compute a gradient. And that gradient with a minus sign minus G is shown here. Ingredients or momentum step computes data updating according to V, where V is alpha v minus epsilon t, right? So V here is our blue vector, Alpha v here is in purple. Our new momentum is going to be alpha v minus epsilon j. So I take this minus epsilon g and just add it to tail with Alpha of p, that gives me this vector here. This is minus Epsilon g. So my overall step, the change in my parameters is going to be somewhere in-between the momentum I was in, the direction of the momentum Io is going in and the new gradient at that point in time. Any questions here? The question is, is this running average computed trust mini batches or inbox minibatches. Other questions? Exactly. Right. Yeah, Jenna is asking to clarify. This means that if we are having six, I can gradients. Momentum has the effect of damping out that zigzagging and just going in the direction where the gradients aren't consistent. That's correct. My time has other benefits we're going to talk about in a few slides. So close to one. Okay, great gadget question is, we are setting alpha equal to 0.99. What's the point of having it at all? Well, through training, we're going to run many, many, many batches, like oftentimes on the order of millions of iterations. And so maybe at the million iterations we still don't want to be influenced by the first one. We only want to be influenced by that collapsed 2000s. And so alpha controls that. So the question is, will momentum help us to get to the optimum faster? Or what was the second inconsiderate? Or is there a local minimum? That's a local minimum. That's a really good question. I'm gonna get to that in just a few slides. So let me write out what momentum looks like. This code I called momentum p because that's what they call an in physics. So let me just actually changed at all to be. So momentum is just an additional line of code to update your momentum vector. Then after you update your momentum vector u, I simply stopped in the direction of the momentum. So let's go ahead and see what that looks like. So We're gonna be looking at the same wall surface now and now we're going to include an optimizer that has momentum and the momentum is important. Alright, so the only difference between good oranges that we added momentum, you can see it does something kind of intuitive, right? So momentum in this loss surface is going to the left, right, that's where the gradients are first calculated. Momentum overseas relative to food because it still has that momentum of stepping down this surface. But then because that Alpha is 0.9 or something less than one, eventually corrects course because now it's getting the momentum of the more recent gradients, which point pop into the right towards that star. Let me show another example. This is momentum from the other initialization. And you can see, in this case, momentum actually goes to a different local optimum because it has so much momentum going up into the right. That's, it kept going in that direction. And by then the gradients, we're pushing it to the land. And so that's why you converge to a different local optima there. Alright, let's go ahead and take a five-minute break. When we come back. We'll talk about why momentum helps and then we'll talk about other, other piece optimizers. Yeah, so for each of these fumble, don't get older. Exactly. Yeah. Just put it off. Yes, yes. Yeah. Here's what's your question. Sorry. Hello. Excellent. Opacity or after you take your overall assessment. All right, everyone, back from the break, any questions on the momentum algorithm? Alright, so later on when we talked about convolutional neural networks, we are going to learn something really interesting, which is that when we talk about CNN, one condition you have competition. Almost all of them. Just use the Casio gradient descent with momentum. All right, why is this the case? Well, their acuity. Questions that I believe that momentum is really good at finding local minima that are good local minima for generalization for performance, right? Why is this? A few questions we have here. One is that momentum optima, that's actually a pretty poorly worded questions. So let me just draw what I actually mean. Let's say that this is our loss surface and these are our weights. And our loss surface looks like this. So when I see this momentum help with local optima, I mean, does momentum help us to avoid being trapped in Babylon? The answer is yes. So if I just do stochastic gradient descent with momentum and I start here, we know that we're going to take steps down this hill until we get here. And then at this point, our gradient disturb. So it's a custom gradient descent. And it's fair if instead I'm doing SGD plus momentum as I'm coming down this hill, right? I am keeping momentum. So when I get to this point, even though the gradient at that point is zero, update if not zero because I have the momentum of my prior vectors, which means that I'm going to keep updating my parameters by increasing W. And if I have enough momentum, I will actually escape this local minima. And therefore all continue to keep going down this hill over here. So if there is a relatively local, a local minima that is somewhat steep, then our momentum can carry us past that local optima, local minima, right? So then this leads to our second question. What kinds of local minima does momentum tend to? Think about that for 10 s and I'll ask someone to answer. Right? Can someone tell me the type of local minima? Really flat, shallow local minima. So basically, momentum is going to find these local minima where it stays flat for a while, because it just stays flat for a while. Then when I get to the local minima, even though I have momentums, right? Because it's shallow and talk for a while, my momentum eventually dies out. And then I said, if you think that about what it means to be a shallow local minimum. So it's shallow, we'll call this a shallow flat local minima. In contrast to, let's say another loss function has a local minimum that looks like this. And so this is a steep local minima, right? What do these mean? So, cheap local minima. Remembering that the x-axis is weight and the y-axis is lost. A steep local minima. It tells me that even if I'm over here, if we go my waist, just do it all. But I lost gonna get a lot of purse, right? So these are poorer local optima because they're more sensitive to the settings of the parameters w. Whereas Uranus, shallow flat local minima. This is saying I could change my waves over this entire range and my boss stay small business, the better local minima to be in, because I can tolerate noise added to achieve good performance. The question is, can you make an argument that deeper local minima is overfitting? I'll rephrase that and say yes in a positive way, which is that in general, shallow minima, better generalization. So it turns out that all the ImageNet winners, even though we're going to talk about RMS, Adam. And those are really by the settings that you want to be working in. A prison, setting up conditions where you have this really big dataset. And you have such a large loss surface because there are a number of parameters are in the millions. And pure basement found that using SGD plus momentum is what is best and is believed because it binds the shallow local minima. Any questions there? The question is, can I explain why the shallow optima to better generalization? So, um, there are more rigorous theoretical arguments for this. There was actually a QC to every side Cambridge. I think he defended his thesis on in 2018, He wrote a paper analyzing the Hessians, these local minima to try to make the argument that they generalize better. So there are theoretical arguments, but I'm just gonna give the intuition that one reason it might generalize better is because in a test set, the test set data is different. The loss function is also a function of your data. If you are in a region that is really shallow and flats, right? That tells me that if my waist we're to change a bit, I saw achieved good performance. So you can think of it as I have some tolerance within this large region. For there could be noise which could, which also is related in a way to noise and the activations. And so because I can tolerate so much changing of my weight and still hadn't been lost. It's more robust. It's a general right to the center. All right, So the question is, do we in general prefer this shallow flat local minima to the steep local minima. And the settings of image classification, which is neural networks. Yes. I see. Yes. Yeah, The question is, in general, again, in the setting of computer vision with neural networks, we prefer to find a shallow flats. The answer is yes. There are sort of two-dimension that there are these other algorithms where when you're looking to call you my search a bit more to your left and right to see if you can continue. At least for the networks that we talked about in class. I haven't seen that applied. But my guess is done in the setting of a flat local minima. Reply staying the same. Saddle points are all like they are in general, more fecund minima. Right? So how does that work over there? So Tom was question, is, does the momentum also help in the case of saddle points? Well, if there is a saddle point that there's still a direction where you could decrease for loss. And so right? Right. I have extra energy, right? Exactly, Yeah, So kind of the same team as we said here. Momentum will keep your gradients alive longer. Or the ketone Barger for longer. And so if you didn't want to get something to saddle point momentum, but it gives you a better shot of bacteria. Remember it stops only when it's in a flat valley where there's no other direction. Alright, so that's momentum. There is another thing called Nesterov momentum. Momentum is very related to momentum, but it's actually a bit better in performance. And in the study of convex optimization is actually better in some senses. Nesterov momentum books, very similar to momentum was the following. The classical momentum was to do that d is alpha v minus epsilon times the gradient at the current parameter setting theta. Okay? The only difference between classical momentum and Nesterov momentum is that instead of evaluating my gradient Theta by current parameter setting, I'm going to update, I'm going to start, I'm going to compute my gradients at the parameter setting plus the moment. Alright? This might be confusing, so I think it's best explained as to why this is a good idea. I picture. So you remember that in momentum, what we do is add our current parameters setting. We compute a gradient that's minus g, and we have our momentum direction. That's absolutely right. And then the momentum, the classical momentum step, can be interpreted as we're stepping into alpha v minus epsilon g. So that means I'm going to step into the direction of my momentum and then I'm going to sit, step in the direction of negative gradient computed from this point. All right? Nesterov momentum says, well, in momentum, the direction that was some good interaction. So why am I calculate the gradients over here? Why don't I take one more step along the right? Why don't I first step lumpy because that note, I know I'm going to stop there. Alpha B. Instead of calculating the gradient here, I calculate the gradients after I've taken my momentum and Nesterov momentum says, instead of calculating my gradient here, because I know alpha V brings me closer to my local minima. Stepping Alpha v right here. And then here I'm going to compute by gradients. And maybe here, instead of my gradient pointing straight to the right, which it wasn't a classical momentum. So my gradient points down to the right. Okay? So that's the intuition for any questions that you guys have had to back substitute. So maybe half the class on a question here. The question is, this could be a disadvantage in the setting where your momentum takes you to a point where the gradient doesn't. Essentially your momentum is, you're further away where you're reading this morning accurate? It couldn't be the case. But if you have a good momentum, momentum is CPU towards the local minimum, well, you should get a more accurate gradient. So in convex optimization, this is provably better. And empirically for neural networks, this is also better. So in the homework number cards you'll see in therapy that problem that tumbled out. I see the question is particularly talking about the example that we gave before. Where in this example the momentum takes us to a different path into this local minima. So that's true for this example. But remember that this is just a 2D loss surface. And in general, for the neural networks are going to be a much more complex surface. And so, and also that as we increase the number of parameters, we have exponentially minimas but poor loss. And so even if for a contrived example or a simple example like this, momentum takes us to a slightly worse local minima then this one over here. And the more complexity and grown-up books. Tom Wisconsin is in Nesterov momentum. The momentum calculation is exactly the same, right? It is exactly the same. The only thing that changes is the gradient instead of being evaluated and my current parameter setting theta is evaluated at the parameter setting, right? First step in my work, I first started my momentum direction. So I first go to this location and then I compute the gradient here. So the only thing that's different is this gradient calculation. The question is, is this moment is Nesterov momentum better than classical momentum in all cases? Though it won't be better in all cases, but it generally performance better. But I'm sure people could find edge cases where it doesn't do as well. Okay, great. The question is, when I say that Nesterov momentum does better, so I need that it converges to a better local minima. Where do I find that it converges faster? When I said better I met that they converge faster. All right, let me show you. Oh sorry. There's, there's one thing here, which is you may look at this expression. And if you're like me, you think I don't want to deal with this, right? Because how do I write this into my stochastic gradient descent algorithm? I just want to be able to take a gradient on my current parameter settings. So the way that we typically implemented is we do a change of variables into this coordinate system. Beta tilda. Beta tilda equals beta plus alpha times momentum. Alright? And then it's fairly straightforward to show that if you do this change of variables, this is what your gradient step turns into. And your gradients doesn't have this change of variables. You're taking the gradient of your data till the space with respect to wait until that. So you don't have to worry about this being where you need a first step in and take ingredient. But the change of variables, this is just a gradient computed out of parameters sudden. Alright, Just in the interest of time, I'm going to skip this. But if you can't figure this out, just come to my office hours and we'll go through it together. So that's how we implement momentum and Nesterov momentum in practice. We do this calculation of the new and the old according to these equations. And then the update is that datanew is our old theta plus d nu plus alpha, or momentum parameter times v minus the old. That's just a change of variables to allow us to compute this easily. So this is the code for Nesterov momentum. We get a gradient. We compute our new momentum p, and then we update the weights according to this equation, which is d plus alpha momentum minus momentum. That's this equation right here. So let me go ahead and show the video of what Nesterov momentum looks like. Nesterov momentum will be in green. In this video, you're gonna see from momentum, momentum, at least for this. Simple example, very similar paths. You can kinda see that Nesterov momentum degree course, course-correct a bit earlier. And that makes sense because remember it's computing the gradient after you take the momentum, sir. So it's getting gradients. After you take the momentum step. Up until about the gradients are pointing towards the right. So that makes sense that the green course corrects the earliest it goes to the right because it's getting bigger. Rod and the first initialization, the second initialization, Nesterov momentum look almost like if you go to the same local minima. All right. That's Nesterov momentum overfitting. Because he upon inspection, yes, and because they haven't maximum interruption. Alright. So that's momentum. Momentum is typically called the moments. We're going to see that when we get to add them, there's gonna be a first moment and second moment and talk about the second moment. The second moment is motivated from this idea of maybe we don't want a constant learning rate. Epsilon. Generally at the start of learning is greater than epsilon is bigger, right? But as we get closer and closer to a local minima, you want to take smaller steps. So maybe we should make epsilon smaller. There's one way that you could do this manually called annealing. Annealing is a scheduler. So what you do is, you said annealing rate, where after 1,000 epochs, e.g. you multiply down epsilon by a factor, maybe by a factor of ten. Or you can do manual and healing. They're basically whenever you have a tax on the loss function and you see a chat to, you, saved your training step. And then what you do is you start a new initialization from those parameters with epsilon scaled down by two are accurate. So these ways of either scheduling or manually making Epsilon smaller or cold and dealing animal, right? And it's kinda like another design choice that we have to make. But there's actually another way to adapt the learning rule through a relatively reasonable intuitions. And so this leads us to the first adaptive gradient method called added grad. This was from John duty in 2011. And the idea is as follows. I am going to change by epsilon in the following ways. Taken on a large step in a certain direction. Then I've probably done quite a bit of optimization. That direction. And that direction I want to make my step size is smaller. All right, So what we're going to do is we're going to scale down the gradients are still dumped. The Epsilon directions that have already taken a large tariff because I've already made progress in that direction. Alright, let's write this out. So let's say that we have unrolled network and it has n parameters. So if we have n parameters or GI, our gradients is an n-dimensional vector. And in this case, I'm going to use the subscripts of g to denote which parameter. So the subscript two adults no time date to know which element effector. So G1 here is the first gradient for the first parameter, G2 subgradient for the second parameter, all the way down to g n for the nth parameter, we're going to have a Hadamard product, G, G, right? All this is, is element-wise multiplication so that G had them are, G is going to equal one squared, J2 squared, all the way down to g n squared. And then an undergrad. What we're going to do is we're going to set a equals zero to start. And then we're going to accumulate these gradient squared. Alright? So if we've stepped, let's say along dimension one with a large gradient, g x1 squared is going to be really big. So one squared is also gonna be really vague. And now when I take my ultimate update on my parameters theta, I'm gonna divide by the square root of a. So in that first dimension vibrator too large because a is being, the first dimension, is going to scale down the step size, that first dimension. So let me write out what this a Hadamard product is really quickly. Just to be clear, this Hadamard product. So epsilon over square root of a plus new. The new is just to avoid division by zero or g. This equals epsilon v1 divided by square root of A1 plus mu. Then we'll have an epsilon G2 divided by the square root of A2 plus knew all the way down to the parameter. Alright? So again, quite large in G1, if one will be paid. Because a one is big and now it's in the denominator here. Then it's going to scale down my future steps in this direction because I'll have a large value here. Any questions there? Alright, let me show a video of how added grad does. So. That was just two lines of code, right? I have a slice to zero. I accumulate that a is equal to itself plus the gradient squared. And then I stepped in this direction of the gradient, where I divide by square root of a plus, plus a small number to avoid division by zero. Alright, so let's do the optimization with added grad, added that is going to be just gonna be here in red. And who, sorry, this is the wrong one. By the way, all of these are linked. And I, The link is in the lecture. So I said if you want to ever revisit these videos, or you could just go to this directory. So read it is going to be added grad. And what you'll see here is that the graph does something reasonable. It gets to the minimum. You'll notice one thing first off into those rights, sorry, I'll be to the left. More so than the other algorithms. Can someone tell me why that is added grad, step up into the left relative to the momentum and the vanilla stochastic gradient descent. All right, Perfect, yeah, so that was exactly right. If we started at this example, sorry, we started this parameter setting. Remember that we have two directions. Let's call this x-direction W1 and this y direction W2. So my first gradient points in this direction. This is the first gradient. So it's component and W2 is this big and it's going to put it in, w1 is much smaller. So my first step a lot more than I do for the left. And then remember what added grad does is this is the gradient for w one and this is grading for W2 because I stepped more in the W2 direction, right? The next iteration, A2 will be bigger than a WaterMe. So I'm going to step left, the W2 direction, I'm going to scale down movement in the upward direction. Which means that on the next iteration, I'm going to step more left than I do. And that's why I added red goes up into the left relative to these other optimizers. So intuitively, it's doing exactly what we told it to you. I want to make one more caveat about these videos. You'll notice that they're like these, these distinct changes in directions. This is because when I generate these videos, I actually only took the location of the parameters every ten iterations. So I'm skipping over like ten steps in between here and here. Ten steps in-between Karen, Karen. And so that's why it looks a bit more character because it sounds like the question is, are there theoretical explanations for why having Brad is better in certain situations or in general? So the scope of this class, but John Dewey developed at a grad in the context of convex optimization. So I'll refer you to his 2011 paper for the theoretical arguments, facts, the context of non-linear neural networks. It's just a good idea in terms of if we believe that intuition that if you've stepped in a direction than I did progress, then I can take smaller steps in that direction. Let me show the other video. So for the other initialization, which is again dump into the left grad, shown here in red. And you can see that I made very good progress toward minimum and, and generally perform better than stochastic gradient descent. All right. Any questions on the cooperation of undergrad? He does anyone see a problem with adding grad? Sir problem? Would that be correct? Yes, there's a problem with that. And that is exactly as you said, which is that added grad. And undergrad is only ever going to increase, right. So because a in the first mentioned equals A1 plus g1 had a marked G1. G1, G1 times itself, right? G12 squared. U1 square root is ever only positive. Because it's only non-negative. It could be zero. So it's gonna be zero or positive number. Because that is the case. A1 will either stay the same or if it only increases. What that means is that our learning rate in a certain direction is only ever, always going to get smaller. This can be bad because remember, our phosphorus is really non-linear. And therefore we might have a larger mountain. The first direction might not have been good stuff. But now, for the rest of the training, where constraints to take small steps in that direction because my initial gradients are so large. So we want to fix this by, instead of making this an accumulation of gradients, making this a running mean of brilliance, where it's running some ingredients only looking at the recent history instead of the entire history. And that algorithm is called RMS problem. So RMS prop is exactly like added grad. Now we're going to do this running sum, where a is gonna be attenuated by beta. Beta is 0-1, so it's like a value like 0.99. Alright? And you can see then in this way, alpha, sorry, alpha a can decrease, right? Because now is multiplied by beta. And so if we're taking small steps along direction one, right, then the direction one a is going to spike, is going to repeatedly get decreased by discovery of Beta and it can go down back to zero, e.g. so that's the RMS prop, very simple modification to add a grad. Let me show the video of what RMS prop looks like. So I believe pharmacy practice this one. So harmless puppies is gonna be in purple and it won't be as clear as this one, but we'll see it more clearly in the next video. And add a grad going the same direction. And then the red added grad takes smaller and smaller steps, go into the local minima. But then the purple RMS prop, which allows the age to decrease, will overtake it. It'll be more clear in the second one. So I take the second one again. Remember, purple is going to be RMS prop and reg is not a grad. Having Brad has a good read. But then you're going to see RMS prop overtaken. Right? And again, that's because that I'll play that one more time. So remember red tick, small entourage, that's everything. Any questions there? Tom Waits says, come we ask, are these level sets handcrafted or is it for a particular loss function? Yeah, So I generated these for something called function. So the OLS function is, is something that is a predefined function. So that's where we take our gradient with respect to. But it's just a simple teaching example, obviously much simpler than the neural network. The question is, can the values of a ever be negative? And the new RMS current formulation answer is no because a is twice as thereof and it's only added. The only Beta is also 0-1. So because g squared is always positive, we're only adding positive numbers. So it can never be negative. I'm sorry, say it again. Right? There's no square root issues because all the values of a argon. The question is, in the visualization that I showed, the student observed that I just read jumps out faster than purple and purple over patient. When I, when I did these, I also varied interests and try to make each one as good as possible, so I didn't have to get a larger learning rates I could further ahead. But, but that's why, that's why it starts with a bit further, right? The question is, to be clear, this is different than learning rescheduling. The answer is yes. So learning rescheduling, you just scheduled epsilon to decrease every number in the box. And that can be done with RMS prop. The question is, can I explain what data does it and how that can make things like negative, you said, right? Yeah. So I just want to be clear. They will never go negative. A will always be non-zero or larger. The intuition is as follows. Let's say that we have a 99. 99 is 99 was our sum of gradients on the timestamp. So here this axis is time. I know I'm overloading and vaccine indices. And this would be if there was no, if there is no RMS prop, it would be like J1 squared. Actually, sorry, No, never mind. What it does is it does a equals. So I say that beta was 0.99. So this is like saying a one-hundred equals 0.99 times 899 plus 0.01 times g hadn't, had them RGS g squared. So basically to our gradients, right? Sorry, to our second moments are A's are still accumulating gradient, right? We're still adding gradient to what are some of the squared gradients are. But let's say that in the first dimension of g, The gradient was equal to zero is the smallest. This beta then will decrease a in that first dimension from, let's say, that first imagined having just equal to ten, that'll be equal to 9.9. And if there are still small gradients and that will continually decrease the sum of the squared gradients and active mentioned until that'll allow stick. Right? Sorry, you were saying this is like talking to how much history of a, how many, sorry, how much history of the squared gradients sum up into a. That's correct. I don't want to put it between both G, so it's non-negative. Get exactly what we want to look at, like how big the gradient direction, okay? But then we undo the square root, will lead you the square root over here. Right? Right, rocks what this is asking, can I write this down? Because I wrote like divide by a vector and then Hadamard adapter. So I'll just refer back to the slide. This term here is equal to this factor here. Other questions. I see. The question is can we said beta equal to zero so that we only care about the green sides of this. You certainly couldn't do that, but I don't think it would perform well. Why? Because because it's gonna be noisy, right? Maybe you took a lot of steps along direction one, right? But then on the 100th iteration in direction one, your gradient is like one times ten to the minus ten, right? So you've took them a lot of more steps. But then the dimension, once you have such a small gradient, you divide by square root of one times ten to the minus ten is gonna be a huge number. Two is seven tickets. Two-step reaction. That's why you want to be accumulated gradients over the course of training rather than just have a single platform. Means now that adding a constant in front of our loss function will make a difference. The question is, because we're taking a Hadamard product of our great dance. Because adding a constant to our loss function to make it behave differently. It doesn't, because if we do plus C, but she's not a function of Theta, then this will equal zero. So the gradient, the gradient from the comfortable. God Daniel is pointing out that because we need to compute and keep this vector a and we needed at every time step, this is going to double the number of parameters and voltage gradient descent. That is correct. The question is, are there any ways to alleviate this? Not that I know of. Alright, so let's move on. That's the RMS prompt. Now, you might, do, you might have an idea which is, what if I combine harm as momentum, right? So I have a slide on this. You can combine arm as momentum. So I'm going to skip this because really our most helpless momentum is what is called Adam. So Adam is the adaptive optimizer. Adam is basically R and S plus momentum with a few wise corrections, we'll talk about what that means. So first it says adaptive moments. What are the moments? There's the first moment. Actually, I'll just do this on the next slide. There's a first moment. And the first moment is momentum. Because the first moment refers to the first one woman at G. So we're just taking values of g and averaging them. That's momentum. The second momentum, sorry, per second moment, refers to g squared. So the gradient normalization g squared is the second moment. And that is like adding grandma or the RMS. So all Adam does is it updates the first moment, which is computing momentum. And then updates the second moment, which is doing you're RMS prop on your gradient squared, the running average of your gradient squared. Then you take a gradient step where you're doing the normalization with the adaptive gradients in the direction of the momentum. So this is different from RMS prop in that, in that farm has taught here, had a G, right? So it's only scaling the epsilon. But now Adam takes a step in the momentum direction. So it's a combination of momentum and it got any questions there. Alright, so this is Adam. Adam has something else called the bias correction. So, Right, right. Tomboy saying y is g squared called the second moment. So Tommy was also saying in statistics, the second moment is the expected value of a variable, a random variable squared. So as G, a random variable here. And the answer is yes, you can be thought of as a random variable because the gradient is Boise and it depends on your particular data. So what we squared, just think of it as a second moment. Okay? The bias correction in Adam is, oh darn, I meant to write this is on the ruler and flies by your Dr. Jonathan. I have a link to just the Coursera video where Andrew talks about what this bias correction does. Essentially, all it does is, well, let's first look at the following. As t goes to infinity teaser number of iterations, right? Beta-1 and beta-2, which are numbers less than one, go to zero, right? So as t goes to infinity, detailed and a tilde equal DNA, so as t goes to infinity, you don't have to worry about despise corruption moments. When t is less than infinity. All these bias correction moments do. All these bias correction equations do, is they give you a more accurate measure of the knee, of the presence of elevated. So when you have build data calculating a mean, it doesn't really make sense because you don't have enough samples. And so the bias correction compensate for that lack of samples and both sensory scale up those values. And please take a look at the docent grew and learned. That hasn't been to the Coursera video. If you want to just watch someone actually do an example with numbers and show how this helps you to estimate the mean better. Absolutely iterations. Okay, so that's Adam. Adam has a few more lines of code, but it's still quite straightforward to implement. Then let me shoot a video of what an atom looks like. This. Alright, so it's really getting credit now, Adam is going to be in pink. And for this video, like Adam harmless copy all the local minima around the same time. In this video over here from the other initialization. We can see here, Adam starts to fit behind RMS prop in undergrad, but then over time, like RMS prop is going to surpass. All right, so that is Adam. And then you will all be implementing this in homework number four. All right. Any questions on that? Okay, well, in our remaining 6 min, then, I just want to talk about an intuition of what's happening order until R. And then I'll point you to a bunch of things if you're interested in learning more, but you won't be tested on. Disorder versus second-order methods. In this class, we're only going to be using for order methods. This is really to tell you that second-order methods exist. So why are they called first and second-order methods? Well, we do gradient descent, right? We're making a linear approximation, which is called a first order approximation because when we write out the Taylor series for only, we're only keeping the gradient term. So for a first-order method, we're approximating the line in red as J of Theta equals j evaluated at my current parameter setting theta t. Theta t is going to be the setting of theta right here. Then plus the first order or linear approximation of the loss function better. So that's just going to be my Taylor series expansion. Theta minus Theta t transpose times the gradient. This red equation corresponds to this red line. I remember that tells me if I change my data, how much I expect my lost it changes. So if I change my theta by this amount, right? That would be saying, so let's say that this is Theta t plus one. T plus one updated via gradient descent. Did at t plus one is going to eat for my data, my prior time step minus epsilon g. Right? Now if I go ahead and plug this into this equation, that will give me that J Theta t plus one equals j at Theta t plus this is going to be theta t plus one. And for that I'm going to plug in theta t minus one t, theta t minus epsilon g minus Beta t transpose gradient. Right? And then if you look at this expression, if theta t's cancel out this thing here, I wrote as g here, but a sick gradient, right? And so this thing simplifies to J of Theta t minus epsilon g transpose G. This term here is bigger than or equal to zero. So it's saying that with my linear approximation, I expect if I step in the direction of the negative gradient to decrease my loss by, by epsilon times the gradient square root essentially. Alright, we know that all in well from first-order gradients. Second-order methods now include also the second derivative, the optimum optimization. So let me first give the intuition of why you would ever want to do this. Let's say that I had to loss surfaces. So my first launch surface looks like this. It's nice and shallow. The second loss surface looked like this. So it's very steep. In these settings. You will want to take different size. So if you were at initial starting point here, right? If I want to get to the minimum in the first setting, I want to take a large step. If you think about why I want to take a large step is because this thing is shallow, right? It has really built curvature. And so because it has low curvature, I can take a relatively large step, whereas when something is steep, I need to take small steps because I'm articulate large step and point to start. Essentially added this local region of the loss. So here I needed to take small steps. What this tells me and what is the intuition of second-order method is. Therefore, is that if I were to make instead of a linear approximation, a quadratic approximation of my loss function. That quadratic approximation, remember the second derivative tells me the curvature. This quadratic approximation will be wider if it's shallow and narrower if it's steep. And what I can do is I can make this quadratic approximation and then the bottom of this parabola. And that'll be my new parameters. So in the case that we had a really flat loss surface by quadratic approximation, which I'll draw in purple at this point might look like this, right? So if I stuck to the bottom of this parabola, I'm going to make a large step. Whereas if I make it quadratic approximation here, my, my parabola might look like this. And so in this case I will take a very small step because my parabola is very narrow. Intuition of what a second-order method is. When we come back to lecture on Monday, next week, well, I finished talking about the second-order methods. 
Hello. As far as follows, homework number four is due this Friday. We announced last time this homework has a lot of coding and it's also optimization at the end. So please be sure to get an early start on it. I will be sending a midterm announcement later on through and learn and Yasser today, please read it carefully because there are a bunch of details that we have to get right with a class this large. And so first off, because we have, we had a fashion that only takes 420, but we have over 370 students enrolled in this class. We are going to ask some students to take the exam at one of the smaller classrooms and voice call, and we'll do that by last name. So we're going to send out an announcement with those details. In that way, students can be more spaced out of this room as well. Some logistics details. That'll all be in more detail in the email once we finalize those locations. Midterm logistics details that you are allowed for two chips, each piece of a 0.5 by 11 inch paper. And you can fill out both sides with whatever you want. And so you have eight total size. The midterm is going to cover material up to and including this Wednesday's lecture. And so it'll cover any food up to including convolutional neural networks, which we're going to start off with today. We're going to get to today. You can find past exams. I'll put it in there. And so we've uploaded all of the exams that we've got very different. But this practice question, you may bring a calculator to the exam even though the define the exam so that it can be done without a calculator. But you can't bring any other type of computing device to the exam. You can do the exam and pen or pencil because the TAs and I are going to be scanning your exams after after we complete them. And then lastly, if you are in the MS combine program, this announcement that was sent out, we'll have more details on this anyway and how we'll distribute the exam and then walks that asks you to upload it. Any questions on homework or midterm logistics. Alright, and then just a reminder that there's going to be a midterm exam review session this week. On Thursday is going to be 6-9 pm on young CS 50. I don't see us 50. The TAs will have a Zoom room open, but they won't be monitoring the chat and that's a recording of it will be posted. And we recommend that you at least attend to or watch that review session. Boxes as we strongly recommend. Alright, any questions? We will get back into material then, our last lecture, we talked about these different optimization techniques that we could use for neural networks. We talked about the idea of adaptive gradients at those two are combined into an optimizer called Adam, which many of you have already heard of. And it's a really great fit. The first-order, the classic gradient descent, that includes both the momentum and the second moments of the adaptive gradients. I wanted to just start at the beginning of the semester asking if there are any clarifications on any concepts from first-order optimization, last factor. Alright, so there we left off by sector we are. Once you give a high-level overview of what second-order optimization is and also talk about why it really isn't using neural networks. And so be recalled that you are finishing on just differentiating first order and second quarter gradient descent methods. In first-order methods, what we do is we make a linear approximation as some parameter setting theta t that's given by this equation, which is just the first-quarter Taylor expansion of the loss J Theta t. And if we were to do is to cast a gradient descent step That's first-order. Then all we do is be updated data by just stepping in minus epsilon times the gradient direction. And if you plug in this update step to estimate the loss of index. Setting up a theta. Your linear approximation is always going to estimate that your loss will decrease by some amount, epsilon times the gradient transpose. Right? And when you take a small step, this approximation is closer. But when you take a large step by step to this blue point right here, let me redraw this loss function. Just take it a bit more clear. Maybe the loss function actually looks like this. If we take a large gradient steps from theta t, theta t plus one, we are going to have expected that our gradients that are lost would have gone from this value here down by this linear approximation. And so this is my epsilon transpose Gy that I expect my loss to decrease by if I make a linear approximation with the first-quarter method. But if we were actually to step two, theta t plus one or loss will actually be higher than, alright? And that's because the linear approximation no longer holds when you take, it holds more poorly when you take a large step size. Any questions here? Alright, so we were saying, Well, isn't there more information that we could use to take a better step? Let us a second-order methods. So the idea with the smallest, we drew these pink curve last time. And we said, if we're in a relatively shallow, reasonable loss, that we want to take larger steps. Whereas if the loss is really steep, like this example over here, we want to take small steps, alright? And we can formalize this by saying, well, to know how shallow or how steep the loss surfaces, I can compute its curvature, the second derivative. And so that's why these are called second-order methods because they make the second derivative. So if the curvature is low, which means my second derivative is small, then we want to take a very large step in gradient descent. And at the second derivative was large, that means I have high curvature. I would want to take small steps. And so the idea behind the second-order method is to start off at your parameter setting theta. Then you make a quadratic approximation. Where again, because the quadratic approximation includes the second derivative, it estimates the curvature at dislocation. And then after you make this quadratic approximation, the way that you stopped is you just stepped to the bottom of the parabola. So if we were at the speed of t, I will make this quadratic approximation turn red. And then my step would be to say I'm going to start to the bottom of the parabola. And that would be how I update from beta T over here to Theta t plus one. T plus one would be over here. So you can see if the curvature is shallow, then my parabola is going to be wider. And if my parabola is wider, when I stepped to the bottom of the parabola, I'm going to take a bigger step. But if I pluck summation here in purple is a very narrow parabola with my curvature. When I stepped to the bottom of the parabola, I'm going to end up taking a very small. That's the intuition of a second-order method. Yeah, perfect. That's a great question. Which is the question is, is there a hyperparameter in the second-order method? There is. Because the hyperparameter for Vicky descent first-order was epsilon. That tells me how big of a struct to take. But in this case, the step size is dictated by the width of the parabola to the bottom. Alright, so that's the intuition of a second-order method. We have here. A derivation will go through the derivation because it will remind you of some things that we've done before. We're going to talk about why we end up not using these methods too much in deep learning. So in going for the second-order optimization, we take this first-order Taylor expansion, which only includes the first-order derivative, and we now make it a sucked into order Taylor expansion. So now we did the second derivative term and the second derivative. So this is the same as from the prior slide. The second derivative term will include this Hessian matrix. This is this matrix that we define what we're talking about gradients. This is the multivariate analogue of the second derivative, right? So if you just take a scalar quantity with respect to a scalar, possibly the derivative twice In order derivative, the multivariate case, there are many dimensions to take best second-order derivative. And so this question is this generalization of the second-order derivative? We usually write it as this notation where we're doing our nabla twice. I need the second derivative. And you'll recall is just matrix where along the diagonals is derivative of loss with respect to Beta one. But the second derivative, the 22 element, would be d squared j theta. With respect to d, beta two squared, et cetera. And then the off-diagonals. I don't have that much straight, so I'm just going to stop sharing. This would be a dy squared J of theta with respect to d theta one, d theta two. And this term here would be d squared J of theta with respect to d theta two, d theta one, and so on and so forth. So this is the Hessian matrix. Again, the multivariate generalisation of the second derivative. So this is the second order Taylor expansion. And so this equation here is this red parabola, my aquatic, my quadratic approximation of the loss function around where my current parameter setting. So if I want to do a step to the bottom of the parabola, what I have to do is I have to take my parabola, set the derivative equal to zero and then solve for theta. Don't want to take our parabola equation right here. We're going to set the derivative equal to zero and then solve for the Theta that minimizes this quadratic approximation. Any questions? Alright, so for this, we're going to use two gradients that you all know already. The first is if we have gradient with respect to Theta, theta transpose G, that's just going to be equal to g, The gradient. And then we also have this one. If we take the gradient with respect to theta of theta transpose h, This is going to be that H plus H transpose Theta. That's the gradient that we had derived from. Bye. Okay. Great. Wonderful. Yeah, so the question is, in this Taylor, Taylor series expansion, we would have a theta minus theta naught times the gradients. And then for the second order term, we would have a theta minus theta naught squared times the second derivative. So where's the squared here? So it actually exist because we have a theta minus theta naught transpose. And then this is the other one, theta minus Theta. But then you have to do the order of operations correctly because they're matrices and vectors. So these two are my theta minus theta dot squared. Sorry, can you repeat the question? Yes. Thank you. So a student is just clarifying this parentheses here. Each of parentheses, beta minus theta naught is not a function. H is multiplying this vector theta minus theta naught. So this is a matrix vector multiplication not assumption page. That's a great point. I've, I've never I've never received that question before, so thanks for clarifying that. Alright. Okay, so let's go ahead and do this gradient. So we have j theta naught. We're going to differentiate this with respect, to, differentiate this function with respect to Theta. This has no theta, so it goes away. This thing here is, I'm just going to call this g for shorthand. This thing is equal to Theta transpose g minus theta naught transpose G. If I expand out this multiplication. So Theta transpose G, I differentiate with respect to Theta, I get HE out. And so that's the G here. This being doesn't have a Theta, so its derivative is around. Last one here is this gradient. And so here, if we differentiate with respect to theta, then we're going to get a one-half parentheses, H plus H transpose instead of minus beta naught. H and H transpose for the hush gender is a symmetric matrix. So H plus H transpose will give me two H. That is going to cancel this one-half h of Theta minus Theta naught. So when we take this gradient and set it equal to zero, we get this equation. And now to solve for what the Theta is that minimize this equation. You get this beta naught minus the inverse Hessian times. Right? So this is essentially the Hutchins in the first-order, right? First order approximation for gradient descent, we would have data is updated as theta minus epsilon times the gradient. And then the second-order, we replaced that epsilon with this inverse hashing. So theta is theta minus H inverse gradient. Question. The question is, can I explain what happens to the second term of j theta, this term? Just want why would I differentiate this? I get just this is not the question. Oh, this term. So this term just goes to zero because this is J evaluated at a particular thetas are up. But I'm taking the gradient with respect to theta because I want to know where I'm going to write. This forms can also be pretty intuitive, right? Remember, we said that what's going to happen is if by curvatures fall or sorry, if I lost surfaces shallow, right? If it's wide, which means my curvature is small, I'm going to take a larger step. Alright? Curvature being small means the second derivative is small, which means a heterogenous small. But here I'm inverting the hashing, my curvatures wall. When I take one over that, I'm going to get a big number and that's going to lead to a large staff data. Whereas if my curvature is high and h is height, one over that is put to get really small number. And this is exactly what we want. It measures the curvature using the Hessian. It takes a step in each direction based on the questions that I kept getting. Rocks. Just question is, is it always going to be guaranteed that the Hessian is not invertible. So no, so the Hutcheon will be estimated from samples. If we have enough samples, in general, it will be invertible. However, it may not be a well conditioned matrix, which means that it might have small singular values in some directions. Those cases, there are ways to condition e.g. by kind of like an aldehyde to make this invertible. So the DLP assume that they even brighter. Right? So, sorry. So the Hessian will not be a 4D tensor because it's still a scalar with respect to a matrix. Oh yeah, you're right, sorry. It's adult. Yeah, you're right. So rock to this, pointing out that this fashion, if theta is r matrix, will be a 4D tensor. Because at first, the first derivative will be a matrix and the second derivative will be the derivative of a matrix with respect to the matrix which would be affording texture. That's correct. Yeah, because if theta where r matrix ingredients would also be matrices, so there's be a 4D tensor type of 2D matrix which will give you a 2D matrix. Sorry. Yeah, Jake is asking you about this step here, which when I take the gradient of if I were to just differentiate, I'm just going to use, I'm going to use x instead of theta. I do x transpose H x equals H plus H transpose. But then in this case, the Hessian is a symmetric matrix because of the way it's structured. So if I differentiate a scalar with respect to theta one, theta two is equal to if I take that loss function and differentiate with respect to theta two and then theta one. So because it's symmetric than these two simplify to equal to h x. And then this two cancels out this one. Great, Yeah. Other questions. Alright, so this is called Newton's method. And this seems like intuitively, it does a wonderful thing and we don't have to do all this optimization of epsilon, although again, Adam, Adam and Adam and RMS possible by our lives there. So the question that you may have is, why don't we always use Newton's method, right? But think about it for 10 s and I want to see if anyone has ideas for why we don't use Newton's method in practice for neural networks. Wonderful said. Calculating the cash-in and computationally inverting it is unbelievably expensive. That's correct. So let's look at this in three points. The first is memory. So it is computationally expensive to store the hash here when we have large networks. So storing the Hessian is expensive. Let's say that we have a neural network with 1 million parameters. We're going to talk today about seeing answer. We're going to see our CNS sometimes have hundreds of millions of parameters. Let's just say we have. N equals ten to the six. So 1 million parameters in our neural network. Let's say that each number was floating point single precision. So four bytes per number. If you compute how much memory it requires to store the gradients, we need to store 1 million numbers each four bytes, and that comes out to 3.8 mb. If you want to store the hessian, you need to store 1 million by million matrix. And you do the math on how much memory that requires. This is 3.6 tb. So first-off sorting and hashing, even a very, very large number is prohibitively expensive. Second, inverting the hashin is expensive. So inverting the hashin, we know that matrix inversion is order n cubed. I said you can imagine this is going to get slower. Let's quickly, much more quickly I certain number of parameters increases, right? And then an empirical result That's relates to all of these is that to estimate the Hessian or approximate it well, typically requires a very large batch. Right? That makes intuitive sense because we have so many numbers did you get an estimate? So we need a lot of examples to be able to estimate those numbers. Well, for all of these reasons. Nazi second-order method to use. Even though that, even though they're very great idea, All right. Any questions here? Right, great. So this question is actually getting us to the next slides, which are supposedly is, what if someone came along a way for us to estimate h minus one in a very short amount of time to be end up using it. And the answer is yes. So the following slides are not tested. This is really just for your information and I want to point you to ECE 36, B, and C If you want to learn about these things. But because it has CNS, so prohibitively, essentially, there are going to be ways that people have developed to be able to do Newton's method called Quasi Newton methods. And so BFGS. Okay, actually that's less than I would've thought. Since all sides never asked me how many have heard of. So BFGS is named after these researchers who found a recursion to approximate the inverse Hessian. And because it's a recursion and it can be written as a bunch of outer products. We actually know how recursion for the inverse Hessian, so we never have to also inverted. And so this is one method that can be used to still do the order dispatch with an estimate of the Hessian from this recursion. Then there are other techniques like limited LBFGS, which, which limits this recursion. And then there are also other methods that you might hear like conjugate gradient methods that tried to do as so-called free optimization. Essentially trying to approximate second order optimization without ever having to compute the Hessian or storage. So those are all beyond the scope of the class, will never be tested on that. But I just want to point them in case you're interested in looking at this further. And again, ECE to 36, the entity will go over these in more detail. Okay, great. On one says it's 236. See, that goes over these B does not. Okay. Any other questions on second-order methods? Great. The question is, for a second-order method, will it be problematic when there is a saddle point? Problematic in the sense that you may not find the direction. It's a second. Yeah, so true exactly at that subtle point. And there's no noise in your estimates, then you would just stop there. Other questions. Alright. So that's the other optimization. Sorry, no team working off medication. There's something called the fundamental problem of deep learning. And we're going to talk about it more and more as we start to talk about practical convolutional neural networks as well as recurrent neural networks. But essentially this fundamental problem of deep learning is that we have stacked many layers for neural network. And we know that eventually there's a loss function. And let's say there are, say, 20 layers, right? We know that we have to backpropagate through these 20 layers. And we have already seen examples, e.g. in the initialization where. Maybe these weight matrices are relatively large. And after you're back propagating across 20 of these weight matrices. If those weight matrices are large, after 20 repeated multiplications, your gradients could explode. Or if the interest rates are small, they could vanish. Alright? If your gradients explode or banish. That means that for your update at the first weight matrix DLD w1, either at they explode, w1 is one step up in some crazy direction, nobody will occur. And if DLD W1 is zero, then it doesn't learn it all because the W1, alright, so the fundamental problem of deep learning is making sure that when you get gradients they haven't exploited or famished. Here's a really straightforward way that we can avoid exploding gradients and it's called gradient clipping. So here's the idea. We do is every iteration of gradient descent, we take the gradient and we look at the norm. The norm if it exceeds some value, some hyper-parameter, I'm going to call flip. Then what we want to do is we're just going to scale down a gradient so that is no longer bigger than, Alright, so basically if our gradient look like this, but clip and the magnitude of this is say 100 But clip was equal to ten. We were to scale down our gradient so that it only looks like, alright, so when we do gradient clipping, all we do is we check to see if gradient is that this magnitude is larger than clipped. If it is we up to, we update the gradients polar. So we take our gradient which is bigger than clipped, we normalize it to be unit vector length, and then we multiply it by. So this will scale down the gradients and practice. This actually helps quite a bit with exploding gradients. All right? Any questions there? Yes. The question is, what's the difference between clipping and reducing the step size? So the clearest way to see the difference is that these gradients, remember our backpropagated. If you reduce the step size, you're just taking a smaller step for every single parameter. But when we backpropagate it, the gradient, if it was clipped over here, will be smaller than what I've got propagates to the prior step. Alright? So instead of having everything would be large and then try to typically small step where there'll be creating a balanced, maybe these didn't explode, and these did explode. If I tip it every step in nothingness. The question is, when we have a vanishing gradient, doesn't that actually mean since DL DW goes to zero that we reached optimum? Not necessarily. So it'll be a local optimum, but it can be a very poor local optimum. And so we'll see sometimes we have vanishing gradients just because of, you know, like a poor initial position like the waste or too small. The question is, is gradient clipping done when we are updating the weight vectors or when we are calculating DL DW. I believe it's typically done when calculating DL DW view. So if you put this gradient, I believe you would use it for backpropagation. Can you check me on that? Just to be sure? Yeah. The question is, is there something like batch norm is something similar for regularizing the gradient norm? Not that I know of. Something like batch norm would help us to get a deeper. So it has non-explicit but some intrinsic effect on the gradient norm. Sorry about that. Great. Thank you. That's because there's plenty out of the city's planning I wrote greater than or equal to. This thing still has a normal credit. Yeah, the question is, can I go over and exploding gradients and what that looks like? Yes, Let me pull up the fire lecture. Actually. I'm going to use this example from when we did initialization. And there was this case where we had a ten layer neural network and initialize the weights to be too large. And so when We initialize the weights to be too large. We saw that the activations exploded. And this is just an example of an exploding gradients because the activations have gone to, let's call this 120 million in value. When I do my backpropagation. Didn't, never mind, I thought I had this slide, but when you do a backpropagation, you're starting off with the softmax scores. And then to backpropagated to the next weight, you need to multiply it by the activations transposed. So there's an example where it then your gradients are gonna be like 100 million. And Mr. Bass arm, I didn't know anything. You're not going to stop and tell her the whole question of the questions. I'll explain the gradients, right? Is that right? Yes, it'll help to some extent because I'm a range of activations, a reasonable range. So to avoid, like the exploding gradient is used to answer your question. I've said it isn't a panacea. So we're going to learn that for networks like we can only train them up to the order 15 layers, maybe 18 layers. Because after that, you shouldn't have that one. But even if they did have cash, they wouldn't shrink. So we'll have to think of other ways to avoid these exploding and vanishing gradients. Alright? So those are challenges to be aware of. Vanishing gradients is just when gradients go to dances around. And they're going to see this as a really problem when we have recurrent neural networks and we're going to actually architectural solutions to address this. Alright? That is it for optimization. Any last questions on order? Adam, Momentum, Nesterov, momentum. Alright, we're gonna get into convolutional neural networks. Convolutional neural networks are kind to this modern revolution and keep learning and they are one of the workhorses of the cell we're going to talk about. Architecture is motivated from how it works, what the architecture is. Never going to do case studies, ImageNet winners. So look at how we set the various hyperparameters of these compositional numbers. So this lecture will correspond to chapter nine and a deep-learning textbook. Alright? Just like we said in lecture one, this is going to be our motivating example where we're looking at this ImageNet competition. And on the y-axis here, we have the top five error rate. Remember that there are 1,000 image classes instead of C4 time where you have ten and you get to take five guesses. And if anyone of those five and you get it correct. And so that's what top five error rate is. And the competition started in 2010 and it was in 2012, where you can see the error drops by a significant margin. And this architecture here, AlexNet, only convolutional neural network in the competition that year and did everything by almost 10%. So from here on out, all of these architectures, VGG and add CF night you will not rest, are all convolutional neural networks that optimize architectures hyperparameters to eventually get the error down in 2015 to 3.57. And this is actually superhuman performance. So a few grad students at Stanford one weekend came in and they tried to do the tasks themselves with taking a guess at the top five labels. And they were at 5%, right? So by 2015 with this resume, it actually achieved superhuman performance on image classification. Alright. We're going to first talk about the inspiration for the architecture of a CNN is also CNNs have been around since the 1990s. So the first number even going to look at is this one in 1998 called Linux from younger. And in addition to this, I forget if I might have cut it for this year, but there's this architecture from a researcher named Fukushima called the NeoChord neutron. This was in the 1980s. And this has many similarities to CNN's as well. So you can even say CAMs really started in the 1980s that this NeoChord controller, alright, so you should look at this picture and not know what is going on at all. We're going to unpack that entirely. But we're first going to talk about where This architecture came from. And then again, we'll talk about exactly what the architecture is. I came from biological inspiration, particularly some of the seminal experiments, experiments done by Hubel and Wiesel in the 1960s, got them the Nobel Prize. Where it looked at how neurons in the visual or animals responding to stimuli like moving borrowers. And Hubel and Wiesel's work, as well as lubricant, other neuroscientists in the 70s like Tony motions, led to several conclusions about the digital system. And these are the three things. The first is bad. B1. B1 just means primary visual cortex is the first part of their cerebral cortex that processes images from the outside world. E1 has a retinotopic map. I'll explain what that is on the next slide. Is that the one has broadly two clusters of cells. One is called a simple cell. And these simple cells can be approximated by a linear model, followed by thresholding operation. And then the other is that D1 is composed of these complex cells that respond to that, that are invariant in the position of the feature. Alright, so these three principles are going to be less R-CNN architecture. Okay, so first off, you want has a retina topic. Now, what does this mean? Basically, it means that if I'm looking at somewhere, right, things that are close to each other in my field of view are also closely represented together in my tray. So this circle is my field of view and so it's breaking up the space I'm booking or into, into these different regions. So 11 is like the students out there, 90 students to my top-left. Maybe one, maybe three. Here is tomboy and one hears rocks it right here at the bottom. Alright? So Conway and rocks ships are sitting next to each other, each other in space. If you were to open up my brand and record from my visual cortex neurons, you look for the neurons that represent tomboy and rupture. They would also be close to each other. They're actually, this is, I should have said 3.4, because the brain also has this property where things in the right visual field or in the left hemisphere of my brain, largely these in the left visual field or in the right hemisphere of my brain. So if Tom way was three interactions with four, they are representing close to each other. Also in 3.4, goals have been drawn represents any questions there on topic now. Alright, so that's, that's another topic. Yeah. Okay. Then they're really simple cells in the visual cortex. And so when you tried to model the activity of these biological neurons, the researchers would do is that they would model them, but basically vocalize linear filters. Basically just have to focus on linear filters here. These linear filters can be or can be implemented by an operation called convolution, which we're going to talk about. But convolution sum is a linear operator and any linear operator can be written also as an affine transformation. So basically the simple cells are modeled by either convolutions are matrix-vector multiplies, whichever one is easier for you to think of now, followed by some thresholding operation. And a thresholding operations says, if you're above that value, report the value. If your report is there. So this is our greatly. Alright. Then CNN's also the visual cortex has these complex cells that encoded in variance. So P2 features. And the way that the early Shannon's tried to model this is through something called a pooling layer, and we'll talk some more detail about that later on. But this pooling layer is supposed to help to represent some of the data. Just tell them variant arises in or is observed in complex cells. Any questions on any of these three principles? The question is, what was the inspiration for using linear layers for this wholesales? So when the researchers recorded the simple cells, they sent me in their experiments, what they would do is they would e.g. take a bar and show it to a cat and move the, move the bar around. And they would report and listening on these neurons that are moved around where simple cells Keynes is, they want to say, Okay, how does this neuron, the cat visual cortex, respond to this bar moving? So if your bar moving is x and the caps neuron is why you need to find some relationship between x and y. And what they found is that many of them could actually just be described by a linear relationship wx followed by a threshold. So I'll write that as thresh WX. And now it looks a lot like bailey W. Great session bear either x, y or the XYZ coordinates of the butter on it. Also. When we talk about how these neurons are representing the bar, this is deeper to the coordinate velocity refers to all different types of kinematic variables are actually also in today's orientation. Sometimes if the bar is tilted by 25 degrees and it leaves, the neuron responds, but it's tilted by -45 degrees, it doesn't respond. Alright? So given the inspiration of this CNN from system, one might ask, how far did this analogy there? So this is from an ancient neuroscience perspective in 2014 from Daniel Simons, who's now a professor at Stanford and gender Carlo at MIT. Where did these really interesting experiments where they showed monkeys natural images recorded from all different parts of the visual stream. So D1 is at primary visual cortex, is also a V2, a secondary visual cortex of before and there are further down the line. And then after that there's a part of the brain called the inferotemporal cortex, or IT, it should be, but you need to know is that for these monkeys, they recorded from areas like V1, V4 and IT. And so they have now the responses of neurons in these areas as the monkey saw these natural images. Then they trained a convolutional neural network to process these images as well. And they found this really interesting results. Which is that if you look at early layers of the convolutional neural network, they resemble B1. And if you look at later layers of a convolutional neural network, they looked pusher than they look closer to inferotemporal cortex. So maybe there is some analogy between the CNS, the visual system, and that early layers. Cnn put more likely to early layers of the visual processing pipeline in the brain and later a bigger area. But these analogies break down when you think about that a bit further. So just like regular neural control, that perhaps they're going to be a significant limitations to biological analogies. So here are some examples. Our brain, the neurons in our brain. They had feedback before me, current circuits, whereas CNN's we're going to see are another type of feed for neural network at layer one, layer two, layer three. There are just differences about the tasks that we do. So our brain is doing a lot more than just the misclassification rate, but it's so much information about an image that a neural network number of arms. And, and want to see that the pooling layer that we add to the CNS is also not really a great way to incorporate and variance. And so putting these limitations enough to hesitate to me to see an edge, e.g. to study exactly the visual processing pipeline. But nevertheless, the CNN square initially inspired from the visual system. Any questions there? Alright, let's go ahead then and motivate why we want to see an n instead of a fully connected neural network which you all have been selected on homework number 3.4. So the motivation we're going to start with is actually one of number of parameters. So if we consider C4 ten images, we know that these are 32 by 32 by three tensors that have 30, 72 values. Alright? And so if I were to have my input image, right, it's this vector that is 30, 72. And then that's going to be fully connected to a hidden layer. I just look at one neuron in the hidden layer. That neuron is going to have 3,072 connections or waves to Mike inputs. Every single neuron therefore has to have 3,072 weeks. You'll notice though that the CFR ten images are really small. So if we're working with an image that's more, battery, the images bigger, like if it's a 200 by 200 image, right? 200, 200 pixels. And then there's an RGB says 200 times, 200 times three, then every single neuron would have 120,000 parameters. So I soon as you get on the order of 1,000 million parameter neural. And so as your inputs grow larger, this network soon gets out of hand in terms of the number of parameters, because there's so many parameters, we'll also know that network is going to pick from to overfit. Any questions? All right. Just taking a look at the time was baffling. Take our five-minute break. When we come back, we'll talk about the CNN architecture question, but it gives a better approximation. That's ability to challenge you to tell us. Yeah. Like okay, now I need to know. What is that? Maybe it's just a very natural one to the power of the test. Remember exactly. I think that's probably fine to go back to the house movie or some kind of proxy for how long. But the other thing to think about what's exactly happening? Yeah. That's good. Yeah, definitely. Definitely. Okay. I will all right. All right. Everyone. I will get back to one of the parties, Earth younger with you. Alright, got a complication operation is done accommodation before. Okay. That's over half the class. The convolution operation is given by this integral in the continuous form, in the discrete-time form is given by this sum. And you may look at this. It's totally unintuitive the first time. Haven't seen it before. You have no idea what's going on in this operation, but it's actually very straightforward. So the convolution operation is, is this operation versus going to do an example. And then after that, you see that example. You'll likely understand how conversation works. I'll be happy to answer any questions. Because many of us are coupled reaction has a very particular meaning. It is this integral or this summation for discrete-time signals. And colloquially we know that the intuition for combinations, right, is that they take a signal, you flip it, and then interacting, and then you do multiplication and so convolution, colloquially called the drag. It turns out that in neural networks, we don't do the flip-flop, we just do the drag. But for those of you who are mathematically inclined and you'll know that it's not actually called carbocation is called cross-correlation. So the convolution code and click operation we're going to do in your own networks is going to be our convolution operation except we aren't going to flip the signal. So this minus sign, plus sign, we're just going to drag and that's called cross correlation. And this operation is not commutative. E.g. we know a couple of weeks you get the cost correlation is not. So that aside, but it's still going to call this operation Convolution because that's what everyone calls this operation. Even though it's suddenly a bit of a misnomer. All right. Okay, so what is convolution? So let's say that we had two signals. In this case, we're going to consider matrices that we want to come. Let's call this signal or this matrix a, and we'll call it this one here. If I wanted to call these two, what I do is I take my signal B and superimpose it on a, starting at the top left corner. So what I'm going to do is I'm going to take this two-by-two matrix B. I'm going to put it over here. So that overlaps with top two-by-two of my matrix. Let me make the slip speed a and B. What I do in combination is ai pointwise multiply all of the overlaps together. So I do one times w plus two times x plus four times y plus five times z. Alright? I just add those numbers together and that's gonna give me a single number. So I'm going to multiply point-wise and then add. And that's going to give me a single number. And this number will be one times w plus two times x plus four times y plus five times z. Any questions on that first part that I did? Okay, Then for cognition or we then do is be tracked. Okay, so we first drag along a row and then after that alone and columns. So I'm going to take this two-by-two matrix, I'm going to drag it over to the right. And so if I drag it went over to the right. Now it is going to be overlapping. This two-by-two matrix. I'm gonna do the same thing. I'm going to point wise, I'm going to element-wise multiply two by W, three by x, y by y six pi z. And that's gonna give me one number. I'm going to add them all together. So together, this would give me another number, which is two w plus three x plus five, y plus 60. Okay? I've been dragging this two-by-two matrix to the right. Hit the end. I can no longer drag it to the right. So now I'm gonna go back to the left-hand side and bring it one down. So now I'm going to move this matrix and put it over here. Okay? Again, do my element-wise multiply and add handout, give me four w plus five x plus seven, y plus z. And then I'm going to drag it over to the right by one. And I'll put it over here. And if I do my operation again, I'll get a five W plus six, x plus eight, y plus nine. So convolution, which is given by the star operator of a matrix a and matrix B, is going to result in this drag operation, where at the end I'm gonna be left with this two-by-two matrix. And these are the values. Of all the numbers up this two-by-two. Any questions? Alright, that's super important to understand. So this is the first time you're seeing convolution. It is really that simple. The clarified even a standard font combination because we have to talk about the theoretical results, but the operation itself, it's simply taking one signal, dragging it over the other. And every single point will apply all the values together and then just sum them. Questions. Alright, so that's the convolution operation. This is just a slide where we just did the prior example. Okay? So there's something else in Coalition, which is that for this class, we'll only be doing what are so-called valid convolution. So what a valid convolution means is that my input signal, this was our matrix a, and then my filter, that's fine. It should be for me to be able to get a value b and a has to perfectly overlap. So e.g. a. Convolution that would not be valid is if I put B over here in this corner. This case, my filter be overlaps with a only in one place. And for convolutional neural networks, we would not do this convolution. Alright? So this essentially tells us that whenever we do a convolution filter being has to basically have a corresponding value and a, for me to do that multiplication bar. And for this reason, whenever we do a so-called valid convolution, my output is always going to be smaller than my input. And the amounts that it will decrease in size by is given by this equation, w minus w f plus one, where W is the width of a. So this here is W, This here is height h. Here is the filter WF, and this is the height of the filter feature. So whenever I do valid combinations, my output is going to be guaranteed to be this size. Any questions there? Because we will be performing other constellation after this one. I was expecting that question. So the question is, this is a bad thing because the output is always going to be shrinking. And so we cascade many combinations. What's eventually our representation of the network go to a negligible size. And the answer to that is that if you just didn't valid convolutions, yes, it would become smaller and smaller and that's bad. But what we're going to do is we're going to intentionally pattern input with zeros that the size could stay similar. What's called zero-padding. We'll see them in just a few slides. The question is, can we do something like decomposition, making which dimensions? You're saying can put a three-by-three after deconvolution is tonight, but I'm not trying to accomplish operation you're referring to. Okay. Yeah. So I'm not sure what operation that would be. The clearest way would be to make your input bigger by zeros. We'll see that in just a few slides. Any questions here? Okay, Then we're gonna get to the convolutional layer. So I'm going to consider that we have to see for ten image, right? So the Sigfox can image is going to be a height 32 with 32.3. And what we're going to do is in our convolutional layer, we're going to have so-called small filters. So these are my parameters. Filter. So I get to learn the values in this column is no filter, right? This is same thing as what we had before, where our filter had these values, w, x, y, z. These are the values that I will get to learn so that these convolution operations be good teachers that can classify, see far well. Okay, So here's the first thing about this filter. Depth of this filter D is always going to match the depth of whatever we're filtering against. Alright. That's just going to simplify our lives so much. What that means is that if this filter Um, this filter will have a depth of three. And then like, let's say the height was a five-by-five, right? So, so be a five by five by three filter first. We did a convolution in 2D. What does it look like in 3D? Well, because we matched the depth of the input and filter. When I take this filter and I start at the top left-hand side, I only do compensation by dragging it to the right. And then after it's done, drank it down, then dragging to the right and the next row. The next row because its depths are matched, I never drag it and as you just mentioned, okay, so what happens is that because the filter and the input, what I will do for my convolutional layer is I will start to filter in the top-left as we have here. Now. This is going to be a five by five by three filter and my input. And just like for the matrix example, we're going to take all of the pixels, are all the values, multiply them together, and then add them up. And that gives me one number, which is this number over here. Okay? Any questions there? Okay? And then what we do with convolution as we start to drag this filter to the right. So I'm dragging, I'm dragging, maybe I end up at this location. Again, I multiply all of the values here together and then I sum them together to get one number. And so at this location, that would give me this number. After that, I drag it I continued to drag it to the right until it's in dislocation. This is like the second to last location, so that would give me number here. There will be one more drag where I drag it to the very edge right here. And that would give me this number over here. After that, always the right, move it down one row. When I move it down one row, it becomes over here. So my filter has been dragged down one row. I do my pointwise and multiplies and then I add them together. Now give me this value over here. Okay? So if I convolve my 3D tensor image with this filter here with matched up. The result is going to be a matrix. The matrix dimensions will be that equation that we saw on the slide is going to be a width of w minus wi plus one and a height of h minus one plus one. The question is, does the output at the same depth is no. So in this case, the color tunnels where this depth of which there are three of them, right? But then my output is just one matrix. So this matrix has used together information across my three color channels. That's really important to realize. Any questions. You happy having to follow the spine. Awesome. Okay, we can move on. This is the output of my input image with one filter. So in a convolutional neural network, what we do is for every single layer, we haven't many filters. I'm going to call this and add filters. Well, and by the way, I forgot to mention something which is our filter, right? It is, in this case five by five by three, right? So that means that the number of parameters and this one filter is going to be five times five times 370, 5 g. So there's 75 numbers in this filter. And that's because if I know that 70% of the entire copper, so the number of parameters in one of these filters that are five-by-five by three will be 75. It's actually going to be one more. So actually I should do this now because we're going to have this later on. There's also going to be plus a bias. So the output, we can also add a plus B. So formally it's actually 76 g. Alright? So what we can do is if I convolve a group feel through my input image, I get this matrix. Now in one convolutional layer, I'm going to have an F filters, right? So this will be, but through one. This is filtered to all the way down to my filter. That means that if I were to convolve each of these filters with the input, I would be left with any of these. So I will have n outputs. And what we then do it a convolutional layer is we take these matrices of which I have enough of them. Just stack them together to make a new 3D texture. Alright, so now this new 3D tensor is going to have a width which is W minus W f plus one, which is h minus h f cross one. And then the depth of it is going to be the number of filters that were in my prior normal birth weight. Right? So then after that, I'm going to pass them through a ReLu. And then this will now be my activations that we previously called H1, right? The activations after the first layer of my neural network. And this will be then what goes to compute h k. So it's gonna be the input for the next layer. And again, it's going to have a width, W minus W f plus one, height H minus H F plus one, and then a depth equal to the number of filters in my player layer. Okay? Any questions that they didn't get it. I understand correctly the question is, do we ever essentially add some type of other operations on top of the convolution. Something more sophisticated than summing all these values and I'm having, I'm sorry, multiplying, doing this. Yeah, So the only nonlinearity comes when we just take all of these activations and we raise the question, if I predict correctly is, why do we always match this with this D, right? And the question is, can we choose not to do this? Yes, you can choose not to do this, but don't make your line a bit harder when it comes to writing code. So the reason that we matched this D to this T is because then the output of this convolution will always be a matrix. And then I know that the depth of my next layer is just going to be however many filters, dynamics. Let's say that this was never going to see this. And Cnn, let's say that this was step three, but I chose my filter to have a depth of warning. Or maybe a depth of two. If it has a depth of two, then what happens is, if you think about this picture, they aren't nationally depth dimension. So I'm also going to have to drag it forward. And now my output won't be a matrix is going to be also a 3D tensor. And then I'm gonna have to concatenate different 3D textures together. So for the sake of simplicity, we matched the depths so that the output of this operation is always a matrix. You could decide not to do it. It'll just be a bit more, right? Great, Yeah, So now another student is saying, in this case, would it make sense to match the depth because that's our RGB values. Yeah, So that's a great point. There's also an intuitive reason for wind CMS deaths, which is for your classifications, you probably care about the color the pixels to get the color the pixels, and you didn't know RG and B. Whereas if you had this adept to, you would only get RNG in one feature and then GMP and the next one. This generalizes to later layers of the neural networks. And so you can think of each of these matrices as a different feature of the inputs. And all of these features are useful and they can contain complimentary information. So by having the next layer have adept and matches and F, We're saying, use all of my teachers together to try to compute the next useful feature. I'm drawing it the same as the previous study. But instead of having, maybe there's all these connections. Now we've got sparser connection. Now. Listen to this. Wonderful. Yes, it's always raising a point, which is that you 15, couple of weeks before he got a couple of issues with linear operation. Every linear operation I can write as w x plus b, which is a fully connected neural network right here. So what is it that this competition buys us? Or how's it different than just building a fully connected neural networks? We'll talk about that in a few slides on my punched out one, which is that there are sparse connections. I'll explain what that means to. There's also parameter sharing. And then three, Tom, I also mentioned the feature activations are going to be much larger. They get to that in a few slides, so don't worry if you didn't follow what I just said. Other questions. The question is, how do we guarantee that these Different filters are picking up different features because I come from different initializations. So on the one hand, we actually can't guarantee that these filters pick up different features. But those different features or what would be helpful for eventually reducing your cost center for Boston guests are built learned and if they're initialized differently than yeah, they will converge to different parameters. Question is, is there a bias term and bulk? Yeah, so you could just imagine that there's a plus b1 that I have drawn here. And oftentimes you can just like, we'll always have this, but it's never drawn B and S. All right? Perfect. Yes. So this student's question is, what if we wanted to combine filters of different sizes? Like maybe this would be a five by five filter and maybe this would be like a seven by seven filter. We can't do that in a single convolutional layer. Because remember I have to concatenate these matrices together, but then they would have different dimensions. So there are a few things here. The first is intuition is right, that different sized filters will check different features. And therefore, it's probably a good idea to have them be different sizes, maybe in different layers. But then the second thing is Google at the same idea. So in 2014 did something called the inception layer, which we'll talk about the Inception where we'll have you do convolutions of different widths and heights, filters in the same layer. And I combine them all together. So we'll talk about that next lecture. We talked about the inception module. The question is, is different than the number of color channels. And if it's a hyperparameter, you can select it, you can make it 100 filters are 1,000 filters essentially. The question is, are the filters themselves also hyperparameters? Hyperparameters, they are parameters, so the values within the filters are learned via stochastic gradient descent. Then the number of filters that we have is a hyperparameter that we choose. One last question here. The question is how they've seen filters, the one-by-one, three-by-three, five-by-five. Most commonly, most of you have not seen. But it is what is going to be something that we're going to do 13 by 35 by five combinations. So I believe that it is the case that you want medium to low frequency features you watch. These are larger filter. And we're going to see that AlexNet starts off with the 11 by 11 comfortable lesions. But then the empirical result after VGG notice that three-by-three exercise. And so I don't have a better answer for you aside from Cherokee works. Okay. So that is the compositional layer of the CNN. So then what happens when we have a multi layers cnn is that we're going to start off with some image. Let's say orange carrots, MYC4 ten image. So it has a depth of three with a W and a height of h. Alright, and then let's say that I make a convolutional layer with 100 filters, right? So then in my next layer, my depth is going to be 100. And there's gonna be a new w prime and a new H prime. This width would just be equal to my original width minus my folks, or width plus one. H prime would be H minus H F plus one. Alright? If I wanted to put another convolutional filter, let's say I would do a layer that has, I'm going to call this ten filters. So each of these ten filters would have some width and height, but their depth would be 100. Because these filters have to have the same as what's going into that there. Alright? So then that would give you a new 3D tensor with a new width, height. And you can repeat this. However many layers of a convolutional neural network you want. Any questions here? Alright, so that's the basic convolutional layer. Let's get to this question now that Tom white hat, which is why is it that there's maybe a good idea, especially as it relates to the motivations we did before. So let's look at a fully connected layer. And here, instead of drawing, flipped our calculators by 90 degrees. So this horizontal rows is there. One in this row here is layer two. So we know that a fully connected layer, we have a weight matrix w. One right there, That's H1, all these flight activations to a12, all these flight activations. Alright, if you look at what a neural network is, a convolutional neural network is doing. What it's saying is in a convolutional neural network. If I were to look at the activations of a single artificial neuron, a convolutional neural network. This neuron is only computed from the values where the filter overlaps the input. So this neuron only really has connections to these orange values in the input. Alright? So I think that'd be set, this sculpture was five by five by three, right? And that's 75. So this neuron is only computed from 75 pounds. This neuron here is computed from a different $75. And so instead of this neuron being connected to every single value and my input images so I can click it to us, but to a sports amounts of inputs. And so that's what we mean by sparse connectivity. If we're doing a convolution across just three units, then if I look at the value of this neuron and layer two is only informed by a subset of the units. All right? The first going from fully connected to CNN layers be replaced for connectivity with sparse connectivity. Because every neuron value is only determined by a small subset of the important. The other thing to note is if we looked at this issue three. So let's say that here we had a convolution over just three neurons in the prior layer. All right, so let's say that both compositions have waged a, B, and C. The value of this wire is, the value of this wire is B, the value of this wire is C. So another thing that convolutional neural networks do is that they share parameters. Meaning of that. Because convolution is this operation where I drag this filter to the right. This blue filter is the same blue filter irrespective of where it is. If its values are 123456 is gonna be 123456 here. I'm just gonna be 123456 over here, right? So what that means is that whenever I look at this, if I were to look at what the wastewater for H2 for, right? This is a same convolutional filter. Now drag it over by one. And its values are also, let me give this in a different color. This value is a, this value is b, and this value is c. Alright? And then I'll do one more. If I drag it over one more, this value here is a. Specialty care is B. So convolutional layers as forest connectivity. They also share parameters. Okay. Any questions on either of those? Wonderful. Tommy's question is, is it true that I can take any convolutional neural network and rewrite it as a fully connected neural network layer. Yeah, ready as a neural network, the fully connected network with only particular nodes feeding into the next one. The answer is yes. Remember, you want to all get accomplished on them. And you're there for a bunch of have to implement this operation of convolution and do that with, I believe, a quadruple for loop, which is going to be really slow. And so it's gonna be so slow that it will be something that we need to vectorize. But in homework number five, we've vectorized. So the way that we do it is by writing it as a fully connected layer, but it's implementing convolution. Okay? So force connections reduce computational memory. By this, we simply mean you have far fewer parameters. So in this example, each of these wires is a different value that we have to store. Versus in this example we just have three numbers, a, b, and c, and that specifies my entire convolution operation. Then this also says sparse interactions reduce computation time. What that means is that if in later one I have n neurons, and in layer two I had N neurons. Then to compute layer to you from layer one, I would need to do order of m times panel operations. In this case, we may have m neurons in layer one. But remember in layer two, every neuron only has connections based on the size of the convolution. So let's call that K. So here, the number of connections k is equal to three. So the order of the time it takes to compute will be order n times k. Instead of order n time. Alright, so convolutional layers have sparse interactions and they reduced the number of parameters. One thing that you may be concerned is that because of the sparks interactions, convolutional layers may be limited in their computation in the following way. Let me just go through this image. Here. The value of this neuron is based off of things in the top left-hand side of the image. And the value of this neuron is going to be based off of when I finally had to drag this filter all the way to the bottom right. Because they're spatially vocal. This neuron only talks to the bottom right, and this neuron only talks to the bottom, to the top left. You may think this is really bad because there's no neuron that looks like the entire image. If I'm doing a self-driving car, I need to use things from all parts of the images. I need to know if there is a pedestrian crossing the street. And if I'm at a red light, which in the top-left, alright? So there is no neuron here that we see both the bottom right and the top left. So how do convolutional neural networks to deal with this potential limitation? What we do is we're going to stack many convolutional layers together. So now, if I look at one neuron, right? Let's call it this neuron H3.3 here, because my convolution only had three values. It only sees three layers, sorry, three artificial neurons in layer two. But my original images layer one. And so even though my convolution only has three values, if I look at my input x, this neuron is going to see five pixels that my initial input image. And then you can imagine if you start from any of you sit together, if I stack another layer, then a neuron in this layer would have seen inputs are seven. To get over this problem where you only see things vocally and the prior layer, we're going to stack many layers together. You can see them. H3.3 will see all of these input values. Any questions there? Not because they use proper thing to get these spaces on the right. So rocks, this question is, I can rephrase it in a different way, which I think the answer is yes. But the fact that convolutional neural networks work tells us that the force interactions are a way you, with sparse interactions you have in front of ways so much information they can no longer do that task. Then Roxanne is saying, is there a connection dropped out because he dropped out, we rapidly throwaway connections and still do you think I would say that CNN's tell us that the sparse connections are sufficient to do the task. And so for that reason I might take that adverse want drunk, I was being uncertainties. Alright. Let's go ahead and just plug in some numbers to make sure that you understand what's going on for the convolutional neural network. So we're going to consider an input that is 32 by 32 by three, that's c bar tab. And we're going to consider two architectures. The first is a fully-connected network with five hundreds neurons, and the first layer. The second is a convolutional neural nets. Were there for filters that are four by four by three. Alright? Okay, so the first question is, how many output neurons are there? And the convolutional neural network assuming only valid combinations. That's what this whole firearms. When I say an output neuron of a convolutional neural network. I'm calling each of these values here. And output neuron, right? It'll be out the neuron that is sparsely connected to a local patch can be input, right? So whenever we asked you that question, or whenever someone asks you the question, how many output neurons are there in a convolutional neural network? Essentially we're saying how big is the out, put it back convolutional layer, right? So let's go ahead and answer that first question. One, we have an input which is 32 by 32 by three. And then we're going to convolve it with filters that are four by, four by three dots from over here. And then I have four of these. I want to know the number of routes of neurons. I'm going to first see what is the output size of convolving one of these filters with my input. Remember that the depth d is always matched. And so the output would be a matrix where the dimensions of the matrix are going to be w minus w f plus one by h minus h f plus one. So in this case, at 32 minus four plus one is 29. And so the output of one filter is gonna be a 29 by 29 nature subclinical one filter. But then remember, I have four of these filters, right? Because I have four of these filters, output is going to concatenate four of these 29 by 29 matrices. So the number of total output neurons that I will have is going to be 29 times 29 times four. So this is the number of neurons in layer one. Any questions there? It just gets us something that Tom I was talking about. Earlier on. We said that the CNNs will have a lot more output. You can see that planning to be true. So if I had a fully connected layer with 500 neurons, right? Then the dimension of H1 is equal to 500 for my fully-connected layers because I just have 500 neurons. But the number of protons here is 29 times 29 times four, and that number is much bigger than five. So there are many more activations in a convolutional neural network. Right? Any questions there? The question is, if we're starting with four filters. Oh, sorry. Yes, because basically the four-fifths, so each convolution is gonna give me one of these matrices. And then because I have four filters, I'm going to have four of these matrices. So the number of activations are output neurons will be 29 by 29 and then tax works. I have four of them. Right? Other questions. Okay, Let's do the second calculation, which is how many parameters are in each model. So for the fully connected architecture, we have 500 output neurons. Each of these neurons will have a weight to the 3,072 elements here. So the number of weights are going to be 32 by 32 by three. I'm going to do a plus one for my bias. That's the parameter. Every single one of my 500 neurons has this many connections to the input. So this has to multiply 500 to get my total number of parameters. And here we see that this number is 1.5 million params University for 1,000 neurons in person and they already have over 1 million parameters. It might convolutional neural network, the number of parameters or the number of the number of parameters in one filter. So the number of parameters in one filter will be four times four times three. And I'm going to do plus one for the bias times the number of filters that I have. The number of filters I have are for. This equals 196 g. So as far fewer than diploid can't demand even if it wasn't, you know, even if it wasn't for filters, if it was 400, right, it'd be 19,000 parameters for hundreds of water filters. And this number 19,600 is still much less than our loan 0.5 million parameters, right? So you can see here plainly that the CNS or less parameters and boys and girls. In fact, when we talk about an architecture called VGG net, which is a very deep 60 to 19 layers convolutional neural network. It's going to have, I think on the order of it's going to have over 100 million parameters. I believe something like 70 or 80 million of those are coming from fully connected layers and the rest of the population. So those bars, neural networks that we've talked about, most of the parameters come from the fully connected layers. The question is R-CNN is much more parameter efficient in terms of their performance? I believe by that, you're asking me higher accuracy with far fewer parameters and the answer is actually wonderful. Yes, it's always pointing out the following, which is that for the fully connected layer, we only have to have 500 numbers to describe each one. But for the CNN, we need 29 times, 29 times four numbers. So don't have any more activations to store and Ms. Cnn. And doesn't that take up more memory? The answer is yes, best one quantifies CNN. So VGG net, which I'll just bring up again because that's going to be a huge neural. Now, we're going to see that I really do need. Then you just put their compensation costs many GPUs because it's easy to use if we actually don't have the memory to hold all of the activations required for you took their raiders will actually go ahead and compute how much memory is needed to do a forward and backward pass and it's pretty large. Great. Now the truth, is there only one bias per filter tensor, or is it one bys PR depth and it's the former is one bias. For this tensor. This tensor will have flux of B1 plus B2 to devices just one scalar tensor. Let's go back difference. Yeah, The question is the bicep just gets added to this foundation? That's correct. The question is both a larger filter, increase the memory consumption. Considerably. Larger filter will result in activation, because a larger filter will, since the output activation is W minus Ws plus one, right? This number will be smaller if WWF disorder. So actually to help with the memory is starting the activity. The question is, if you wanted to have it out to do the matrix multiplication, do you mean by rewriting this convolution as a matrix? If you do that, there'll be fewer zeros in that matrix representation of it. But you would still, the major membrane concern is for storing the activations. Alright, we're gonna move on for now. So those are just values in the convolutional layer. And this is actually a slide that we've already talked about, but it's just a reminder all accomplishes in this class, there's going to be valid convolution, convolution to CNN. So if you're coming from signal processing, you don't do valid convolution. So usually, usually your inputs are not always over, the filter is not always overlap. So that's why I say this. So people coming from signal processing may not have that be the first thing they encounter. Okay? So there is a student question earlier that said, ya plus size is always decreasing. How do we deal with this? What we'll do is we'll take our input and will pad with zeros. And this will prevent the size of the noise decreasing. So if I have a three-by-three input and I do cat equals one. N equals one means put a one layer of zeros around the entire input. Cat equals two is two layers of Sarah. So here, this new width would be equal to w prime, and that would be the width of my original filter if this is WF plus two times the value of pad, in this case equals one. So now the width went from three-by-three to five-by-five, went 3-5. Okay? So the output of the valid convolution will now be w minus w f plus one plus two times pad. So if hat equals one, which means I just put one ring of zeros around my input. And I said WWF equals to three. Then I'm going to tap at the output width. W prime is gonna be my input width minus w f of my filter, which is three plus one plus two times tag, which is one. This is equals to W. I said you pad and one, and you made your filter size three, your inputs never decrease in size. Any questions there? Yeah, The question is, do you use padding so you can keep multiple filter outputs the same size? Essentially, yes, and to also stop beside this from decreasing. So a student here, I mentioned that we usually do one-by-one and three-by-three or five-by-five convolutions and are able to make the five-by-five convolutions have distinct emphasizes that three-by-three for the three-by-three, pad by one. And for the five-by-five we pipeline. The question is since there are so many zeros at the edges would rescale the activations. We wouldn't be zeros, wouldn't make it smaller. But then if it was helpful to scale them up and hoping the optimization with darker. And then this relates to Goodfellow who says empirically, the amount of optimal padding is somewhere between type equals zero and the value that causes B inserting the African person with padding. Another hyperparameter we haven't or convolutional neural about just called stride. So we've talked about how in convolution we drag things, right? And we've been dragging them. One by one by one. Stride allows us to drag by more than one. So here, if I have my filter, which is this three-by-three red thing here. If I do strive to two, then when I drag it over, I drag it over by two indices, not worn it. Alright, and then it happens everywhere. So after I reached the right end, when I drag it down, I would actually drag it down by two here sponsor. So here, when I drag it down, it would come over here. Alright, so let's try just is a hyperparameter that allows me to do more, to drag it more indices, bunch of Swarm. When we set this tried, this triad has to always be talking about compensation. So if I were to stride the value of three, this would not work because if I were to stride with a value of three, when I get to the right by three, the first one would be fine. But now if I drag this over to the right by three again, it would end up over here. And this is not a valid convolution. So that would have worked out the chest since you guys now is compilations. And then if we just look at this stride equals two example, we can see that if I were to take, if composition, I would end up with an output that is a three-by-three matrix, right? Because I can drag this book by stride 23 times along one row. And then going along the columns, I can only attracted to that three times also. So when you have stride, the output of your combo with snow layer is now going to be given by this matrix here. Basically the width gets divided by the stride and I guess it's bounded by this trap. Any questions here? Right? Yeah, great. The question is, maybe you want to choose a stride equal to the size of your filter so that these work, the outputs one had any overlapping. But is there a use case for that? Yes, there is actually one use case for that will be performing layer, the pooling layer. This is what we talked about before, will model the complex cells that give you some position on variance. So what we do with a pooling layer is we define a filter size. This case is two-by-two. Then we define an operation like maps or average. Alright, so this is a two-by-two max pool. Then what we do is we start at the top left and we looked at two-by-two matrices. And then because the operation is Max, I'm going to take the max of these, which is six. And then usually when you do a max pool or an average pooling, your stride is going to equal your filter width so that you're never overlapping. So when I strived by two, I drag this red two-by-two matrix over by two and it overlaps 3478. That's the screen matrix. The max value here is eight. And then I stride down by two. That will give me this section over here. So this is the output of a pooling layer and typically they're destroying them. Okay? Any questions? If you see average pool, right? And sort of facts is just taking the average of these values. So if you average pool these, these will be one plus two plus five plus 6/4. So the output of a pooling layer has these dimensions. And then because you're pooling layer is essentially reducing this two-by-two matrix until the one number. You can see why this incorporates some position in variance. Because if things shift slightly, the maximum within your filter might still be the same. So we've done that with an example here. Remember the pooling layers introducing conditional on, very excellent. So what I've done here is I'm drawing the output of a max pool over three inputs. So this is layer one on the bottom layer to talk. And every single input here, for every single output, I look at the three input values and I take the max over. All right, so when I did this pooling layer, the purpose of this example is to see that what I could do is I can take layer one and shift everything over by one. So the 0.2 concepts here, here, 1.5 goes here, et cetera. But the max pooling operation, even though the layer one has shifted so that every single value is different. In my output layer, I still have some overlap. 1.5 and 1.5 here are still in the same position. Alright? So this gives some positional and variance or robustness to another, do an excellent job of it. But obviously, any questions on this pooling layer, customers, but his position on brand image is a silicone. It took effect even if it's 6/5 years. So that's what it corresponds to. The lecture. See you all on Wednesday. 
All right, everyone ready to get started? I'm sorry that this screen isn't working. If you have trouble saying, I asked you to move to the center of it. Alright. First reminder, homework number four is due this Friday. I'll put that degrade scope. We sent out a midterm and announcements on Monday and please read it carefully for details on the midterm. And if you have any follow-up questions, please post them on Piazza. We've announced some of the other midterm details, but the new one from the email is to make space. We reserved three other rooms and rice Hall for students also take the exams there, and we're doing that based off of last name. And so you have a last name beginning with KRL them last elliptic examine voice 1234, etc. In these rooms there are about 56 per room, and they look like this. So the C-star tiny bit more spaced out than in this room. But we think that if we flip the students, several of you in these rooms and that'll make more room here. Alright, so please follow those assignments. Any questions on any image from logistics? Alright. The midterm exam review session is tomorrow in young CS 50. And the TAs are going to upload the review problems by 09:00 P.M. tonight in case you wanted to take a look at them ahead of the review session. And again, we encouraged you and strongly recommended you attended the session or watch it later on. Friendly reminder that this Monday is the President's Day holiday. It's a holiday for UCLA and therefore there's no bacteria or office hours on Monday. And then my Wednesday office hours after class on Wednesday when we have the midterm exam, they're going to be canceled because the TAs and I have to scan all of the exams after the mid-term on Wednesday. All right. Okay. Any questions? All right, let's get back into material bank. Alright, so last lecture we introduced the architecture of the columns shelf drill number. We're going to just recap what that architecture is. And then today we're going to essentially into a case study of the really influential convolutional neural networks that have led to this deep learning revolution. So remember, what happens for a convolutional layer is we have some input to a layer. That input is going to be a 3D tensor with some width, height, and depth. And what we do is a convolutional layer is going to comprise some of mounted filters. Filters for each filter is going to have some width and height, and then it's going to have a depth that's matched to the depth of the input. Alright? And when I do the convolution of one of these filters with Mike input, I get out a matrix. So if I had NF filters, I'm going to have an F matrices. And matrices will be the convolution of each of these filters with the input. What I then do an accomplishment layer, if I take these NF matrices and I stack them together to make a new 3D tensor. So let's say that I started off with images from CFR that were 32 by 32 by three. If I were to convolve with 100 of these filters, I would have 100 of these matrices. Can I stack them together? And that becomes my new 3D tensor. That is by activations for the next layer. Alright, and then the next player might convolve with some other number of filters and that would give me a new 3D tensor. That's okay. The question is, can you think of this as a fully connected layer on the channel? The channel double, meaning that, right? Yes, because I think I understand the sentiment of your customers. You said you're always combining all of the datasets for the local patch state you're in. So you can think of it. Great. The question is, where is the plus one from when we did the parameter activation is coming from. So basically every single one of these filters, if this was say, a five by five by three filter. First one here. The number of wastes would be five times five times three, the volume of that tensor. But then we have a plus one. Because each of these filters also includes a bias term that's just added. So that's where that plus one comes from. Other questions. Okay. So that's the basic convolutional layer. We're going to do several case studies. They're going to become more familiar with this. As we look at these examples. I'm sure we talked about how all of the convolutions are so-called valid combinations, which means that we only do a convolution with a filter fully overlaps the input, right? So that would be at these four locations given by red, green, blue, and purple here. This means that if you were to compute the output size based off of input having a width w and height h, and the filter having a width WWF H, F. The output will always have a width w minus wi plus one, and a height h minus hf plus one. So as long as the filter is bigger than one, your output is gonna get smaller and smaller. So we then also introduced last lecture this idea of padding. So sometimes to keep the output the same size as the input, we'll go ahead and pad with zeros. And if you had with hat equals one, this means including one surrounding zeros and foot. I'll put it the complication that becomes these dimensions. Alright? And then we also talked about strive last factor, which means we do the drag of the convolutional filter. Stride equals one, which is the default that you're dragging this filter over by one all the time. But if you wanted to track it over by two, then you would set a stride equal to two. Now you have tried that will reduce the output size. All right. Any questions on padding or stride? Yes, that's correct. So the question is, when you tried by two, you're moving over by two. When you go to the right. But then when you go down, you also go down by two. So when you strike by two, you strike by two also in the vertical direction. Question is, is there ever a case where the stride size is bigger than the filter size? I've never seen that. Because then you wouldn't be missing entire sections of your input. Question is, how do you know whether the stride is valid? So basically, I realized I didn't have this slide here. Let me just from our last lecture. And this is the output width and height for a value of stride. And to make sure the stride is valid, this number should evaluate to an integer, not a decimal. Other questions. Alright? And then we talked about the pooling layer, which applies in operations such as if you have a max pooling layer, it'll look for the maximum value in your pulling size, in this case two-by-two. So something a two-by-two matrices. And it just extract some Epsilon. So here the maximum is fixed. Here, the maximum is eight, et cetera. So that's a point here. And this is the layer that is designed to try to introduce some positional invariant into your CNN. Any questions on the pooling layer? Yeah. Great. The question is, is there a reason the operation we do is max rather than me? So you, you can also do the mean. So that's usually called the average pooling layer. And we're going to see some of the neural numbers that we talked about today do an average pooling better than a Mac scoring? I believe that's pointing is usually chosen for empirical results, which that's a better performance. But you can do average pooling as well. Great. Yeah. The question is, to clarify, pulling is done channel wise. And that's correct. So when the input is a 3D tensor, so if you imagine that this input was a 3D tensor, you would do your pulling on every single matrix within that cancer. And then the outputs will be stacked. So you would do pulling on each individual matrix and then stack them to return. Also a 3D tensor. Right? Gentle thinks there's a spatial down sample and not the channel gram sample. That's correct. Their cousins here. Alright, let us spend debt into Linux. This will be the first. This is the convolutional neural network from me on LinkedIn and colleagues in 1998. It's the simplest one. And then after that we're going to go into. Alright, so this is the architecture of the input is going to be. So they use a dataset called or they used to. They wanted to classify digits. And instead of having these be RGB images, they were grayscale images. So these are just 32 by 32 matrices or not. Alright? So what they do is in the first layer, what they have is they have six, five-by-five convolutional filters. Okay? So let's go ahead and write this out. And these filters are applied at stride equals one and Patty equals zero. So the first question that we'll ask is, what is the size of the feature maps? That means the output of the convolution layer C1. C1 contains these 65 by foreign competition of filters. So what I'm asking you this essentially, when I apply six, five-by-five convolutions not alters. What's the size of the output? So for this, we're just going to apply the formulas that we talked about before. So when I do the convolution, the width and the height are going to change by w minus w plus one. So in this case, my initial image head was headed with it 32 and a height of 32. The thoughts or width is five, and then I add one. So the output of the convolutions are going to be 28 by 28 in width and height. And I can someone tell me what the doctors did. You just set it up. Perfect. Yeah. So I have six of these. So in total, the size of the feature maps are 28 by 28 by six. So that's what's drawn here. These are the six feature maps, each of them 28 by 28. How many parameters are there in one layer? So here we have six five-by-five convolutional filters. And so each five-by-five filter, It's going to have five times five equals 25 parameters, right? And then remember there's that plus one for the bias. So we're going to do it plus one for the bias. So this is the number of parameters in just one folder. And I have six of them. So in the first one layer, I have a total of 156 parameters. He does anyone want me to do that calculation again? Alright, great. We asked them signs up. Okay. Okay. Yeah, yes. The tongue way of saying if he asked for feature maps, the feature maps refer to just want to be. So let me just say size of, let me change these off the size of output, just to be clear, of C1. And then this will also be size of output. Size of output. Right? So the next after that, we're going to apply a pooling layer where the pool, the pools are two-by-two and they're applied at a stride of two. So we know that if you do this two-by-two pooling layer, where the filters are two-by-two hundred provided a stride of two. And I don't have the equation here, hasn't in the prior lecture. The output of the pooling layer will be w minus four, which is two, divided by the stride, which is two plus one. So we'll have a Tony eight minus two, which is 26/2, that's 13 plus one, that's 14. So the output of this pooling layer is going to be 14 by 14. And then just like Daniel is asking for the fight on each channel in the deaf separately. So if I had applied, I applied on this first matrix that gives me this. Then I play on the second matrix that gives me this one. The third matrix that gives me just wanted to say that this will be 14 by 14 bytes, six. Alright, and then how many parameters are there in the pulling? Someone who just have the answer? Yeah. So remember for the pooling layer, all we're doing is we're looking at some values and then taking their maximum or take their average or some other operation. There are no trainable parameters there. Alright? Saying for the point operation, is there a ceiling or floor? What if it's not an integer? Oh, you mean this equation? So again, just like for the convolutions, how the streets had to be valid. The stride will have to be valid for the full officer. So you need to choose a stride such that w minus w p over stride is. Before we do cited the output of s3, I want us to do a different question. And this one I'm going to ask you to think about. Number of parameters in S3 or third, comparable or display or C3, which is a convolutional layer. It contains 16 five-by-five convolutional filters. Okay? This is what width and the height of this bump there. Alright, take 15 s to think about what expression would give you the number of parameters in later S3. And then I'll ask someone to give me the answer. All right. Can someone raise their hand and tell me and you don't have to tell me that exact number to say like five times, five times, whatever. How many parameters are in this compositional layers. Okay, So the initial answer is five times, five plus one times 16. Who agrees with this answer? And if you don't agree, what would you change about it? Perfect desk. So I asked you guys to think about this one because I noticed that it's just something that's easy to forget. We will usually always write the filters in terms of their width and the height. And we're going to leave off the depth. The depth is always assumed to be matched to the prior later. So because the prior layer was the output of this pooling operation and there are six that adaptive six. Remember them two convolutional layers will be five by five by six. So the number of parameters in S3 is five times five times six. Because this is the depth input to this layer. The question is, isn't it five instead of six nodes? Six, because the pooling layer, I'll put his 14 by 14 bytes six, so it's matching. So that's the number of parameters and C3. If we want the size of the output of s3, then what we do is we do the width minus the filter with the plus one. So in this case we'll have, let me do this calculation here. The input is 14, the width is five. Now we do a plus one, so this is equal to ten. We have 16 of these doctors to be. The size of the output of s3 is ten by ten by 16. Any questions on any of this population? The question is, do we assume the input is base coat for this Linux, the inputs word grayscale. So the first one is 32 by 32. The question is, why wasn't there a gap? And the first one layer is because the input image was grayscale. So this is like, this is just a 32 by 32 matrix, ready to register 32 by 32 by one 3D tensor. And so the depth is equal to one. I'm sorry, what's the question? Does the output of which layer? I'll put up c3? Yes. So the output of s3 will have a depth of 16 because there are 16 of these structures. Sorry. The picture to have in mind again is when we do the complement cell layer, the filter Jeff has always matched to the input. So the result of the convolution is just a matrix. And then if I have 16 of these filters, I'm guessing, I'm going to concatenate these two matrices together. Are you referring to these five by five filters? The depth of these filters is six. So we could have written these are five-by-five by six convolutional filters. Because remember in a convolutional layer, the depth of the filter always matches the depth of the input. So that's kinda the checking using convention. We just write the width and the height, but you always have to remember that there's got to be smashed to the fire. Alright. Any other questions? No. So let me just explain one more time what's going on here since I, to make sure everyone's on board. So the layer C3 is a convolutional layer where the input is 14 by 14 by six. When we say that we have a five-by-five convolutional filter. The convolutional filter will always have a depth that's equal to the Delta B input. The convolutional filter always has six investigations to be inputted. Six, that's quite a number of parameters, will be five times five times six plus my bias. And so one of the filters convolved with this input is gonna get the a matrix that is ten by ten. But now I have 16 of these filters. And so that's why the output of the sphere is ten by ten by every day. Write down the dimension, becomes very clear that one of the great, yes, it's almost right down the dimension itself. We can put it everywhere because CNN tells you what the filter data should be, right? And we're gonna do this many more times today. Don't worry, use the CT scan. Are there questions? All right. So that is some examples of sizing on this convolutional neural networks. We're going to just show a few notations and then we're gonna get into studies out. Alright? So usually in a convolutional neural network, they will be comprised of several convolutional layers are stacked together with a ReLu in-between. And we're going to use this notation called Plate notation, where if I have a convolutional layer, an array blue, and then I drop box around it and I write a little n in the corner. I'm going to repeat that concrete root n times. So typical architecture for a convolutional neural network is that we're going to have the peaks of a Combray loop com, for a loop. And you usually, we're going to see this N is going to equal two or three, usually will staff, you know, two or three concrete. Concrete is concrete lose. And then after that is going to be followed by a max. Cool. Alright. And then we're going to have a bunch of ANDs, three called Rayleigh's, followed by a max pool stat that's displayed to n. After this, we're going to use k fully connected layers. And this is just something that we'll commonly see, although we'll see later on in architecture called Google that removes fees. And then finally, we're going to have our softmax output. Alright, so this is a typical, fairly conventional CNN architecture. All right. So with that, we're going to go ahead and give each state buddies, right? Yeah. So there's only one maximum. Yes. So what this is saying is that for every, let's say n is equal to three for every three called ReLu calibrated comp rabies. Sir. For a comprehensive cooperative new comp, remove it. Then we'll go through one max score. Alright? And then that architecture of comp ReLu, ReLu, cooperating back school is repeated n times rate. The question is, do we generally will, do we generally have batch norm? Yes, batch norm is something that you will watch to insert here. It's not going to be present in the neural networks that we discussed today because batch norm was made after these ImageNet competition. All right, okay, so we're gonna start off then with this case. Sorry. Okay. Now, before we aren't gonna do a bit more until next. So it's the one that we were talking about. There are just a few calculations I also wanted to do here. The first is how many connections are there? The first convolutional layer. So if you want to put Lynette paper, they're going to give they want to say something in that paper like that, there are 122,304 connections. And the first way I'm walking over that number comes from. So this also is going to give us some idea of nomenclature. So the first thing that you might wonder is what is meant by connection. So these 28 by 28 by six, I'll put one layer means that I have 28 by 28 by six artificial neurons. And I've drawn in red, one of those neurons is, alright. We know that this neuron is the output. The input convolved with a five-by-five filter, where this neuron is the value when that five-by-five filter is applied at the top-left. Alright? So to compute the value of this neuron, right? We know that we multiply the input with the filter. And we. Sum up all the values. And there are therefore 25 connections for the five-by-five filter from the input to this one neuron. There's also the bias. So there's gonna be the plus one for the bison, but we'll call that a connection, also positive connection. I want to know how many connections there are in the first layer. The first thing I'm going to say is how many connections this every single neuron in the output of the first layer had. So every neuron. Tomboy, isn't the bicep applied per filter? The answer is yes. So John McCain still causes the connection. So basically there's a bias b. Let me draw this in red, also a bias p for this one filter. And even though this bias b and this filter is five-by-five filter, or the same filter that I then used to calculate the value of this neuron, right? I shipped to filter over. Even though the bias b and this filter weights are exactly the same, the call each other connections. Alright? So every neuron has five times five plus one connections. Again, those corresponds to the five-by-five wasting my full term plus the one part that I want to know how many connections that are then all I have to do is then say how many neurons are there, and then multiply it by the number of connections per neuron. So let's write that out. The number of neurons I have. The output of my first layer is going to be 28 times, 28 times six, because that's the output of my first comp layer. And therefore, the number of connections is going to be the product of these two, the number of neurons, which is 28 times 28 times six, multiplied by the number of connections per neuron. And that's five times five plus one. And this gives you that number that you'll see in their paper, 122,304 connections in that first layer. Any questions there? Yeah. The question is can I explain why we're concerned? But the number of connections, it can give you a sense of the volume of calculations that you're going to be doing. But primarily in exactly like this. I think it's more for teaching purposes to them, you're understanding exactly responding on compositional errors. Like I said, this will help you off to get a sense of the hardware required because it tells you the number of operations that you need to do for this application. All right, so with that we'll go over the entire texture. So I'm gonna be using this notation. Where for every single layer, when I say layer here, I mean I just use a different version layer. Usually layer, layer is going to refer to either, either a conversation on transformation. So even though there are seven or eight steps here, this neural network only has four layers because that only counts the convolutions or fully connected networks or linear transformation. So C1 here is one layer, C3 is another layer, as tutors mark Kennedy layer because a point it's not canister layer. There's another pulling that happens here that doesn't cancel. Layer C5 is another convolution, that catalyst layer, that's the third layer and then F6 is my fully connected and that calf for here. So therefore total layers. But then every single computation, we'll give it a number. For each computation, I'm going to write the app, the size of the output. So the output of the first column player is gonna be 28 by 28 by six. And then I'll give a description of what's in that layer. So this will have six convolutional filters, each hip and five-by-five apply to strike one. So then after that we have the pool that we talked about, two-by-two pool with strike two. And then the lunette didn't use a max pool. They actually did a pool that how to train your book coefficient, but this is not used today, so we won't go into the details of this. And we've already calculated that the output of this pole is 14 by 14 by six. Then after that, we have another convolutional layer, 16 convolutional filters but with five and high-five. And then as per the questions that we just did on the prior slide, remember that these have a depth of six. Because this depth of six is always going to be matched to the depth of the input of the prior layer. So always remember that when computing, now for our parameters, there's layers. Okay? I said Tom was saying John McCain and others call something a layer if it attaches trainable parameters. And so these comps, layers that are trainable parameters. All right, So tomorrow is asking you about the details of this pooling layer. Told me I'm going to stay just take a look at the paper for that one. So it's not used today, so we won't spend more time on it. But he has a specific point there here that's not no longer be used. After this convolution. We go into a pooling layer with two-by-two filters applied at stride two, but a couple of width and the height by two. Alright? And then after that, there's one more convolutional layer. There are 125 by five filters. Five by five filters are going to have a depth of 16 because the input from the past point where has it up to 16? Because the width of the filter is equal to the width and height of the filter or equal to the width and height of the image, then the output is just a scalar. And because I have 120 of these convolutional filters, that just gives me 120 numbers. Right? After that I have a fully connected layer, that's my linear layer. And then after that, they do a comparison to them. They have a loss function which computes in a square error. So this is the architecture. You wrote it in terms of something like the plate notation. We would say that there's a column for column four, that's this times two, I'll apply a comp, sci and MT output. And that is just a concise representation of these $0.70. Okay. Any questions on this architecture? Great. Question is, can I explain why? For number five, the size is just one-twenty. So remember the output of the convolutional filter, sorry, a vicinal layer will be stacking the outputs of each filter. And for each filter, the output width and height will be w minus w plus one. So in this case the input width was five. The output with the thoughts are with this five. And then we have a plus one. So the output is just one-by-one. The way to picture this is that in this convolution, because the filter is the exact same size as the input. There's no drag that. You just put the filter on the input and you can drive it at all. It's only one vowel combination. And so the output is just going to be a scalar. And then I have 120 of those. So those 120 scalars gets stuck together and be after the fat layer. It's just 120 numbers. Justify understood. The question is just summarizing the architecture, which is saying it's architecture. We have cough, columns followed by pools. Essentially reducing the size of the features until we have a fully connected layer at the output. That goes to my ten classifications. That's correct. Answer your question or was it? The question is, what are Gaussian connections and why are we using MSE? I'm not gonna go into those details because they're not used today. Instead of the best practices if you have ten outputs, right, we would use a softmax classifier with ten, with ten classes. And that's what's going to be done for all the image guy, CNNs. But back in 1998, that was the prevailing way to classify these statistics. They actually had a template for each digit that they computed. The mean square error. For the shoes are not used to date. What if the reason they're not as well as performing territorial empirically, they're worse. Alright, so let's move on then to the ImageNet classifier. So now these ones that we're going to start talking about, beginning with outstanding 2012, carrying practices that are very commonly used today. So we're going to start off with looking at our tech, which is that first CNN that really reduced the top 5% error classification on the initiative dataset. Right? So before we talk about the architecture of AlexNet, I'm going to talk about some of the data augmentation and organizations have been used. And these are the things that should be familiar to all of them because we've talked about them in prior lectures. So first off, AlexNet is round and this next initiative we know is that dataset that has natural images, that volatile wasn't one of 1,000 classes. Alright? Images are usually relatively large. So what happens is because the Internet has variable sized images. Our first crops them to all be the same size. So what they do is they'll take an image and they will either resize it or if down-sample it so that the shorter side is 256 pixels. So the image may not be a square. Maybe it looks like this after cropping or down scaling. The shorter side is 256 pixels. The longer side to something larger than that. What they do is they crop out the central to the six by six pixels. So this operations says take the center square of this image. This will be a 256.2 56 width. And then the actual inputs of the CNN are then going to be this augmentation off of the image. So for this district, what they do is then they cropped out 224 by 200, 204-20-4204 crops of this image as ways to augment their datasets. So they might take this to 24 by 224 crop here. And then maybe on the next crop, it's going to end up being this to 24, 24 crop. Then those costs comprise different crops that are all now inputs to the AlexNet architecture. So in their paper, they say the inputs are 224 by 224 by three. We're going to see that if you apply their architecture to image at this size, it doesn't work out, which means that they have a bug somewhere. So we're gonna just assume that the inputs, perhaps it popped up to 27 by 227 by three. Alright, that's just a minor bugs somewhere in how they wrote the paper. Then we're gonna subtract the mean image over the training set, just like we do for our kind of nonlinearity. Alexnet is the architect enough to popularize the paper. What they did is they trained various of these convolutional neural networks where they use the ReLu after the convolutions and compared it to applying tan h units after the convolution and presented as over here. So what they do on the x-axis is they show the number of training epochs needed to get some training error. Lower the better. And the dotted line is tan h and the solid line is rabid. So they saw that change approximately six times faster than tan h. And therefore they use ReLu for their architectures. And so when we write out the architecture, remember that every fully-connected layer and convolutional layers followed by a rabid. Also train on multiple GPUs. So if you look at this image from their paper, this is the actual image. I didn't crop it to remove the top part of it. What happened is that back in 2012, they had GPUs, but just regular bytes of memory. And that wasn't enough memory to hold all the images that they needed to train on. So they literally split their architecture into two paths. So their first layer would have 96 convolutional filters. 48 of this would be put on one GPU and 48 on the other. Alright? So this was hardware constraints. But then say to do this step has a few other details. They use something called local response normalization. We're not going to talk about this because the next two architectures from now, VGG net, they determined that local response normalization doesn't help. If you're interested in learning more about this. Feel free to drop by my office hours is basically an intuition that if a given layer, one neuron is really loud, it's just silence for the other ones. That's something from biology. But again, made her paper so that this doesn't help. And then they're pooling layers are done with some overlapping. So usually pooling layers are done with the stride equal to the width of the pooling layer. But in this case for Alex that they have some overlapping pair dataset augmentation. So in addition to the cloud, So they did they also, after extracting out the 22224 pixels, they took their horizontal reflections. And therefore they had in total ten images from one, because it took five patches over to 245224 cluster reflections. With these ten images and put into AlexNet you again at 10:00 batch distributions. What they would do is they would output the 10th softmax probabilities together. And you can think of this as a sampling across and put examples. And this reduces the error rate by 1.5 per cent. Also did a few types of color augmentations to the images. And this also had the effect of reducing the top one error rate. And then finally, they didn't drop out with p equals 0.5. And they found that this substantially reduce overfitting, but lead to ten times over twice as long. Questions on any of these augmentations has accepted. The 1% is an absolute error rate. So I believe that the error rate, the report is either 16.4, 0.2, 0.4% error rate. So this 1% means like if they were to do this, pull their occupation. If they were not to do it, it would have gone up to 17.4%. Yes. So let me just repeat what Tony was saying, which is clarifying the crop. So basically when you get a test image, when after the crop down to the residency, but it's expected that they would take five to 24 by 224 prompts, as well as their horizontal reflections. So this gives ten subset images. Those images would go to AlexNet reading ten softmax probabilities are ten softmax distribution, then it would average those together. Any other questions? Question is, is the augmentation done at train time or just for testing? It's also going to turn the question is, is there a reason they chose 24 to 24 by 224 rather than say like 250-65-2506. I don't know the reason they chose that, but I imagine it has something to do with making sure you grab the initiative, classify it like if you make that number too small, but you may just be taking out like a small patch of say, attack or something. And then if it's too large, then you have more computational intensive to 24 of them are in the sweet spot. Alright? Okay. You more things for AlexNet. So the optimization with SGD, stochastic gradient descent with momentum and weight decay. Remember weight decay just means L2 regularization. Recall when we talked about L2 regularization, you could probably see it as decaying the waste every STD stuff. So weight decay is just L2 regularization. Batch sizes were 128 images and their momentum factor was 0.9. They did manual and kneeling of the learning rate. So what he did is, um, they started the learning rate of 0.01 and they would monitor the loss over, across epochs. And when the loss fat code, what they would do is they would manually stop the training and then decrease the learning rate so that the learning rate is now smaller. And then after that, you would have a decrease in the loss and ended up going. And then they would again when it plateaus, comment interests burning rate, allowing the loss curve to go down further. So oftentimes you'll see boss curve that looks like this. And when, you know, it takes another drop that just is and indication that at that point the learning rate epsilon was decreased either manually or by schedule. And then the L2 regularization with any questions there? All right. I'm trending time to train alex that took roughly five to six days on the GPUs that they had available to them happened time. We talked about how it's good to ensemble of networks. So even though these numbers take a long time to train for this competition, they trained up to sudden AlexNet and they ensemble. There is both together. So if you just have one AlexNet, it achieves an 18.2% accuracy. And then when they had the final ensemble with seven CNN's, take it off the accuracy down to narrow it down to 15.4%. This slide here is just a reminder of what the image looks like. So these are what the images look like. And again, they're in one of 1,000 labels. And whenever we report error rates, we're going to be reporting a top five error array. Usually. And whenever we say the top five error rate is 15.4%. That means that it only means that it aired on 15.4% of the images and error occurs in the following way. For an image, AlexNet got to make five guesses at what the image was. And if one of those five is correct, then he caught it right? None of them are correct and they got it wrong, right? So I believe these examples, AlexNet only got these two wrong. And again, this one Dalmatian behind some cherries. And so I think that arts, that part of it a reasonable job here in terms of producing the foundation and shared. Alright. Any questions on any other, I guess, preprocessing detail from our time? Great. Question is what it means to ensemble multiple at the same type of neural network? Are you asking this from the perspective of these neural networks actually producing independent great, yeah, The company, is it different initializations or is it something like bagging? So I believe although RCTs check me that the service CNN just started from seven different initializations. And even with just different initializations they converge to I'm somewhat independent predictions. Okay, I just check the time aside to 50. So let's take a five-minute break and when we come back, we'll go into the details of the architecture. Like about the five different patches. And if your function is touches the same, right? So this works like footballs, interesting cases. So in training as you want to turn on those data, augmented patches. So that effectively gives you more samples, although they're somewhat correlated, we get some more sample training, which is a gift doing any testing. You're also doing it because even though they're highly correlated, they'll produce different softmax probabilities. And so maybe on one property getting more of it's scary and not the foundation for exams. So April that has an effect of reducing error. So in that case, is it unique for each image? You got? Like an image after Prop to be like to stick to J. You got patches, five plus five. You got some pictures. So you've gone from ten times, is that right? Yeah. There's actually an x here. And like for the taxi for each test intimate you about patches of it. I make the final prediction. Exactly. Yeah. Okay. I see that reduces heart rate by 1.5%. Considerable. Yeah. No, it's me again, my colleague S6 filters for this matches. So 16 is a hyperparameter that patient. So they decided you want 16th notes. And then they also showed that the size five by 5. " at any arbitrary tensor for the filters, that will be a hyperparameter that you need to really try to image data memory. Sorry, not all, but a bathroom. Oh, just a bachelor. Yeah, that's fine. Right. The picture to have in mind despite marked here over here and you're looking great spacebar to myself clear here, like standing around a similar law school, but then six bombers and he's definitely right. Every system, what we should think of that as a texture. So I have in mind, this is the most accurate drawing of it where the filters will always have adapt best match to the oh, yeah, yeah. That's all right. Thank you. Right. All right, everyone. We will get back to it. Any questions on Alex? Jeffrey processing? Alright, well, go into the architecture of Alice is going to be an eight layer neural network. So there are going to be eight layers with trainable parameters being in publications or fully-connected layer. We're going to stay the input as to 27, 27 by three. And the first layer, there are 96, is a convolutional layer with 96 filters. Each of them 11 width by 11 pies applied at stride four. Okay, So then what is the output size? We're just gonna do this. But it'll be an application of that formula where it's going to be w minus Ws. There's gonna be a plus two times pad, but pad here is equal to zero divided by stride plus one. So in this case, the input W is to 27, the filter width is 11. The stride is equal to four, and then we add one. So the output of this first layer is going to have ensures that our 55, 55. And then, because there are 96 of these filters than the depth at the end of the first, at the output of this convolution will be 96. So the first conv layer output is 55 by five by nine. Any questions? Question is, is there a reason the filter sizes are odd numbers? I think it is likely. Well, later on, we're going to see it's tried to is equal to one, right? If we want the width of the output to match the width of the input, going to need this minus Ws plus two plus one to equal my original width. Because two times pad is an even number and then this plus one makes an odd number. Then width of the filter, we've got an odd number. Alright? That's the size of the output features at the first layer. Next question is how many trainable parameters are there in the first convolutional layer. Services like eliminate question, I'm gonna give you 15 s to think about it, hoping to talk to you. It doesn't. I'll ask someone to shut him and asked her so how many trainable parameters are in the first column? All right, someone give me an answer for this. 11. 11 plus one. Perfect, That's right. So the students have the correct answer. I'm just gonna write it out. So you took the filters, has 11 by 11 by three. Remember because we input image as RGB channels, so step two is three. So the number of parameters in one filter is 11 by 11 by three plus one. So the total number of parameters will be 96 times. So the number of parameters and the first convolutional layer is 96 times 11 times 11 times three plus one. And this is equal to 34,944 parameters. All right, So that'll be written out here. Remember, this is gonna be the size of the output at the first convolutional layer, 55 by 55 by 96. And then this layer will have 96 filters. Each of them 11 by 11 by three apply to strive for. I'm going to skip this. This is just more application of the sizing formulas to just get to the architecture. There's one question here which is company trainable parameters are in the first poem layer. And just like in Baghdad, this is a trick question, right? The answer is zero. A pooling layers contain zero parameters. So this is the first three computations in AlexNet. The conflict are followed by a pool. They're pooling filters were three-by-three applied at stride two. And that will lead to an output that is 27, 27 by the same deficit and put 96, then they apply this normalization of error. Again, you're not responsible to know what it is because no paper uses it. Since AlexNet. I had forgotten I bought some of these calculations on the slides because, because they're really mechanical. So let me just move on to the whole architecture of AlexNet. So you can see arch that will follow this comp. Remember there's a ReLu after every single company convolution followed by a pool. Then there will be another convolutional filter. These ones were 256 filters of size five by five by the same tap. This layer 96 replenish dry 1.2 because they are five-by-five and the cat is equal to two. Remember that the output width is going to be the input width minus the filter size five plus two times pad. In this case, pad is two plus one. And so you can see that this cancels out to give Wn. And so the output of this convolutional layer is also going to have the same width and height as the prior norm layer 27 by 27. Then they do another pooling layer that takes the width and the height down to 13. And then after this they do three convolutional layers stacked together. Each of them with three-by-three filters, quite a pad one. A three-by-three filters applied a tad one are also going to have Wn equals w out, since it will be minus three plus two plus one. So three-by-three iPad one keeps this ties to say, five-by-five, catchy, saying, alright, after the three layers. So this is three layers here. They have a pooling filter. Again, take you down the width and the height. And then after that they have three fully connected layers. And finally, their softmax layer, which gives you the softmax probabilities over the 1,000 classes. Okay? So that is AlexNet. Any questions about AlexNet? Today's question is, is the reason that we use maybe she's an odd number for the filter size to match the input and emphasizes, yes. Any other questions here? Besides? Just suppose, yes, this student is asking, if we have an even padding. Let me just write out the formula again. This is with stride equals one, wn minus Ws plus two times pad one. Right? Now, the padding has always given meaning of Paris equals one. We put zeros on both the left and the right. That's why there's this factor of two here. If you did uneven padding, you haven't even size filter, but that would be not to take care of him. Kennedy. Other questions here. Can you have decimated? Perfect, Yeah, so rocks the same. Another reason for odd filters is an image processing. You can think of a point that you're interested in faltering. If you have an odd filter, essentially, the center of that filter is at that point, and you have the same number of filter traps, both going left to right. And so that's another reason for an optimal tree structure. Other questions. The question is, how did they come up with these hyperparameters like 384 3d4 to 56 filters. They didn't do the, they would have done some hyperparameters me. But this actually leads to the next ImageNet winner. Because in 2013, the ImageNet winner was called ZF net. And we're going to spend very little time on that because all Z F net is, is AlexNet with different hyperparameters. So basically these authors as bi-layer and they came up with a technique that they pulled the cockpit. So try to identify the features that were being burned by AlexNet at e.g. in their paper, they say AlexNet, which uses at the first layer and 11 by 11 filter. They observe and Caribbean island and filters tended to extract Phi and low frequency features of the image, but not middle frequencies. So they made some adjustments. The first is that they made the first 11 by 11 filters, seven by seven. And we saw better frequency coverage there. And then they also found that it was better to apply smaller filters, that smaller stripes and to increase the number of filters. So you were putting out the number 384 and I believe they increase this number to 512. Sorry, I I actually took the wrong slide, so I'm missing a few slides here, but I'm missing a slide. So in the third, fourth. And actually let me just draw them to fire one. For these layers here that use three to 43 to four to six and Z F net used. They use 51210, 24.5, 12 filters. Alright? And the effective, this hyperparameter optimization was huge because the F net dropped the error rate from 16.4% down to 11.7%. And if you do the math, that's approximately at 30% reduction error rate, nearly by just changing the hyperparameters of the AlexNet. Alright, so much more to be gained simply by optimizing hyperparameters. Okay, Any questions there? What are the significant difference between between VNET and AlexNet? Alexnet is outfit has a trainable there, so it has eight convolutions and pulls. Whereas That's only had four, sorry, sorry, not companies had 8 m, which means eight convolutions or fully-connected layers. Whereas only four combinations and Boy, heck of air is deeper than that. And then AlexNet also used the ReLu activation, whereas Lynette uses sigmoid. Then the output of Lynette is this comparison to a template mean-square error, whereas for our extent is the softmax loss. Alright? So that's Alex and z F net. Beginning then in 2014, there were two really notable architectures there, VGG net and you go back. And these were the two architectures that achieve the lowest error. On ImageNet. You can see people that just squeaked out the windpipe by 0.6 percentage of an error. But the main significant thing is that instead of a layers, which is what our tendons you have network, they went to 19.22 layers. So let's talk about how they did that and some of the experiments they did. So BTT net is an architecture that came out from this research lab at Oxford called the visual geometries Britain. The basic summary of the architecture is in this quote, which is our main contribution, is a thorough evaluation of networks of increasing depth. So going from eight layers to 11 to 16 to 19, you've seen an architecture with very small three-by-three convolutions. And they show significant improvement over require our configurations. Prior state of the art being our tendency FOR, alright. So there was to focus on this small three-by-three convolution more extended down. Alright. This is the summary of architecture. And then let's talk about some of the design made. The architecture is going to have I'm calm ReLu, ReLu, followed by a pool. And then they repeat this three times. So in total there are six layers, six columns, convolutional layers within this first segment. Then they stack three convolutions, followed by a pool, and they do this twice. And then they finally have three fully connected layers that then go to this softmax. Every single convolutional filter will be three by three. Stride one and add one. So the width of the input and the output of a convolutional filter will always be the same in this network. Then all pool filters are the two-by-two max pool is applied at a shutter. Every point butter is going to reduce the width and the height by a factor of two. With this architecture, they took the ImageNet error rate from 11.7% down to 7.3%. And that's approximately a 40% reduction in error. So this is fairly significant. Alright, so the first thing we should think about is what are the implications of using three-by-three filter? Rather than what AlexNet and z ethnic did, which was at the first layer. Having an ALEKS test case, an 11 by 11 filter, or in CF next case or seven by seven filter. So do you have had a 7-by-7 input filter? Whereas VGG net uses a three by three filter. Can someone tell me that they can potentially of using the smaller filter but with larger one. Right? At the same, you may lose. You said larger frequency information, but I'm just going to make it more general. Three-by-three and 7-by-7 are likely to pick up on different frequency features. But you mean yes, that's true. Any other cons? It can take more time to train. Uh, we'll revisit that one. Other other ethical concerns. I have it. I think it's I speak receptive fields. What I mean by receptive fields is the last lecture in this image that we had where, where we talked about how neural networks have sparse connectivity. So if you have a three-by-three filter, right? If we just look at the width dimension, then neuron is only going to see a three-by-three patch in the image of the preceding layer. So the colonies and it required some more mature spore final. Because it's seamless and perfect. Yes. Yeah. So let me just explain what the student answered, which is correct. Remember, we gave us motivation that a reason why small filters maybe bad, is if I'm a self-driving algorithm to try to do a self-driving car, I needed to make use of information from different parts of the images together, right? So if I only see a three-by-three patch, I'm only seeing activity or I'm only seeing parts of the images that are very close to each other. And I had just one layer. I will have no neuron that uses information from the top left of the image in the bottom right. Alright. In GF net, they have a seven by seven filter at the start. So because it's seven by seven, we will draw this as number of books like this. We call the receptive field of this neuron seven because it sees seven pixels or seven features from the preceding layer. Whereas here, VGG net would have a receptive field of just three. So the way that VGG net address this, this, and you would get a sense of this from the architecture where they stacked three comps layers together. If you suck, convolutional layers together, if I were to draw it like this. These convolutional layers together. So now I have two stuck together. This neuron up top has for just two layers receptive field. That's effectively far, right, because it's through two layers. It can see five pixels of an original input. Now if I were to add one more layer than the effective receptive field of this neuron would become center. So one potential con, of using smart filters is that you don't get to see all of the pixels of your input, right? But you get around this by stacking many convolutional layer together where every single stack is going to increase their receptive fields by plus two. Okay? So that's why did you do next? Well, get around this by second many compositions together. Any questions there? Three by three filters. With a three-by-three filters and NOPAT equals one the output with always Matzke and quickly. Did I answer your question? The question is, do we want to be smaller? In deeper layers that we're going to use the pooling layers to reduce the size. That leads me to the second question. So genitals question is, is it the case that having a stack of three of these three-by-three filters, is it better than having just 17 by seven filter, either in terms of expressivity or in terms of number of parameters, right? So let's go ahead and firstly, the parameter question, which is, which has more parameters, is it? Yeah, I just want to point out that there is something simpler. So you have an extent where he explains that while they're in that if you have a deeper there so they can eat glass. Why they're not independent. So the model generalizes. Okay, yeah, so tomboy is referring to a Professor, Sorry, I can agree. Who published the paper four months ago explaining why the deeper layers are better. And I'm just going to I'm not sure I follow it exactly what you said. Write it down for the acinus look up. Alright. Okay, so VGG night, which layer has more parameters? 17 by seven filter into the F net, or three of these stacked three-by-three filters and VGG neck. Alright, so let's go ahead and do this calculation. We have Z F net, and it uses 7-by-7 filters. We're going to assume that the input depth is going to be some variable CAN. And then the number of filters, which will be the output of this layer. I'm going to call C. Alright? And then I'm going to, I'm going to drop the biases moving forward. So we're always going to count the weight parameters. So if I count the number of parameters here, the, each filter would have seven times, seven times the input death number of parameters. And the number of filters I have is c out, alright? So the total number of parameters and one CFF layer that has a seven by seven coalition is viscosity of repair. For VGG net. We're going to stack three of these three by three filters. Alright? So for a three by three filter, the number of parameters will be three times, three times the depth of the input. And then I'm going to have c out of them. And that's the number of parameters in just one of these three-by-three filter convolutional layers. I know that three of them. So I'm going to take this number and multiply this by three. Okay? So just to simplify it, if I just call CNN Ci equal to some constant, big C, G, F net would have 49 c squared parameters, and VGG net would have 27 times c squared parameters. So the first thing that you can see is that high stacking these three-by-three filters, we actually have a reduction in the number of parameters compared to Z F net buyer, the function will be a factor. That might be one trout, which is number of parameters being less. There's also a capacity argument. And this is going to be a bit more intuition and maybe where the fruit is in experiments. So why might need three-by-three filters turned into a good thing. So I want you to remember that after every single comp layer is a rabid. So there's a ReLu over here. There is a ReLu over here. Then if we add one more layer down here, there would be rabies over here also there. Where it says Ye Fnet is just going to be one convolution followed by a ReLu. And VGG. We're going to be stacking three layers. We're going to have three values. And this means that in terms of the computation, there are more non-linear operations. Practically empirically, what we see is that stacking these and adding more nonlinearity leads to better performance. Because in the gradient descent, the stacking of these phenomena charities allows there to be more expressive features capture than just having one nonlinearity. So why might it be a good thing? Jackie, these, we introduced nonlinearity and then empirically, this leads to better performance. The question is, can I go over what the three stack three-by-three filters looks like. In terms of the implicit, the outputs. I will show us, find that back in just five. So if it's still on there, please raise your hand and ask it again. I'm sorry. Can you repeat the question? Oh, yeah. The question is, is the increase in receptive fields for these three-by-three filters, always two. The answer is yes. So if you were to just draw out what the next layer would look like, you would see that it also has said that the effective receptive field of this neuron is now seven by seven. The question is, what if you have five-by-five filter? We would have to draw that out because I can't, That's something that seems simple, but I can't do it in my head. So I guess it would be so the first would be five-by-five, and then the leftmost would see a self, a five-by-five. So I believe interests by four. So the first way it would have receptive field of five. The next one it would have a receptive field for nine. I believe that's the answer. I see that it can rupture. Okay. So Russia saying that the receptor field will increase by W S minus one effective receptive field. Yeah, I'm pretty sure that's correct. The question is, we mentioned that different filters may pick out different frequencies, their features. Can I explain that a bit more? So I don't know if there is an intuition to over whether you have a larger filter should get the high and the medium or the low frequencies. But at least the optimization that ZF that did where they visualize the features with this decomp, identified that 11 by 11 large filters. We're good at picking up high and low frequencies, the missing medium frequencies. Because these filters are optimized to reduce the loss. Difficult for me to think of an intuition for why that would be. But that's all I'm going to leave that as a terrible results. Other questions? Alright. So why might stacking three-by-three filters turned into a good thing? This is because there's more nonlinearity. There's more nonlinearity. E in this case, three re lose VGG net versus one ReLu four. Yes, ma'am. All right. There's one more potential. The question is, why is born nonlinearity a good thing for the model? So this is where we then run some new variable results. So at it, having more layers and more nonlinearities up to a point to be getting better performance. You can think of maybe an intuition being born on linearity allows you to learn more complex features that maybe number for classifying. All right, there's one more con, of using small filters and more layers. And this is just going to be memory. So if we look at the memory of Zf match and VGG net used to store the values of the activations in the past which we get cash or cash. Remember, the F net, we have 17 by seven filter. And just apply that hat equals one. And so on. W out, the width of the output would be wn minus the width of the filter, seven plus two times pad, which is two plus one. And so the output would be wn minus four. So the output of 17 by seven filter is, it's going to be a width of wn minus four and then a height of h and minus four. And of course, however many ports are. What we have, we have that many of these matrices. For VGG net. We're going to have three of these three x three filters applied at cat equals one. Remember that 1.3, the input into the output width are the same. So at the output of my first three-by-three convolution is going to be a size wn, Hn, that I'm going to pass back through another combination. And that output is going to be WN h, n. And I'm gonna go through one more three-by-three convolution for which the number of neurons, all right, In the output will be Wn by agent. You can see therefore that since I have to calculate all these activations for VGG net versus just one output activation for Z F net. That essentially comparing these two layers, I need to store more than three x amount of activations compared to the F9. So this will be more memory intensive to hold the output of all the faders. Questions there. Alright, so this is a VGG architecture and I hope it gets towards question from before where we show exactly what's going on. Vgg net is going to process 224 by 224 by three images from the ImageNet. Do a similar company to AlexNet. Then what begins is first two columns. Remember there's a rabid following each column, where we have 64 filters. And remember, every single filter is three-by-three. So that's why I haven't been the filter sizes. So in this first convolutional layer, there are going to be 64 filters, each. Three by three by the depth of the input, which is three. These are applied at PAT equals one. Then if I look at this layer, the second column, there's gonna be 64 filters. And now each of these are going to be three by three by 64 applied at pad equals to coordinate. Alright? Because the width and the height are matched, the first two-dimensions remains to 24. And then the depth is just going to be the number of filters in that way, or 64. Alright. The pool is this two-by-two pool of plaintiffs trying to and that reduces the width and the height by a factor of two. Then following that pool, we do our three-by-three convolutions again, there are two of these data, but another pool that reduces the width and the height by two and then restart the three com filters to all three columns, 43. And then finally, the boy connective errors. Any questions on this architecture? Instead of each question is, is there a ReLu following each column? Yes. So every single one of these has a rail in-between. Other questions. Question is, how do we go from full to FC? Great question. Who is a seven by seven by 512 texture. And away we go from four to FC is just like how we go from in the homework. We go from a 32 by 32 by three image to a layer with say, 100 artificial neurons. The way that we do that is we reshaped 32 by 32 by three into 3,072 vector. And the fully connected layer would therefore be 100 by 30 72. Here this fully connected layer is going to be 4096 by the product of seven times seven times 512. So we received this thing into a vector and it'll be perfect. Yeah, So the student noticed that every single time we do a pool that reduces our width and a height by a factor of two. The next column player, we increase the depth by a factor of two. Why did we do this? The reason we do this is so that the number of operations, the number of multiplications in every single layer, It's the same. Let's go ahead and write this down. So if I look at the number of multiplications, this is the same as the number of connections that calculation that we did for the minute. If we look at the number of, ofs in this layer, the number of operations will be the number of operations per neuron times the number of neurons. Let's do the first part. How many operations are there per neuron? So for a given neuron, the neuron is doing a three-by-three convolution, right? Match to the depth of the input. So every neuron. The output of a three-by-three convolution and adapt is a depth supplier input which is 64. So every single neuron is doing a filter that has three times, three times 64 multiplications. Then we multiply this by the number of neurons in that layer, and that's the total number of operations. So the number of neurons in this layer is one-twelfth times one-twelfth times 128. So this number of apps will be one-twelfth times one-twelfth times 128. And again, I'm dropping the biases here just for convenience. If we then look at a different layer. So if we look at this layer here and we count the number of hops, the number of apps will be the number of operations per neuron. So the neuron does a three-by-three convolution. And here the depth of the convolution will be 128. This is three times, three times 1.28. And then this multiplies the number of neurons, which is 50656566 times 56 times 256, right? And then the purple, the purple and the red numbers are equal to each other because in the red, I had one-twelfth, One-twelfth, they got divided by two. That's 256 is. But then once 28 got multiplied by two to the 6.64, we got multiplied by two to 120 years. So these two numbers are the same. And that's why whenever they decrease the width and height by factor of two, increase the depth by a factor of two. And that leads to the same number of operations for every single. Question is, why would we want the same number of operations per layer? I would say. So that there is no bottleneck layer, e.g. that is either taking up more time or doing something that's much more computationally complex on the other layers. Alright. So that's the VGG architecture. We mentioned that there's this problem of memory. And for VGG net, we have to store many activations which are the output of all these three-by-three convolutions. So in this next slide here, what I've done is I've written out the memory needed to do this one for cost of VGG. So the memory needed to store the activations is just going to be the size of the activation. So that's gonna be the width times the height times the guy, right? So what we do here, first we calculate the number of activations and every single layer. And then every activation for every value is going to be single float precision. So it's going to be four bytes. Reactivation is four bytes. If you add up all of these numbers for just one, for passive VGG net, you're going to have to compute 24 million numbers. That's just the sum of all of these. Then if the other 24 million numbers in each event is four bytes, then just to compute a fourth pass requires 96 mb of memory. Alright, and I remembered you have to catch these valleys because they have to be used the backward pass as well. Backward pass, you're going to have to continue all those gradients. And so you're also going to need about 100 mb of memory just to store your backward pass. Any questions there. All right, and then let's do one more thing which is to calculate the number of parameters in every single layer. So we're doing here the same calculation we've done already, except I be dropping the biases. So let me just write that here. I'm ignoring the biases so that it's clear to anyone reviewing the next day they're on. I want to know the number of parameters in this convolutional filter is going to be the size of the filter, which is three times three times the depth of the prior layer, and then times the number of clusters. I can write. The point of writing all of this out is for you to notice the trend, which is VGG net, has over 100 million parameters, right? If you sum up all of these numbers, the total number of parameters and VGG net is 138 million parameters. But if you look at where that 138 million just concentrated in these fully connected layers, right? You sum up just the number of parameters and the fully connected layers. The Fc layers. They comprise 122 million of these 138 million parameters. So really the thing that is making this VGG net very large, It's just the last three fully connected layers, not the convolutional layers. And that's entirely consistent with what we talked before in the past regarding these convolutional layers some way after the CNS also have a baby's? Yes. So there's a rabid after this one? A rabid after this one. There's no rabid after this one because this is the soft max score. So this goes directly to your song. 1,000 scores. The question is, what is enough? What is the way for us to reduce the number of fully connected layers at the end. The asteroid will be the next architecture. We've talked about people that will remove all of these estimators and we'll see that it's much more likely than VGG. So let me summarize. For VGG net. It has these calculations. A lot of memory is required just to compute the forward and the backward pass 100 mb for one-four past. There are 138 million parameters, but then a lot of the number of parameters are in the fully connected layer where there were 122 million people networks we'll talk about next as well as at school. That would definitely remove the fully connected layers. And that will be two numbers moving forward that are far fewer parameters. And VEGF, VEGF is one half of March or CNNs are a few other experiments. So using this idea to step into three by three filters, they tried to do an extended to say is deeper better. So this is a table from their paper. And the identities are given by these later bachelor's ABCDE. That just corresponds to the number of layers because I did a bunch of other experiments. So, so all you need to know is for this table, as you go down it, you'd have more layers. And the networks that do the best that achieve the lowest error are the 16.19 layer neural networks. Alright, so deeper appears to be better. Although we're soon gonna find out that you can't just arbitrarily Nicky, just deepest. Here are some other details about VGG net. So the image input is 224 by 24, just like AlexNet from a crop, they do the bubble means traction. I had mentioned that Alexander has this local response normalization layer of VGG, did the experiments to find that it didn't increase the performance and so therefore they removed it. Vgg net use stochastic gradient descent with batch sizes of 256. This is double that of AlexNet. And they also just use momentum. They had an L2 regularization penalty. They had dropped out for. The two-point can have two layers, the ones before you get to the softmax, they adjusted the learning rate the same way as an artifact. So they also yield that learning rate whenever the training loss factor. Alright, when VGG net was treated, Xavier initialization that we talked about was thought, yep, now, we know that the visualization is particularly difficult when the number of layers gets more. So they actually did something creative here to try to get their network to converge. Which as I said, they wanted to train a 16 layer VGG network. They would train a shallower one, let's call it 11 layers. And then after the 2011 layer of VGG, not that was used the parameters of the first investment layers as the initialization or attending a 16 their neuron. Remember, that's how they got around this initialization problem. Later on they found that solve your initialization was sufficient to initialize your network to get good performance. They performed. Data augmentation is including horizontal flipping, random props and RGB shifting that AlexNet VGG that took two to three weeks to train it off for GPU machine. So that's a considerable amount of time. And by the time of their submission to have finished seven of these factor Xa. So the output that got them there, I error rate as ensemble of a sudden. We're all set. Question is, was this two to three weeks per net or for all seven months? I believe it was pertinent. But you could have had them on parallel computers. Alright. Any other questions here? Other question is what would be the chain time for a high-end GPU today? I'm not sure the answer is anyone to speed up much faster? How much data do that? I agree with that answer. Correct? Yeah. These are the key cues, again, from Twitter from 2014 and so it's been almost a decade. Question is what stopped them from using 16 GPUs? I don't know, but I have some insight, which is that these are academic labs and we only have so much. Any other questions. Right? The question is, what is this 1,000 in the last later, this is the number of classes and image tag. Image tag has 1,000 process doc, but I can see part-time. Alright, so that's the VGG net, but not absolutely win the ImageNet competition in 2014. And at least at the start or at the end of lecture here I want to introduce some take-home points of people met and show you the basic inception module, which is this idea that they introduce to take advantage of the fact that different convolution filters, they pick out different features. Alright, so here are the main take-home points that buccal net was able to get to 22 layers. They introduced this inception module, but I'll show you in just a few slides. They get rid of the fully connected layers. And remember, the fully connected layers are where most of the parameters are. So Google net has only 5 million parameters. That's 12 x is less than AlexNet and 27 less than VGG net and Google net energy. In 2014. Inception module that they introduces a good idea. Part of the reason they were motivated to remove these fully connected layers is that they wanted to keep the computational budget. They wrote in the paper that they wanted to do this so that it's not just an academic exercise and they need some really fancy and GPU to be able to train these neural networks. And so I believe that the authors of Google net actually trained this on a CPU. And they won ImageNet in 2014 for the top five error rate of 6.7%. So this inception module really has two goals. First, we'll talk about is to let the network take out the most important features. Rather than us saying everything is gonna be a three-by-three convolution will see that they allow other types of publications or other sides of the composition. And then the second point is to reduce computational expense. So we're not gonna be able to get to the second point today in terms of how they reduce computational expense. But we're going to see that their architecture and incorporates some really clever, a really fun idea to make the computation a lot. Alright? So people that again was motivated by the spinning of saying going deeper is better. But like we saw with VGG net, as you go very deep and you have this boy connected layers, we have more parameters, more computationally expensive. So how do we address this? And this is through the inception module. So this is what the naive inception module, but this is my E because rebuttal to make several modifications to this, to make it computationally efficient and to not explode memory costs. But the basic idea is the following. They call this network and network. This whole block here is One inception module. And the idea is the following. You have the previous layer, this is your first layer, then it's actually the input image, right? And what you do is, you do one-by-one convolutions, three-by-three and five-by-five convolutions as well as max pooling in parallel. Right? And then because he's convolutions are of different sizes, are extracting different features from the data. So you can think of these different operations as all extracting different features of the data. And then what they do then is they concatenate all those features together. Again, this is the naive version because it's that I did. It just concatenate everything. We'll see. But they concatenate these pieces together at a high level. And then that becomes the input to the next layer. So now the next layer will have the features from the compositions of these different filters as well as the matrics pulling. The next leader can choose how to weight those features better. So there's inception module. I give the flexibility of saying, I'm not sure if three-by-three compositions are the best. Maybe five by five bar, we're just going to do them altogether. I mean, the next layer. The next layer. When we come back to lecture next week, because remember Monday is a holiday, Wednesday to determine the Monday after that, we'll get into the details of how this inception module works. 
I hope you're all staying as dry as possible. That's urinalysis important again, the first is that homework number five is due this Friday, March 3rd, up with the base scope. It is our last homework assignment. And I hope you all will the sheet that over the course of the CIFAR ten dataset, we've gone 35-40% accuracy, which was an assignment to with just a softmax classifier to oversimplify percent accuracy with CNN. So almost doubling the performance by using neural networks. On Friday of last week, we uploaded the project and its accompanying data sets to learn. The project is going to be due Monday of finals week. So that's gonna be March 20, 2023. And on the project, you're going to be allowed to use whatever code you desire to use. So that includes applied towards TensorFlow and Keras. You can use any deep-learning library you want. You all haven't had exposure to deep learning libraries. So what we're going to do is on Wednesday. The next lecture. Having two of my PhD students cover how Keras and fights work, work. And you'll see how these deep learning libraries really make implementation of neural networks straightforward, such that you really don't have to understand how neural networks work to be able to train them. Of course, because all coded everything from scratch. You'll know how everything is working beneath the hood. But yeah, we'll have these two PhD students on Wednesday cover the particular syntax and how to get started on the project using Python. With that lecture, we're also going to provide some starter code to implement either CNNs or RNNs with Pi, torch and Keras, alright? And then in the project guidelines, if you haven't read them yet, you also give you the option to do a custom projects. You give guidelines for what that customer project should cover. And I just want to highlight. So you all know that if you want to do a custom project, you need to send that request to me. Nobody can describe it. Are there any questions on the project or the guidelines that we uploaded? Right. And then lastly, the TAs are working on grading the midterm exams. And our goal is to release midterm grades back to you by the end of lecture on Wednesday. Any course, logistics questions. Question is for the project, are there any architecture? So you have to please take a look at the project guidelines. Basically, for the project will require you to connect something that we took her in class, that is Post CNN. So the most natural one will be the one that we, We'll begin today, which is RNNs. But you're also welcome to implement things that we may not have covered in class. So in class we'll also cover variational autoencoders, generative adversarial networks. We may also cover transformers. We don't have that material for today, I did, but I'm working on a new lecture that hopefully will make it in time for, for this iteration of the class. So you don't have to expend something that's after CNN's in the project. Other questions, great. Rock asks, Can we use late days on the projects? Are not allowed to use on the projects. So the projects all have to be submitted by Monday, March 20, 2023. Any other questions? Alright, we are going to get back to material. So our last lecture was quite some time ago. So just to recap, what we were doing is we are beginning or we were welcomed into the survey of neural network architectures. We talked about the AlexNet, the first CN end up on the ImageNet competition. We've talked about ZF nets, which is basically AlexNet foot with hyperparameters optimize and not one in 2013. In 2014, there were two architectures that were remarkable. One is VGG net, which we talked about at the end of our last lecture. And that was this neural network architecture that went deeper to 19 buyers. And remember, the key features that they did was they made every convolution, a three-by-three convolution, and started to stack them. So there would be for ductile three consecutive convolutional layers. Alright. And then last lecture, we were just about to delve into Google neck. And so we just started the motivation of the woman, which was a few things. I wanted to first propose a new architecture. That relies on this thing called the inception module. And the basic idea behind the inception module is what we choose hyperparameters for a CNN, you can choose the filter within SOC as a filter with a title. But which one is best? We're not sure. Why don't we do a bunch of these different ones and then let the network pick out where the most important features. Alright, so that's number one. The second motivation is that they wanted to reduce the computational expenses of these neural networks so that they're not merely an academic exercise so that people can train them if they don't have huge computational servers. So Google has only 5 million parameters. And so that's far less than AlexNet VGG net. We're also going to discuss one key innovation that they make any architecture that keeps them computational expense dance. So this architecture won ImageNet in 2014, and it did so without training on a GPU. Alright, this is in contrast to VGG net, which had multiple GPU servers and it still took three weeks. But pupil that is able to train without a GPU. Any questions on motivation? Alright, so let's see how Google net does this. And this is a basic building block if you go out with just the inception module. So in this image here, this thing here is the input from the prior layer or the actual charge. Sorry, this is the input from a prior layer. And then what you see is that this input gets sent to convolutions of different sizes. So Google net will do one-by-one convolutions, three-by-three convolutions by by comma, five-by-five convolutions and max pooling. And you can think of each of these different parallel blocks as extracting different features of the data. Then what people will do is it will concatenate all of these features together. And I'll send that to the next layer. The next layer just to see the output of the one-by-one, three-by-three or five-by-five convolutions as well as the max point. And then it gets to determine which of those because it wants to use to be able to maximize, minimize the loss. Alright? Any questions there on this basic idea? The question is, we don't do pulling after convolution here. At least in this illustration. We just do pulling on the previous layers and quick to maybe try to extract and variance position invariant. Student asked, how is this less computationally complex than same VGG that because it sounds like we're now, instead of doing just three-by-three convolutions, doing a bunch more convolutions and concatenating them together. So we'll get to that in just one button. The question is, what is a one-by-one convolution? So a one-by-one combo regime will be a filter that has a width of one and a height of one. But remember that gap is matched to the input. So let's say the player input had a depth of 256. So it'd be basically a one-by-one by 256 filter, as you can think of it as combining across the features of cost. But the third dimension up here and protect, sir? Sorry, can you repeat the question? Yes. Perfect gases didn't ask. Here we're using combinations of different sizes were built. These produce different size feature maps. And then if they do have you concatenate them so well, adapt right now. So let's say that our total net receives an input from the past layer, that's 28 by 28 by 256. Alright? It does these convolutions. So the first question is, what's the size of the output of a one of the, when I say 128, I'm going to have 128 filters here. Each of them are one-by-one by the prior layer. So these are one by one by 256. These are 192 filters that are three by three by 256, etc. The output size of the 128 one-by-one convolutions. This is straight forward. We know that w l equals Wn minus Ws plus one plus two times pad. Alright? We're not going to pad here, right? So if it's no padding is just gonna be WN exercises one-by-one w at this one and we have a plus one, right? So the output size will also be 128 by 128. Sorry, I misspoke there. The output size of these one-by-one convolutions will have the same width and height. So 28 by 28, because there are 128 filters open, 28 by 28 by 120 years. Okay? So the output of these 128 one-by-one convolutions, it's gonna be 28 by 21, 22. Any questions there? Okay, so to be able to concatenate these tensors together, the size of these convolutions also have to be 28 by 28 and width and height. Otherwise I can stack them together. So the next question is, what pairing do we need to keep the output size consistent for the 192 filters that are three-by-three. Alright? And we know that if we set hat equals one, right? We said add equals one. Then at the output here will have the width and the height. So 28 by 28 by one. So basically what we do in Go on that is that for these convolutions was set equal one for the three by threes, and then paddy equals two for the five-by-five. And that guarantees that all of them have a width and the height of 28 by 28. And therefore, we can go ahead and concatenate them all together. There's also one more thing which is a three by three fourths of a three-by-three pool is not that the pole shifts is overlapping and the overlap is chosen so that the feature at the output of a three-by-three pause until 28. 28. Then the depth of the three-by-three pool will be 256 because it's three-by-three pool will be applied to every single every single matrix in this 3D text or every 28 by 28 matrix. Any questions on the sizing of any of these? Great, yeah, so Jenna's question is when we concatenate with the candidate or the third dimension, That's exactly right. So we just take all of these features or which we have 128 from the one-by-one convolutions, one entity from the three-by-three of sector and we just stack them. And so one really involved with the textbook. So this will be 28 by 28 by 672. Any questions there? Does anyone see something concerning there? Perfect. Yeah. So the student says, we just keep doing this. It's going to grow and grow. You'll notice that the output of this concatenation is never going, that the depth is never going to be smaller than the input. Because this three-by-three pool will always have a depth of that matches the input. And so if an addition to this, I'm concatenating other features. This number after the concatenation is always going to be bigger than 256. Or the emphasis mentioned. The question is, why are the dimensions of the three-by-three for the width and the height, the same as the input. Yeah, so this is traded at such a number which I can get off the top of my head such that the app is 25, 28. So it's not strike it at three, but it's treated as either one or two. Alright? So you're going to address this problem in just a few slides. But before that, I wanna do one more. I want to do one quick calculation because it's going to relate to how we fix this problem and how the author reduce the computational expense of people max. So the question is, how many operations are there in this inception layer? This will be how many operations are there in these combinations? We've done this calculation many times, and so I'm gonna do this quite quickly. But stop me if you have a question. So let's look at the one-by-one convolution. So for the one-by-one convolution, we're going to ignore biases. Number of operations for just the one-by-one convolutions is the number of output neurons I have, which is 28 by 28 by 12828 times 28 times 128. This is the number of outputs, output neurons. And then this is times the number of operations it takes to compute the value of one output neuron. So one output neuron is the result of a one-by-one compensation. And this one by one convolution has a depth of 256. And so the number of operations here will be one by, one by 256, the filter size. Any questions on that calculation? All right, so then let's do another one. For the three-by-three Kong. Number of operations will be the number of output neurons, 28 times, 28 times 192 times the number of operations to calculate one output neurons activation, that'll be three times, three times 256, the size of each of these three-by-three filters. So I go ahead and calculate that number for the comp layers, the one-by-one to see if I can multiply by five comparable things. Then the total number of operations in this particular inception module, we've drawn out, it's going to be 864. No, Ian, any questions there? Okay, so here comes one of the new innovations of glucose. So remember we have two goals. One goal is that we want to make sure that when we concatenate the outputs of these, that this number is to just continually drilling. Alright? And then the other goal is to reduce computational expense. So what we're going to do is we're going to insert additional complications. So what we're going to do is we're going to have a new block. I'm going to call this a block with 64 filters, each of them doing one-by-one convolutions. And if I put this block somewhere in this computing chain, right? Then what we're going to do is we're going to effectively decrease the number of computations. And it will also be a way for us to decrease the number of features if we so desired. So I'm gonna give you two options. Just looking at the three-by-three convolutions, I'm going to tell you that we have the option of placing these 64 one-by-one convolutions after the three by three ones. Or I could put this filter in purple right before the three by three ones. I might have misspoke oranges. I put the 64 one-by-one convolutions after the three-by-three filters. And then purple is I put it before. Okay? So let's just worry right now about the question of which one is better from a computational expense perspective. So I want you to take 30 s to a minute to think about. Is orange or purple better in terms of reducing the number of operations in this backward. And feel free to talk to your neighbor. Alright, excellent, good discussion. This is not an easy question to answer in 1 min, so don't worry if you're right or wrong, but I'm curious who thinks it's better to put these one-by-one convolutions before the three-by-three? How about after? Okay. I would say more answered that. It's better to put it before and that is correct in this case. So let's see why this is. So what we wanna do is we want to calculate the number of operations. In the case that we put these 64 one-by-one convolutions before versus after and see which number is smaller. So we put it before. Then. The output of the 64 one-by-one convolutions will be 28 by 28 by 64 because we only have 64 filters. And then it'll be times the number of operations that it takes to calculate one output. And that's the size of this filter, which is one-by-one by the depth of the input, which is typically six times one by one by 256. And add to that, we then add the number of operations for this 192, three-by-three convolutions. And so that will be the output of the output size of this filter, which would be 28 by 28, by 192 times number of operations per activation, which will be three by three times the filter times the input depth, which is 64. So it's gonna be three times, three times 64. Any questions? Alright, let's do the after one step we put it in after the number of operations is going to be the output size, 28 by 28, we do the three-by-three convolutions first, so the output is 192. And then each of these is three times, three times 256 operations per neuron. And then we have the one-by-one convolutions. So the output will be 28 by 28 by the depths of those one by one convolution is 64 times one times, one times the depth of the entire layer, which will be when added to it, because that's the output of this three-by-three convolutions. Want me to review or go over either. Alright, cool. So if you look at these two numbers, Let's look at this 128 by 28 by one. I did save that here, and that's also here. We have a three by three by 64, gets a three by three by 256. And so we can see that if I just compare these two numbers, this one is smaller by a factor of four. Then if we look at these two numbers, 28 by 28 by 64 times one by one by 256 here, and one by one by 192 here. This number is bigger than one, I need to, but it's not for x bigger. Alright? So some of these numbers is smaller than the sum of these numbers. And therefore putting these 64 one-by-one convolutions before the three by threes results in fewer number of operations and then less compensation. So Daniel's question is, in this case the output filter max are still 28 by 28 by one by six sounding too. And remember the, the key problem here is that the three-by-three pool, it's always going to have an unmatched. In addition to putting these 64 one-by-one convolutions in front of the three-by-three or five-by-five. I'll put us the one-by-one convolutions to after the polls reduce the staff from typically 60, 64. Possible, in this case, the concatenation, it will still be larger than the conflict, but now it is possible to choose the number of doctors so that the concatenation is smaller than in this case if we were to calculate all of these. And I saw right before lecture that I have a typo here. This is 28 by 21, 28. If you were to do these operations. This case, again, the concatenated outputs on larger depth. But I could have reduced the number of filters to make that number smaller if I wanted. All right, I want to pause and ask if there are any questions. Great. The question is, where do these numbers but 192.96. Those are arbitrary hyperparameters that we get this. So we can make this smaller. Alright, so we have said that putting these 64 one-by-one convolutions before the three-by-three or five-by-five should decrease the number of operations. If you go ahead and do this calculation, which I've read snappier, then the number of operations in this inception module is now 271 million. And that's about a four times reduction from before we had the one-by-one convolutions where it was 870 million or something like that. So these one-by-one convolutions reduce the amount of computation by a factor of three to four. Alright, question for you all. Isn't there a concern that we are going to lose the information through these one-by-one convolutions. Basically, I see a bunch of students thought and gas jet is thought to be a difficult question. We definitely lose information because by doing these one-by-one convolutions, we're taking information that was across 256, 28 by 28 matrices that we're compressing it down to 64, right? So there is a very real concern that whenever we do these one-by-one convolutions, we won't be losing information. That may be now this three-by-three convolutions couldn't pick out because that information is gone after these one-by-one convolutions. So that is a concern. But interiorly. And again, this is one of those things where I say the empirical results to make the argument. The performance of these networks is still, it doesn't degrade. And it doesn't degrade. Meaning that this tensor, which was 28, 28 by 256, can really be dimensionality reduced. It can be squeezed down into 28 by 28 by 64 tensor and still contain the relevant information that will be necessary for performing classification. Alright? So, yes, information is lost when I put in these one-by-one convolutions, but empirically still do well with the additional benefit that would reduce the amount of computation. Yes. So the question is, why don't we just do three-by-three convolutions and have 64 and then e.g. right. Yeah. So there is one additional benefit of having this one by one convolution here, which is that we get an additional layer of nonlinearity will be given additional layer in a neural network. So this actually counts as a two layer inception module. And because there's another linear followed by ReLu transformation that also empirically helps to increase performance. The question is, how do we change the size of the feature map? Because we always have a one-by-one convolution. So in the Google net architecture, what they'll do is they'll have these 28 by 28 by hover feature depth layers at the output of the instruction module. And then they'll reduce the width and the height by using that spores. The spores will decimate by a factor of two every single time they happened. Is it weird to think of the inception module as a mini auto encoder. In each layer. I see where you're getting with that Daniel, that. So I wouldn't say it's on an autoencoder because it's not trying to reproduce the layer input, but it is a dimensionality reduction, which we also know is that picture of an autoencoder. Intuition of reducing information, squeezing it into the 64 is correct. Alright, so that is the inception module. And this is a figure from the Google Map paper showing through the entire architecture where you can see I've had to shrink it to fit it in this slide. But basically each of these things where you see like these rows of convolutions and the max pool. These are all your different inception layers. And many of them are something that are very deep neural network. Each instruction layer. This is the overall architecture in terms of the order of operations. So the column max pool, comp max pool, and then after that they have stacks up in sections, followed by next calls. Every single one of these inception layers. To write the depth of two comes from having these additional one-by-one convolutions. And so when you sum up then all of these inception layers and convolutional layers, you get that this is overall either 16 or in 19 bear neural network. Let me go to my fireflies, I tell you the right number. Oh sorry, 22 layer neural network for people. Then there's one more feature about That's interesting, which is, if you'll recall, when we talked about VGG at 100 million of VGG nets parameters, we're in the fully connected layers. At the output. Gets rid of those fully-connected layers entirely. So it has columns and max pools. And then you'll see at the end, all they have is they have a linear layer. This is necessary to get from your features to your 1000s softmax scores for each of the 1,000 classes and ImageNet. So that's just unnecessary linear layers to transform it to softmax scores. But aside from that, there are no other additional fully connected neural network layers here, but that's also probably kept the number of significant B since there's no FCS kick. Yeah, great. So just saw that there are three softmaxes here. Yeah. Yeah. That's a really astute observation. We're going to talk about that on the next slide. Yeah. Any other questions on the architecture here? Yeah. The question is in the max pool is bear, you mean within the interception module? Is there padding? Almost surely, yes. So I don't I don't know the stride and the cutting off the top of my head. So I'm going to ask rocks at their tongue what it says, calculate or look up in the paper what the straight into the setting of the players are. So if the infection Any other questions? Yeah. Yeah. The question is, what's the difference between average pooling and max pooling on in terms of like what might give better performance. I don't have a good answer for you. I'm pretty sure that in this case they probably just tested both in one byte for better performance. And the question is, why is Trump got only applied to one layer? As opposed to all layers usually applied on fully connected layers. And so that's why it's only applied on this last linear layer at the antipsychotics like a fully connected thing without the, without the activation. Other questions? We're going to answer Jake's question next switches. What are these additional softmax classifiers here? So we're going to discuss a problem with Google Maps that lead to them having auxiliary softmax classifiers. And that will ultimately be solved by our next and last Architecture top advisor. So this is the architecture of BubbleNet where I've reduced the inception of layers to these yellow blocks. And this two x means that there are two of the assumption layers, vaccines, there are five. Alright? Then I just also want to point out again that I'll extend them. Vgg net had many more estimators here, which is why they have so many more parameters and go that only has linear layer to the soft-max. That's why it has so many fewer parameters in this architecture. This architecture, this architecture has 22 layers. And what that means is that at the output, I'm going to calculate a loss. I'm going to call this loss L two for now. And how to update the parameters of my convolutional filters and session layers which include convolutions. I need to backpropagate through 22 layers. Right? I mean, we know that there's this fundamental problem of deep learning. Where would I do repeated iterations of my chain rule across 22 layers, grab my gradients, might vanish or explode. And therefore the gradients when I get my stayed at the activations in layer of wine were called H1. If I were to compute d, L2, the loss DH one, or this one could have exploded or vanished, right? And therefore, my weight updates and these earlier layers might not be good updates. And if the early layers aren't identifying the features, I'm never going to get good classification. Alright? So in some, because there are so many layers and backpropagated gradients. When I back propagate all the way to the start, can be very important gradients. And therefore, there will be no learning that happens in this architecture. Google Maps solve this by adding two auxiliary classifiers. So what they do, what they did is they took the layer 11 activity. Let's just pull that activity H 11. They took the layer 17 activity, we'll call that age 17. And then took the HL7 activity. And they put that through a couple of fully connected layers and then to a softmax. So this would give a loss L one. This loss, remember we called L2, I'm going to call this loss L. What this means is that when I back propagate, I don't just have a backpropagation through L2, right? But I'm now also going to backpropagate from L1. And when I do that, I'm gonna have to calculate a gradient at this point. We'll call this gradient D01. D. Alright? But now the gradient from L1 only has to pop back, propagate through 11 more layers. Alright, so what's 22? And therefore, the gradients from this pathway will lead to reasonable signals or reasonable gradients, sorry, to update the weights of these convolutional filters. So that by layer h 11, these features are good enough to perform your ImageNet task, right? Same thing for a 17. So you can think of this as reducing the effective depth of the number because it's saying by later HE, but then you have to have good enough features to do my task. Also by their age 17, you have to have good enough to just do my task. And then of course, throughout the entire network should be able to solve any questions on that. The question is, doesn't this add a lot of parameters? It does especially because there are these Fc next. However, when they then do inference, so when you want to actually make a classification, you don't need these auxiliary classifier. So this is only in the training process. But then our last, our final architecture or the final architecture they submitted for the competition is just the sector path. Great question is, how are the losses for L1 and L2 and L3 calculated? All of these are cross entropy loss. So these will be the same cross-entropy loss for by 1,000 classes of ImageNet. Just Try to decode it from just trying to classify from h 11 versus age 17 versus the entire network through losses. Are you aware that the question is, do we put equal weights to all the walk or do we wait L2 more than L1 and L3, I believe in the paper. They waited them equally with 0.3? Yes. Yes. That fingers from seven. Decent enough. It's a tomboy. Question is, if you have a good loss over there, 17, why do I need the additional five raters? So ignore this point for now. Because it says that it had a minor effect. But it could be that your loss after these five layers is less than the loss from the upper layer 17 activity. So that is the congregations in these layers can still be extracting out better features to make L2 and L3. You are using. Explicit function. Says In general, L2 should always be less than L3 because L2 gets additional parameters to reduce the loss. And that intuition is correct, although we're going to see that practically it doesn't always happen later on. So I can see that two events ingredients. But I would say, that's a great question. So the question is, this student says, I can see how this would tackle vanishing gradients, but has tackled exploding gradients. That's a really great point because if I backpropagated here, I say D L2, d h 11 by say also backpropagated through this path to get a deal three, d h 11, right? The total gradient at age 11 will be the sum of these three paths. If we have exploding gradients, then this thing is going to dominate, right? I'm just going to be a really big gradients that won't help learning. So really this type of approach helps with the vanishing gradients because in the case that they banished, this goes to zero, but we thought had good gradient signal from L1 and L3. So then the question becomes, well then how do we deal with exploding gradients? So dealing with exploding, I'm usually not as bad because we can use the gradient to things that we talked about in the prior lecture to always make sure that our critics have no norm bigger than whatever we said. That hyperparameter to. Your question is, could you actually use this as an ensemble to increase performance? You could. But then remember the ensembling works best when the predictions are independent. And in this case, because the predictions are coming from literally like the same pattern of network, there'll be more correlated. So ensembling is less likely to be beneficial. And given that they didn't ensemble trees, I'm not pretty sure it didn't help much. If at all. Other cups, keys. Yeah. Yeah. That's a good point. Yeah. So the student is saying that it's trying to reduce the computational expense by not using fully-connected layers, but they do have them in these auxiliary networks and so on. I want this increased training time and the answer is yes. Well, I've included them because I'm presuming that without these additional effects innovators, they didn't need this for computational capacity to do better. Their final numbers, they don't report these layers, but you're right that they are there in China and they will increase competition with other questions. Alright, so that is pupil that later work showed that these auxiliary classifiers minor effect and you only needed one of them. I'm presuming that means that initialization, alright? And then like I mentioned, these auxiliary classifiers are discarded and France, so when they actually did ImageNet competition, they only use the central path. Other details that was trained with stochastic gradient descent with momentum. The momentum hyperparameter was there a 0.9? They decrease the learning rate by annealing it by 4% every eight passes through the training data. They used 144 prompts per image, as well as other data augmentation operations. And then when they actually Competed. They trained setting to go next and averaged the results of this happening, bottlenecks or ensemble the results of the seven Google Maps. They did not use GPUs to train this network. And I have four or less do for fewer parameters as we know, then Alex and epigenetic because they don't have this fully connected layers and they want Initiative in 2014. Right there. That's my last slide on B woman. So any questions about people in general before we go to our last CNN architecture? Right? Is that a great? Yeah, The question is, this bottleneck outperformed VGG net with far fewer parameters? And is that a theme of trying to find architectures that are more parameter efficient and get higher performance. There are definitely groups working in that area. It turns out that after the next architecture we're talking about guys, that's there. We're than people who are combining the idea of resonance with Inception and those were running ImageNet competition is for years, but really the more modern day. The key architectures, even in computer vision, you use transformers and those are, those are pretty parameter unhappy. Other questions? Yeah, Tom, I ask these people inspired by minutes. I believe I believe that's why they capitalize this out. For that, not looking at Jordan says just appointment. Alright. Other last questions on diplomat. Alright, let's get into resonant then. So restaurant will be the last major architectural innovation that when you talk about this being is really important. Even when you look at architectures like GPT, right? They will make use of residual connections because this really helps you to be able to take neural networks that are on the order of 20 and go to hundreds of letters. Resonate. Won ImageNet by getting 3.5 to 7% error rate. Remember that human level performance is like a 5% error rate. So this performance in 2015 was super cute. Alright, so here's the motivation for resume. So far, was that going 2012-2014? As long as you look when you look at the ImageNet winners, AlexNet was eight layers, VGG net was 16 or 19 layers, that was 22 layers. So why not just keep adding layers? I should be able to get better and better performance. Alright. So this is a figure from climbing host paper on the resident. He's the one who first author on the rest of that paper. And what he did is he just took a convolutional neural network, one that had 20 liters and one that had 56 layers. Each paint them on condition x. And these are the training errors and the errors that he observed. Alright. So let's just start off by looking at this industry. Very oxygen. So why does this book odd to us? Yes. Yeah. So Daniel says a 56 layer neural network has many more parameters than the 20 layer neural network. So at least forget about test error for now, but look at the training error, right? If you have a higher capacity network, if you have more parameters, different training error should be less than your testing error. That is, you should be able to overfit the data better when you have more knobs to turn your 56 layer neural network. So for training error, I would have expected to see this red line lower than the yellow line. So what we can tell from this is that the 56 layer neural number isn't even capable of overfitting to the training set. In fact, because it does worse than the 20 layer neural network and training error, right? This is actually a hint to us that relative to the, relative to the 20 layer neural network is actually underfitting. Alright? It's doing worse on the training data. And then neural network with far fewer parameters. Any questions there? Alright, so this result is not intuitive. It's already non-intuitive that I did not work with. More parameters would achieve worse training error and then network with fewer parameters. Let's try to extend that a bit more. This result of this bone intuitive and asked me why for a more mathematical argument for why a 56 layer neural network should always be at least able to mask the performance of the 20 layer neural network perfectly down to 20 layer network inside of it. So let me write that down in words. Let's say we had a 56 layer neural network. One way that I can have the 56 layer neural networks match the performance, but 21 layer neural network is that I could copy the parameters of a 20 layer neural network. And then we know that each of these neural networks can be implemented some pretty complex functions, but they can also implement the identity. So after I copy the grounds of the 20 layer network, I can set the remaining 36 layers to implement the identity. And in this fashion, I could've already, just by hand design, designed to 56 layer neural network that's better than the optimized one using the results of this 20 layer neural network linear systems. So tomboy says this is not a linear system. So how could you concatenate blocks and expect it to do that? So I'm going to do there. So the first 20 m don't have to be linear, but the remaining 30, 60 happens infinite money. So you're right, and I guess surviving that are non-linear. Some implementing the identity isn't as simple as setting like a W2 equal i. We may have to tune the weights in a certain way to make it equal to identity. But we would also know for sure that a neural network layer could implement identity if the parameters were separate directory. So that's what we're saying here. The neural network has the capacity to implement the simplest transformation identity. And if it can do that, then the 56 layer neural network has to be at least better than a 20-year corona. Yeah. Oh, great. I'm assuming it's asking we're making an inference based on two points to 20.56. Why not show more numbers in between? Yeah. It would have been nicer if they're stuck waiting for more than just two points. But this is a general trend if you were to increase the D6. So they did, do some experiments later, like 100 something, it would be even worse. Yeah. Great. The question is if we made like 22, 22 with an intuitive result, be still non-intuitive. Yes, I don t know which layer will have the minimum training error. Like, Yeah, you're ready to be 22-year, could be 27 and can be 33. All we know is that statistics is first in twice. Yeah. I'm sorry, can you repeat the question? Oh, yeah. Yeah. So the student is kind of extending out this idea by saying, what did we say in 202-120-2203 events, we see where the inflection point of this training error is. We can definitely do that. We would see is that yeah, there will be some inflection point. But the key thing here is that once you've got too many letters, it's relatively undercutting the content stuff. I'm always asking what do I mean by identity for these 36 layers? What I mean is that there's gonna be some neural network with 36 layers. And if its input is, you know, the 20th layer activity, the output would be the 20th activity Also after these three distinct layers. Without a miscarriage. Or just looking at the time, Let's go ahead and take a five-minute break. When we come back, we'll then talk about how we use this to design the next architecture of the resume. Existing research projects we support right now is like little slow. But what do we write about things that have to do the work for the exact makes sense. Like I write about my results, what I've done. Hi. All right. In fact, like I can't quite get time. 13 or I forget the exact answer. My guess is that it pops. My guess is because this topic Good question about your question about dropout. I was just wondering like, why is the accomplishment layers usually? I don't have a good answer beyond like, it's probably an empirical. So if you think about like dropped me as a parameter, tcp, like you're, you're setting, setting some values to zero. And I couldn't because of books already doesn't have that many parameters. And you can think of that as like really dramatically changing the output. I actually don't know 100% for sure. On convolutional layers doesn't help. So feel free to research guides. I'll do my desk if it That's not regular. But yeah, others might have. As you can maybe get five to the alpha, two humps, alerts, set of output neurons. But I'll look into it. Rate or the top five error rate. What's the path? Thanks. Alright everyone to look at bacteria. So rocks, you got the answer for the next school. Mismatch score is applied at a stride of one. And so that'll give us the same output size. Alright, coming back to this idea of going deeper, Are there any other questions on what we've said so far? Alright. So this idea that as long as the neural network could have implemented the identity, identity is transferred, identity transformation easily, excuse me. And colleagues came up with this architecture called the resume. Alright, so this is a quote from their paper. This degradation problem, meaning that as you add more layers, performance gets worse. Suggests that solvers might have difficulties in approximating identity mappings by multiple, non-linear there, right? That's saying, if it could, it could have implemented these 36 liters of the identity. Recent done. So at least it seems like neural networks don't learn identity transforms, right? So then what they do is they come up with this residual learning reformulation, alright, that I'm just a bit. But the residual learning reformulation makes it very easy for neural networks to learn identity mappings. Let me actually just write it out first. So let's write out the goal. The goal is to change the neural network architecture so that it is easy to learn the identity mapping. Alright, so if you imagine that for a standard neural network, you had an operation like the activations that layer I plus one are equal to ReLu times WH. The prior layer plus B. The net changes. This architecture is not exactly how it's implemented, but just to give you the idea, it's gonna be h i plus one equals each eye ray. We'll call this W tilde, the HI plus b Tilda, right? You can see that in the resume, it's a lot easier to learn the identity mapping because as long as these are close to zero, so w tilde and b tilde are small. Then h plus one is approximately HI. Whereas for this thing to learn an identity mapping, right? One way would be for w to approach a diagonal matrix that's close to the identity. Not exactly right because there's a bias and rabies. But in this case, learning identity is really easy if you just said w and b close to zero are any questions there. Alright, so that's the key idea which you can think of. This is changing the architecture so that it is easier for the neural network to learn the identity mapping. If the identity mappings happen to be optimal, then they're really easy to learn because all you have to do is drive the weights and the biases to zero. And they will implement the transformations. Any questions there, perfect discussing gets asked every year and I'm glad you caught it. So the student asked ReLu is non-negative. Therefore, doesn't h i plus one always become bigger than HI? And this is where I say, I not only have the exact, the exact resonant architected, this is just for intuition. In the exact resident architecture. They think it'll be easier if I just show it on the next slide. After the convolution, ReLu, then do another convolution so that this thing is non-negative. Then after that, you do your identity mapping, and then you have your ReLu, applied after this. You can think of that as if I were to write this out, there would be like maybe a W2 over here. This would be like a reactivation being like an a that goes into a ReLu and there'll be like a W2 Tilda here. That's how it's normally done, but I'm going to just erase that so it's not confusing to students. The question is, why are we finding as opposed to just dropping bears? You mean like I going from 56 layers back down to 20 layers. Yeah. So the idea is we know that with deeper neural networks, with more layers, performance should be increasing performance and keep getting better if we were able to train them. So because we have that prior, that deeper should be better, we see that keepers worse. But this is not because of a theoretical reason. Theoretically, this should always do better than 20. This is for a training reason. Then the solution is, but tried to pitch decks for any reason by making it easier to learn the identity. Answered the question. It's all grounded or optimize the network, decide how many years it needs to. Oh, I see. Tom ways saying, you can also think of this as, because it's so easy to learn the identity. Maybe you're in the case where for downstream layers the biases are zero. If they were, let's say the weights and biases of you must have a 1,000 layer number. And the last 100 layers have different biases or equal to zero. Now I tell you that our 900 layer neural network is his best. That is correct him like an intuitive sense, but I rarely does it actually play out in practice like that. Other questions, yeah. The question is our w tilde and V tilde going to zero, easier to learn because of the way we initialize them. So initialization will play a role, but it's not the only like e.g. there's usually weight regularization, right? That will cause weight decay on these. And it's in general, easier to just push these two twist. Is there ventilate some exact value like w equals identity? The question is, why not put the ReLu before? I believe it is this problem of wanting to avoid the activations just getting larger and larger with each successive layer. Alright, so let's see, I have a quote from this paper. All right, so you're probably thinking identity mappings are far from optimal and over a police officer, the court says, it is unlikely that identity mappings are optimal. Formulation may help to preconditioned the problem. At the optimal function is closer to an identity mapping and a zero mapping. It should be easier for the solver to find perturbations with reference to the identity, to learn the function as V1, alright, that's a lot of words. Learning the function has to be one means, learning to W and E to implement whatever function you want. What he's saying is an optimizer is easy to make w and b go to zero, right? And it's probably the case that it's more optimal to have an identity rather than a napping sits around. And therefore, you can think of this as an architecture that includes, or the prices would not work, find these mappings that are likely going to be better than a zero that is optimal. But this is something that helps to bias the architecture to at least have nappies that are closer to identity. And then based off of this prior intuition from the slide, hopefully lead to a better performing, right? Yeah, Tom, I said isn't as their own that people always bad. Exactly. You don't want your activation. So you got Is there a, alright? So with that, we take our normal architecture, which is taking some input or higher their activity you would normally have comparable on understanding the resident architecture, we do Combray, the comp review. And then you can look at the output here. If we call this f of x, this is x plus f of x. If we call this thing h of x, you can think of what the residual layers are doing, are learning. What is blood residue at the Baxter add to X to make the representation optimal. So if h of x, what's optimal, then f of x should equal h of x minus tax. And I believe I have that on the next slide. So h of x happens to be the optimal transformation here. The residual layer with just learn h of x minus x i e what to add to x to get that representation. Right? There's one more thing. So sometimes when I have a plus sign, right, the dimensions of x and f of x has to match to be able to add them together. And sometimes they don't. So in those cases, sometimes the linear layer or one-by-one convolutions that some padding to make it match here. All right, so we're working the assignment. You want to add anything up because its gradient and with that, I'll do on that whether Biden is helping or gradient, that's great. I'm going to say if you didn't hear you on for taking my question, don't worry. We're going to have several slides later on, so I'm going to preserve that until the later slides. When we asked why does that actually work? Alright, so. These are quotes from the paper that said our reaffirming what we've already started. Alright, so network architecture, the rest of the design goals of VGG net every single convolution or three-by-three filters. And remember, VGG had this thing where if the width and the height divided by two, then the number of filters would be doubled to the computational complexity is the same everywhere. Then the output has average pooling followed by 0 linear layer to get 1,000 softmax scores when it goes to a softmax. Then all of the layers that have this three-by-three convolutions become residual layers, which means that they add this identity connection over here, right? They perform dataset augmentation. They also use the Catholic gradient descent with momentum of 0.9. They have weight regularization on their parameters. And then whenever the way they yield the learning rate is whenever the learning rate plateaued and they decrease the learning rate by an order of magnitude. Any questions on any of those details? Always increasing. To me. The problem where the question is, whenever we decrease the width and the height doubled the number of filters, do we ever run into the problem where the high becomes so small? So we decide when the width and the height our app, because those are done through pooling layers. But these columns will always preserve the output width and height. So actually we can fill that all entirely if you wanted to hundred layers and we have to have it six times, I'll put the pool right approximately every 100/6 layers. Great. The question is, why did these networks not use adam or rabbit or more advanced stochastic gradient descent optimizers. Anyone have an answer for that? Wants you to say burning meant to do so as I believe Adam was 2014. So as I read, Adam did exist, but it actually took me on the date of atoms, but adding graduates 2011. So I think that definitely is empirical results. Empirical results. Yeah, that's true. It comes back to this slide that we had in the gradient descent lecture, where we start that momentum. This, this is the reason tends to bind this really shallow local optima. Where if I plot the loss and the weights, I can wiggle my waist quite a bit and still have the same loss. And so these local minima are believed to generalize better because they are robust to perturbations in weights. Whereas, because Adam can really quickly ticked on your learning rate if you take a large step, Adam and RMS prop and added grad can find more of these steep local optima and their steep, then you may not generalize as well. Because if, if, if a bit of points in your system, all of a sudden you're at a high loss location. So that's the general reason why it's believed that all of this works for why SGD plus momentum leads to better empirical results because it's finding these really flat. Question is why do we anneal the learning rate plateaus? Rather than doing a smooth drop-off and learning rate. We could do that also affected by decreased by 4% every 80 bucks. And so both of them are possible. Other questions, okay? So this is the ResNet architecture and may have to zoom in even more than you go in there. But you can see they have these three by three columns. And then you can see the skip connections that come for the input. And it goes all the way through these many convolutional layers. And then you end with a linear layer that takes you to 1,000 softmax scores. Alright? So I believe that this iteration might have been the 34 layer for us net. And when they went ahead and tried to train this architecture, what they noticed was the following. If you just take the left-hand side is just vanilla CNNs. We saw this before for 20.56 layers. Here they're comparing 18.34 layers. For vanilla CNN if you increase the number of layers. So the bold line is validation accuracy, validation error, and the non volt blindness is training error, I believe. But neither case with 34 layer error is worse than the error. That's the same problem we saw before. But now if you make these ResNet, CNN's. Then when you increase from 18 layers to 34 liters, now have better performance. So what should it have intuitively happened that is going deeper, there's a better performance. Actually funnily happens when we change the architecture to the resident architecture. This doesn't go on forever. So in this paper, and some students are asking you for video number six here. Yellow is 20, this blue is 32, the screen is 44, and the spread is 56. So they did have more of these plotted. So this was for vanilla CNNs. And we see that actually 20 was the best. And when we go to the rest net architecture, you can see that as you increase the number of layers, the error does get better. Alright? The black one is a 110 layer ResNet. They also said, how far can we take this? And so they trained a resume, but 1202 layers. And this, this did do worse than the 110 way or rise parents. So it's not that you get this for free and you can always make these arbitrarily large somewhere, 110-1202. Section point questions here. All right, so this is the empirical result that says raise nets work. Now the question becomes, why do rest? And that's after we work. So we have motivated it from this idea that identity nappies might be optimal for neural networks and so on. Make it easier to learn an identity mapping, then I should get better performance. And this is actually not the case. So what you should do is you should look at this neural network. Let's look at a 34 layer one year. And this is what Tom was pushing wasn't earlier. What do you notice about this network when you do backpropagation? Yeah. I can't say group as gracefully as Daniel, but he said, the goodies go look. Basically at every single one of these nodes. There's a plus sign. What do we know happens at a plus sign for backpropagation? A plus sign just passes through the gradients, right? So if I have my lost here, and this is like a D L, D h, Let's call it the 34th layer. I backpropagate through a plus sign and my gradient sustained change. So basically what I have here is a gradient highway. Suspect I won't encounter vanishing gradients anymore because through this plus sign, my gradients are always going to get all the way back to the very start. Alright, good. Alright. So basically the ResNet. So all this fundamental problem of deep learning, because the residual skip connections or a gradient phi way that backpropagate accurate gradients from the end of your network all the way to the startup. Haven't proven to you that that's the answer, but that is the answer question. Jenna's question, is there a few times skip connections. Are they different from somebody? They probably are. I don't know off the top of my head. So TAs to do but there are questions. The question is, are the gradients pass through essentially similar or exactly similar? It depends on if these skip connections have any weights on them. So they do have weights them, there'll be, there'll be a multiplication by those grapes. But the ones that are just purely the identity the gradient passes through exactly. Question is, will there be any chance of exploding gradients? The answer is yes. So we would still want, if you see that the gradients, the questions, You spoke a little bit but they did, it would be similar. Then he get the same thing that I wanted to give you something. So you're saying Can we just tap out many auxiliary next year and I would help with starting as well. You wouldn't have a similar effect. It could, but you would probably have to put in several auxiliary members. Remember that those auxiliary verbs also add more parameters versus a very clean way to get the gradient from the foot. So let me now show you at least one study. This is believed everywhere, everywhere believes, everyone believes that work because you get the gradient highway. One study. And this was by this person Larson, who in 2017, when people are still debating this thought of a really elegant experiments, I think the reason that this work is because they give you this bridging highway that reduces the effective. Alright. And so what he did is he thought of this idea called scrap. This is just an experiment to test out the effective depth or the identity transformation that matters where this is an eight layer neural network. But then he also gives a four layer path, a to air path and a one layer. So even though this is an eight layer neural network, it's effective depth is because there is a one layer path, right? Just effectively shallow, even though there are eight layers. That's kinda like Resonate where even though there are 34 layers here, if all of these projects and there'll be effectively one pass-through of ingredients all the way from the output to the input. And what he showed is if you just take this fractal net architecture and you run the same exact experiments that resume, which is that you make a neural network with 510, 2040. So on the left is the vanilla CNNs. For the vanilla CNNs, we see the same problem that resonance saw, which is red is five and bluest 4D and green is 20. So when I went 20-40 layers, my training loss worse. That's the problem that we've seen. Fractal net going 5-10 to 20 to 40, we see that the training loss always gets better. As long as this fractal that you would add more layers. And so this 40 layer franklin left. Again, there's a path that has 40 convolutions, but there's also a path that has one copy of. This is a proof that this is an illustration. Having a pathway that reduces the effective depth of your network leads to solving this problem where where when you make the neural networks deeper, the training loss gets worse. Just having defective, shortened falls, this any questions better? Alright, so that is the last of the CNN architectures that we'll talk about in this class. There are architectures I came in 2016, 2017, 2018 for the ImageNet competition. But there were basically iterations on these same ideas. Here's an example. In 2017, this architecture inception V4 did very well. And you can see that what inception V4 is, is that it's an inception module with a residual connection, right? So after the resume, there were some architectural innovations, but they were iterations on similar ideas that we've talked about. Alright, I've got some summary plots here that show you the relationship of algorithms to another. So in this particular plot, x-axis is the number of operations. Y-axis is performance, or higher is better. And then the size of this circle isn't number of parameters. So you can see VGG nets are sitting here a large circle, so many parameters. Whereas inception before even the resonance is kept far fewer parameters and get better performance in pure operation's survey of convolutional neural networks. One is that architecture can play a key role in the performance of the network, especially for the resume, the architecture is important to get that gradient find ways that you can train. There is evidenced in general, having deeper and wider neural networks result in better performance for CNNs going deeper. So a point by this, I mean, we saw that you could make but at 22 layers, but after that, performance difficulties, no numbers but then the resonate, confess that problem, then it appears for the rest. Next, one important thing that plays a role is the effective depth of the network. That is, to avoid the problem of the fundamental problem of deep learning, which is exploding and vanishing gradients. You want to reduce the effective depth of the network so that they're ingredient pass from the output to the input layers and they get a bit until there's clear. Any questions there or across our entire survey on CNN. Today's date, it might not be something that's say by this thing. Right direction. It's asking that we've been talking about in the context of computer vision. You could be in different applications. Wireless signals, where there are a few questions. One is convolution still the best operation to use? And if so, how do I decide those architectures? I don't have a good answer for that and I don't think, I think there will be specific for each and every dataset. So e.g. the project, if you're doing the default class project is on data. And if you look at some of the papers that we cited, one thing that we want to is often seen as useful feature data is to look at the power in different frequency bands. So at the very end of art, project guidelines cited paper called EEG net. And they do convolutional neural networks. But one convolution is temporal, right? And we know that a temporal convolution basically either a low pass band pass or high pass filter. So it's extracting out different frequency information from the signal. So basically I'll say is domain specific depends on the exact characteristics of other questions. All right? Yeah. The question is, if you have more skip connections does help in performance. Almost certainly someone has done this experiment. I just don't know off the top my head. Okay. Okay, great. Yeah. So there are many iterations beyond resume. And I'll leave that up to a literature search and research. Other questions here. Alright, so with that, we are going to get into the next topic, which is recurrent neural networks. So basically I think in the 20 min that we have, we'll just be able to motivate why we would want to look at the current neural networks. On Wednesday, we'll have a deep-learning library sector from the OH, minus and then, and then we'll be finished. Recurrent neural network architecture is a week from now on one day, right? Okay, so why did they turn real numbers? So we have talked a lot about fully connected neural networks and convolutional neural networks. But one thing that you'll notice is that they don't have recurrent connectivity. What do I mean by returning connectivity? Essentially, I just mean feedback connection. So if I have a fully connected neural network like this, all the connections just go from layer to layer I minus one. And we never have connections from a downstream layer to layer in the neuron in the previous layer. So when I say recurrent connections, I mean feedback connections like these orange ones. And then I wrote the sentence, parents have no dynamics. You probably don't understand what that means that I will talk about them in just a few slides. We'll talk about the impact of what these are connections. These feedback connections that then leads to the network. What they, what they, what they mean for the network. Recurrent neural networks through language modeling. I know that most of you know today that the best language models include transformers which are not recurrence. So in a sense, motivation is limited in that while it does motivate recurrent neural networks, there are before obeys to do even better, I think was falling. But still, it'll get across the idea that returns could be important in particular. So let's look at the following problem. Let's say that we're training a neural network language setting to do character prediction. So I'm kinda character prediction. All right, What we're gonna do is we're going to represent this with a 2060 vector. So this is going to be a 26 D vector. And the letter t will just be a one-hot vector where. The number in the alphabet were key occurs, we have a one and then everywhere else. It's, that's my input representation of characters. One-hot vectors of length 26. Alright? And our goal is to do text generation. So what we wanna do is we want to pass this letter t into a convolutional neural network. And I want it to be able to make a reasonable prediction as what the next letter, alright, so we know that in the alphabet, right? You have a t. Like we next letters are like e.g. bowel. So like maybe a has a probability of 0.15. H occurs pretty frequently. It's like THE, so maybe Hs like with probability 0.3, then maybe, you know, Z has probability 0.01 because there are very few instances in English, where's the fall is to do? Okay, So that's our setting. We take the inputs will be one character. There's one-hot representation of t. We want to output a distribution over what are the likely next characters. So this is gonna be a softmax distribution over the 26 letters in the alphabet. Then I want to hear, then I'll put the character that has the highest probability. Maybe it's h in this case, or maybe I don't even want to have the highest probability, I just want to from this distribution. So here's a question for you. Consider that we've typed two strings so far. One of them is th, and the other is th. Th. Alright? My question is, if you were to be in the setting where the prediction is done by CNN, will the next letter predicted, which in the first case you apply the a vowel, right? Like e. Or a. Prediction in the first case, differ at all from the prediction in the second case. So in the second case, you know, maybe it should be a key that follows to start the work. So just think about that for 20 s, feel free to talk to your neighbor and then I'll ask someone to give an answer. I guess I'm wondering as I can and tell me the answer would be the same. And why is that? Right? Exactly. In both cases, the CNN excuse me, exact same input. We know if it's the exact same in a CNN is always going to output the same softmax distribution over those 26 characters. Alright, so it would be very likely that for this particular problem setting, the CNN would ever put a vowel after THE, which is not what we want. Okay. Great. The question is, could we not just go ahead and concatenate instead of just giving you one letter H, give it like several letters in the past e.g. I'm sorry. Yeah. Exactly. So the question is like, why can't I just give it, you know, instead of just the last letter, the last ten or 20 letters. And the answer is you can do that. And that will be what transformers do to solve this problem. So I do have a slide where we say that one of the ways that we could take into account the fact that the outputs after th, th, th, th should be different is that instead of just giving the most recent factor, would you give it a history of characters? And this is one way to solve the problem. This is how transformers do it there. And that's all well and good. But then this also leads us to another way to do it, which is through been recurrent neural networks. And so this gets us to this idea of states. Alright? So a CNN being feedforward and having a deterministic computations is always going to produce the same output given the same irrespective of what happened in the past, right? So because the input here is h and that's all the CN MCs. It has no representation that preceding the H could have been very different things in these two scenarios, right? In some cases is a totally fine thing, e.g. for classifying images. I just want to know if an image is a dog or a cat that doesn't have any temporal structure. But in anything of that. Micro structure by classifying videos are generated. What happened in the past matters for your prediction. So at this point we need a new construction. And the way that recurrent neural networks do that is through this variable called state. Alright, and so we're going to expand on this in just a bit. But you can think of State is doing, is capturing and a single variable. What has happened in the past? Let's say that I wanted to infer some output z of t from inputs x one to x of t. Again, the first way to do it would be to just pass an entire history of conflict. And again, you can do this. One potential quantities is that they're going to many more parameters because you need different parameters to essentially combine all of your inputs in the past. Alright? And then there's also this question of which inputs are important. And so if you get to a transformer architecture and that's where this thing called the attention comes in. And again, it's something specific to those transformers. So that's one way to do it. The other way to do it is to have recurrence. And so in a recurrent neural network, we introduced this variable called state. And the state is going to be influenced by past input. So this is input values as well as the current. Alright? And the way that we make the state to see is that we do something called a Markov assumption. And the Markov assumption essentially state the following. Everything I needed to know about my inputs up to time t is some function of everything I need to know about my inputs up until time t minus one and the current. Alright, so let me write this out. This S of t minus one, you just think of it as the following. This is a state variable that contains all of the relevant information about the inputs up to time. T minus one, the subscript care about, so it's about X1, all the way up to x t minus one to perform a task. Right? So to do my task, right, maybe it's rambling videos or text generation. I would have to keep a history of my inputs because now temporal structure matters. I'm going to summarize exponent all the way up to x t minus one into this one state variable S of t minus four, okay? And that's good. You know, X1 to X2 minus X1 contain a lot of information, but all, all of it is what's relevant to the task goes into f t minus one. And then I have this relationship that tells me how I can take the info about x one to x t minus one, represented through S t minus one and my current input x t. To now develop this state variable, which contains all the info, the relevant info about x1 to x t. So as long as I keep doing this recursion, basically remembers what I need to know from my historical inputs. And that gives me information to be able to then solve my task. So this is a nice formulation because now I don't have to remember all of my past inputs X1 to X2 and pass them in this network. But really all of my tasks and fluids are you just going to be summarized by this t variable, t minus one. Alright? So this is a compact representation of the information I need to know about my historical inputs. And then with this, this it will be dependent state of a recurrent neural network. With that I could do tasks. All right, Any questions there? Alright, so this is called the recurrent neural network when you incorporate the state variable. So it differs from a feedforward neural network because to be able to understand how t minus one t, we necessarily need to introduce the back connections. Let me give you a really simple example in linear case. So let's say that my relationship with that S of t plus one equals eight times S of T, right? Let's say that S of t with a 3D vector. So S of t was some three artificial neurons. Okay? So if I had three artificial neurons, so this would be S1, S2, and let me do a superscript one, superscript two, and f superscript three. And let's say that I had that a equals 123-45-6789, right? When I had this relationship, S of t plus one equals a sub t, What I'm saying is that if I had my S of t minus one over here, and this bent equal. S of T, that the value of my first neuron will be one times the value of my first neuron at the current time step plus two times the value of my second neuron, fire timestep, plus three times the value of my third neuron to fire to construct. And so we would draw this as these three values being a feedback connection with value one. The second neuron intuition, the first neuron with the value two, and then this connection with the value three. And you can see that if you were to draw this out, this neural network now has a bunch of leaves. So it's not paid for, but through these loops, it is able to store information about protest. Okay. Any questions back? Tom way to saying, are the values of these weights fixed? They don't change across time. And the vanilla recurrent neural networks and also LSTM architectures that we're talking about the arm. They won't change across time though, of course, change during the training process when you learn them. But they are static inference. Alright? So in biology, I'm just going to, just for the sake of time, I just want to get to a few examples. So I'm going to show you a few examples. And this comes from a nice blog that Andre cartography, but in 2015 called the unreasonable effectiveness of RNNs. And what he's doing is that same character prediction task that we talked about. Where in this case, just for simplicity, we will have four letters that we could output. And so instead of 2016 vectors afforded vector and the letters h, e, l. And the basic way to think of a return on that bridge is we'll start off by inputting our letter. So this is the letter H because this is a one-hot and the first element is one. And what that is going to do is that's going to go into my recurrent neural network. And this hidden layer is that state. So this hidden layer is that variable. And so this would be S at time one, S at the time to time three, time four. The inputs x at time one, exit time to X at time three. Time four. Actually, I think this should be 012.3. So what happens is that the input goes into the neural network and it modifies the state. And then it gives us an output distribution over the next characters. And so maybe after we put it in an h, we get an E. We then take that E and we put that into my Recurrent Neural Network again. But because my Recurrent Neural Network, my state is also updated to S1. State variable is not different than the past time step. And based off of this, I'll put an L, right? If I put in the L Now, it will output for this given states and other L. Notice here that hello, right? I'm going to put an L again. So the inputs here for X3 and X4 exactly the same. They're both but letter L. But because now the state of the recurrent neural network is different, the output can be dropped frames. So after the second L is the structure in the English language, it should output. Alright? So again, the way that the output of this L is another L, where L is 0. The difference is because the state of the recurrent neural network, it's also been updating all of these inputs, right? So if Andre called Papi, went ahead and he really simple vanilla recurrent neural network. I think he implemented this one in about 100 lines of code. And he started to train it to try to learn tax. And so here's Shakespeare. He did this exact task with just a simple RNN. And if you do 100 iterations where you're just trying to predict the next character. You get out your brush and as you change it, more and more, going all the way down to 2000 iterations, you can see up to thousand iterations as far as to do reasonable, even if you read this, it doesn't. Semantic meaning like, why do that day, right? But it does better to do quotations correctly. It does learn to capitalize proper pronouns. For even longer. It can start to output text that looks like Shakespeare. And so it is combining characters from different plays, right? But that's because it's been trained on all of our experiments trying to imitate what Shakespeare looks like. And again, some of this text is not really to intelligible, but sometimes when you read it, even like in the rhythm of it, you can kind of iambic pentameter. Alright, so that's Shakespeare. I'm highlighted here these lines because I think that these might be like more iambic pentameter likely to show the reigning of the raven and awards. You can turn it on Wikipedia texts and go learn how to do Wikipedia and it'll even do the citations correctly. Here is them training on law, tax law as groups for an algebraic geometry textbooks. And it generates texts such that if you were to just do a bit of debugging to fix the errors which are minimal. It actually generates fake algebraic geometry. And I really like this example because you know how in math textbooks sometimes though, save something as true and then they'll say like the proof is trivial or the proof is omitted because it's an extra assistance. They were never simple to me. If you zoom in on this, this neural network learns to also. We'll stop there and we'll come back on Monday to finish RNN. And it can lead to deep learning. 
All right. All right everyone, We're gonna get started for today. Can people hear me? Alright? Alright, it was a rainy treasures walk away over. One more. I am Jonathan cow. I am your instructor for this course and on behalf of our team of TAs tomboy, can shift to lie, shouldn't be, and remain who will introduce themselves later. We are really excited to be your instructors for this course, which is C1 47, 47 neural networks and deep learning. All right. Today we are going to be basically doing an intro into deep learning. We'll go over the syllabus and you have time. We'll begin lecture material. Alright? So high level of things before we begin. The first is that this is actually one of my favorite classes to teach. Feels really rewarding because as a result of this process sees so many students who come back to me and say that they found deep learning does for further research are useful as they went on to industry. So deep learning is a very relevant topic for many different applications areas today. And we hope that this class will be useful for most, if not all of you. This class will hopefully be fun in terms of, I know that for many of you this is required classes and elected at the same time as we'll talk in the syllabus, there will be quite a bit of work for this class. We'll discuss that with the syllabus, but please take that into account as you determine whether this process for it or not. And then if you've ever taken a class with me, you know that I like to keep things interactive. Lecture hall this bid. So please feel free to ask questions. I will be very happy to entertain your questions. And usually if you have a question, you're not the only one with that question and you ask them that will give me an opportunity to explain something once again. Alright. Any questions before we got one quick thing? So I think I already saw there are many students who are joining via Zoom. So if you think about the lecture, first off, this class will be reported by doing tasks. And then brewing costs will upload those bacteria to grow and learn. Use it the under a tab called UCLA media reserves. We'll send out an announcement with all of this information. Because this class has also taught for the MS Online students. We're going to have a live Zoom Room, which we'll share the contents of my iPad screen and also record the audio. One of our TAs can is going to be monitoring the chat. And so while we won't take any verbal questions from the Zoom room, will be verbal questions for those who are in-person in the classroom. If you have a question on lecture material and just put it in the chat and camera or one of the other TAs will get through any questions there. Alright, well get started. So if you've been following the news lately, you'll know that there's this new chat bot activity who here has interacted with just one half the class. Shattered beauty is created by OpenAI and it's a very powerful language model that can do some pretty remarkable things. And so last night I asked chat TBT. I would be best for me to motivate this class. So I said motivated course to find the corresponding enforce sentences. And this is what she did. She says, deep learning is a revolutionary field of AI that is driving many of the recent advances in machine learning. It has the potential to transform a wide range of industries and has already begun to do so in areas such as images, speech recognition, natural language processing, and machine translation. Uh, deep learning course will provide you with the skills and knowledge you need to understand and apply these powerful techniques to the real-world problems. Whether you are a beginner or experienced practitioner, a deep-learning course can help you take your careers conducts bubble. The rapidly growing field of AI. That's a pretty good introduction, might even be better than this and that I could write on this. And it actually comes pretty close to the intro I usually give for this class. But just to motivate the applications of deep learning and get you excited about how it can be applicable to the things that you're interested in. There's one thing here is that whether you're a beginner or an experienced practitioner. So we just decided to be an introduction to deep learning course. If you're an experienced practitioner student in this class, this class will be easy for you, so this should be fairly for beginners. Alright. I usually motivate this class is going to talk about its impact. And one way to talk about the impact of deep learning and neural networks is to point out that you have publicly today interacted with at least one, might be several neural networks. So starting off with, if you've done a Google search, we know that you're going to use it the page rank algorithm to show you the search results. However, Google also augments the page rank algorithm with other deep learning-based techniques, including rank brain as well as neural matching. Neural matching, e.g. isn't NLP based technique. And one example of how it is this sport is it tries to infer the meaning of what you're saying. A commonly given the sample is if you are searching for a boot on Google and you are, the less you're likely looking for footwear. But if you're in the UK, you're probably referring to the trunk of the farm. All right, So Google uses deep learning to augment their surface. And if you searched on Google today, if it's rapid with a neural network, you might also help me know that I do go, there are reports that there is a code red over there in response to chat TTT as to whether and how that threatens Google search engine. The search engine. You start to knock people if you do not get to today. And you've been sharing videos, suggested videos, those recommendations come from deep-learning based algorithms. In part. If you've been on Facebook today, facebook uses a variety of deep neural networks to cases that comes up. One is something called deep text. So here is a video of Facebook Messenger. Now, I don't use Facebook, so I'm just trusting that this is actually a Facebook Messenger. But if you say something like, I need a ride, then they'll give you a link that says request a ride. Right? But it's not just looking for the word ride and showing you that week whenever you save, right? Because there are many sentences. Include the word ride, but don't need you to request a ride like this one here that says I don't either I or this one below that says, I like to write it down. Alright, so within this speech processing and NLP road, neural networks are really useful for trying to understand sentiment or even the meaning of what they're trying to compare. In addition to this, better uses something called deep base. And so here are two pictures of Emilia Clarke, but one, she's dressed up as dinars target area. And even though the therapists are very different, face, which is metaphase algorithm is able to verify that these are facts, the same person and even more from the images alone, they can deduce many things, including the emotion conveyed and can even guess the age share. So share is predicted to be 31. And I think they say that they're using accurate plus or minus five years. In fact, Image Recognition, Computer Vision was one of the key fields that really helped to spur a revolution in deep learning that we've been experiencing for the past decade. So this is a video of a professor at Stanford talking about the impact of Computer Vision and massive data from ImageNet and the modern CPUs and GPUs to train such a humongous model. The convolution massive data from ImageNet and the modern CPUs and GPUs in such a humongous model. Let me just see if I can find it probably is. I'll just try to hold the mike really first. Raise your hand. And the faculty had trouble getting massive data from ImageNet and the modern CPUs and GPUs to train such a humongous model, convolutional neural network blossomed in a way that no one expected. It became the winning architectures to generate exciting new results in object recognition. This is a computer telling us this picture contains a cat eye where the cat is. Of course you are more things that cats. So here's a computer algorithm telling us the picture contains a boy and a teddy bear, a dog, a person in a small cavity in the background. Or picture of very busy things like a man, the skateboard, railings and lampposts and so on. Sometimes when the computer is not so confident about what is C's. We have taught it to be smart enough to give us a safe answer instead of committing too much, just like we would do. But other times, our computer algorithm is remarkable at telling us what exactly the objects are like the make model year of the car. All right, any questions on this example of the make model year, the car is pretty remarkable because we'll notice a few things. First, all these cards are different sizes, right? So let me backtrack. If I asked you, how would you identify that a car? You might say things like car is relatively large compared to other things in the scene. Or you might say, a car has four wheels, right? But then in this image, the cars can be at different depths and so you don't want them the same scene. There are different sizes. And then this for this vehicle, only two are visible. And actually this image is cut off. And so this top vehicle, only one is visible. You and I all know that that's a car. But if you're just trying to hand craft an explanation for the features that tell us what a car is. Might be a bit harder to put that into words because these images are all cars. Have four questions here. Alright. So beyond those examples, if you've used recently are today, to try to determine what's the page. You'll know that York has a section called popular dishes where they show you things that are popular to order at the restaurant. That includes deep learning, which identifies the images. And what I didn't say, we're going to write that the image is coming from the different reviewers are of the same object. Will also use deep learning to parse the reviews of the users who uploaded those images to determine their sentiments and evaluate, was this a positive review or was this a negative review? And use that to figure out what the popular dishes. If you've interacted with Alexa or Siri or Google, you have interacted with the neural network because the speech processing algorithms on these all rely on neural networks. If you use Twitter, than the Twitter spine line would have determined to show you is based on deep learning. Lastly, if you have lost something, a lot of these recommendation systems today also use neural networks. And there are algorithms that you might be most likely, very likely you have interacted with the neural network today. And that's just the ubiquity of deep learning. Just in our everyday lives. Beyond that, deep learning is able to achieve things that are remarkable to us. So this is based off of something called GPT-3. It's also published by OpenAI and shares many similarities. Similarities are overlaps with that chapter. In this video, what we're going to see is that if you use a neural network to get a description of something degenerate, and then the neural network would generate the code to create that. He said here, some auto-generate a button that looks like a watermelon. And here is the code that generates that for them. Somebody who you might find this even more impressive injections, since it actually generates combined with Voltaire. There's syntax error. Then after correcting it generates exactly what the fastball. Right? Beyond this, you can also generate things that aren't as naturalistic. So here, this is a tool called Adobe, who here has interacted with Dolby on it. I say that somebody is like 30% of the class. You can give a text prompt and it'll generate images according to that prompt. And so this is an illustration of a baby daikon radish in a tutu walking a dog. I'll give you a few seconds to think about what that might look like. Alright, and this is what the AI generated image makes. An armchair and the shape of an avocado Looks like this. And then a storefront that has the word open, hey, I've, it's done. It looks like this. Yesterday went on dali also. And here is a UCLA professor teaching deep-learning in a large lecture hall. The large vector. Hello, What's more grandiose because this, but just a bit. Here's Joe grew and doing cartwheels at the Rose Bowl. Alright. So I also mentioned at the very start that deep learning has this really diverse applications. And many students find it useful in their research areas or even in big industries that they go into. So here's a video of Jack gene. This is from when Google was releasing TensorFlow. And he talks about a very interesting application of neural networks. It system. We built this called TensorFlow and we use it for everything that we do for this area. And so the system we've built this called TensorFlow, and we use it internally for everything that we do for these emphasis area. And last year we decided we would open source it because we wanted people to have the ability to take this software, download it for free, and use it for their learning problems. It's been really great to see different things that people have used it form. So here's one example. There was a Japanese cucumber farmer. And it turns out when you harvest your cucumbers, you have to sort them into all kinds of different categories for sale. Small ones, medium ones, large ones, particularly one's not particularly ones straight one's curved on. This is pretty complicated and pretty time-consuming and harvest time. So the farmer was able to take a camera and using a computer vision model that he trained with TensorFlow, actually have the division model determine what category of cucumber was looking at and then rigged it up to some conveyor belts and some little switches that would push the cucumber into the right box. And so this eliminated many days of labor that the farmer and his wife would have to do it at harvest time. Just one tiny example of something you can do now that would be hard to four. Alright, so I hope you hear there's just been talking about a Japanese cucumber farmers used TensorFlow and in turn disability to straightforwardly implement neural networks. To be able to easily classified cucumbers into different categories. And that's saved a lot of their time at work. When I first heard this, I just thought Japanese cucumber farmers are super hardcore. I teach this stuff and I would've never thought if I was to tell the farmer to do that and also their cucumber farmers of Kennedy go deep learning. So I looked into this a bit further. And they kick up and farmers there sudden as a software engineer. And he was able to build a system that uses a neural network to get a visual classification of the cucumber. And here's that system network. Questions there. All right. Continuing on, its applications are diverse. Learning has a lot of important applications in a variety of different fields. I've been on several PhD committees after having taught this course for students are using deep learning, e.g. to try to classify the fMRI scans and PET scans correspond to certain diagnoses. Here's an example. Here's a recent example from DeepMind, which is one of the challenges in biochemistry and biology, is to understand what the structure of a protein will be based on the sequence of amino acids. And so DeepMind created an AI based off of deep learning that is able to take the sequence of amino acids that comprise a protein and very accurately predict its structure. And this has been a challenge for many decades that DeepMind has a big lead towards solving as a result of deep learning. And this one is called alpha. Beyond that, it's used in other areas and other example I like to give is a cancer moonshot. There is this thing called candle, where it tries to integrate disparate sources of information about the chemistry until drug, how a patient might respond to it, and the types of treatments that the patients are going to try to get the success of that cancer drug in helping the patient. Another area where it's helpful is in developing new technologies. And so this is a video that talks about how there are many challenges and self-driving vehicles. Same time, neural networks are at the foundation of self-driving vehicles in terms of being able to take what's been seen by the camera, parcel of what's going on in the road and then make decisions based off the bat. This is a challenging problem. So let me play this video. And then once you have that problem-solve, the vehicle has to be able to deal with construction. So here's the cones on the left are forcing it to drive to the right. So not just construction in isolation, of course, it has to deal with other people moving through that construction zones as well. And of course, if anyone's breaking the rules, so the police are there and the car has to understand that that flashing light on the top of the car means that it's not just a car, it's actually a police officer. Similarly to the orange box on the side here. It's a school bus. And we have to treat that differently as well. When we're on the road, other people have expectations. So when a cyclist puts up their arm, it means they're expecting the car, yield to them and make room for them to make a lane change. When a police officer student the roads are vehicles should understand that this means stop when they signal to go and we should continue. Alright, so examples are just things that you wouldn't understand all the red, right? But there are so many of these unique roles and edge cases that self-driving vehicles have to be able to take into account. And people are solving this with neural networks. The questions. What did you do on? An example that I really like to use is the recent progress made in games. I like this because when I was a senior and undergrad over a decade ago, machine-learning luminaries at the time, dr. the game of Go wouldn't be solved in the near future. Of course, we know that there was deep bluish, I say I, that Garry Kasparov in 1997 but goes and much more complicated game. And this is demonstrates harvest, the CEO of mind explaining why that is games or kind of microcosm of the outside world. That's why games were invented, That's why humans find it fun to play. There's a rich history of compete attacking board games. Started with games like backgammon, drafts. And then finally there was deep blue 97, the beak Caspar for chats, watershed moment for game. Since then, the really big remaining Holy Grail, if you like, has been done. Chess number of possible moves to about 20 for the app. And go it's about 200. Another way of doing the complexity of Go is the number of possible configurations of the boss is more than the number of atoms in the universe. But if you ask a great Go player why they fade a particular move? Sometimes they'll just tell you it felt like. So you can, one way to think of it is that Go is a much more intuitive game, whereas chest is much more logic-based, right? Yeah, So because there are so many possible configurations. And further that when you asked a really good Go player why they played a move, it comes down to, it becomes really challenging to think how do I train an AI that can beat the best human Go player in the world? Because what this intuition even mean, how do I create an AI? Just have this intuition. Even with these challenges in 2016. And this is actually right when I was interviewing for my job here at UCLA. During those interviews, there was this game going on where AlphaGo and AI traded by DeepMind was playing all who is widely regarded as one of the best skilled players in the world. Before the day reset all before the game, AlphaGo had already exceeded the European champion. But Lisa Dole was at a level of play much higher than the European champion and he was very confident that he could win. The first game was a massive surprise. When AlphaGo won. Alphago won came to an end. Alphago won game three, at least for me, there was this feeling of excitement when AlphaGo won. But then it slowly changed a bit into a tiny bit of despair, as in humans ever going to be the AI. But recent Dole was able to teach game for. Then ultimately AlphaGo won the fight in sets or 21. And this was something again where she learned luminaries had said earlier that the statement of the salt for a long time and get deep learning was able to solve it. One way that these AIs are trained to play Go was to look at the human expert data and in essence try to determine some of that intuition. So from these examples as to what's the best group of plaintiffs, third position. But here's a video from David Silver who led this DeFi projects at AlphaGo pocketed be talking about how they were even able to later on upbeat the algorithm so that they can make even better go fire that requires no human data at all. Alphago Zero is the strongest Go program in the world, outperformed all previous versions of AlphaGo, specifically defeated the version of AlphaGo, that one against the world champion at least at all. And it beat that version of AlphaGo by 100 games deserve to all previous versions of AlphaGo started by training from human data. And they were told, Well in this position, the human expert play this particular movement in this other position, the human expert played here AlphaGo Zero doesn't use any human data whatsoever. Instead, what it has to do is learn for itself completely from self play. So the reason that playing again itself enables it to do so much better than using strong human data, is that first of all, AlphaGo always has an opponent have just the right level. It starts off extremely naive. It starts off with completely random plane. Sorry, that was my bad. I actually don't be attached the video, but the video wasn't much longer. It starts off with continuing, I can phrase, and then they do something where they trained the eyes and unmatched at the same levels. And just by playing other versions of the AI, it's called self play. They are able to become gonna go. This new algorithm which they call AlphaGo. Alphago recently hold the world champion. 100 days is around. The AI has improved a lot even just by, by algorithms. Any questions bear in mind is working on several other names instead, here is the video of their AI office car to play the game of Starcraft. Here's another application that is hopefully showing how diverse application to my own area of research. So I worked on something called brain machine interfaces. And our goal is to help people who are paralyzed. So this is a picture of Christopher Reeve, who many of you know was the Superman and the actor who played Superman and the original trilogy. And Christopher Reed suffer horses. Horseback riding accident, paralyzed from the neck down. For the rest of this month. For the rest of his life, he required a ventilator support to help with breathing. And we use deep learning to build brain machine interfaces. This is to help people who are paralyzed due to spinal cord injury or disease like ALS communicate with the world once again and move once again. So the basic idea of how this works is for someone who's paralyzed, the natural pathways that take information from a region of their brain called the motor cortex. This part of the brain and goes down the spinal cord to the alarm, these natural pathways are broken. But even though these pathways are broken, intense but thought of I want to move my arm still persist in the motor cortex. What we can do is we can read out that activity by going to the brain directly. In the first example, I'll show you what we did is we did neurosurgery to record from neurons in the motor cortex. And the neurons in the motor cortex communicate via signals called action potentials. And so I'm showing you 96 electrodes here, where every single electrode is getting really close to a neuron and bust into its voltage. And these spikes here are the voltage signals neurons in MIT. And that's the fundamental currency of information. And the brand is how your neurons talk to other neurons and how neurons talk with the rest of your body. So what we do is we read out these electrical neural activities from these electrodes, and then we pass it through what we call a decoder algorithm. Basically, we translate that electrical activity into the intent of what the person actually wanted to do. And what you use that to control across thesis by the robotic arm or a computer. Alright? And this decoder, the best ones out there today are based off of deep learning. Deep learning we'll learn the patterns between the neural activity in the brain and how to best decode that, the movements of some device. So here's an example of how this works. In this video, you'll see a 52-year-old woman ALS. She's paralyzed and she's controlling a keyboard that's already completed computer cursor just by thinking about it. To answer this question, how did you encourage her sons to practice music? I think you've got it. Target to target. And then when she gets over the letter that she wants to select. That select the letters if I might be part of a football at Cambridge University. Another video of the participants where instead of using this custom keyboard that we created, she's now controlling the computer cursor on Android tablet. I see the control isn't perfect. Sometimes you struggled to get to the exact location. And there is definitely more improvement to be made based off the algorithm. So many advances we tilt the performance of these incidents. He's able to use this to surf the web. I want you to write emails. The question is, how do you collect the same data progress? Yeah, that's a really good question because these are trying to be a supervised learning, which means that we need to know the movements she intends to break and collecting their own data corresponds back. So actually what we usually do is we will move a computer cursor on the screen for her. And it will say, imagine moving your arm to follow this computer cursor. So when those perfect goes to the right. So imagine wearing her arm to the right. That's how we get the target signal to decode. That's a great question. Yeah, The question is, Venice seems like you're paralyzed and you want to use a system like this, you have to go through some pretty intense training. So actually for these algorithms, they were able to be trained with just 10 min of data and probably could be more fully. Deep learning has done better algorithms. But if you put in once the firefight, but those will be data that have been collected through normal abuse over the course of months. There's also the other thing that is pretty glaring about the system which is either requires neurosurgery to get these signals. And so, excuse me, try next video which is at UCLA, what my lab is working on is trying to make a non-invasive. And so this is actually just from a month ago showing a system where instead of doing neurosurgery and report neural signals non-invasively through this one. But this is one of our graduate students wearing a cap. And now he's able to control this. And this cursor is entirely driven by burning. There are nine neural networks working together in the air. Have you include supervised learning and involving buses, but because of where it equal to that, some performance is not as good as one way doesn't require neurosurgery. And this student is able to just practice on both of those into a high level. But I haven't talked for my office hours. In neurosurgery. What happens is that the participant is first given fMRI MRI. The MRI that asked to imagine you live in. There are, or there can be areas that we did where we want neural signals and then MRI when they're asked to imagine moving your arm. Areas of the brain that become activated will become known to the nurses and the neurosystem. Although this board then will spread through the non-invasive system. The question is, do they use, are we using the same or lupus as what? The invasive subject? Because easy. The answer is. So actually, in the video that I showed you that 52 year old woman with ALS, I said that she's imagining moving her arm, but she actually isn't. She is to move the cursor up and down. She's imagining her thumb going up and down. The cursor left, right? She's imagining her pointer finger going left and right. In this video, I'm, the graduate student is doing right-hand group is for right that kind of movements for both hands and feet for gas. And so part of the reason why that is is because when we record non-invasive signals, they are, they are all such, such for spatial temporal resolution that they can't resolve various movements in the fingers. And so we have to use whole body movements. The question is, if you train this on one subject, is it generally enough to apply it to another subject? In the intercritical case? No. And that's because it depends highly on which neurons we end up recording. Even for the same subject. If you do two surgeries, the signals from the first surgery will be very different. Music comes on the second side, so they have to be retrained. Whereas for these non-invasive methods, because the signals are so poor spatiotemporal be resolved. They are correlated across subjects, and so these do generalize across subjects to a better extent. Take one more question and then we'll continue on. The question is for action potentials, are they in binary or is it important to maintain the analog signal? But also talk about this when we talk about neural network architectures. But the output of biological neurons is binary. Either a spike, what happened? There actually isn't any information can be in the voltage, can be analog signal. All that matters, but that's just my calendar, not the brain actually uses digital communication. That's one of the reasons why we can have pretty good control of our fish, even though the brain has to send electrical signals all the way down our body, that's a pretty long transmission line. And there's a lot of them aren't all the way. But the brain has developed these digital robust mechanisms to convey that information. Because there are so many questions about this. Let me just plug that. We aren't going to cover those in these cost is the cost of the machine learning class. But I do teach about this next quarter in a class 14038 for undergrads at 02:43 for graduate students where we talked about how the brain generate signals and how we can use that for brain machine interfaces. All right, so moving on, whole bunch shown you that deep learning is really becoming a part of everyday life. And you've interacted with the neural network today that has resulted in key breakthroughs in many areas and in different fields. Many people are looking to deep learning to try to boost performance. One thing I learned earlier this year is that if you've seen those little cocoa robots that deliver food out on the street. The founders of the company where students in this class, six years ago in this class, was something that inspires them to do. So deep learning may be useful to research in your area or future. Any questions here on any of the overview that we've given. Alright, so I'm gonna give a very brief background on bidding that even though my expertise is in the brain and then brainwashing interfaces. In this class, you're primarily going to be studying neural networks in the context of computer vision. And that's because like we saw in the earlier video with Professor, That's really where neural networks, neural network research dates back to 1943. Mcculloch and Pitts in 1958 with Rosenblatt, who here has heard of the perceptron before. You, I couldn't have to try and learn your first neural networks class. That's really the first artificial neuron where there is a node that will sum all of its inputs. But so does the affine transformation. Then after doctrine applies, a thresholding operation so that the output of this artificial neuron is going to be zero or one. That was inspired from biology. But the answer from the prior question, distinctive aspects, biological neurons communicate in an all or nothing binary way. So really in the 1940s and 1950s, researchers to start to think, okay, can we design computing elements that have similar properties to biological neurons? What happens when we looked them up? Alright, so Rosenblatt perceptron, if you just have one neuron, is something that's linear. And to make it nonlinear, you need a stack, several layers of perceptrons, giving rise to something called the multilayer perceptron. That gives you a non-linear architecture because now you have an affine transformation is followed by non-linear thresholding operations, followed by a more alkaline than nominator thresholding operations, etc. The challenge was that people didn't go how to train these neural networks. And what happened is that in 1986, Rumelhart Hinton and others published an algorithm called back propagation. And we're going to talk a lot about that proposition in this class. But that was really the key thing that allows you to train these artificial neural networks. Such that by 1989, Yann LeCun, who is one of the fathers of deep learning, was able to train neural networks to do classification of digits. So here's a video of this from 1989. I think before many of you were born. The goal of the algorithm is to look at the digits and write out what we are so accustomed, classified correctly and then put it on the screen. And it can do this if our computer printed or handwritten 45 different styles. You may be thinking, this is 1989. Why is it that deep learning didn't really take off until 2012? This video, they're saying the researchers, which is great, they deserve recognition. In addition to this architecture in 1989, the multilayer perceptron, which we'll call him this class, fully connected neural network can also develop something in 1998 called the index, which is an architecture that is called the convolutional neural networks. And convolutional neural networks, which again, we're going to talk about a lot in this class, are the modern architecture that gives really high performance on image classification and computer vision problems. So we are kind of in a renaissance since 2012, revival of deep learning and its utility to everywhere today. But you might ask, why is it the case that even though these came in 1990, 8.19, 89, why do you only ticked off in 2012? And a lot of that is due to simply the computing power and the amount of data. We're not big enough in the 1980s to really see the power of neural networks. And one thing that really helped to advance this was that beginning in 2010, there was a new dataset called ImageNet. It came out. These numbers that I published or that I had up here are from many Jessica. So almost surely these numbers are even higher. But basically what ImageNet, It's just a collection of many images, tens of millions of images. These are examples of the images. And the images are guaranteed to be one of 1,000 barrels. And so those variables include motor scooter, container ship, leopard, might, et cetera. And this dataset was more in an unprecedented way at the time. And really the goal of this dataset was to try to help people develop better computer vision algorithms for classification, where it gives you an H. And basically they had a competition where we said, for the side, take an image and I'm gonna give you five guesses for what the image is. And if you are one of the slides and adjust them, you got it correct. So if you look at container ships, this is an image container ship, this is true label. And the five guesses that the algorithm takes this container ship and the length of the sides of these bars is basically the competence of the neural network that it is a container ships. So it's very competent to container ship. But then after that, even the next guesses are reasonable. Lifeboats fire both platforms. Even if you look at this example of a leopard, this is a leopard has that with very high confidence. But then the next guesses are like jaguar and she does know wrapper, etcetera, right? And even when it doesn't get it right. So if you look at row two, column three is an image of a cherry, even though there's a domination behind it. And you may ask, why is that? Well, these datasets for labeled by humans, whichever treatment saw this, determined that this is a picture of a chariot. And so this is one where the neural network, it's wrong to judge. The top five guesses are domination, rape, other berry, I'm not sure what the fourth guest is and the fittest is correct. So if you look at that image, you might actually think the neural network did the right thing because the domination is a pretty prominent. Did you end up with conducts most common? Any questions on ImageNet? Competition? And basically, every single year, they were going to have a competition where you can submit your algorithm to the competition and they would measure your performance. The performance will be measured in terms of error rate. So the lower you are, the better. Zero per cent music you've done everything right? And in both 2,010.20, 11, you can see that no algorithm dark better than a 25% error rate. By the way, there are some graduate students at Stanford who wanted to determine what is this human performance on this. So they sat down on a weekend and they did just finish that task themselves. In human level performance is about 5%. So even we make errors when we look at these images. Even in 2012, which is the third column, the performance of these architectures are all still worse than 25% error rate except for one. And this is called AlexNet. Alexnet was not just below 25% error rate. It was far below is x 16/s. So this is actually quite a considerable leap. In AlexNet was a convolutional neural network. And by doing so much better than all of the other algorithms on this dataset. It really kicked off this deep learning revolution we're in subsequent years. You can see the error rate significantly decreases, where all of these architectures or the winning ones are augmentations to the convolutional neural network architecture. We're going to talk in great detail when we get this CNN's convolutional neural networks about some of those architectural innovations that really decrease. Any questions. A key question, how do you know the initial database stuff? But yeah, the question is, how do you know in the initiate a space that is labeled correctly? You don't care. So the initiative base does have to interrupt raisins or at least enable maples that humans with the students. And so all of these were labeled by humans. We're actually going to learn about regularization later on when we talk about image classification called maple, excuse me, We're actually, if you build into your train the assumption that the labels aren't perfect, you actually do even better. Other questions, yes. The question is from Zoom, which is this figure, top five error rate or top one error rate. The answer here is the top five error rate. So for everyone who doesn't know what those terms mean, talking about the error rate is what I described. What you tend to get five guesses at what the correction inches. And if you get one of them right, That is capitalists, right? There is something else called type one error rate. And top one error rate means you only get one guess. And if this radius right, if this one is wrong, one error rate is obviously more difficult. Question. You can ask the TAs to fact check me on this being taught by our rate, unlike 99% sure, but CSPs. All right, Other questions. Alright, so part of what is fueled this deep learning revolution of the past decade is that today we have much larger datasets. And we also have the computational power to build much larger neural networks that are capable of processing this data. There's this quote from Goodfellow textbook deep learning, which will be our textbook for the course. Fortunately, the amount of skill required to get good performance, but deep learning algorithm reduces the amount of training data increases. This is a way of saying if you're in a setting where you have a lot of data, you may not need a lot of skill to build something fit that data. In essence, trained with a pretty standard neural network could get you most of the way there. Now of course, this will depend on the context ribbon, like we saw in the earlier video from Professor David Silver and defined, there are ways in which algorithmic advances, like for alpha goes thereof, can lead to very, very impressive improvements in performance. Not just the step sizes. These are some parts that just show examples of how data sets have increased in size over time. And furthermore, that our networks have also been able to increase the size over time as a result of increasing power of computing and better and better. Alright? Any questions on any of that introduction? I always give a break in the middle of lecture because I think that this is way too long to fit about this. I will take a five-minute break and I'll come back after that. I have not actually enrolled in this course by nice no deep learning and neural networks for my research. Yeah, So I was wondering if I could add it. If you can if you wanted, you could just shoot me an e-mail. I could get. My concern with taking me is because I'm supposed to do my oral examination with somebody else. Yeah. I'm ready to take another class with me or do I thank you very much. I'll get my friends next to me too. I like it. A little bit. Lighter material from fired here just to make sure that we're still determining. For the class, right? Yeah, right. A lot of linear algebra, statistics. Then ideally all say pharmacy. So we don't get to be more challenging. Hi. I don't know exactly what your you taking. Okay. I just wasn't your question or you just not good material for a moment. All right, everyone. So one logistical and asked me about PTAs. If you are early enrolled in this class, you'll notice that we changed the classroom from Mom Learning Center to 39. And that's because you still able to enroll more students than the Cochrane capacity, even though not all students come to the classroom. But because of that, we do have more capacity to involve students. And so if you need a PTE for the class, just shoot me an email with any other questions? Oh, they said if someone said there's an opening for the class, which is pretty impressive because the class is full before I started. Okay. There are one to two consecutive signs. Any other questions? Alright. Yeah, that's a first for me. Alright. We're gonna go through the syllabus now. And we'll be happy to take any questions on the syllabus and then after that, we'll get right into course material. So here's a rough schedule for the class. These will be the content of the lectures. We're going to be following the deep learning book, which you can find online at deep learning book dot for. The class does not have a final exam. It has a midterm exam, which will be in week seven, and it's scheduled the same week as the Veteran's Day holiday. So you had a bit of extra time to study for it. And if you are in MS Online students, we know that your exams are on the weekends, so we will organize your remote exam on Saturday, February 20th. Alright, but for the rest of the class, exam will be in-person in class during the classroom. Any questions there? All right, class logistics. This class will be done entirely in Python and we will provide resources to you for setting up Python three. And then with every single assignment and we will give you a requirements that gives you all the packages that need to be installed. Just fine. Alright. If you don't have fire Python experience, e.g. you just have MATLAB experience. Please budget in that. While there is a transition from Matlab or Python that isn't as difficult as other transitions is still takes significant amount of time. So please plug that in as you, as you determine whether you're going to take the course or not. Alright, we're going to post a set of formal notes on ruined learn. Basically these are going to contain the content for the course. In usually tourists form. The lecture notes I am projecting up here and will be annotating or more informal and ask that you don't publicly post them because I do a lot of images that I haven't resolved to copyright. So we will distribute them on every annotation that is done in class. Coast to the nearness com. Okay? Alright, on discussing sections. Within this class, we have many discussing sections. But in our experience, many of this discussion sections are very poorly attended. And so what we do is we consolidate our discussion sections and instead of holding more office hours. So what's going to happen is after class today, I'm going to send out an announcement on rumor. And that will include details on the consolidation of discussion sections. Basically, if you're someone who values by discussing sections, we want to hold those for you. So there's gonna be a poll where if you are going to attend live discussions, we're going to ask you which discussions can you intend? And then based off of that, we are going to hold on the subset of the discussion sections live taught by our TAs. And the remaining time that T has been normally be teaching discussions for our kids as extra office hours. The TAs will record one of the live discussion sections and upload it to grow and learn. So if you're not able to make it, you can still see it. In addition to that for when we have homework assignments, the TAs will also create a separate coding discussion video that essentially is going to be instructive and helpful to complete the homework and go over syntax and other Python coding things to help to complete the assignments. Any questions on discussion sections are two, rehabbing schedule the office hours then basically the TAs and I tomorrow are going to determine which discussion sections will be held. And then after that, we're going to allocate our office hours, which will be on across the entire week. Alright, for reading in this class, there will be three components intergrated on. The first is homework. So there are five homework assignments. They are primarily coding assignment. So though early on in for homework when Homer tune, they're more handwritten assignments, solving questions. These assignments are going to be due based off of the due date that we put on the assignment as well as on the syllabus. And they are due by being uploaded to Gradescope by 11:59 P.M. on the day that day or two, we use Gradescope so that they can develop a central rubric. It helps to make breathing more consistent and fair and also gives feedback to the students. That's where they got things wrong. We know that life happens and sometimes there'll be unexpected or unforeseen circumstances. So we're giving three freeway days to every student should only be used in extenuating circumstances. And you don't have to notify us if there is no way that you can just submit the assignment grades up to three times 32323 days within the class. There is a caveat, which is that on any given assignment, you can use that to lay days. And so this is so that we don't fall behind. Then any assignments submitted more than two days late receives the great. Any questions on homework or homework policy. Alright, we'll have a midterm in class that's going to be worth 30%. And then in the final weeks of class, maybe in week seven or week eight, we will also release the final project of the class. And the details on that final project will be released at that time. We'll give a project assignment that people in the class can choose to do. But if you are someone who has a side project or research that involves deep learning, we do have a way for you to apply to cap that as your final project. And in most cases we will accept that project as your final project. Any questions here? I'm sorry, repeat the question. The question is, is the midterm exam written or is it coding? So the exam will be taken in-person and you will fill up the exam by writing on it. But we may ask you to write to it on the exam. Are there questions? The question is, is the final project in certain bicycle person writers? So we allow groups of up to four people, I believe. But it can also be done individually. Any other questions? The question is, will the expectations for the group project is different based off the weather is like an individual or a team of four? Yes. Usually if it's an individual, will relax some of the constraints on the performance. Meaning calculation uses less power and less compute power to optimize. Other questions. Alright? Grading in this class is on absolute scale. If you're like me when I was an undergrad. This a bit because what if the professor gives an exam where this exam for the average just like 25 and everyone gets a graph, that's not going to happen in this class. We design the exam so that the average is at least 80. And we reserve the right if we do get a hard exam, relaxed these absolute scales. The reason that we make an absolute rating rather than a curve is so that we're not competing with each other. And if everyone in the class learn the material well, I'm more than happy to give everyone who is constant. And the absolute scale allows me to do better. All right. Any questions on the absolute scale or grading? And I want to say this upfront. When it comes to grading, I will grade you according to the syllabus and according to this rubric. And I will not change your final grade unless I made a calculation error and that's UCLA academic senate policy. So if you send me a request to grade you outside of the syllabus or this rubric, I will not respond to that request. Alright. In addition to these grains skills, we also will reward bonuses to the final grade. One is a bonus of 0.5% on your final grade. Based off of the scale for filling out the cost evaluation feedback which allows us to improve the class for future iterations. And the other is a Piasa bonus class. This large. We find that Piazza is really helpful for getting students questions answered quickly. And we also find that students answered your question or students learn better when they are also answering questions. So to encourage this, we give a bonus of m plus 1.5% to your final grade based off the posting and participating on Piazza. So on Piazza, you're allowed to post anonymously to other students, but your answers will never be anonymous to the instructors and TAs. And that's because that's how we know how much you posted and how we can assign this piazza bunnies. Also please keep that in mind as you post. The reason that we encourage this is also to help your learning. At the same time, we're not trying to offload. The TAs will still be continuously monitoring Piazza. So we know that there are some questions out there where it's a very difficult question and other students may not have the answers. So the T is in general will be uploading Steven question student answers that are correct. And if there's a question where the students don't have an answer to the TAs will go ahead and answer that question on Piazza. Okay. Any questions on that? All right. This should be the primary means of asking and getting questions naturally, the class I can mention be like Piazza to be student-driven. And the TAs will answer any questions as students are unable to answer. If you have any course-related questions that are not appropriate for y'all. So then please e-mail our entire teaching staff. The reason I ask this is because some TAs go above and beyond and answering questions. But then when students find that out, bacteria gets bombarded with all the questions and it becomes a lot of work products here. So if you know all of us, then we're able to divvy up the work amongst the TAs. All right. And then if you have matters that are more sensitive than that, please feel free to always contact me directly. Alright. Further notes on the crops like I mentioned, we have five plants homework assignments that contain both written and coding. Coding in the homework will walk you through Jupyter Notebooks. And usually it's going to be clear whether you have done things correctly or not. So usually when you submit the homework, you're going to get full marks on it or not. We will release the solutions for the written. I don't know why it says homework exams, homework questions. But we will not release the solution code for homeworks. And that's because we use these assignments every single year. Homework is submitted via Gradescope. We talked about that already. And then we also talked about how we gave three questions. Alright? Some of you have a question on how C1 and C2 47 differ. So this is where I have a C at UCLA in terms of how they separate undergrad and graduate classes. I feel and you should be able to take his past respect to the fifth if you're an undergrad or graduate class. So C1, 47. 47 just allows undergrad and graduate students take it together. Undergrad or frequently concerns if they're going to be graded on the same scale as graduate students where I went for undergrad. They actually always just said No, you're going to be located on the same scale. And actually oftentimes undergrad and graduate students. After being on this side of the exams. I know that to be actually the case. Sometimes the undergrad average is higher than the graduate student average, and many times it just statistically indistinguishable. Ucla doesn't like that. And so they require there to be differences. So I'm happy then to bridge T1 47, 47 students on different scales. So graduate students and undergrads are not adversely impacting each other. And then there will be one question on homework number three, that will be optional for C14, E7 students could not see 247 students, the classes or otherwise. Any other, any questions here? Alright. In general, I'm going to post the slides that I'm projecting up here to grow and learn prior to lecture. So if you're someone who likes to take notes on the slides will be able to download them and follow along. Okay. The question is, can I clarify what I mean by different scales? Yes. So this is the scale that is used by default. And if e.g. the undergrads have a median score that is like say ADH, but the graduates of these have a median score that is say like 92. I will, I be relaxed with undergrads scale, but not the gratitude scale. So that mantissa can be relaxed. So that'll be based on just the performance of the students with C1 through C7. At the performance, we can see 24x7. Other questions. Alright, so the non annotated versions of the slides will be posted to the ruler and try to lecture. Um, we'll list the readings in the textbook at the start of each lecture topic. And this class will have a toll TAs. There's one note about the TAs, which is that there's one TA which is taboo and the TAs will introduce themselves very shortly. He was a TA for the MS Online version of the class. And so his office hours are open to all students, but he will prioritize questions for MS online office hours by policy. Then there's also the TA who will run this and check and be sure that there are addressing any questions here. Alright. This class is going to be very practical and theoretical. So we're gonna be focusing on them implementations of neural networks. And I'm algorithms to train them. We will not cover the theory of deep learning, which other classes cover. And in general, this will not be a theoretical class. We will be rigorous whenever we use math, of course. But this won't be true space. Really, this class is more focused on equipping you with the knowledge about practical neural networks and how to implement them that may be helpful in actually using neural networks for research or industry, industry applications. Of course not a statement on the importance of theory. Theory is super important. It's just not what we're going to be focusing on in this class. And actually theory and deep learning is quite difficult. So I'll just give you a preview that several of the questions you're going to ask me in this class, I'm going to have to answer with saying this isn't terrible results because we may not go a theoretical reason for why something works. But we do know that it works. Questions that just like for the last question is, am I saying this thing about theory? Because we don't have the time and not the actual thing in terms of I'm sorry, what do you make is if you say I understand that you don't have to go and you just don't have time? Or is it like, Oh, I did this. I don't know why it works, but it does. Yeah. So students asking me, is it the fact that we're not covering theory because we're a time crunch? Or is it sometimes the reason where really like that theory isn't developed and we dumped out and it's a mixture of both in this class, be documented even at the theory was known, we're still focusing on the practical things because of time crunch. But in several cases, we're still looking for theoretical answers for empirical results. Other questions, alright, fine on academic integrity. To let you all know, can I take academic integrity very seriously? And if you're tangent, violation of academic integrity as defined by trooper and principles that I do follow through on reporting new to the dean of students and any disciplinary hearings set packing as a result of that. So please just note, I take academic integrity very seriously. Any questions? Alright. Other customers, what is the academic integrity policy with regards to pretend homework? You're welcome to work together on homeworks. Just be sure to submit your own carrier. Yeah, In general, you are welcome to collaborate in this class. Alright? So for prerequisites, this class requires a solid understanding of probability as something like ECE one-to-one, or equivalent. Linear algebra. We also, this is a prerequisite, prior exposure to machine learning. And I want to say up front that several students have taken this class without this prerequisite and they've been able to manage. You're welcome to take this class and that I don't enforce the prerequisites. And we will do an overview of machine learning to refresh things and be sure we're all on the same page before going on to neural networks, um, but the class would be more difficult if you're not coming in with any prior machine learning. These are examples of topics I assume, you know. And you're going to upload basically the formal notes for the entire class on Boomer. And so you're unsure if you meet the prerequisites, please go ahead and take a look at those formal notes and then see if the material is has definite that they feel comfortable. And again, I don't enforce the prerequisites. And so you may not have any other requisites you want me to tell the class, but the class will be very difficult. And so that's, I put that on the student's questions. Alright, I've mentioned this already. If you are familiar with Python is time-consuming to transition to Python. And I want to say up front also that this class is a lot of work. And so I understand that it's an elective, but on my end, I feel to really understand this material, you need to spend the time really stroking through actually implementing it. And that takes time. So we give fairly demanding homework assignments. And if this is a quarter where you already have a heavy workload, I wanted to let you know that upfront so that we can take that into account as you determine what classes are, right? So this talk will be a lot of work. Any questions? Alright, about me? This is my sixth time to do this class at UCLA. Like I mentioned, it's one of my favorite classes to teach. Outside of teaching. My research in my lab is focused on applications of deep learning and machine learning. And it's tied to the brands, so to the brain. So I am primarily a computational neuroscientist and a neural engineer. What that means is the neural engineering side. I'd built devices like brain machine interfaces or I drew, we were computing machine interfaces in that cycle I showed you earlier. And then on the computational neuroscience side, we apply machine learning techniques to try to understand how the brain represents information and how it does computations. But that information. Beyond that, I'd like to now introduce the TAs. So we have several TAs, but I'll do is I'll show define mental ask each TA who's quite as shown to just stand up and then you project to say a bit about yourself. And so we'll start off with envoy, who is our head TA for this class and before family. Two times before and also received a samuel the Excellence in Teaching Award for this class. And so he's very experienced here. I'm a PhD student. I'm working with Professor one here, I've drawn it. And my research interests are in computational neuroscience and it should be the documentation. So as Jonathan mentioned, this is my third time. This was in declining and overall, I have 11 times many posts within undergraduate and independent arguments. So my favorite courses that unfortunately is not disclosed. My favorite courses, 36 ft, which is the force on convex optimization. So one advice that I give to students every quarter is that you should take that course. That hasn't been the most rewarding course I have ever taken. It hasn't helped me understand all aspects of machine learning. So please feel free to talk to me if you have any questions about that person. I'm more than happy to help. So far we've been working to do all this wonder and I won't be directly with each other and get the best out of this TAs if you come up from here. So next we had a first-year kid. He's good bye, professor. Many models for all scenarios and models that are reliable and dependable. And I'm working on my model than the condition. So it is my first time. Second time, or the cross where I'm here to help you with any questions and concerns. You may rapidly quicker. Thanks so much for taking. The homeworks are going to be what I would do. But more than that, I'm looking forward to learning from one another. Hello, students. This is my first video that I apologize. And I do hope that Ali, The next day. She is not entirely economics. This is centered video to introduce herself. This is not Keynote. Hi everyone. Welcome to enter 23. My name is Hindu and I'm a second year master student in Electrical and Computer Engineering. Most of my coursework and projects involve machine learning. Scores last window and thoroughly enjoyed it. It's quite exciting to be on the other side. I'm looking forward to interacting with you all. I hope you'll find the course as rewarding as I did and wishing you a great quarter ahead. My master's student in computer science. So I hope you also find that you also to share my research involves using machine learning. And you see I've logged on using the lung cancer. We have two more hydrogen and then the secondary losses today. But this loss yet feel free to reach out to me with the advocate. Initially is elastic, which is student working on time series, Transformers and MATLAB and high conflict difficulty questions. And this is one of the greatest courses. There is still a great asset that you've taken this course and you would enjoy. Thank you to the TAs. I think it's appropriate. You all get in or out of a clause. They do all the hard work behind the scenes. We know from last quarter that the TAs are very important and directly work with for the TAs fire and teaching. And and I know that they care deeply about their learning, are ready to be really helpful during office hours. And for the four others that I haven't worked with, we've talked and I know that they feel that way also. So please be sure to use the TAs as a resource to get pumped for this class, we are really here to help. Alright? Any last questions before we dive into material? Alright, we're gonna get started then. So we're gonna start off the class with a machine learning perspective. This is something just to get us all on the same page. If you feel very comfortable with machine learning, then the first factor and then the next lecture. Maybe a bit of Congress, both sides for you, but we just ask for your patience. Again. I just want to get everyone refresh on the same page and have a common understanding of loss functions, parameters of gradient descent. And so we're going to refresh all of them. The reading for this receptor will be Chapter five up to 5.5 for the deep learning book. And then if you need a refresher of linear algebra and ability, please look at chapters 2.3 of the deep learning pluck from fifth dollar. You likely know that there are many types of machine learning. There's unsupervised learning. There is reinforcement learning. In this class, we're gonna be focused on supervised learning problems. Which means that in most of the settings we're going to give you both the data and the labels. And then this class is also going to focus largely on supervised classification problems. Which means that because classification means taking a data point and putting it into one of k potential classes. That's largely because we're gonna be working with this dataset called SeekBar time. And cost will be working primarily on supervised classification. We will do some regression in this class to motivate or to refresh machine learning. We'll do a regression problem later on this class when we talk about recurrent neural networks and other architectures, we'll talk more about regression. But at least for the majority of his class, we're going to be focused on this dataset. This dataset is called C4 and C4 ten. There are going to be images that are given to us. The images we can call a vector x. And each of these images of which were showing, I believe at ten by ten grid of them. Each of these images is going to be 32 by 32 pixels. So it's going to have a width and a height of 32. And then you all likely know that the way that we code for color is the R, G, and B values of an image. So in addition to having 32 by 36, every pixel is going to have three values for RGB. And so every single image is gonna be 32 by 32 by three. Now, each of these images is going to correspond to one of ten classes, so that's ten and C4 tag. And these are those tiny classes. So this is the possible classifications of each image. Each image will definitely be an airplane and automobile, et cetera. There are at least to start, we'll say one way that you can classify marriage is to take our data point x, which is this 32 by 32 by three objects. Object called a tensor. So it was just 32 by 32. We call it a matrix, right? We have rows and columns, but when we go to higher dimensions, but generally call them textures. And because this has three dimensions over which there's width, height, and depth, we would call this a three-dimensional tensor. But at least to start, what we could do is we can take these 32 by 32 by three values. If you multiply them together, you get 3,072. And so I could reshape this into a vector that has 3,072 elements. Right? And then the problem of classification would be to say the following. What I wanna do is I want to take my image x and I want to pass it through some function, I'm going to call it S. And the output of this function should tell me which of these ten classes have you lost it. All right, so the machine-learning problem takes my image x and tells me, is it a bird? Is it a cat? Is it a deer? So let me just write that clearly. The class refers to what the image belongs to. And C4 times a dataset you'll become very familiar with is actually going to be data set that we'll introduce to you in homework number two, where you'll start off by learning path that is just a linear via something called the softmax classifier. Then on homework number 34.5, you're going to start implementing neural networks and an augmentation to neural networks on the same dataset. And eventually, you're going to see the accuracy of classification on CFR can go up to several tens of percentages as a result of using neural networks. And so this will be a key dataset for this class. Any questions on this dataset? The question is, what are the hardware requirements for doing classification? Front seat part-time within this class? They are not demanding, so you should be able to do it on whatever computer you have. Will also put out a tutorial on how to use Google Colab for which you will also, if you choose to use, have access to it. A GPU. It's not a very powerful GPU, but it's watching you, but you'll be able to do all this on your standard X86 computer. Other questions. Great. Thomas question is, for this datasets, are the labels binary or are they the spores? Spores? Thomas question is basically, if I have an image of an airplane, are they going to tell you the probability that's an airplane and automobile, a bird, or is it just going to tell you an all or nothing? It's an airplane, not an automobile bird, and it's the latter. So both the dataset will tell you what is the class of that. Alright? So we are going to just again, give a refresher on supervised learning just to make sure that we all have stain fundamentals in terms of the homes of machine learning. But the complaints of the machine learning problem which I have lost function, the parameters, the gradients, and then doing gradient descent. Alright? So let's say we want to rent a home and rest. Good. This is all synthetic dated and I just conjured up. And we want to know if we're getting a good deal. So here we have sample homes where on the x-axis we have the square footage of the home. And the y-axis we have the rent of that. Alright, again, I can try example. The real data will be more complex, but square footage gives you a pretty good indication. Of course, there are other factors that will determine the amount of that. But in this case, what we'll have is, we'll have components of machine learning problems. The first will be the input to our machine learning algorithm. This example is going to be the square footage. Because in this case what we are going to want to do is we're going to want to say for home that I'm looking to rent, what is the square footage? And given its square footage, I want to predict what a good value for the Rex group. And so that tells me my output is going to be the rent. Not the goal of the machine learning problem is going to be learning some function f. Essentially the oracle of Westwood wrench, where F is going to take as input, square footage, and output to me what it predicts the rents ought to be read back home. Any questions on this setup? Alright, so considering the supervised learning problems, machine-learning, essentially, continent says, I have a goal, which is, I want to learn this function f that maps my square footage to rent. I know that my input, so many outputs. Alright, does it the square footage into retrospectively? And now machine learning, I have to start to make some design choices. Alright, so the first is, what is this function going to be? In other words, what model should I use for, right? So can someone tell me what the simplest model here might be that is reasonable? Yeah, Great. Linear regression. So I could choose that f of x is equal to align, right? So I could say this data, it looks like it's pretty well modeled by a line. I'll draw that line right here. Going through all these data points, alright? And we know that a line, It's described by two numbers, the slope and its y-intercept. And so f of x would equal a x plus b, where a is the slope of that line and b is the y-intercept. If I were to call the monthly rent some variable y, square footage is x. This will be the model. The red line would correspond to the model y equals a, x plus b. This isn't the only model we could have chosen. The model that we choose is up to us. I could have chosen to look at these little I guess I'll call it noise. Fernando isn't easy to say. I think a line might not be sufficient to model the complexity of the dataset. And maybe I might want to make the model that is a bit more complex. So maybe I want to make a model and where it's polynomial, so y equals b. And then let's say it's like 100 order polynomial. So I could say d plus 100, 100 plus 99 times x to the 99th, plus all the way down to a one x. Alright? And what a model like this would look like is one that actually pretty well goes through many of the data points, might have to make it a bit higher-order to go exactly through everything but a model like this orange polynomial talk more powerful in terms of fitting these points, right? So point-of-care, same. We get to choose the model. The model could be as simple as linear as more complex like this. Polynomial. Whatever we choose for the model, we step back. And then after this, we need to know a method to determine how to pick a model. Alright? So when I look at a model like the red line, and it's y equals a x plus b. The x and the y, our data. And then this a and this b are called parameters. Basically in this model, the parameters of the things that I get to change to make this model as good as possible. So I get to choose a and b, which are the slope and y-intercept for this line. To try to model this data, to try to get as good a prediction of the rents square foot as possible. Okay. Any questions so far? So how do you think? Yeah. Great. But I don't want to sit here. So the question is, yeah. So the question is asking you basically about computational time. This model is more powerful, right? It has higher capacity in terms of it can model more complicated things. But it may take longer time to compute. Now it turns out that this regression can be solved by something called the squares. But your point is still valid for neural networks where the training time you can be on the order of hours. In fact, it can be on the order of months, hopefully not years, but could be a really long time. So at what point do you trade off the simplicity of the model with few times and those are all optimisation you have to make as engineers from educated guesses. And there'll be a way to, you know, we'll talk about how we validate and test these models to get a good idea of what kind of performance benefits first thing. Any other questions here? Yes. Yes. So the senior was asking just to confirm, we're trying to find what the values of a and B should be, the optimal values of a and B so that this swine specific data as well as possible, right? And that's a good transition to our next slide. When we say that we're going to choose a and B to make this model as good as possible. We need a mathematically precise way to tell me how good a model, right? So if I were to draw it to linear models, there's gonna be one choice of a and b where the line looks like this. There'll be another choice of a and b where the line looks like, say this purple one. Alright, and then many of you will, what did this and you'll say the red line is obviously the purple line. Because the red line goes to the blue data points. Alright, but I needed to still this into something mathematically precise. So can someone, so think about it and they can someone raise their hand and tell me a mathematical way to know that the red line is better than the purple line. That didn't see any hands going up. Yeah, the students says, we can look at the average distance from the actual data points we get to the line. Alright, so that's correct it, and let's go ahead and write this out. So let's say that we have, I don't know how many dots there are here, but we're going to say that there are 20. Alright? And we have our model, which is this red one we'll call y equals y equals b1 plus a one x. And in purple, we'll call this Y equals b2 plus a two x, right? So they're both linear models, but they have different choices at the end. Alright, and then the students said, how I judge the model is, I'm going to look at my actual data points, like these blue points. So for each of these viewpoints, viewpoint is going to come with three pieces of information. The square footage, that's the x coordinate. The I'm sorry. It's only going to have tip is a progression as the square footage, the x-coordinate and the announcer fantastic work. Yes. So for the right data point, Let's say that this is the i'th data point and I'll index that by using a superscript i. The input is going to be the square footage. That's the variable x. And for particular data point, data points, that'll be x superscript parentheses. Alright? Then the output is going to be the rent, and we'll call that Y. So each data point is composed of the square footage of the data point and it's rent that is referring to this appointment. And what we wanna do then is we want to calculate said, which is the average distance of our prediction from these data points. And so what I'm going to do is I'm going to quantify the distance here as epsilon. All right? All right. So what is F19 mathematically, epsilon I is going to be the difference between the rent of this data point, which is why I minus the rent predicted by my red barn. So my red line is going to predict the rent of the data points as f of x pi. I'm going to call this y hat. So if I subtract off y hat of I, which is this value right here, that is going to give me the length and sine of this one. Alright. So we'll come back on Wednesday. All right, everyone, we're going to get started for today. So a few announcements before we begin. The first is that homework number one is uploaded to CCLE and sorry, I think it's actually in the week one materials, not week two materials, week one materials. And it's going to be due on Wednesday April 14th, uploaded to Grayscope by 1159 PM. And then we're going to continue on today talking about ion channels and membrane potential. And so these are the corresponding readings in the Principles of Neural Science book. Again, we've uploaded that to CCLE. All right, last lecture I mentioned we are going to have Shashank who introduced himself. Shashank is the other TAMS class along with Tonguey. So Shashank, can you unmute and give a brief introduction? Yeah, it's so professional. So I'm a second year master's student in the ECLE department and currently person research in privacy preserving machine learning. And we go through topics of differential privacy and cryptographic tools like homomorphic encryption, etc.

So the idea is to like ensure privacy through the neural network computation task. One of the, I mean, most of the classes that really help me to understand neural networks are neural networks 247 by Professor Kaub. Then there is a great course on adversarial robustness of machine learning models from the computer science department. And obviously to learn more about the biological systems they try and emulate neural signal processing is a really good, give you a really good understanding of how neural networks work. So yeah, that's about me. Great, thank you, Shashank. All right. With that, I want to just ask if there are any course logistic questions. All right, then we'll get back into material. So last lecture we were talking about neurons and how neurons come in all different shapes and sizes, but they have the same fundamental operating characteristics.

Not unlike how for a transistor, they all have the same components, but you can, and you can, sorry, much like for a transistor, you can have different whips and lengths, but they all have the same components and then computational complexity and capacity come from how you hook up these neurons together. And we were beginning to talk about how the neuron is a fundamentally, fundamentally electrical system. And so we talked about how if you have a neuron and here we're showing here inside the neuron and here outside the neuron and you were to stick two electrodes in one of positive electrode and the other are ground. So I put in two electrodes, then if I could measure the voltage from inside to outside the cell, which I call the membrane potential, VM, that this is a voltage that will give you a reading of about negative 65 millivolts. All right. And we talked about how this voltage is present because we have both positive ions like sodium ions and potassium ions, as well as negative ions like chloride ions that are in the extracellular and intracellular fluid.

And there's an imbalance of these ion concentrations at rest. We're going to talk about how those concentrations are set this vector, but because there's an excess of positive charge outside and an excess of negative charge inside and positive and negative charges attract, right? Then we're going to have a sheet capacitor form where these positive and negative charges are going to try to get as close to each other as possible. All right. And so this is going to cause there to be an electric field that points into the cell. And last lecture, we had done this poll question where we said, let's say that we open up a so-called ion channel and the mechanics of the ion channel will also get into more detail this lecture. But this ion channel is unique in that it lets through just one ion. It lets through Na plus ions. And Na plus ions are more concentrated outside the cell than inside the cell. So 10x more outside than inside. We asked the question, if I were to open up this ion channel, so now that Na plus ions could flow through, we asked the question, are they going to flow into the cell or are they going to flow out of the cell?

And at the end of last lecture, we did the poll and most of you got it correct that Na plus is going to flow inside the cell. And we said that there were two reasons for this. First Na plus is a positive ion and positive charges are going to be attracted to negative charges and be repelled from other positive charges. So because this electric field points in, it's going to provide an electric driving force that pushes Na plus into the cell. And this we call the drift current. So the drift current was our electric driving force. That would push Na plus into the cell. And then we said there was one other driving force, which was a chemical equilibrium driving force. So Na plus is much more concentrated outside the cell than inside the cell. And so if I were to open up a channel to get chemical equilibrium where Na plus is equally distributed everywhere, then Na plus would want to flow down as concentration gradient to go inside the cell.

And we called this current, let me draw it in the color purple. This is called the fusion current. And it's the chemical driving force. All right. Okay, I wanted to just recap this and ask if there are any questions on this. Because this will be an important principle for the rest of today's, for some topics in later slides this lecture. All right. No questions. So then we talked about how we're going to view the action potential from multiple different levels and give some concepts about the action potential that we would unpack it bit further. And the first is that if I were to have Na plus go into the cell, right, if Na plus we're going to go into the cell through these ion channels with both the electric drift current and the diffusion chemical driving force pushing Na plus into the cell, when I have positive ions going into the cell, that's going to increase the membrane potential.

All right. So if I were to open up some Na plus channels, the resting potential, which is normally negative 65 millivolts, if I open up these ion channels because sodium flows into the cell, now that voltage is going to increase slightly. And so that voltage is going to go up a bit. And we talked about how, and again, we're going to talk about this more in today's lecture, these ion channels are also special in that if the voltage increases, they open. And these are particularly these Na plus channels. And so if Na plus flows in, it increases the voltage, Vm, and that causes other channels, which are going to open based off of the voltage increasing to also open, if these channels open, more Na plus is going to flow into the cell and the voltage is going to go higher opening up more of these channels. And this initiates this positive feedback loop that causes the voltage to skyrocket because all of this Na plus is flowing into the cell. And that is the positive rise of the action potential.

That's how the voltage of the cell goes up for this action potential. Question from Ammona. Yeah. Hi. I wanted to ask, sorry, about the previous slide is one more responsible than the other in terms of drift current or diffusion current for the Na plus to be moving into the cell? Yeah, that's a great question. So in this case, because they are both flowing in the same direction, they're both responsible. It's actually possible to compute the current that is going to arise from, or is possible to compute the voltage at which the concentration, the diffusion current and the electric field driving force cancel out. And so they both contribute in this case because they're both pushing in, we know the answer. But the buyer to give you a separate question where I said, let's say I open up now in green here, a K plus ion channel. So in this case, for a K plus ion channel, that's through potassium, we can see K plus will want to be driven in by the electric field.

And they'll want to be driven out by the diffusion because K plus is much more concentrated inside and outside. And so if I just told you I opened up this channel, you couldn't say if K plus is going to go in or out because the two currents are across each other and they both contribute. And so to actually answer this question to break this, this individuation, I would have to give you more information about the strength of those currents. And so, well, I'll show some equations later on that get it when these two aren't equal every. Yeah, thank you. Great. And a question from David. Yeah, is the opening a permeability or is there actual physical opening? The opening is believed to this still in the open area of research is believed to be a changing confirmation of these ion channels, which can be triggered by different processes that we'll describe later on today.

All right. Cool. So. This is lecture two for EC 189. I was planning to continue where we left off, which is we were going to start to talk about devices that exist that interface with the brain. Or with the central nervous system in general. Before diving back in to where we were, I wanted to just ask if there were any questions from last lecture that people want to follow up on. All right. So we were last talking about the retinal implant. The idea would be that you would wear glasses. Sorry. Before that in the retinal, in retinal blindness, what happens is that normally there are cells called rods and cones that process light that naturally comes into your eye. And those rods and cones activate a circuit in the back of the eye that ultimately sends signals from so called retinal ganglion cells to the optic nerve. And then the optic nerve carries those electrical signals that were transduced from the white in the circuit and sends that off to the brain. Where then we have our perception of what we what our eye saw. And so in retinal blindness, these circuits at the back of the eye that send signal to your optic nerve no longer work. And so the idea is, although they no longer work, it actually turns out that we have really good knowledge about how this circuit.

Actually, it's weird to see. We have really good knowledge about how this circuit actually computes and how light signals are transformed into electrical signals. And so the idea is, if we could take a video camera and video the scene, we could telemetre that information to hear this chip through our communication. And then that signal that could generate the spiking signal that we would then want to send to some electrodes that stimulate the actual retinal ganglion cells that then send electrical signals to the optic nerve. So the idea should bypass this broken circuit by recording the video and then stimulating the back of the eye, how we how it might have originally intended to how light would have originally intended to simulate it. Right. And so last lecture we asked the question, what are some problems you might see or what are some not problem, but challenges that the system might face. And so some people mentioned that it could be hard to do the surgery. It might be hard to get the signals reliably to the chip. Are there any other problems that people can think of with respect to the electrodes here? I believe you also be able to unmute yourselves and just speak, but if not, someone let me know or chat. Could it just be why we have the electrodes in the right place? That's related to it, which is you have to make sure that you're stimulating the correct cell that eventually go to the optic nerve. So in this case, the answer that I was looking for is the challenge that can come up as the following. Let's say that we have our electrode array. And there are some cells hanging up here that we're trying to stimulate.

It's not that it's very hard to precisely stimulate one cell and not the other. Sorry, I'm just noticing that my keynote is lagging by quite a bit. So let's say that I want to stimulate this cell over here. If I were to send electrical stimulation on this electrode right here, you can think of this as we're hitting a very finely tuned circuit with a hammer. When I send an electrical current into this circuit, that current, Audron Red, is going to propagate out in all directions. And it's very hard to just stimulate this one neuron that we want here without stimulating all the other neurons. And so actually a challenge of this system is how do I, I know what I want this neuron activity to be like, let's say the top neuron here. How do I design stimulation patterns to stimulate just this neuron and not the others. And so an example of an active research area in here is to design stimulation patterns across your different electrodes. And that for example, maybe this third electrode sends out this electrical signal that emanates out in this foggy metric circle. Maybe the electrode next to it sends another signal electrical signal that also emanates out. As you can see here, the red and the purple electromagnetic waves are going to start to interfere with each other. And you could have constructive interference and destructive interference. And so you might think to try to pattern this stimulation on these electrodes to just target particular neurons that you care about in the vicinity. It's a difficult problem, but that's an example of how people are trying to solve this question.

So he'll. Maybe if you ask me how you want your own, could you like specifically design it in a way where say you want the red one there to simulate only the left green one and not the one on the right. Then you can have the purple one signal be in such a form that it acts as a low pass filter for the part of the red wave that you want to chop off and not the red right one to not see. You can just filter it out using a special touch to pass on the purple one so that the right one only sees the purple signal and the left one sees only the red signal. Could you is that possible to do or. It's not currently possible because it's not straightforward to implement a filter from just an electrode that can stimulate. So that's why instead of filtering the ideas here are constructive and destructive interference because really all we can control are signals that we send out. And we could we could try to position and set the parameters of the signal so that they will destructively interfere some cells, but then sending out a signal and asking it to do a filtering operation. That is non trivial. I'm not sure how that would happen. So that's a rational implant is an example of how we can write in to the central nervous system. You might also have a cochlear implant. So for someone who is deaf. They can't hear outside noises. The thing is that actually a cochlear implant is not. Also operates on on a principle that is not too difficult to understand here. I'm showing you the cochlea. The cochlear got. And what happens is that along the cochlea you have a bunch of these hair cells.

Normally when you hear sound, we talk about this in the first lecture. What happens is there's a pressure wave that's being sent to the air and that pressure wave is going to vibrate the liquid in this cochlear duct. And that vibrating liquid is going to move these hair cells left and right. And so the hair cells will be activated. And that will cause an electrical signal to be sent to your auditory cortex. So then you hear a sound of that frequency. And the really interesting thing about the cochlea is that it has so-called tonotopic mapping. Which means that if you activate hair cells here, it's the perception of a 20,000 hertz signal. And as you go down the cochlea, these hair cells activating give off the perception of a 3,000 hertz signal. Hair cells over here give off the perception of a 200 hertz signal. Hair cells over here give off the perception of a 600 hertz signal. And so as long as you can get the spectrum of the signal, as long as you know what its frequency components are, then you can replicate the sound by stimulating the hair cells in a particular area. So you can imagine if you're listening to someone speaking, you can have a device that records the audio. And then after it records the audio, it does a Fourier transform on the audio. And that Fourier transform is this operation you will all know very well through class. And it takes you from the time domain to the frequency domain where I know as a function of frequency, how much power is in a signal. And so this signal would have very high power at low frequencies, the lower power at higher frequencies. And so if I could take the Fourier transform and know what these powers are, then I could say, OK, let's say that this was centered at 300 hertz, then I could stimulate here with a large power. And that if this was, let's say 1000 hertz and I could stimulate over here with a bunch of power. And so that's how cochlear implants work. Any questions there?

All right, everyone, we're going to get started for today's lecture. A few announcements before we begin. The first is we will release homework number one by this Friday. It'll be uploaded to CCLE. And then it's going to be due the Friday after that on October 16th. All homeworks will be due uploaded to Gradescope by 1159 PM. And so please be sure to leave enough time to make sure you can upload all the documents to Gradescope and that it doesn't push you past the 1159 PM deadline. We sent out an announcement with instructions on how to sign up for Piazza and Gradescope. And so if you haven't done that already, please go ahead and follow those instructions. And then lastly, we're going to send out an announcement shortly later today on the consolidation of discussion section so thanks for submitting the Google form that we sent out and we were able I believe just got an update from Tom, we were able to find three discussion sections that everyone will be able to make who wants to attend live discussions.

All right, any questions before we begin? All right, and then before we begin, I'm also just gonna see if I can find my TA, okay, and make them co-host. There's Tom Moy. Okay, and make them cause there's more. Right. Okay, I'll maybe deal with that during the break. Alright so we'll be clear on that in the future will always announce when the next homework is going to be released and announces like we just did and when they'll be due. motivation of not having not having assessments that are worth so much so that it might encourage students to to to be academically dishonest.

And so we talked about how the grading scheme is set up and how their bonuses on pizza. Right, so now I just want to talk in more detail about exams for this class. And so this is a slide here on academic integrity. I want to put it up to communicate to you all that I take academic integrity very seriously. I believe it's important that our assessments are fair. And when we make choices on what to do for this class, we do it with fairness in mind. And so in this class, if we suspect you of academically dishonest behavior, please know that we will, that means the teaching staff, we will follow up on that and report that to the Dean of Students.

And so this is also my slide where I'd say, please don't put us in that situation where we have to do this. Please be academically honest and that matters a lot for this class. So we will have a midterm exam and a final exam. And during this time of online instruction, online instruction is very difficult for us to prevent cheating on exams. We acknowledge that, for example, due to collaboration. And so to try to make exams more equitable, all exams are going to be open note, open book, and you may access notes on CCLE via your computer. It is closed internet in that we don't want you to be googling for solutions. However, the TAs and I also aim to write exam questions that could not be easily googled. Finally, we're also going to, after the exams are collected, perform some analyses on the exam answers. And if the TAs or I suspect any students are collaborating on the exam, then we reserve the right to administer an oral exam to any students who are suspected of collaborating. And the oral exam results will supersede the written exam results. If there are any other policies that we'll have towards the exam, we'll announce that closer to the exam.

We also recognize that some of you may not be in the United States or maybe you're on the East Coast and if you're in a different time zone such that taking the exam during class time or during our final exam slot would be difficult, please email me by the end of this week so that we can have a list of the students and we can organize a separate exam time. Okay, so many are saying they aren't able to connect. That's peculiar. The lecture will be recorded. I do see a lot of people who don't have their mic or video set up. So, here's what I'm going to do. I'm going to end this meeting right now, and then I'm going to start it again within the next minute, and then we'll see if people can connect them.

So I'm going to end the meeting now.

All right, everyone. For today, we have a few announcements. First, a reminder that homework number four is due this Friday, uploaded to Gradescope. And on Monday after class, we sent out an announcement on CCLE with details about the midterm exam, which will be on Monday during class time. Alright, so please be sure to read over those midterm details, which talk about the exam timing. So the exam will happen between 2 and 3.50 PM during class time. We're going to upload the exam to CCLE at 1.55 PM so that you can download it and print it out if you desire.

You don't have to do your work on the exam. And then you need to stop working at 3.50 PM and then we give you 10 minutes to upload your exam to Gradescope. All right? So those details will all be on CCLE. The TAs and I will also be in a Zoom room, not this room, but a different link that we have put in that CCLE announcement. So please drop by that link if you have any questions about the exam and we'll be posting any updates or clarifications to Piazza.

All right, so those are the details from the midterm announcement on CCLE. HKN is going to be holding a review session for the midterm for this class. It's going to be Friday, November 6th from 4 to 6 p.m. Pacific Standard Time at this Zoom link. Now I didn't request, or I haven't coordinated this review session with HKN. These are other students who have taken ECE 102 before and wanted to do this service to help students review the material. And so again, we haven't coordinated with them. But likely, topics that they cover are going to be related to what we'll do, what we've covered in class. All right, and then Tom boy is going to hold a midterm review session.

This is of course coordinated within the class. This will be Sunday, November 8 from 12 to 2pm, and it will be at Tom boys usual office hours link and discussion section link Tom boy is going to record this review session and also post the solutions for this review session. OK, that was a lot. Any questions on announcements or questions on things related to the midterm? So I saw in the chat, Rodrigo says, ask so we don't have to log into class to take the midterm. That's correct. We're not going to proctor this exam over Zoom. Okay, any other questions?

All right, so we'll get into material then. So, oh and a reminder, the midterm covers up until the end of Fourier series. So we're going to talk about Fourier transforms today, and this material will not be on the midterm exam. So we've talked about how for Fourier series we can decompose any signal as a sum of complex exponentials and this is for periodic signals. But in reality or in real life in several applications we'll work in, the signals that we work with are not periodic. And so we need a generalization of Fourier series to signals that are not periodic and And this is called the Fourier transform.

And the intuition for the Fourier transform of. So let's say it's a rect. And we'll talk about this more in class. Let's say that the rect existed from negative t over 2 to time t over 2. All right. And so if I were to then go ahead and make a periodic extension of this signal, it would look like the following. We repeat every capital T. All right. But I could also modify this signal by making the zero time longer.

And so if I only cared about really calculating the Fourier transform of this aperiodic rect, what I could do is I could define T to go towards infinity. And so it would extend this zero time all the way to infinity and it would never repeat therefore. Right. And so the idea for calculating the Fourier transform of the signal is that we are going to take a signal and extend and make it period infinite. And so if we make it period infinite is going as a signal that is no longer periodic because it's never going to repeat since the period T, big T is infinite. All right. Any questions on the intuition here? All right. So our basic starting point is we're going to take our Fourier series equations, right?

And so f of t can be written as this infinite sum of exponentials, complex exponentials, where each is weighted by coefficient ck. And ck is the equation that we derive for computing the Fourier transform coefficients. And now this big T here is the period. And in the Fourier transform, we're going to set big T equal to infinity again, so the signal never repeats. All right. So we had started this example of the rect last lecture. What we did is we said we're going to have a rect and the rect is 1 from negative 0.5 to 0.5 and then after that it stays 0 and the amount of time that it stays 0 is, it's going to stay 0 until a time capital T over 2 on the right side and until a time negative cap T over 2 on the And so for this rec that stays zero until these two time points we have the slide that shows the Fourier series coefficients for this rect is CK equals one over big T the period, think of K over big T.

Right. And then the next slide, Cisco through the derivation of those four years series coefficients, which we've done before. then started to just try to get some intuition over what happens if big T goes to infinity. All right, so we started off by just saying, well what if big T was equal to one? So if big T equals one, then the Fourier series Ck is equal to just sinc of k. All right, and sinc of k looks like this function. All right, and so now let's calculate my Fourier series coefficients. Let's calculate Ck, so let's calculate C0, C1, C2, C3, etc. All right, so for C0, that would be k equals 0. So for k equals 0, this corresponds to the complex exponential e to the j times 0 times omega naught times t, which is just equal to 1. All right, and so at k equals 0, then c0, the first coefficient, is going to be equal to sinc of 0. And so sinc of 0 is just this value of the sinc function at t equals 0, or sorry, at omega equals 0. And so that's c0 equals 1. Right.

for C1, this would correspond to the complex exponential e to the j times 1 times omega naught times t. So it'd be the complex exponential that has a frequency of omega naught. This omega naught is equal to 2 pi because omega naught is 2 pi over the period big T and big T here is equal to 1. And so that means that the Fourier series coefficient for the frequency at omega naught, which is two pi, is going to be equal to sinc of one. And sinc of one is just equal to zero. All right, and so the Fourier series coefficient at omega two pi is just going to be equal to zero. And it's the same thing at four pi, six pi, etc. The Fourier series coefficients are all equal to 0. All right, so if big T equals 1, all the Fourier series coefficients are 0, except for the 0th Fourier series coefficient, C0, which is equal to 1. Okay, so I asked at the end of last lecture does this make sense that if my Fourier series coefficients are zero except for C zero, then that means that my signal is just equal to one, because we know that f of t is equal to some of CK each of the JK, But now everything is zero except for C zero. And so we would just have a C zero times e to the J times zero times Omega naught times T. And this would just be equal to C zero which is equal to one.

Right. And so if big T equals one. Then my signal f of t is just the one signal. And we said that this makes sense because if we look at how we define this rect, the rect is a 1 between negative 0.5 to 0.5. And then it stays zero until negative t over 2 on the left hand side and then positive t over 2 on the right side t over two is plus 0.5. And so actually when t equals one, the signal is just this blue line here that stays at one. And so if I made the periodic extension of the signal, it would just remain at one. And so that's why the fourier series coefficient indeed comes out to one and, and that makes sense. asked to go back to this slide. Yeah, so I want to pause and see if there are any questions here again. All right. So now what we're going to do is we're going to set, we did this for big T equals one.

All right, everyone, we're going to get started for today. Our announcements are that homework number two is due tonight. And yesterday, the TAs released homework number three, which is a coding homework. And as per Tanmoy's announcement, we recommend that you get an early start. It's all on Poisson processes, but it's a lot of coding. So be sure to get an early start on that because the coding can be time-consuming, especially if there's debugging. Then our midterm is going to be in class a week from today.

It'll be on Monday, May 3rd, 2021. Earlier today, I sent out an announcement with details about the midterm, including how we're going to distribute it, how we expect you to upload it by 4pm, as well as if you're in a different time zone or you received permission from me earlier, how we'll schedule your other exam time. All right. The midterm is going to cover material up to and including croissant processes and so we're going to finish croissant processes at the first part of lecture today, and that'll be the end of material for the midterm. We put all of our past exams on CCLE and so you're welcome to look at those midterms, of course, to prepare for this midterm. With the following note which is that the first midterm that we gave in 2017, which I think Tom Wael said that was, was extremely long, way too long for a midterm.

So you should not expect a midterm at the length of the 2017 exam. All right, and then again, see the CCLE announcement for any other details about the midterm. And I'm happy to take any questions about the midterm if people have them. Right, I see no hands raised. And so we'll, on Wednesday I'll be happy to again take any questions on the midterm if people have them as they come up.

Right. So with that we're going to finish Poisson processes during the first part of today's lecture and then after that we're going to start to get into decoding neural data and so we'll get into discrete classification today and motivate that and talk about the particular decoding task we're going to be doing. So these are recap slides from last lecture. We're at the end of Poisson processes and last lecture we talked about inhomogeneous Poisson processes where the key difference is now that firing rate of the Poisson process lambda r can change over time, right. And we define the Poisson process, the inhomogeneous Poisson process with the following properties that there are zero spikes at time zero, that the number of spikes that happen in a window between time S and time T plus S is given by the integral of the rate function under the curve, and that becomes the mean of my Poisson distribution.

And then that the number of spikes in any given window or the variable N of S as independent increments, which means the spikes in non-overlapping windows are independent. And so starting from this definition, recall this is different than how we did homogeneous Poisson processes. In that case, we built a Poisson process by concatenating exponential interarrival times, and we derived these three properties. For inhomogeneous Poisson process, we're saying that it's a process with these three properties. And so then last lecture we were interested in asking, are the inter-arrival times exponential? Right, so we derived the inter-arrival time of just the first spike. And we found that it was not exponential. Right, so that tells us the ISIs independent.

Right, so in a homogeneous Poisson process we assume that the next spikes should happen with a high firing rate, i.e. they should happen in less time. Whereas if I knew that the first spike happened after a long amount of time in this firing rate curve, then we would expect T2 to be during a time when there's a smaller firing rate. And so we would expect the time to the next spike to be bigger. And so, we already have this intuition that the size should be dependent. But to calculate it, we needed to to show a rigorously we need to be able to calculate this probability. And so remember our approach was to calculate the distribution of the second ISI, it's time being greater than some time, s, given that the first ISI happened at time t or happened after a length of t. And if this probability depends on little t, then this distribution of t2 being greater than some time depends on the value that t1 took on. And therefore, they can't be independent because t2 depends on the particular value of t1. Right, so if t1 and t2 are independent, then this won't depend on T, but if it depends on T, then T2 and T1 are not independent.

And so we had a lot of discussion at the end of last lecture about why this probability can be rewritten as probability number, letter C here. That is the probability that we have zero spikes between time T and time T plus S. So we're going to continue today, actually writing out this probability, right. And this will be the last thing that we do for inhomogeneous Poisson processes and then after that we'll tell you about how to code them up and then that should give you everything you need to do homework number three. So our question is, are the ISIs of a homogeneous, of an inhomogeneous Poisson process independent. And so, let me just, we have in the prior slide that we want to calculate the probability of T2 bigger than S given T1 equals T.

And that this probability was from the poll, this, the answer is C. So let me write that down. It's probability that N of T plus S minus N of T equals zero. All right, and I'm going to, we had this notation from Wednesday's lecture last week, but recall, we defined this quantity mu of T, which is the integral of my rate function lambda r from time 0 to time t. All right, and so let me just write down that again over here. We have that mu of t is the integral of my firing rate over time from time 0 to time t of lambda r, or let me do t plus s and t here, since that's what we have, this expression.

If I want to calculate this integral, right, all this integral then is would be mu of t. And we're going to need to calculate this because we have, we know how to calculate this probability from our definition of the Poisson process, because you'll recall in our definition we said that the number of spikes in a window is going to be Poisson distributed, where the mean of the Poisson variable is going to have a mean, which is the integral of the rate function between t and t plus s, that's just this quantity here, so it's going to be the mean mu of t plus s minus mu of t, raised to the number of spikes that we see in the window. And so the number of spikes that we see in the window is zero, so this gets raised to the zeroth power, times e to the minus the mean, so that would be mu of t plus s minus mu of t. And then all of this is divided by the number of spikes we see, which is zero factorial. All right, all I've done is I've written my Poisson distribution here.

And so we can see that these two terms both cancel out to be equal to one. And so therefore we get that this is equal to e to the minus mu t plus s minus mu of t. Let me put parentheses here. Alright, and so from here, you can see that clearly this probability is a function of the time t. And so since this probability is a function of t, and little t again is the value taken on by little t1, then the distribution of t2 depends on t1, and therefore t2 and t1 are not independent. And so there we have given a formal mathematical proof to show that T2 and T1 are not independent.

All right, everyone, happy to take any questions. I had a quick question about number three for the homework due tonight. Yes. Sorry. All right, question three. Yeah, for part B, I wasn't sure if my approach to this was correct, and I heard that one of my other classmates had used a different approach so I just wanted to verify my method. Sure. What I was thinking was that I would take the area from 0 to 1 millisecond and divide that by the area from 0 to 20 milliseconds. What was the reason for dividing by the area from 0 to 20 milliseconds?

Because with 50 spikes per second, I got that there would be 20 millisecond intervals. Got it. Yeah, so 20 milliseconds is the average interval. But if you just want to find the percentage of spikes that would violate a one millisecond refractory period you actually only have to compute the first probability, because you want to divide that by all potential spikes, some of which may have a inter spike interval that's greater than 20 milliseconds. And so you would only want to calculate the the first expression that you mentioned there. Let me just pull up my iPad, and a share screen here so I can just. I can just write out everything so that I make sure that we're all on the same page. Okay. All right, yeah, so for question 3b, just like you were saying, we would want to calculate the probability that our exponentially distributed variable has. We want to calculate the entry, the area under the curve from zero to 0.001 seconds which is one millisecond. And so, all you would want to calculate, we want to divide this by the probability of the ISI, of all potential ISIs, which is just, would just be the probability that t is less than infinity, but this denominator here would just be equal to one, and so we would just want to calculate And we derive that this is one minus the XP of land to T.

Okay, I see. Thank you. Great. Let's go into. Let's go for the next questions in the order of hands race so I'm not sure. Okay, I think it's, I think it is in order so Chase is next. Hi, I just wanted to like, talk about five and see if my approaches correct. Sure, yeah. Which one of number five, or which. would just be multiplying them because they're independent, and then it would be one minus the CDF of both the first neuron and the second neuron.

And then for B, it's kind of the same thing since it's memoryless, but C I think was the one that I wasn't the most sure about. And the way that I reasoned about it was that, I guess since you're seeing if one fires and two doesn't fire at a certain point, it would be the integral of the CDF of the first neuron times one minus the CDF of the second one. And then, or no no no, sorry. I think it'd be the probability that the first neuron fires at any given time, times, the CDF, or the one minus the CDF of the second one. Yeah, so, first off, for number one, your approach is correct. For number two, you said something about it being memoryless, but if you meant the, if you meant, actually, sorry, let me read question B, given that no neurons are detected in the first s seconds. Yeah, so it should just be the independent increments property. I'm not sure if that's what you use there. Yeah, yeah. Great. OK, yeah. So for part C. So part C is a bit more tricky.

And if OK, in the interest of time, I can I can state one hint and actually maybe just write out where you want to start off. So, for part, see, you were mentioning probabilities in terms of CDs, which, which is a valid approach for doing part a also in part a you could also have done it with the number of spike So, let me just say what I for part a I think you mentioned that it would be the probability that the CDF that the first. Sorry, that the first neuron. second neuron, its first ISI is greater than 60 milliseconds. And that's totally correct and you should get, you should get the correct answer that way. Another way that you could have written it is the probability that there are just that there are zero spikes on n1. At time t equals 60 milliseconds, the same time. And so, the reason that thinking about this helps is it helps to frame Part C. They want to know the probability that a neuron is detected on electrode one before electrode to. And so Let me first write it out in terms of, in terms of just thinking about in terms of number of spikes. And then, and then I think that will give a pathway for the answer and then after that I can listen again if you like chase to how you were trying to set it up and see if it's equivalent or not.

But for part C. If we want to know the probability that electrode one detects a neuron. So that would be the probability that a neuron is detected on electrode one before electrode two, right? That's saying that at some point in time, N1 of T is equal to one because we've detected a neuron on electrode one, but electrode two has not yet had a neuron. So, n two of T is equal to zero. Does that make sense. At a high level. Great. Yeah, so then the caveat here is that we want that spike to have only been for electrode one and not electrode two. So, n1 of t equals one, n2 of t equals zero, but we also want that overall there's only just been one spike.

All right, so office hours are being recorded now and we'll do the raise. I'll take questions in the order that hands are raised. Collecky. All right, I was kind of used on number 4D on the homework. Sorry, could you say 4D or 4D? 4D. Have the harmonics one? What was your confusion on the question? Just like how to go about the question. I, yeah. All right, yeah. So did other people have questions on 4B? So should we do 4B1 or 4B? Actually, that's 4B1 and then what do you? Yes. So I think the question is, inventions on harmonics? I don't know if there's some property or something like that that we're supposed to know and use. Yeah. So do the five harmonics?

Yeah. And we could have defined this more clearly on the homework. So let's do 4B here. So when we say that the Fourier series has only odd harmonics, that means that the only coefficients that are not zero are C1, C-1, C3, C-3, etc. These are not zero. But then C2, C-2, C4, C-4, etc. are zero. And so I've seen already an answer posted on Piazza that uses this property to derive a relationship for X of T. Because ultimately, we're asking you all to plot what X of T is going to look like. So we can go ahead and try to get that property and then we can see what X of T looks like. So can anyone who saw for this property tell us your approach here? Sorry, what exactly did you ask me? Oh, sorry. Let me clarify the question. So we're told in this question that the signal X of T only has odd harmonics and isn't even function. So we're just going to focus on the odd harmonics property. And I'm going to actually just write what I've seen on Piazza.

So on Piazza, people have said that this odd harmonics property means that negative X of T is equal to X of T plus T over 2. Big T over 2 or big T is the period. And so that's pretty much the critical insight that you need to be able to solve the rest of this problem. And so can anyone has anyone derived this and or have an approach, I guess, as to how to go about showing this? Oh, yeah, I did. Wait, I did derive it with T minus T over 2, which I think is the same thing. Yes, it's the same thing. Okay. Yeah, it shouldn't matter then. Oh, what I did was just. So we have the Fourier series representation of a signal is like the sum and then times the coefficient and then the complex and exponential. So I since it's odd harmonics, I replaced all the case with like inside the summation with 2k plus 1. And then I just subbed in T plus T over 2 into the thing, the fourth A series representation. Since it's like addition, you get on two complex exponential terms. So one of them has 2k plus 1 and this is multiplied by T over 2 and also omega non. So T over 2 times omega non is should be pi and then this is pi times the odd number.

So if you write that in like the trigonometric form, you'll get it to always equal negative 1. And that's how you get the negative x of t. Okay, great. So one thing along the line, you said you subbed in T plus big T over 2. And what was the rationale for that? What do you like do you mean like the motivation for plugging in? Yeah. I would I just proved this property. I didn't actually got it. Yeah, that's that's that's good. Thanks. Thanks for that Eric. Yeah, so let me tell you how. This is how I thought through this question, which is I want I know that and it's long these drawings exactly of what Eric said. And so we'll do what Eric said. So, but just a slight with minor differences. So we know that these even Fourier coefficients are equal to zero.

Right. So let's just take one of them as an example. So let's say that C2 is equal to zero. Right. And it's going to be true for every single even harmonic. Every single C sub even number. Let's go ahead and see how it can be the case that C2 can equal zero. And then we're going to see a general property for which Eric's explanation is to more general explanation. But this would be how I start off the question as a I'm not sure. Or I'm thinking I'm not sure what the property is like let's say that I didn't know this property. How would I arrive at it? Right. So C2 is equal to zero. We also know that C2 is equal to. The integral from over a period. So from zero to big T. So we have a C2 of our signal, which we call X of T. And then each of the J. And then we have a K omega not little T. And here since we have C2, then K is equal to two. So we have a J times two times omega not times little T. And then we have a C2 of T equal to zero. So if this is equal to zero, then it means that the integral over a period is going to cancel out to zero.

And so. One way in which I can try this is I can split this up into two halves. From zero to big T over two. X of T e to the J times two omega not T dT plus an integral from T over to the big T of X of T e to the J times two times omega not times T. And so here I'm just doing an example for one setting of K. And again, this is to give us the intuition as to what the answer looks like. After which then what Eric said before is the general showing this. So at this stage can someone tell me how this might how we might show that that these two some together equals or. Oh, sorry, I missed the minus sign. Thank you. I just saw that in chat. Everywhere. That minus sign is actually very important. Can you do a substitution exactly the bells exactly. Yeah, so I'm saying that if I integrate one part, the other part should be negative of that. And so if it's going to be negative of that, I just want to do a substitution to see if I can combine these two together. Right. And so.

Essentially, I want the first term and the second term to cancel out. The first term and second term look very different right now. This is zero to T over two. T over two to big T. And so let's take Kai's recommendation and do a substitution. So I'm going to define a tau. And I want this tau to move these integration down to zero to T over two. So I'm going to say that tau is equal to T minus big T over two. So if tau is equal to T minus big T over two, then I can apply this substitution to this integral over here. So let me write out the next step. This is zero to cap T over two. X of T e to the minus J to omega not T and T plus an integral. And now I'm going to do my substitution. So I'm going to be integrating over a detail. And because I'm integrating over detail, my integration bands are going to go from zero to T over two. I'll have a zero to cap T over two. And I'm going to have an X where now everywhere there's a T, I'm going to put tau plus big T over two.

All right, everyone, we're going to get started for today. First, a reminder that midterm will be during the next class time. So it's going to be a Monday, May 3rd in class. Oh, we'll go in class already. In class on Monday, May 3rd, 2021. We, again, on Monday, posted details about the exam as an announcement on CCLE. Be sure to take a look over those. And if you cannot take the exam during the in class time, because you're in a different time zone, or I previously approved it, please send me an email following the format of the announcement on CCLE. Then a reminder also that past years as midterms are all uploaded to CCLE and that the 2017 midterm was too long. So keep that in mind if you study by doing that exam. It was also a bit on the hard side too. You would expect that the exam should look more like it did in recent years. All right, any questions on the midterm? All right, so then another reminder homework number three is Tuesday, May 4th, 2021, the day after the midterm. And it has a good amount of Python coding.

So we recommend you can early start on that. And then last, we received some emails about read grade requests. And so if you have questions about or concerns, or you believe that a question that you submitted on the homework was misgraded. You should submit those read grade requests via grade scope. So just with the grade scope portal, you can submit pre grade requests. And this will wrap the request to the greater who graded your question to take another look. Any questions on any logistics? All right, so let's get back into material. So last lecture, we ended by showing this delayed reach task and a classification brain computer interface or neural communication prosthesis that we built from it. And so you recall the task was at the monkey touches and holds a center target. A target then appears in one of eight locations or seven, sorry, one of seven locations. In this case, the downward location. And then the monkey isn't instructed to reach yet. But what the monkey does during this period is he plans to make a reach towards the target. And then a go queue is delivered, which corresponds to this peripheral target becoming large.

And this center holds queue disappearing after wish the monkey then knows that it's time to reach to the target. All right, and so there are two types of activity here. There's plan period activity, which is the neural activity during the monkeys plan to reach toward the target. And then the actual movement period, which corresponds to the monkey's actual physical reach to the target. All right, and last lecture, we said that for this first discrete communication prosthesis we're going to build, we're going to decode off of the plan period activity. So why is this well, we will do movement period activity, brain, brain computer interfaces next. But you can imagine if you were looking at say a keyboard, right? And we had the characters, etc. If I wanted to type on this keyboard and I were to move my hand, I would have to reach from location to location to type out a word. But what if I just imagined that I wanted to make a reach to the w or reach to the p or reach to the t, right? I wouldn't have to actually make a physical reach. And just across the planning it, that is represented in motor cortex. And you can imagine that this type of prosthesis could be much faster because if you have a discrete number of selections that you can make for the alphabet, it would be one out of 26 potential selections. And you can just plan to the t, plan to the age, plan to the e and you can type faster. Then if you were to physically make reaches to each of these target locations.

All right. Any questions on the task setup? All right. So then last lecture then we talked about the system about how what would happen is the monkey touches and holds a target. We show a peripheral target. And if it's a BMI trial or if it's a neural prosthesis or brain computer interface trial, the monkey would plan to this target. And that would correspond to this activity over here, the monkey's plan activity towards this target. We would decode this activity and then draw a circle where we think the target was. And if the circle overlaps the square, then it's correct. And then we will show the second trial, which would be the target a new location. The monkey would plan that's the neural activity over here. And then we would make a decode. And if it's correct, we would draw the circle in over the square. And then we would present another trial. And this is a plan activity again. And then if we decode correctly, then we would draw a circle over the correct target location. And so that's where we ended last time.

And we showed some videos of this decode. I'm going to show the video just one more time for the. Medium and fast BMI trials. Purpose is just recollect this and then happy to take any questions on this. And so this is for the medium speed trials. And so you can imagine there's an optimization game that can be played here. I could try to decode even faster in which case. The amount of time that I have to look at the plan activity and then decode it to be one of the K targets become shorter. Then I have less data and I would be more inaccurate. And so there's a trade off that you can get here between accurate accuracy and speed. And last I could also show this video here where the trials were very quick. All right. All right. So any questions here? Any questions on the BMI task? Sorry. I keep on the neural communication prosthesis or brain computer interface, the CI task. And sometimes I say BMI also, which is brain machine interface. So I use those terms interchangeably here. Any questions on this task?

A question from Brandon. So just out of curiosity in that last video, we saw a lot of the open circles, meaning that those were plan movements. There were like five plan circles and then one that was actually a physical reach. Is was there a reason as to why the experiment was set up that way? Yes. So ideally we would not have any physical reaches in there. We would just start showing targets and the monkey would immediately plan to them. And then we would classify them immediately. And we can do this in a super long chain. The problem is that the monkey may become disengaged in the task. And it becomes disengaged in the accuracy decreases. And so the intermission reaches that we interleave there are to just make sure the monkey stays engaged in the task. And also knows that he has to reach himself. This engagement related thing. Any other questions here? All right. So that's a look at what we're going to be doing in the next lectures on discrete classification. The question from Andrew. Yeah. Could you go to the slide with a task timeline? Yeah.

So I'm confused how like if there's like time required for like information to propagate. How is there a baseline neuroactivity? Or is there baseline neuroactivity like before anything happens? That's correct. Yeah. So I didn't explain these. I mean explain those really quickly. Thanks for pointing those at Andrew. So the baseline neuroactivity corresponds to the activity that's just in motor cortex when you're holding the center target. But there is no target that has been shown that you should plan to reach to get. And so this is just activity when you're holding a center target and you don't know what you're going to do next. And blue here. Yeah. And then in blue here will be the activity where a target comes up on the screen. And there's going to be some way to see maybe about 100 milliseconds for your eyes to proceed this target for that information to go through the visual cortex. And then eventually make its way to the motor cortex. I was just confused because they thought like baseline activity meant like activity. That was already part of like thinking about the task. Yeah. It isn't. Yeah. It's only just the activity before any target is shown. Thanks for that clarification. Any other questions here? So does it do we use the baseline activity anyways in the record decoder? For the discrete decoders that will have you using class, you will not use the baseline activity for the decoder. Since it doesn't correspond to any particular reach direction.

All right, Sal? Hi. I just wanted to ask a quick question, and I'll stick around too. For this homework that we – homework five, I've been working at it. I was just curious, how much of the – with what we've already gone through, how much of this homework do you expect us to be able to complete with the material that we already know. Yeah, so Fourier series number one should be able, you should be able to do. Yeah, I did one. The number two is symmetry properties of the Fourier transform. So we did go over this in class briefly since they were the same as the Fourier series. So we have to go. So are we going to go over it more or is this going to be like kind of that's going to be the exposure to it and then that will be the exposure to it. So we won't cover that anymore. And so, yeah, for that one, you'll need to spend some time, likely with with the symmetry properties and just thinking about what they imply. And so, And then the figure one will take some time to.

Yeah, we will take some time but you you have all the results you need to show it. Got it. So I'm going to wrap up with questions based off of some of the derivations we've already done with the Fourier transform. In terms of, in terms of manipulating the Fourier transform, and we'll, we'll go over more on on Monday. We have our next lecture. I see. All right. Thank you. I would ask. Cool. Thanks. And just, I think everyone knows this, but this homework is not due tomorrow. It's due a week from tomorrow.

And so, I think everyone knew that, but just wanted to state that to be obvious so that no one would be panicking. And the TAs will be going over – the TAs will be going over related questions to these during their discussions this week as well – related questions to these OREA transfer questions. Yes, Al. Yeah, I guess that's kind of a part of what I was going to ask, if we can go like over an example like of something like a problem like 2A. But you're saying that the TAs might go over examples like that tomorrow? We can go ahead and go over 2A.

I remember we did this question, we've given this question in prior years also. The TAs may go through it tomorrow. I'm happy to discuss 2A right now. That'd be great. Yeah, because I'm kind of looking at it and I'm not really sure how to even approach this. I hear you. So, let me go ahead and first copy over the Fourier transform properties because I've mentioned I sometimes remember them myself or I remember them incorrectly. So, for a transform properties. There we go. And again the derivation of these.

For anyone who didn't recall when we did this slide in lecture. Copy. The derivation of these is exactly analogous to the oops The derivation of these is exactly analogous to the derivation of these properties when we did the Fourier series. Maybe, maybe I'll do the first, I guess there are eight so I'll do the first four to get us started on this and then, and then I happy to talk about the other ones but actually, I'll do the first four, maybe a few less if people have questions but I don't see too many questions right now so why don't I go ahead and do the first four. So 2A1 asks, determine which of the signals whose Fourier transforms are depicted in figure 1 satisfy the following, x of t is even. So then we will look at our Fourier transform properties, and if x of t is even, then we know that capital X of j omega is even.

So then we need to, this tells us that capital X of j omega is even. So then what we need to do is we need to look at that figure one. Look at, let me, I'm just gonna pull off this page so that I can look at it. Okay so I'm looking at figure one right now and what we want to determine is from figure one which of these Fourier transforms are even. All right so I'm gonna have you all look at figure one and then I'm gonna ask you all which of these Fourier transforms are even? Yeah, Sal.

Um, well, it looks like, um, E, but also A, but I'm not sure if A is correct. I'm saying there's two different graphs. Great, yeah, so, um, A is split up into the real part and the imaginary part of X of j omega is also even. And so if they're both even then their sum is even. Some is even. And more precisely, when we take x, when we take real part of x of j omega plus j times the imaginary part of x of j omega, both the imaginary and the real parts are even and so therefore the entire signal must the there. When we combine them into a complex number that will also be even. So a is correct. It's even and then Sal also said that E is even. That's also correct. That one you can just look. X of j omega is clearly a symmetric about the y-axis there. There's one more of these that is even. Can anyone tell me what that is?

Oh D, right? Yeah, it's d also. So in d we give you the magnitude and the phase, right? So if you were to go out ahead and write this out, magnitude and phase, right, in d we have a number where x of j omega is going to be equal to the j pi over 2. And we know that e to the j pi over 2 is just equal to j. And so this signal is just j times a rect that has a width of 2. So it's t divided by 2. Sorry professor, is that the magnitude times e to the j pi over 2, is that just a definition for that? It would be then equal to, so the magnitude of x of j omega is a rect, that should be omega over 2, rect of omega over 2, and then the phase would be e to the j, and then the phase is pi over 2 for all omega, so it's just pi over 2, and that's equal to j times rect of omega over 2, and rect we know is an even signal. I mean, you can just look at it, it's an even signal. I didn't need to say that. And so we know that j times rect is also an even signal.

Oh, I see. So you multiply then. Yeah, okay. Yeah. So for 2a1, we should get the answers a, d, and e. Let me make sure that's correct. Yep. That's correct for 2A1. Sorry, one more time. So from that multiplication, J times rect omega over 2, how do you know that that's even other than just looking at the graphs? Yeah, J times rect omega over 2 is a signal that starts at minus. Oh, okay. Sorry, I think it's omega over 4, sorry. Because the width is 4, sorry.

All right, homework to the gray. Let me pull it up. Yeah. Which notebook is it on or is it the? It's just the problem three. Oh, yeah, the problem three notebook. Okay. Problem three. The in homogenous problem prophecies. Yeah. Okay. Yeah. So my question is, so I did problem two, which is the homogenous one and I understand it. And like everything is striking out properly. When I'm trying to generate the histogram, so part B.

It says that it wants to plot the expected firing profile determined by the union equation, as well as this histogram on the same plot. Yeah. But for the histogram, since there's 50 bends of 20 milliseconds each. There's like the X that length of the axis 50. But if I'm plotting the expected firing profile over a second. And the like times and milliseconds on the axis of a thousand. I'm unsure how to. Yeah. So you will want to sample the, you'll want to sample equation one and two also every. Every 20 milliseconds. Okay. And so let me share my screen. That's good. Cool. So even though you have 50 bins and they go from zero to 1000.

You would have the access to going from zero to 1000. You would have 50 bins and then you would sample the actual function. Also every 20 milliseconds. So you would have 50 times points to plot and Python. Okay. Yeah. That definitely looks good. Follow questions to that then is when I was plotting the bins. I was like labeling the bins. So like I was labeling them like zero to 20, 21 to 40 and so on. So like I don't know how to like make the x axis like time like this, but also show 50 bins. Um, actually, when you, uh, what function are you using to plot the, uh, the bars are using the PLT dot bar function. Yeah. Yeah.

Yeah. So for the PLT dot bar, you'll have like an, uh, a vector of x look x values and y values, right? Yeah. So for the x values, you could just make that the like zero, 20, 40, 60, 80, 100, all the way up to 1000. And then that'll, that'll plot the y values at those x locations. I see. Um, along. I see. I'll try that. Do you mind if I try it right now? No problem. Yeah. Yeah. Yeah.

Yeah. So I share my screen. Yes. Well, awesome. Um, will you be showing code for the, uh, for the problem? Oh, yeah. I'm going to just pause recording then so that, uh, all right. So anyone watching this postdoc, we just returned from looking at Ian's code. And, uh, a reminder that. So if you're plotting, if you're using PLT dot bar. And the time bins are 20 milliseconds. So you're giving a time sample the x locate the x locations of PLT dot bar are like 20, 40, 60, 80. Um, there's some odd behavior in PLT dot bar. So you have to sit the width of the bar to 12 to see all the bars.

I'm just thinking sure that. So the book itself that we wrote the, uh, the width equals 12 thing, neat. Yeah. Yeah. Okay. Yeah. And so if you, if you, if you don't set the x positions to be 20, 40, 60, 80, but you said, 3, 1, 2, 3, 4, 5, then the, um, width equals 12 thing won't make sense. So make sure that you set the x positions, anyone watching to 20, 40, 60, 80, et cetera. Okay. Awesome. Thank you. I'm probably three now, but, uh, yeah, Zach, if you have any questions, go for it. All right. Sounds good.

So for, uh, so problem two, like towards the end. So for the spy times, so I know I for the last part, when we plot the CV, so the average asset in the CV of the ISS. So for understanding is for each of the eight, which part of the sec? I was at part G. Part G. Okay. Oh, okay. So for each of the, uh, reaches, so see how there is 100 trials in each of the trials, we have the spy times and the original spy times are like the capital TN where they show, like it's increasing versus if we do like a difference array, et cetera, we can find the individual spy times. So when we calculate like the average by a side of a given reach, I guess my intuition is, do we calculate first the mean of a given trial? And then from there, do it over the different trials or to be like, add everything up and then divide about a total number of, I guess, ISIS.

Yeah, I would do the latter. Yeah. But let me look at what we do in the, in the homework in, in sorry, in the solutions. Yep, that's what we do. Yeah. So the reason I said the latter is because maybe you have some trials where like, there are fewer spikes, then you would be up waiting their contribution to the average. Whereas we just want to look at all of the ISIS and then see like a cross all ISIS that happen across all trials for reach. What was the average yet? Yeah, same for the CV as well. So if we, as I'm trying to separate each of the reaches, there's a hundred trials and each trial has data. What the CVB of again, all the data if we create, put everything in a given reach angle. So there's a hundred trials times how many ISIS will we copy the CV of like the total of that? Yeah, so we should take the standard deviation and the mean across all the ISIS and then the CV will be one number for each wish condition. But it's a standard deviation of all of your concatenated all of the ISIS across all the trials divided by the mean of all the ISIS across all the trials.

Can I see? Yep. Okay, I'll set it in for part F. So the right for part G. So this is where we're plotting the ISIS distribution and I know we'll probably use like a histogram and we use the exponential. So I was able to kind of get it going. And then I guess my issue was not really so I know like the political swell that was applicable for one of the previous parts. So for fear because it's normalized, I saw like a Python on the other some post there's like a setting where you said like area equals one or something equals true. Yeah, norm equals true, I think. Yeah, so we do PLT dot hiss. And the first argument we just give the ISI the list of ISIS or the array of ISIS that you have and then common norm equals true. Okay, gotcha. And then we did give also a common dint equals 30 to have a plot to have it make a histogram with 30 bins. So I see that makes sense. So see originally I was like for each of the ISIS I was looking at like mainly created like 50 bins and then I was you know, say, okay, this less than this, creating this, I put in this pile. I was trying to like a bar PLT dot bar kind of from scratch. So for this is we want to have to really do any dividing and sorting it kind of does it for us.

All right, everyone. We're going to get started for today. So a few announcements. The first is at homework number five, which we uploaded to CCLE last week is going to be due this Friday. I'll put it to grade scope by 1159 PM. And then later this evening, we're going to release a midterm grades, the statistics as well as the solutions. Are any course logistics or admin questions? All right, so it's been over a week since our last lecture. So I want to recap a bit about what we were doing last time. We had derived the Fourier transform, which is the generalization of the Fourier series to arbitrary signals that may not be periodic. And so it's like the Fourier series and that it tells us the frequency content of a signal. All right, and so the Fourier transform capital F of J omega is this formula and computer Fourier transform of our time domain signal F of T. And there was some discussion last lecture about why we have a J here. So in other settings, you may just see the Fourier transform written as F of omega. In the end, the Fourier transform is a function of frequency.

Right, when we plot spectrum frequency is on the x axis and then on the y axis, you see how much of that frequency is used in the signal in this class. We will use the notation J omega. However, the J there is primarily symbolic. It's to tell us that this omega is a frequency. And we'll see why we use this notation more and get to a plus transform. But remember, ultimately our original signal F of T whoops. Original signal F of T is a function of time. And it tells us how our signal evolves over time. Our Fourier transform F of J omega is a function of frequency. And it tells us the frequency, how much of it tells us how much our signals composed of cosines and signs of frequencies over entire spectrum and entire span of frequencies. And we also derive last lecture, the inverse Fourier transform, which takes us from our F of J omega back to the time domain F of T. Right. And so last lecture, we did a few Fourier series of Fourier transform examples. We derived the Fourier transform, for example, of this signal each the negative ATU of T by just plugging it into this equation and doing the integral. And so this is an example how we actually compute that Fourier transform integral.

And then we said that we were going to start to compute some or to derive some properties of the Fourier transform, which are going to be critical for us using the Fourier transform. And so last time we derived that the Fourier transform is linear. And this is a really simple proof. And we also derived this time scaling property, which is that if I have a signal F of T and I either compress it by multiplying the T argument by a where a is greater than one. Or I expand it by multiplying it most playing T by an a that is lesson one. And the transform will also become scaled. And so I want to just recall the intuition of this to make sure that we have. Have intuition over the results that we're seeing. And so this is my signal F of T. Right. If I compress my signal, which means is less than one right my signal might look like this. Sorry, that is less than one means to extend the signals. Sorry, let me if we expand our signal. So the a is less than one, then our signal might look like this. And then if I compress my signal a is greater than one, then my signal might look like this.

Right. So for my compress signal with a greater than one. If we compress a signal in time, what we see is the Fourier transform the spectrum gets expanded because a is greater than one. So this omega is going to multiply is going to be divided by a greater than one. So it's going to expand the spectrum. Where is if I expand the signal in time, this blue signal, my Fourier transform is going to be compressed. Right. Can someone remind me the intuition of why this makes sense. Why is this an expected result? Can someone raise their hand and respond? Kai. Because for compressed signal, your all the frequencies are going up. So you're going to have more higher frequency content. And in the frequency domain, that's further away from the origin. Yep.

Great. Yeah, so let me just draw out what Kai was saying, which is the correct answer. If this is our spectrum, f of j omega. So the exact same as omega, let's say that the, let's say that F of T had a Fourier transform that looked like this. So this is going to be capital F of j omega. Where F of T has a Fourier transform capital F of j omega. And so what Kai was saying is when we compress the signal, right. And we cause a signal to actually have higher frequency components, meaning it changes more quickly. If we were to think of trying to reconstruct this signal with co-signs. If I want to get these wiggles and squiggles out, I would need to use co-signs that are pretty high frequency. All right. And so cosine with higher frequency is going to have a larger value on this omega axis, which is quantifying frequency. And so we would expect then. If a is greater than one for the spectrum to be expanded, because I need to use higher frequency co-signs. To make my red signal, whereas my blue signal, my extended signal.

It's. Trusted at in time. And so if I want to reconstruct this blue signal, I would use co-signs that have. A relatively low frequency, right? They have a very large period. So for the blue signal, I might expect the spectrum to look like. This because all I need is low frequencies. All right. And then the the amputees are just following this one over a scaling here. Okay. Any questions there? All right. So if you for if you don't recall this proof, we we derived this last lecture. So feel free to look at the last video to recall this. We're going to do a bunch more proof today. And so the next operate. So today we're going to go through all of these properties.

We're not going to go in depth into some of the proofs because some of the proofs look very similar to ones we've done in the past. However, we'll do a fair amount of these proofs. I copy and pasted the proofs into these lecture notes, even if we're not going to do them or we're going to go to them more quickly. But if you look at the formal notes, hand out number 10. There will be a derivation of every single one of these properties there. So we're going to start off with time reversal for today. And time reversal tells me that if I take my FFT and I time reverses. So I do F of negative T. Then what happens is that the spectrum is also reversed. Alright, this is actually really easy to prove to prove this. All we have to do is use the scaling result here setting a equals negative one. And so if we take the time scaling result, we already proved and set a equals minus one. Then we achieve this result. And so we don't even have to do a formal proof here. So let's do an example of how this time reversal property works. So we're going to ask here to find the Fourier transform of each of the negative a and then absolute value of T. So this signal is one that looks like this at time T equals zero.

It takes on the value one. And then when T is positive. Time is growing. We have a decaying exponential. And this is an even signal. We're taking the absolute value of T. So when time decreases, we also have a decaying exponential like this. And so this is our F of T equals E to the minus a absolute value of T. And last lecture, we had derived the Fourier transform of F of minus a T times U of T. F of minus a T times U of T is the signal that is zero. Until time T equals zero and then it rises up and then is E to the minus a T. So it's this signal. So if I want to find the Fourier transform of each of the minus a absolute value of T. Then what I can do is since I already know each of the minus a T times U of T, the Fourier transform of this signal which we derived last lecture, I can write my F of T as being equal to E to the minus a T U of T. And then what I could do is I could add to this signal.

Okay. All right, what is your question. So I'm a little confused about how to approach one, a, I think I watched Shoshana's office hours and he was explaining how it could be turned into like a binomial distribution. That's right. I'm, I'm not sure how to go about it. Like how to reach the, where the lambda s comes in, and why that's like separate from 1 minus p. Got it. So, in this question, let me just pull up a, let me pull up a blank window so I can Right on there.

Right, so. So, for this question, it sounds like from what you said that Shashank had discussed how probability of M equals M given N equals N is binomial, right? Mm-hmm. Great. And at this point, we know what this probability is. That's the binomial distribution. Are there any other probabilities that we know here that we could use to calculate probability I'm not seeing it. Oh, that it's Poisson? Yeah, we know the probability of the n process. So we know that probability of n equals little n is lambda. Exactly, that's right. So this is Poisson lambda. And so that's where the lambda s is going to come in, in your derivation. So now the key is, if these are the two probabilities that you know, this binomial one and this Poisson one, how do we use it to calculate probability of big M equals little m?

And so if you... So, you mean like Bayes' rule? Yeah, Bayes' rule would be the first one that I try also. So if we had that, we would need like a probability n equals n. Sorry, n equals n divided, given that n equals little m. And then the numerator would be a probability n equals little m given big N equals n, and then probability of big N equals n. And we do know these two. But we don't know- But right now, exactly, yeah, we don't know the denominator.

So actually for this one, we could actually solve it using the other thing we know, which is a law of total probability. So we know that this is equal to the sum over all possible values of little n. And this probability here is just the numerator, this expression over here. So if you plug in this expression into here, then you can start to do the simplifying algebra that should lead you to the correct answer for this. So it's going to stay in terms of n though, right? Well, this expression here is going to have an n in it, but then when you sum across all potential values of n, then it should disappear.

that look like the Poisson distribution with 1 minus p times landom times s? Yep, that's right. Okay. Yeah, so at that point it's going to be, it's going to be a bunch of algebra. I can give you a little hint which will hopefully simplify things. So, oh and sorry, this should be from n equals m to infinity because we Because we know that we know that admin is going to have fewer spikes than n. you should be able to simplify to a term where you have a sum from n equals m to infinity. And you're going to have like a p to the n minus m times a lambda s to the minus lambda st, divided by n minus m factorial. So you're going to get this expression when you do this simplification. And the key thing to realize is when you get to this expression, if I go ahead and I do a substitution, k equals, this is really bad, k equals n minus m, right? Then that changes the sum from being k equals 0 to infinity, and all these n minus m is changed to k.

Oh, okay. And then what you'll see is that all this is is a Poisson distribution, summed over all potential values of k, and so this whole sum will equal one and then the sum will go away. This is equal to one. Oh because it's exponential? Yes, plus on and you're summing over all potential values and we know that a probability distribution when you sum over all potential values has to equal one. Okay. Okay, thank you. That helps a lot. Cool. Yeah.

Okay, I'm gonna clear these drawings. Yeah, sure. Thank you. That was all I had. Okay, great.

Thank you. All right, everyone. We're going to get started for today. A few announcements. First a reminder homework number five is due this Friday, uploaded to grade scope by midnight. If you didn't hear the end of last lecture, or look at the cover page of the midterm exam, we announced that the final exam score will replace your midterm score if you score higher on the final exam. And then as per our syllabus, I wanna remind everyone, we don't have lecture a week from today on November 25th, 2020, the day before Thanksgiving.

Any questions before we start with material? All right. So, we're going to continue where we left off. Last lecture, we spent some time finishing discussing and proving much of these properties about Fourier transforms. Right. And there is just one more property will get to today which is the integral of a signal what is its Fourier transform and we'll derive that today. And we've also along the way derived a bunch of Fourier transform pairs. step, or rather, we'll discuss how it's derived. We won't test you on it because it's a generalized Fourier transform that we don't want to emphasize too much. But in general, when you take the exam or do the homeworks, we'll provide a Fourier transform table that contains properties as well as Fourier transform pairs and you're welcome to use all of these without proof. All right.

Okay, any questions from last lecture just to follow up on anything about any properties any Fourier transform where when we just apply the definition naively. It doesn't work out. Right. So, these will be the generalized Fourier transforms that we discussed today. We did talk about a few of them so a few of them, we can do some straightforward computation. If we use the Dirac delta and we apply the algebraic properties we know of the Dirac delta, we found that the derivative of the Dirac delta is 1. And we said that this makes sense intuitively because the Dirac delta is a function that is 0 and then changes infinitely fast to be of an infinite amplitude. All right. And so, to create that infinitely fast transition we're going to need sinusoids with frequencies that approach infinity.

And so that's why it makes sense that the Fourier transform of the function by tau. What that corresponds to is a complex exponential e to the minus j omega tau. And we saw using our convolution theorem, as well as our time shift theorem, that this result is consistent with previous things that we've derived. All right, then we try to take the Fourier transform of the signal of just a constant one. And we saw that if we try to evaluate the Fourier transform which is 1, then 1 is going to have a Fourier transform which is 2 pi times delta of minus omega, but then delta is even, so that delta of minus omega turns into delta omega, and that's applying the duality property that we derived last lecture. Andrew?

Yeah, I have a question. So since the integral is not Evaluable, but it does turn into that like 2 pi times Dirac Delta Does that mean like that integral like does? Equal that end result and that like if like we find this integral and like another problem We can like use that result Yeah, I have eaten negative JW omega t. Can we say that that just equals to pi direct. Yeah, we can. Yeah, so, I'm solving this integral, including duality and the fact that the Fourier transform of delta t is one allows us to, to, to give this value to the Fourier transform of one, and so the answer is yes, and then she is a bit of subtlety that I missed there please chime in and let me know. Cool, thanks. And I'm just going to take this opportunity really quickly to make my TAs co-hosts so that they can unmute themselves.

Okay, and so we found that the Fourier transform of one is two pi delta omega, which is just saying that the Fourier transform of one is a signal that only has a non zero value at omega equals zero. And so this solution here corresponds to, or makes sense intuitively. Okay, any questions here? All right. And then we also asked, well, if the Fourier transform of one is delta omega, there's a similar analog for when we have delta, what happens if our Fourier transform is delta omega minus omega not right. So the sum of some signal is delta omega minus omega not and we calculate the inverse Fourier transform, we do this approach because then this gives us a delta in the, in the integral from which we can use the properties of the delta to compute its inverse Fourier transform, we get another Fourier transform pair, which is that the Fourier transform of a complex exponential is two pi times a delta omega minus omega zero. And so this answer should also make sense intuitively because if I want to construct a complex exponential with frequency omega zero, right? All I should need in my frequency domain, in my Fourier domain are sines and cosines that have a frequency omega zero.

And so that corresponds to a delta function right at omega zero. All right, any questions there. Right, so you can see here. We're using the inverse Fourier transform equation the Fourier transform of the impulse response which is called the frequency response. we know the Fourier transform of e to the j omega naught t, which is a delta at omega naught, we can compute the Fourier transforms of a few more signals. And so I've written these out because these are relatively straightforward given everything we've shown already. We know that cosine omega naught is simply the sum of two complex exponentials at omega naught and minus omega naught weighed by one half. We know that from Euler's formula. And so if we wanted to calculate the Fourier transform of cosine, then we're in good shape because we know the Fourier transform of each of the J omega naught t.

So we apply the fourier transform to cosine one half comes out because of homogeneity. And, well, a bunch of things missing here. Okay. Then we have the fourier transform of. We have superposition, so that the fourier transform applies to each of these complex exponential. And then the twos cancel here to give pi of delta omega minus omega naught plus delta omega plus omega naught. And so just like the Fourier series of cosine, where two were values, where Fourier coefficients c1 and c-1 that happened at the frequency omega naught. The Fourier transform of cosine is going to be two delta functions that occur at the frequencies omega naught and minus omega naught. Any questions here? All right, so that's the Fourier transform of cosine. And then the Fourier transform of sign is exactly analogous. We will, it would be the exact same map is on the prior page.

All right, everyone, we're going to start lecture for today. So two announcements. First, we anticipate returning your midterm scores to you on Monday, May 10th, and we'll return those after class. And then the second is that today we uploaded homework number four to CCLE, and it's due in nine days on May 14th. The homework is it involves a lot of coding and also a bunch of math. So we really encourage you to start this early and you should be able to do most of the homework, which is today in class we're going to discuss how we use maximum likelihood to find the optimal parameters for our discrete decoding model. a likelihood under a constraint that some set of probabilities add up to one. And so the way that you do this is something called Lagrange multiplier that you might remember from a calculus class here, where instead of maximizing log l by itself when you have a constraint like this, what you can do is you could put that constraint into the objective function as well, and that constraint multiplies a lambda.

And for this homework question, if you differentiate then this expression with respect to the parameters and lambda, then you're going to be able to solve this optimization problem. Right, so I think it'll become clear when you look at the homework, but I just wanted to put that up front, so that you don't get stuck on that. All right. Any questions on any course logistics? All right. So it's been a week since our last lecture, so I want to remind you where we were. We were considering this case where we record from neural data. In this case, we record from two neurons, neuron 1 on the x-axis, neuron 2 on the y-axis. And we record from these neurons as we ask the monkey to plan movements to different targets. And so here we've shown three different targets, right as red X's, left as blue circles, upward reaches as green triangles.

Right. And for each of these different reaches, the neural data, the neurons are going to fire in different ways. And so what we do is we perform a training set where we know the answers. We ask the monkey to make a plan to a reach. We record neural data. And we know the answer of what target they were planning to reach to. We know if it's a red X, a blue circle, or a green triangle. And then the goal of the training phase in machine learning is that then we want to take this data and learn a decision rule for how to classify new unseen data where we don't know the answer.

And so in training, that would be akin to learning these boundaries that we draw here in pink. And then when we go to the test phase where we want to now put our algorithm online and make new predictions without knowing the answer, what we do is we remember those boundaries. And now, depending on where the data, the new data point here as an orange square pops up, it pops up in this vicinity, then we can guess that the monkey planned an upward reach and if in this vicinity, the monkey planned a rightward reach, etc. Any questions here? All right, so then last lecture we talked about how do we set up this model such that we learn these decision boundaries in the training set. And so we said we were going to work with so-called probabilistic generative models. And we said that what we would want to do is we want to learn a distribution P of X given CK, right? Now, CK is one of the potential classes that the data could come from.

And so in this example here, there's C1, C2, and C3, three possible reaches. P of X given Ck, then, is what the neural data looks like when you have data coming from that class. And so P of X given C1 is going to be the distribution of these red Xs because it's a distribution of data X, given that the monkey planned a reach to the rightward target c1. And we'll make one of these distributions p of x given c2 for these blue circles and a p of x given c3 for these green triangles. And so that is what we need to learn in the training phase. And then in the testing phase what we will then be able to calculate is given Given some new neural data that I don't know the answer to, so given some XJ, what target CK did the monkey plan to?

Okay, so we could calculate the probability of a plan to the right, the probability of a plan to the left, and the probability of a plan upwards and get numbers. And we could choose that the monkey reached to the, the monkey plan to reach the target with the highest probability. Right, and then last lecture, we had done the math to show how we can calculate P of CK given XJ from knowing this training distribution. Any questions there.

I'll recap for now. Alright, so then, if we need to learn P of X given CK, then we need to choose what a distribution for P of X given CK is. And so then towards the latter part of last lecture, we decided that we were going to model P of X given CK with a multivariate normal distribution or a multivariate Gaussian distribution. In a multivariate Gaussian distribution, there are two parameters that set, that specify everything about the distribution.

Once I know the mu, which is the mean of the distribution, and the sigma, which is the covariance of the distribution, then I can write down the distribution exactly. And I can calculate the probability density function for any point x. All right. And then we went into some detail about what this covariance matrix means, this diagonal terms and this off-diagonal terms. And that was last picture. Any questions here? All right. So if I model p of x given ck via this multivariate normal distribution, then what the problem is going to boil down to is this multivariate normal distribution is totally specified by two numbers or two, one vector and one matrix, mu the mean and sigma the covariance matrix. And so for every single distribution in the training phase where I learn it, I'm going to want to set the value of mu and its covariance matrix, which remember we conceptualize as these ellipsoids.

I wanna set the values of these so that they well describe the data. So for these plan ups, I want the mean to be in the vicinity of the points, essentially it's centroid. And here the covariance ellipsoid should be skewed so that it has a positive covariance between x1 and x2, because we see this general trend that when x1 is higher, x2 is higher. But then maybe P of x given c1, its centroid is here in the midst of the red x's, and its covariance ellipsoids are like circles, because there doesn't seem to be too much correlation between x1 and x2.

So now the whole game of training is, how do I choose mu and sigma, so that they really well describe the distribution of the data for each of these planned reaches. Okay. Any questions there. last lecture, which was introducing the maximum likelihood framework. So I mentioned that before we derive the maximum likelihood solution for this classification model that we're working with. So you may be confused about exactly what are the parameters, what are the goals of this modeling, etc. We have a question from the swan. Yeah, I just want to make sure. So, you're drawing a leaf solid on the 2d plane that doesn't mean that data point will be like a leaf site. Right. So just a conceptualize of the correlation matrix.

That's correct. Yeah. Think of the ellipsoid that's. Remember when we drew like, let's say that, let's say we had a 1d normal distribution where data points were well congregated here and then like maybe they're more sparse out here. If we want to model this with a normal distribution we might say that the one standard deviation point is between these two values because 67% of my data points fall in this range. But data points can exist outside of the standard deviation lines. They're just less probable. But the ellipsoids themselves don't contain the point. They just describe where most of the data points should be. So even if my data looks like maybe in each axis is a uniform distribution, then I can still have that ellipsoid.

All right, everyone will go ahead and start office hours. Let's use the raise hand system to take questions in order. Tyler. Professor, I, well, I guess, let's, I want to start off with the very start problem, one, a on the homework. Can you go over the math behind showing how the properties change. Okay. So question one a says, when f of t is periodic f of t is real. We have seen some properties of symmetry for the four a series coefficients. How do these properties change when f of t is purely imaginary. All right. Can someone who maybe to this question. Let us know your approach here.

I started off so far. So, I said if so in the slide, it was assuming FT was real. So, I said, FT is imaginary, then it's kind of like you just multiply in a J. So, the. Yep. Yeah. Yeah, that's right. Yeah. And then what Tyler said is integrate over a period. And then Tyler multiplied some real signal. Since f of t is purely imaginary, let's just call the other signal g of t. So here f of t would equal j times g of t, where g of t is real. And then we would have our And that's J sine two pi k over big T zero T, just like in the lecture notes. TT. So, now, we want to get the, just like an electric us we want to get the real part of Tk is going to be a 1 over T0 integral over a period. Whereas J times G of T times minus J sine is going to be purely real because the J's here are going to cancel.

And so we're going to get the real component will be the portion that multiplies the sine. And then the imaginary portion is going to be the remainder, the part that multiplies G of T cosine 2 pi K over big T star T DT. All right. I was going to show you the first two properties but when it started doing the conjure parts. I was a little confused how the math was working out, so I just want to see those worked out. Oh great. Yeah, okay so the last three properties on slide 41. Perfect. Okay, thanks. Well, since essentially the even oddness switches for the real and imaginary parts, so I think the first part should be like the real component is equal to the negative, the real component of C of negative K. Yep. And then, and then for the imaginary part, same deal but remove the minus sign from the front.

Great. All right, so now we want to calculate, let's say, let's call it CK star. So CK star is going to be the complex conjugate of CK. that Ck was equal to its real part plus j times its imaginary part. Then we know that Ck star is going to be the real part of Ck minus j times the imaginary part of C k. And so relationship. Does anyone who did this question want to tell us what the next steps that you did are? And feel free to unmute yourself, you don't have to raise your hand. I rewrote the real part of C sub k as the negative of the negative of the real part of C sub K, and then that allows us to do some substitution based on what we saw earlier. Yes, so we can write this as the negative of the negative part of real of C minus K. Did I get that right Bradley.

Bradley? Yeah, that's what I did. Perfect, yeah. And then we have, so if we have a minus sign here, then we're going, well let's try to then get the imaginary part into a C minus K term. So can someone tell us what to do for the of CK. Wait, I have a thought about what you just wrote down. If you, if in the green you equated the real part of CK to the negative of the real part of C negative K, is that equality correct? Should there be two negatives if it's more equating to the real part of C of K? Oh, you're right, yeah, it should just be minus real part of c minus k. Yep. That's my bad.

All right, and then here we have minus J, and we have that the imaginary part of CK is the imaginary part of C minus K. Right. All right. And then I think, sorry, and this is where I had a misinterpreted Bradley's comment. This can be written as minus, and then we have a real part of C minus K plus a J times the imaginary part of C minus K. And all this is is C minus K. And so this would equal minus C minus K. All right, okay. Any questions there?

All right, so that's the next relationship that we have, which is C star k is equal to minus C of minus k. And then we have two other relationships for the amplitude and the phase. So can someone tell us how you approach these questions? For the amplitude, it should stay the same because the negative gets squared again. Perfect. Yeah. So, Melissa says that Ck, its magnitude, equals C minus k, its magnitude. When we calculate the magnitude, it's the square root of the real part squared plus the imaginary part squared so the negative sign will go away just like Melissa said. Okay.

So we have one last property then the phase. So, does anyone want to tell us how you did the phase. Can I ask a clarifying question on the phase property? Yep. On the handout, the property is written differently than what we had said in class. So I'm wondering which one we should use. Oh, the one in class. If there was a mistake on the handout, then I probably typed it wrong on the handout. So the one in classes the correct one and I apologize for the difference. Let me bring that up really quickly. Properties. Here we go. use, was it this minus sign over here, Melissa? I guess there may have been.

All right, everyone, we're going to get started for today. So just a few announcements before we begin. The first is a reminder that homework number four is do this Friday. And we're also going to be uploading homework number five. At the end of this lecture, we'll take some time to discuss the midterm. And I just want to put this in announcements so you all know we open up re-graves for the midterm exam for one week after we return the grades. And so if you want to get a re-grave on any midterm question, those are due by May 17th. 21. All right. Any questions or any questions on any any course logistics? All right. So we are going to continue with the screen classification. We're going to finish that in the first half of lecture today. So a reminder where we are. We had this model where we have neural data that's given by the X and the Y axis that comes from different classes.

In this case, we have two classes when the monkey plan to reach to a right retarded and when the monkey plan to reach to a left retarded. And those are red circles and blue, sorry, red X's and blue circles. And last lecture, we talked about how we were going to model this by having the distribution of the data coming from each class, being this multivariate Gaussian distribution with a mean and a covariance matrix. And then we said now from training data, which are examples of the monkey planning reaches to targets where the neural data during the reaches X i and the target identity is ti. We're going to try to learn the parameters of pi pi is the proportion of plan left targets. The means of these distributions and the covariance, right. And these are the parameters of my model and I get to choose them to make the data as likely as possible. Right. So last lecture, we derived the data log likelihood. That's this expression here. And we had this log of a normal distribution that was this expression over here. And we said we were going to now differentiate the log likelihood with respect to pi mu zero mu one and sigma to find the optimal parameters. And last lecture, we had done the log likelihood, the pie and got that pie, which is the number is the proportion of the probability of there being a left trial was just the proportion of left trials capital and one over the total trials capital and. Right. And the tackle and zero is the number of trials that were planted, but right target. And then we then differentiated log likelihood with respect to new one. And we found that new one was equal to one over the number of trials where the monkey planned left times the sum of all the neural activity when the monkey planted a left. And so this ends up being the sample mean of the neural data when the monkey plans to the left. And so that's the centroid of this cluster over here. All right. And we said that that made intuitive sense and there was a maligous answer for mu zero.

Right. I want to pause here and ask if there are any questions recapping this. All right. So then when we left off, we were now going to do maybe the most involved part, which is to differentiate with respect to the covariance matrix sigma. Right. And we mentioned as so these are just copy and pasted the log likelihood from prior slides. And then we mentioned that we are going to use these properties about the derivative of scalars with respect to matrices, as well as the cyclic property of the traces. And we can use these without proof. So we'll just assume that they're known for this class. Right. So we did any questions before we head on to do this. Question from Amina. I wanted to ask about the trace. Like I understand what it actually is, which is just the sum of the diagonal, but I can't. I don't really see why it's fundamentally important to know that. When you say fundamentally important to know the trace, do you mean like why do we need to use it as an operator in our derivation? Yeah. Yeah. So the nice thing about the trace is that it makes this product of matrices into a scalar. And in this class, we're never going to differentiate more than a scalar with respect to a matrix. It is true that you could, for example, differentiate a vector with respect to a matrix. And that becomes a 3d tensor or the derivative of a matrix with respect to a matrix that becomes a 4d tensor. You could try to apply those rules to solve these derivatives. But for us to make our life simple, we end up using this trace property. And we find that this will allow us to just work with matrices and vectors and simplify the algebra involved. Any other questions here? All right. So we're going to take the derivative of the log by the hood with respect to sigma. And so this first term, TI log pi has no sigma in it. So we can ignore it. And then we were differentiating then this term, TI log of the multivariate normal distribution. And so my derivative operators linear so it can go inside the sum. So we'll have a sum. And then I bring out the TI because it doesn't depend on sigma, my covariance matrix, big sigma.

And so now I'm going to differentiate this term with respect to sigma. So for this term, which we'll draw here and move. I'm going to have first this log normal term. And that log normal term has a bunch of expressions. This middle expression here, the D over 2 log of 2 times pi. This doesn't have any sigma. So I can ignore it. So I'm going to do this one first. Right. So here I'm going to get this term minus 1 half. And then we have here an X i minus new transpose sigma inverse X i minus new. We know that this is a scalar. Right. Because this is a row vector times a matrix times a column vector. And since it's a scalar and we know that the trace of a scalar is a scalar, I'm going to go ahead and just write trace of this expression. So instead of just running the expression, I'm going to put a trace around it. So I'm going to write trace of X i minus new transpose sigma inverse X i minus new. And then I'm also going to have a term here minus 1 half log a determinant of sigma. So I'll copy that of minus 1 half log determinant sigma. And I'm going to differentiate both of these terms. And then I'm going to have a plus. So this red term doesn't have a sigma. So differentiating with respect to sigma gives 0 and then I'll have a 1 minus ti. And then pretty much the same thing. I'm going to have a minus 1 half trace of every got to put my you want to see. 1 half trace of X i minus new zero sigma inverse X i minus new zero minus 1 half log determinant of sigma. All right. Any questions here. Okay, so if I look at these expressions for the 1 half log determinant of sigma. I'm sitting in pretty good shape because I know how to differentiate these. I have my look up table and here. The derivative of log determinant sigma with respect to sigma will just be sigma inverse. So these terms I can differentiate. But these terms. The traces of these vector times matrix times vector is not exactly in this form trace of sigma inverse times a. All right. So how can I transform these expressions to be in this form so I can apply this derivative rule here. You can write it in the chatter or razor hand. So charan says we can use this cycling property of the trace. And so when I want to use this derivative rule, I want the sigma minus 1 to be in front.

All right, everyone. We're going to have homework number six not do this Friday but do Monday of next week. All right. And then this homework will cover material up to and including this lecture. And so we had a typo on the homework that said it covers material up to lecture 14 should just be up to include, including today's lecture. Alright, any questions. Before we begin. All right, so we're going to continue with frequency response today. You'll recall that after Fourier transform and knowing the convol- Melissa. Are you going to be holding office hours, adjusted office hours this week? Yes, I, so I won't be holding office hours on Thanksgiving. I did see a Piazza post requesting me to change my office hours. I'm looking into that right now because I, I'm not sure I'm going to have availability prior to the day the homework is due. But if I if I'm able to find a spot potentially on the weekend, then I will do it.

And Daniel. Yeah, I was wondering about Friday, if that is going to be regularly scheduled office hours with the TAs. I believe that will not be I believe Friday is also off so UCLA has both Thursday and Friday off. or planning on holding anything there. If there's a difference, if there's a change in what I just said, I will announce it. Okay. Thanks Daniel. Those are good questions. Any other questions?

Okay, let me make Tom White co-host really quickly. And, circle on. I wanted to add that we will be posting the discussion problems for this week along with the recorded videos on the solution so that it helps the student with solving homework six. Great, thanks, Tomwe. So if I'm clear, based off of, in response to Daniel's question, there will be a discussion video this week, as well as posted discussion questions and solutions for students.

All right. Okay, and Sal. Thank you. I have a question following up on that. So would it also be released on Friday or there should be expected to be released on Friday the discussion questions in the recording? Good question. At T.A. do you have an answer for Sal? So we will try to release it by this Wednesday night so that students have a lot of time over the weekend to look at it.

Thank you. Thank you all. Okay. Any other questions on logistics for the class. All right. So, getting back into material. Last lecture we were talking about frequency response, which was to say, prior we've talked about how LTI systems can be written as convolution between input and impulse response. And this is due to the Fourier transform of the impulse response capital H which is called the frequency response times the Fourier transform of the input x. And therefore we were able to see that the magnitude of the frequency response and the phase will be shifted, the phase of the output will be shifted by the phase of the frequency response. So last time we did one example, we'll do another example to start off this lecture on frequency response. And so last lecture we considered a system that gets an input to cosine t plus three cosine three t plus two plus cosine of two t, and the system has an impulse response, which is given by the sinc squared.

And so, last lecture, we talked about how, if I asked you to compute the integral we would have sine squares times cosines that we would have to integrate. However, if we do this in the frequency domain by taking Fourier transform we find that this output to find the output is almost is very straightforward. And so all I have to do is take the Fourier transform of capital H, which we did over here, and then multiply them together to do this multiplication, we drew this diagram last time to show that this is X of j omega. This here is H of j omega, and then their product is here, why have jam. Why have j omega and take the inverse Fourier transform by recalling that So the inverse Fourier transform is just cosine of one times t. And then similarly for this term over here. All right, so with that we were able to compute what y of t was. Any questions from that example that we did last lecture? All right, so we're going to do one more example, and then after that we're going to get back into our discussion of filters which we started last lecture. And so in this example we have an input to the system which is e to the minus t for t bigger than or equal to zero, so multiplied by the step function. We're going to put this into a system with this impulse response. And so we want to know what the output is as well as this Fourier transform. And again we're going to use the fact that convolution in the time domain is multiplication in the frequency domain. So from our lookup table, we know this pair that e to the minus a t times u of t has Fourier transform one over a plus j omega.

And then h of j omega is going to be a constant 2 because of linearity of the Fourier transform, and then e to the minus 2t is going to give us a Fourier transform of 1 over 2 plus j omega. And so from these we can calculate y of j omega, the Fourier transform of the output, by simply multiplying these two together and we get that the output Fourier transform is 2 over 1 plus j omega times 2 plus j omega. And that is the output Fourier transform. So computing capital Y j omega was very straightforward. Any questions here? All right, so then to calculate the y, little y of t, the inverse Fourier transform of this expression, we typically don't want to do that inverse Fourier transform integral, that's tedious, and so we would use a lookup table.

However, in our lookup table, we don't have something where the denominator is a product of these a plus j omega terms. So I'm going to tell you how to do this inverse Fourier transform by a technique called partial fractions. We're not gonna go into it in much detail here because this is gonna be a critical topic for Laplace transforms. And in Laplace transforms, we're gonna give you three or four methods to do partial fractions.

And so this one we're just gonna do, this one we're going to do and the details will be clear, but we're going to give a much more extensive discussion of partial fractions later on. And so, in partial fractions what we do is we have a yj omega here, for example, which is two over one plus j omega. Two plus j omega. So we know a Fourier transform pair which is e to the minus at u of t has a Fourier transform 1 over a plus j omega. And so what I'm going to do is I'm going to assume that this expression can be written as the sum of two fractions where the denominator of one fraction is 1 plus j omega and the denominator of the other fraction is 2 plus j omega. So I'm going to assume that this can be written as the sum of two fractions where one fraction has a denominator 1 plus j omega and the other has a denominator 2 plus j omega. If I gave you a fraction like this, right, you would know to simplify it, and I asked you to simplify it into one fraction, what you would do is you would multiply this left-hand sign by 2 plus j omega over 2 plus j omega in the numerator and denominator, and then for the right-hand side, 1 plus j omega in the numerator and denominator. And Tomoy tells me that you all did partial fractions in discussion last week, so that's also great. And so for this partial fraction we're just going to do it a very straightforward way, which is, and again in Laplace transform we're going to give you several methods to do partial fractions, but what we're going to do is I'm going to assume this to be true, that I can write this as a sum of two fractions, and then what I need to do is find out what a and b are such that these two expressions are equal to each other. So what I'm going to do is I'm going to multiply, if this is the left-hand side of the equation and this is the right-hand side of the equation.

We're going to start lecture now. So a few answers before we begin. The first to reminder that homework number four is do this Friday and then we will also release homework number five then. And homework number five is going to be a pen and paper homework on graphical models. The other announcement is a reminder that midterm read grades are going to close on Monday, May 17th and so be sure to get your read grades in by then. Are there any questions regarding the course? All right, so we'll get back to material. So we're going to finish up graphical models today and then after that we're going to get into continuous decoding for brain machine interfaces. So recap from last lecture is we introduce this notion of graphical models where we have nodes that represent random variables and links that connect random variables. And when we have a graphical model, what this does is it gives us a factorization of the probability density of all of the random variables.

And that factorization is that we're going to iterate over all the random variables and it's going to be a product of their probabilities given their parents. And so we had done a few examples where we drew a graph and we were able to write out the factorized distribution. And then we wanted to look at these graphs and gain some intuition over them as well as answer a few questions. The first question is are certain nodes in the graph independent and the other was are they conditionally independent? So we're in the middle of going through these three examples of the potential ways that you could hook up three nodes with links. All right, and we're asking for each of these examples what the factorized distribution of what the factorized distribution of A, B and C was. If A and B are independent and then if A and B are conditionally independent, it's given C. Any questions plus bar? All right, so we did example one and today we're going to continue off on example two.

All right. So for example two, this is our graph and we have A being the parent of C, which is the parent of B. And so the factorized distribution was P of A because it has no parents, so there's no condition. There's no condition on any variables. Times P of C given A times P of B given C. And then last factor we had run the poll to ask the intuitive answers for these questions which will not show rigorously. But we asked two questions. First is A intuitively independent of B, which means if they're independent knowing A does not give you any more information about B than you already knew. And so when no variables are observed, in particular C is not observed, A and B are not independent. And that's because if I know A, I know something about C. And if I know something about C, then I know something about B.

All right, so through this node C, A gives me information about B. And then the second question we asked is is A independent of B given C. And the intuition here is that the answer should be yes. And the intuition is this, when I know C, I observe its value for A. And so then A being independent of B given C tells me in intuition terms that is asking the question, does A give me any more information about B than I already knew from knowing C. The way that A and B were dependent is that A gave me information about C that then gave me information about B. But now if I know C, A doesn't give me any more information about C. I know C perfectly. And so I don't gain any information about B from knowing A because the information I have about B, I already know from observing C. All right, so those are the two intuitive answers. I know we covered this intuition a bit quickly at the end of last lecture. So I want to ask if there are any questions here. All right, so then we're going to go ahead and do the more rigorous derivation.

So I'm going to move this to the next slide so that we can just say this slide for number two. So is A independent of B intuitively now? All right, and so if we want to show that P, that A and B are independent, then we need to see if P of A comma B does this equal P of A times P of B. Remember that since these are not independent, you can also just come up with a counter example and show in that counter example that A, that A and B are dependent. So let's go ahead and just try to answer with a general proof. So we have P of A comma B and we want to see if it's equal to P of A times P of B. So maybe think about it for 20 to 30 seconds as to what the first step here would be. Can someone write me in the chat or I'll space your hand for what the first step you might do here is? Total probability, that's correct. So we're going to remember we want to show if this is true for this particular graph, right?

And the information we have about this graph is that P of A, B, C has this decomposition. So we want to introduce P, A, B, C somewhere. So we're going to write this as sum over C of P of A comma B comma C. And then we'll use the factorization of the graph. So this is sum over C and then we'll have a P of A times P of C given A times P of B given C. Right? At this point, we can see P of A has no dependence on C. And so this is equal to the P of A times the sum over C of P of C given A times P of B given C. And just like last lecture or just like the last example, when we get to this answer here, these will be independent. And if this expression here equals P of B, right? But in general, this is not equal P of B. So in general, this does not equal P of B.

Right? So therefore, A and B are not independent. Just like the last example, one of the conditions where we explored this a bit more, one of the conditions for this equal P of B is that P of C given A is equal to P of C. Right? In which case, this would equal P of B comma C and then we sum over C given P of B. But in general, that is not true. And C is going to depend on the value of P of C. Any questions here? All right. So then let's do the conditional independence one. So I'm going to copy this graph over also. I just saw in chat. Can I explain why this is not equal to P of B? Yeah.

So we'll just do one condition. So one condition for this equal P of B would be that we know that P of B can be written as the sum over C of P of B comma C. Right? And this equals the sum over C of P of C times P of B given C. And so if this summation looked like this, then the sum would equal P of B. Now this summation has a P of C given A. And to say that P of C given A equals P of C means to say that A and C are independent. All right? But A is a parent of C. And so here A and C are not independent. And so in general, P of C given A does not simplify the P of C. And in general, therefore, this is not equal to P of B. All right. So we're going to do the conditional independence now.

All right, everyone, we're going to get started for today. I hope everyone had a happy Thanksgiving and a good break. So announcements for today are first that homework number six, we sent an announcement on Piazza and CCLE last week that we have delayed the homework to be due this Friday, uploaded to gradescope by 1159pm. All right, and then start the Laplace transform. And then the rest of the class will be about the Laplace transform how to invert the Laplace transform and therefore has a high overlap in properties with the Fourier transform. So since we have gone extensively to the Fourier transform, hopefully going through the Laplace transform would be quicker.

All right. Any questions on course logistics or any administrative related things? All right, so we'll go ahead and get started. So, I just wanted to remind you. Last one last Monday actually before a week ago before Thanksgiving. We had started this lecture on frequency response, where we had talked about low pass high pass and band pass filters which extract out frequencies in particular ranges, and we went on to derive their impulse responses, and some characteristics about them. We talked about this am radio example, and how filters could play a role in helping us to recover the signal signal after demodulation. some practical limitations of these filters. And one thing we mentioned was that if we were to take an ideal low-pass filter whose impulse response is a sinc, well, the sinc is not causal because we know that a causal impulse response must be zero any time when t is less than zero. And so we said, okay, well, to make it causal, we can do something. We can shift the signal in time, so that now the signal is zero prior to zero but then after zero we have a shifted impulse response.

And so the input filtered by this convolved with this delayed impulse responsible will also just be shifted. And so if I shifted the impulse response by three, my output will also be shifted by three. Right. And we said that this is called a distortion list LTI system. And so, a distortion list LTI system is one where the output is going to be the same thing as the input, except it might be scaled by some amplitude K, and it might be shifted by some time. TD, and so TD would be some delay, for example, the delay and the impulse response for making a causal like on on this slide over here. deriving what h of j omega was for a distortionless LTI system and we kind of went through this really quickly so I'm just going to walk us through this again, which is that the distortionless system has this equation that the output is the input that is shifted by some time TD and scaled by some amplitude K. So what is the frequency response, big H of j omega for a distortion list LTI system. And to calculate H of j omega, you all know that the frequency response, H of j omega is going to be the Fourier transform of the output, divided by the Fourier transform of Y of t, is equal to k times big X j omega, and then because it was shifted in time that corresponds to a multiplication in the frequency domain by this complex exponential. And then after I have this, then I can just solve for big H of j omega, which is big Y over big X, and that's equal to some constant big K times a complex exponential. And then the magnitude of H of j omega is just equal to K.

And then the phase of j omega is what's multiplying the j in the exponential, which is minus omega Td, the amount of delay we have in our system. All right. So in the distortionless LTI system, this is what its frequency response looks like. It has a magnitude which is this K, and so if I plotted the magnitude response, it would big K. And then the phase response is going to be negative omega Td. So that means that as omega increases, it's going to get more negative. And the slope of this line is going to be minus Td. Okay. And so if you deviate from this amplitude and phase spectrum, for example, if your amplitude spectrum look like this, right, or the phase spectrum look like this, then your output is no longer going to be distortionless.

Your output will have either amplitude distortion if the magnitude response is not just equal to k, or phase distortion if the phase response is not a straight line. So what do these distortions look like intuitively. Well for amplitude distortion. It's relatively straightforward. So, the amplitude response is going to be a constant with the value, the amplitude K. And so when the amplitude deviates from an impulse, for example, if we try to make an impulse, but we didn't do a good job and because an impulse is really hard to make in real life, and so, or it's impossible to make in real life. So instead of an impulse, we have something that looks kind of like an impulse, but has some, has some, isn't perfectly an impulse.

If we were to involve X of T and H of T, right? So we flip and drag this h of t here, right, this h of t is going to when we do a flip and drag is going to lead to a smoothing out of the abrupt corners of these racks. the function or in the frequency domain deviating from a straight line. Any questions there? All right, so then phase distortion is a bit more tricky to conceptualize. And so recall that our frequency response, h of j omega, can be written as the magnitude of H of j omega times the phase, e to the j phase of H of j omega. And so when there's phase distortion, what this is telling you is that the phase at different frequencies is going to be different. Right. And so, if I have. And I derive this for a decomposition, right, and the for a decomposition tells me that this is going to be comprised of science and cosine so maybe this is a low frequency cosine.

And then there's a higher frequency cosine. And then, let me do this in a different color. Purple, maybe even higher frequency cosine. this rect right here, right? The phase tells me how much each of these different cosines is gonna be shifted, right? Because phase gives a shift in the cosine. And so when there's phase distortion, the way that I picture it is that the cosines are being delayed relative to each other such that when you add them back up, the signal looks distorted.

And so maybe we have the case that the phase of the high frequency signals is close to the close is small so that their face doesn't get shifted a lot. And so maybe these stay here. Alright. So now because all the different cosines are being shifted at different phases relative to each other. When I go ahead and add them up. They won't perfectly add up to give me my original signal. Instead, when they add up, maybe I'll start to see some distortions in the output where here. Maybe the lower frequencies were delayed later so they only add up to give me my original signal when the low frequencies are also incorporated but maybe early on, I have just more representation from the higher frequencies. So when I add them back together that beats to some distortion in the output.

Right. So I want to. Sorry. I'm not here for this. Sorry, was that Caleb? I'm not sure if you have like a procedure for who goes first or something. Usually we do a hand raised procedure, but there are. There are so few people here that you could just scale for it. Yeah, thank you. I was really stuck on the Poisson distribution part of the homework. And I've been looking at some of the discussion notes regarding solving the optimization parameters. And I guess I just had a few questions about how our variables are being formatted. For instance, in Shoshank's discussion. And I will for thought our homework, we have these why I terms, which are elements of a vector why. And then in discussion, it seems like each why I term is also a vector from a size D.

And that sort of tripped me up and I was hoping you could clarify the variables and maybe how to proceed with the problem. Yeah, so why I is a scalar. And so I so shouldn't be a vector of size D. Let me, sorry, let me grab my iPad. I should have had it already so that I can write out some things. And I just realized if I do connect my iPad, I have to get my AirPods working. Otherwise my iPad audio will. Will make so let me just do this. I hope it just takes 30 seconds, sorry, everyone. Thank you. All right. Can you all hear me? Yes. Okay, great. The AirPods are working out.

All right. So for this question. Regarding the setup, it should start off similarly to the other questions. And so for example, the derivation of pi k should be exactly the same. You just look at that pi k is the proportion of trials. The difference will come when we now. So just going up from the start to this line, this is writing the total data likelihood. So we're going over all of our classes with OK. And then for each class, where iterating over the trials, J in that class. And then we had the log probability of the neural data and the class. And so I feel free to anyone stop me if any of this is unclear. So this is our data log likelihood data log likelihood. And then going from here to here, I think all of you have done this already. I just use Bayes rule to write its p of ck plus p of y given ck. And this is log of pi k again, the same derivation has in the prior parts.

And in the prior parts for log p of y given ck. You all ended up using the Gaussian distributions. Now for Caleb's question as to what happens when this is poson. So for the poson distribution, what we do is. In the multi variate gas in case we model this covariance between neurons. And so there is some inter there are terms following how. Enter how the relationship between neurons in the poson case, we don't have that. So in the poson case, we just have a random variable describing a single neuron. So the way that we said this in the homework statement is that. When we take the probability of all. D neurons. So why to the J is. Your vector in our D and it contains your firing rates of the dean neurons. So why one why to to why D. And this part what we say is that. The why I the little why I the elements of these of this vector are independent conditioned on the class.

And so what that means is that. If I take the probability of this entire vector given the class, I'm in ck. This is going to equal p of y one given ck. Times p of y to give them ck. All of these little why ones why to's why these these are just scalars. They're single neurons. And all of these have a poson distribution. Real quick, these things you could differentiate between the superscript and parentheses in the subscript. Yes, the superscript and parentheses, the J here is referring to which trial we're looking at. So. All of these have a superscript J saying this is a neural data on the j trial. I committed them at a convenience, but let me just put them in to be absolutely clear. So for the j trial, we're saying that the neural data that we see on the j trial, which is a vector in our D for dean neurons. For the naive base plus on model. Can be broken up into a product of probabilities, which is a probability of the first neuron, the first element.

Times of probability of the second neuron, the second element, etc. And that's this expression that I've written right here. Okay, I just think it was just a little bit confusing for me because when I was deriving it for the Gaussian model, but looking at the summation line and read, I would use J as sort of a subscript rather than a superscript in that case. Yeah. And for these notes, I intentionally put J in a superscript. Actually, these are from Monday's office hours notes. I originally was using I, but then I saw that in the question statement, we use I as a subscript for the neuron number. And so that's why that's why I changed all of these to j's and have used it the trial to be the superscript parentheses to differentiate from the subscript number, which is the number the neuron number. Okay. Yeah. And so what you should be able to do then is if this is the probability of this probability is P of Y, the entire YJ, which is the entire vector in RD, then you could take this product. And so you know from the problem statement that the I neuron given the class CK is going to have a pause on distribution with mean lambda K I. That means that each of the P of Y I given CK can be written as the mean lambda K I raise to the spike counts, which is Y I times E to the minus mean divided by the spike counts, which is Y I factorial. So this this line here, all I've done is I've taken the plus on a lamp a K I and written that written the actual probability mass function in place here. Okay. I'm I guess I'm just wondering that at that point, you're taking we're optimizing over lambda. Was it lambda K I or lambda K J? Okay. So you just have one of these in the in the product.

Well, you take the line. So and then you pick out the the one with the I one. That's correct. That's correct exactly. So yeah, you you put this. I'll just say it just in case to be clear to everyone off on the call. What what Caleb was saying is that we take this product, we plug it in here, right? And then that's just into a sum of log of this expression. And then from there, you can differentiate with respect to lambda K I. Because that's the parameter of this model. And as Caleb was also mentioning, this son is going to have. So that's the lambda K I's for many case and many I's where you're different shooting with respect to a particular particular lambda K I. And so that should simplify the calculus and. To. To make sure that you've done the the math correctly, I can tell you the intuitive answer here, which is that lambda K I ends up being. The average firing rate for neuron I in class K. So when you actually derive what lambda K I is. You should get an expression that looks like lambda K I is equal to the average firing rate of neuron I in class K.

All right, we're going to get started for today. So just two announcements. The first is that homework number six is due this Friday, and then, including today we have three lectures left so we're going to finish sampling today and then the rest of our Any other admin related questions. All right. Cool. So, we can't store the value of the signal at every single point in time because there's infinitely many of them. And so we have to store samples. But then, now comes the question. How quickly do I sample my signal because if I sample very quickly like up here, then I'm definitely going to be able to know what my original signal was if I sample too quickly, then I'll also end up using a lot of memory storage to store the signal. And so what we're going to finish deriving today is the sampling theorem, which tells us that, tells us the minimum, sorry, tells us the maximum time in between samples, or the minimum rate at which we need to sample a signal, so that we can perfectly reconstruct it.

Right. So then, last. Oh, and here's an example which we'll talk about later on, which we'll talk about later on, but just shows that if you only have samples like the red points for the samples shown in the red points, we can show here that it could correspond to cosines at different frequencies and so here's an example where, without any other information these red samples here are ambiguous because it could be this cosine at 0.75 hertz, or they could be this, they can correspond to the samples of this cosine here at 1.25 hertz. So we'll talk about that later today this is an effect called aliasing. All right, so last time we took a very roundabout path to essentially build up the intuition for how to think about sampling. And we introduced the signal called the impulse train or the delta train, where this big T tells us that we have a bunch of deltas that are separated in time by a big T. So these are a bunch of deltas that are all big T's spaced apart.

and we call omega naught is equal to two pi over big T. Right? And so an impulse train in the time domain has a Fourier transform, which is another impulse train in the frequency domain. And because we know that in time or to sample a signal in time, what we do is we multiply that signal by a Delta train. Right? If I sample a signal in time, if your signal is sampled in time. That means that in the frequency domain. The spectrum of the signal is going to be convolved with an impulse train because multiplication and time is convolution in the frequency domain. And so this is this periodic sampling duality that we derived last lecture.

by multiplying it by my impulse train which are impulses separated by time big T. What that means is I'm going to take my spectrum for FFT, so here I've just drawn this as a red triangle here. And then I'm going to convolve it with an impulse train which means that I'm going to create replicas of it at every omega naught. Alright, so I might pause and see if there any questions for this recap from last time. Alright, so Okay. So, this is the slide that we ended on last time, which was to say, if we take a signal at 50 and sample it by multiplying it by the impulse train. Then, the Fourier transform of the signal is going to be the Fourier transform of FFT replicated every integer multiple of Omega naught. We saw from this that we can deduce the intuition for what rate we have to sample at. So, if Omega not is big.

Right. If Omega not is big, which means that capital T is small, the time in between my sampling impulses is small, then the replicas are going to be far away from the center remember the replicas occur every omega not right. However, if omega not a small shown here in purple, meaning that the time in between my samples is relatively large, then the replicas are going to be close to the original signal, whereas in the case of when Omega not was big and we had a replica far away. I could recover my original signal which is just this triangle over here by applying a low pass filter in this range. Okay. And so we're going to be the maximum frequency of the signal. And so, the frequency at which the signal goes to zero and stays at zero forever. We're going to call that be. That's going to be a bandwidth and frequency but in the omega domain, it would be two pi times. times B. Grace. I wanted to clarify some notation.

So when you write tilde f of t equals f of t times delta sub tau t, or delta sub big f is f that is sampled in the frequency? Sampled in the time domain. Yeah, so f tilde here would correspond to the sequence of these impulses. Okay, thank you so much. Great. Thanks for that question. Any other questions here. And then I saw a question in chat, Jared asked with bandwidth we are saying that in time the signal contains no frequencies larger than plus minus be over saying that in the not in time but in the frequency domain omega. So the largest frequency for Omega would be to pi be. So basically it's the width, or it's half of the width of the signal spectrum.

And that's what you're looking for. When you're looking at Omega, the angular or natural frequency. Alright, so And so, we're going to multiply an impulse train. Omega not, and then an impulse train with impulses separated by Omega naught, and that creates the replicas at Omega naught. Right. And so here we're showing this convolution. Again, just to get the flip. Not again but to get the flip and drag intuition which is, if I take this impulse train and flipping flip it right if I flip it it's even so it's going to be exactly the same. Now when I do the drag this impulse, when I drag it to the left and the right is going to trace out this triangle. All right. And then if I drag this impulse to the right, eventually it's going to trace out a triangle that's going to be this triangle and if I drag this impulse to the left, it's going to eventually trace out a triangle and then the same for all And you can eventually get these triangles replicated every omega. Rampton. Um, so you said that f of j omega must be zero above to pi be and below negative two pi be. Is that always the case for the periodic signals. Yes. So, you do mean periodic in time.

All right, everyone. For today, we have just one announcement, which is that homework number five, which is pen and paper homework on graphical models with upload to grade scope last week and it's going to be, sorry, uploaded to CCLE last week. And there will be due this Friday, uploaded to grade scope by 1159 PM. Any questions on any course logistics? Oh, and a reminder that today is the last day to get in your midterm read grade requests. And so if you haven't taken a look at your midterm yet, please be sure to do that today. All right. All right. So last lecture, we started to get into BMI continuous decoding where the goal is to decode the signals from the motor cortex, the spike signals. And instead of using that to classify, which you all did on homework number four, to classify one of eight targets, now we want to decode algorithm that translates the neural spikes into continuous movements like a robotic arm or in our case, a computer cursor on a screen.

All right. And last lecture, we had gone over several videos showing the history of these continuous motor BMI's. And now we're going to get into the algorithms. And so there are going to be three algorithms that we cover in class, which are the optimal linear estimator. Here we call it linear vector, the weener filter, and then the common filter, which will be for the next few lectures. And we go over the Matthew hindies in this review paper that we wrote in 2014. All right. Were there any questions from last lecture about anything related to the videos or to any historical BMI performance? All right. So then let's set up the decoding problem and let's get to algorithms. So in the data set that we are going to give you for homework number six, what we'll have is a monkey, monkey J, and he reaches to eight different targets without a planned period this time.

He just makes movements continuously from targets to target. And simultaneously, movements, we record neural activity. All right. So we have two observations. The first observation is the monkeys observed kinematics. And we write that as respect to XK. K here denotes time. So it's K instead of T because we're going to discretize the time. And then PX and PY, these are the monkeys X and Y position at time K. And then VX and VY, these are the monkeys X and Y velocity at time K. These are the position and velocity of his hand. So as he reaches, we track a little bead on his hand and we're calculating the position and the velocity of that little bead. All right. And so simultaneous to this, let's say that we have one new TOR ray implanted.

And so on this Utah array, we'll have capital N equals 96 electrodes. So on each of these electrodes, we're recording spike, spike knee activity. And so here we have a vector that tells us the bin spike counts at a given time. All right. So this is this illustration here. The input to these decoders are a spike vaster where we have 96 neurons. And basically at some time point K, what we do is we define a bin with a window. And I believe in the homework, this window will be 25 milliseconds long. Right. And what we do is we count the number of spikes that occur on every single neuron or every single electrode in this 25 milliseconds. And if we divide the number of spikes by 25 milliseconds, that gives us the firing rate, nestinit of firing rate. Right. We usually just avoid the dividing by 25 since I've just scales everything.

And so it's not totally necessary. So Y1K, if we do divide by 25, that'll be neuron once firing rate in that 25 millisecond bin. Y2K will be neuron 2's firing rate, et cetera, et cetera, down to YNK, which is the 96 electrodes firing rate. So we have a question from Jonathan. Yeah, is this sort of analysis performed during the experiment or is it performed after the experiment? Or does it matter? Great. So for building the decoder, which is what we're going to talk about first, how we train it, we collect the data, and then we do this analysis after the experiment. However, it's entirely possible to also do this as the experiment is running, collect your data, and then update your decoder. So when you take these bins of neuron firing rates and you've already performed the experiment, are you putting a bin center around time point or is sort of like the bin, it's taking the time point and a number of time points before the time point.

Do you understand what I'm getting at? I'm not 100% sure. Jonathan, let me just draw diagram first because maybe this helps. And then if this doesn't help, then let me address your question again. So if we have spikes, what we do is, if this is zero, time zero, this is time 25 milliseconds, this is time 15 milliseconds, this is time 75 milliseconds. So this activity between zero to 25, we'll call Y1, Y at time 1, this activity will be Y at time 2. And so we just increment every 25 milliseconds and there we count the number of spikes, we get the firing rate, and they just go in 25 milliseconds increments. Okay, that's fine. Thank you. Okay, great. Jonathan. Yeah, it's on one.

Yeah, it's on one. So since this is a continuous decoding task, right, we are doing it for 25 millisecond time bins. So for that 25 millisecond time bins, how do you record the kinematics? Like for example, the position might vary over to 25 milliseconds or is it fixed? Yeah, great. So over the 25 milliseconds, the monkey will be moving. So if you imagine, let's say the monkey is holding a center target and then he makes a reschedulatory out to one of the targets, let's say he reaches out to here. So this would be time zero and then in the homework, we'll tell you how to do this, but basically every 25 milliseconds. So actually, let me fight it like this. In between these two time points, we'll be 25 milliseconds in space. And so this will be here. The x and y position here will be p of x1 and p of y1.

And the x and y position here will be p of x2 and p of y2. And then the way that we get the velocity is that we simply do the simple oiler approximation of the derivative. So if I want to calculate the velocity in this 25 milliseconds bin, I would take the x velocity would be px2 minus px1 divided by dt, which is 25 milliseconds. And that would give me an estimate of the velocity. And then we would do this regression. Then the neural data from 25 to 50 milliseconds would be regressed against this velocity and this position. And there are some details about the timing that are important to get right. And so we walk through all of that in homework number six. And I want to say one more thing I responded to Jonathan's earlier question, which is we learned a decoder post-talk after the experiment, after we've collected the data. But then of course, after we had that decoder, then we can decode new neural data on the fly to then move a cursor on the screen.

And we predict that cursor's movements, those movements are entirely nearly driven. And so when we really decode the neural activity to get the kinematics, we put a hat over it to denote that it's decoded and the decoded kinematics will be x hat k. So similar to before in the training phase, I want to learn a relationship between yk and xk. That's my training phase. In my testing phase, now I only get yk the neural data. And I want to predict what the kinematics of that cursor are x hat k. Sorry Jonathan, I'm trying to get it again. So do you think that 25 millisecond ad hocly or that's the amount of data that's required to decode this kinematics with high fidelity? Yeah. So great question Tomway. So the 25 milliseconds we choose from empirical results, so actually in empirical results, it gets even your BMI, your decoder performance gets even better as you make dt smaller and smaller.

0:00:00
All right. We can do the race hand system. And I see first the question in the chat. So Caleb asks on the Wiener filter slide. So sorry, let me connect my iPad again. So I can get the Wiener filter slide up. All right. So Caleb's question on the slide. How about getting from the Wiener filter to the correct form? I think that blows probably this slide. Can you confirm that the top yellow example is correct? It seems like the last term should be L5 times Y0. If you follow the pattern, L3 times Y2 L4 times Y1, you are correct, Caleb. So let's see.

0:01:22
We're starting at time P plus 1. So my error is this should be an L4. So this should be an L4. Since P is equal to 4, so it goes from L0 all the way up to L4. And then this goes to K minus P. K is equal to P plus 1 is equal to 5, so 5 minus 4 is equal to 1. So it should have been this. If you follow the pattern, good. OK. Thanks for catching that Caleb. I'm going to re-upload these notes now so that I have the correct version uploaded to CCLE. So then while this is happening, I'm happy to take the next question. Edwin. Hello, Professor.

0:02:31
I'm wondering if I can ask a question about the latest homework? Yes, let me pull it up. All right, I had the homework up. OK, so I'm working on the last problem about the precision matrix. And so far for the covariance, I have a 4 by 4 that just has to add no terms. But I'm a little bit confused because in part C, I see that we're able to invert it. But my dad knows just turned out to be the variance of each input, so I feel like I'm not completely comprehending the problems. I see. Sorry, let me just really quickly read this. Yes, so I'm sorry. Edwin, you were saying that your precision matrix to the inverse of the covariance, you had just the variance terms on the diagonal? Yes, correct. OK, yeah, so there is something, I think there was a, that's not the question answer, so I think that B was probably incorrect.

0:03:58
And so for B, we can walk through that if that would be helpful. Yes, please. All right, great. So we have this graph, which is x1 to x2 to x3 to x4. And then we say a possible application of this directly graph is to model a stimulus that changes over time. And so in this case, we have x1 is Gaussian like this. And then xt given xt minus 1 is Gaussian with mean xt minus 1 and covariance, and variance sigma squared. And here this is for t equals 2, 3, 4. All right, and so then we ask what is the 4 by 4 covariance matrix of the vector x, which is x1, x2, x3, x4. All right, so can someone, it can be Edwin or anyone else tell me what a first step you thought to do here was? I started with working with the definition of the covariance and an useful thing I thought was trying writing the graphical interpretation of the probabilities.

0:05:39
Okay. Yeah, so let's say that we want to calculate the covariance for xi and xj. So this would be the expected value of xi, xj minus the expected value of xi, expected value of xj. And then Edwin, sorry, was the other thing that you said with the graphical model? I just wrote down the equation. Yeah, so what were the equations that you had? The probability of x1 times the probability of x2 given x1 times the probability of x3 given x2 times the probability of x4 given x3. Great. So this is the joint density. All right. And then again, Edwin or anyone else, given this information, how does anyone want to take a step at how they might have calculated these quantities? Okay.

0:06:53
Let me go ahead and walk us through this time. So here, we need a relation for xi and xj. And so if I were to write this expression out, let's just focus on the boundary equation. What I'm saying is that xt, given xt minus 1, is normally distributed with mean xt minus 1 and variance sigma squared. In this case, the number xt minus 1 is an observed quantity. It has no randomness. And so the way that I can simplify this equation, xt given xt minus 1, being normally distributed with mean xt minus 1 and variance sigma squared, that's the same thing as saying that x of t is equal to x of t minus 1 plus a random variable, I'll call it w, where w is normally distributed with mean 0 and variance sigma squared. Now why can I say this? It's because of following, if I take the expected value of xt given xt minus 1, that's going to be the expected value of xt minus 1, xt minus 1 is an observed quantity. It doesn't have any randomness, so it's just equal to xt minus 1 plus the expected value of w, but the expected value of w is 0.

0:08:44
So the expected value of xt given t minus 1 is equal to xt minus 1, and so the mean here is xt minus 1, and so that's all good. Now I want to calculate the variance of xt given xt minus 1. And in this equation, this would be the variance of xt minus 1 plus the variance of w. But in this case, xt minus 1 is an observed quantity, it's deterministic, so it doesn't have any variance, it's just a single number. So it's variance is equal to 0, and therefore the only variance in this expression comes from w, which has variance sigma squared. And so this variance is equal to sigma squared, right? And so this equation xt equals xt minus 1 plus w is a restatement of this fact of this distribution over here. Okay. Does anyone want me to go over any part of that again? I lost the motivation behind the addition of w, can you please go back to that? Oh, you mean why we put things in this notation?

All right, everyone. For today, we have two announcements. The first is a reminder that homework number five is due this Friday, May 21st. I'll upload to Gradescope by 1159 PM. The second announcement is that last lecture, someone asked a question about homework number seven. And I would look at the syllabus for this year, and on the syllabus for this year, we only listed up to homework number six. And given the pace of the class this year, this will be the last homework for this class. So usually we do have a, when this is taught in person and the pace is a bit quicker. We do have a homework number seven, which is on the mentionality reduction. And this year we are not going to have that homework, but we will likely release, release solutions for our difficult carriers. All right. Any questions on any course logistics?

Questions from Edwin. Hello, I'm just curious whether we will still cover dimensionality reduction. Yes, I believe we will be able to, I think given the pace of the class, we should finish common filter sometime next week and then after that our last topic is the mentionality reduction. Thank you. Any other questions? All right. So we are continuing on continuous decoding for BMI's. And today we're going to talk about the common filter. And so our motivation at the end of last lecture for the common filter was the following. We could decode with the optimal linear estimator as well as the weiner filter and we saw the weiner filter did quite well. But neither of these decoders took into account anything special about the movements we make. Both the optimal linear estimator and the weiner filter were essentially these squares regressions of neural data to kinematics.

All right. We now we know that when we move our movements are smooth and our arm has inertia. So we aren't making a jittery movements like we saw for the optimal linear estimator. All right. So the idea behind the common filter is can we incorporate additional prior knowledge we have like the fact that our movements are not jittery in real life or typically not jittery. Can we incorporate that into the decoder? All right. And so the premise of the common filter is to ask what if we have additional information in this case that our movements are smooth. Right. So here in lecture we're going to we're going to primarily talk about a velocity common filter which incorporates the smoothness of movements for a computer cursor on the screen. But maybe you're controlling a robotic arm and you know something about the dynamics of how that robotic arm moves.

The common filter would allow you to incorporate those dynamics into the estimation. And in this lecture what we're going to do is we're going to talk about the common filter first at a level of intuition since the math gets pretty involved and it's easy to get lost in the read. So we're going to talk about the common filter at a high level intuition and then after that we'll dive into the more math details. All right. So we're still in the context of the motor prosthesis where our goal is to be code the continuous movements of a computer cursor on a screen and you could generalize this to a 3D robotic arm if you're controlling the endpoint of that robotic arm. Okay. So really the common filter has been the workhorse for motor neural prostheses in the last decade and the best state of the art decoders today include a common filter. That said the common filter is of course not specific to this area of brainwashing interfaces and so one of the rewarding things for me in teaching these classes is to see students come back and tell me about how they used some of the techniques they learned in research and for this class what I hear most is how they were able to use the common filter in a particular application.

Essentially the common filter is a technique that is applicable whenever you have time series data and there's some dynamics or there's some structure across time where you want to model that. And so anytime when data is modeled to evolve through time in principled ways the common filter can help you incorporate that evolution through time those dynamics through time into your actual inference, interior predictions, interior decoder. So it's a tool that hopefully that isn't just useful for this class but could hopefully be helpful for you in other context applications or classes. All right. So we are going to go into these next slides I covered the common filter to have very high intuitive bubble and we're going to provide you with the algorithm so that you can code it up on homework number six and then after that we're going to go into the detailed derivation of the common filter because as we'll see the algorithm for the common filter the equations at least look very complex and so when we derive them we'll see where they come from and of course that they are not magic.

All right. Any questions so far? All right. So this was a figure from a study that essentially motivated the common filter. I kind of mentioned this study in passing last lecture where in this case what these people from the brain gate clinical trial did what these researchers did is they had the subject perform a participant perform a center out in back tasks to four targets and on a single trial if they decoded with a wiener filter these are what the trajectories look like and they look pretty noisy. If you take all the trajectories that go downward and you average them together you get these this black line here for the right word trials would be this blue line and you can see that even averaging across all the trials to a particular target the trajectories even in the memes look pretty noisy. So this is a wiener filter and this paper established that we should work with common filters although again for this study like I mentioned last time their delta t times p the number of things they look back in history was 1,000 milliseconds which actually poses problems for the common filter because it means neural data from one second to go is forming your movement at the current time point and you can imagine if motor commands from a second ago are influencing your reach right now that could actually be difficult to control and that's why in the homework we asked you to set p delta t on the order of 100 to 200 milliseconds which we find these to better performance.

This is what their wiener filter did and then they also implemented a velocity based common filter so a common filter that just because velocity and you can see on the individual trial the trajectories are much cleaner and if you were to take the average across trials you would get straight lines each target. And so at the time this is a very compelling demonstration that the velocity based common filter should be used over the wiener filter although today with some more hyperparameter optimization there's evidence that the wiener filter actually the vanilla wiener filter is better than the vanilla common filter. However after this paper once in the field worked on common filters and we may optimize into the common filter such that the state of the art common filter is indeed the state of the art algorithm out there today. Any questions here? Alright so I'm going to show this video in this a bit since I have to go to keynote to do that so let me first talk about the intuition. Alright so let's see that we have the following equation this is following our conventions from last time xk are the kinematics and yk is the neural data alright and you'll recall last lecture we talked about the optimal linear estimator where we did the relationship being xk equals L times yk so that we take neural data and use that to decode the kinematics xk alright.

Sometimes people are interested in modeling the reverse equation which is yk equals c times xk and this equation is saying how can you construct your neural data from the kinematics alright and if you have this single equation and I now ask you to build a decoder to say given that I have neural data how can I infer kinematics this is a question that you should all be able to do because we know what the least square solution is for c right and it would be that c is equal to y times x transpose xx transpose inverse that's the least square solution that we derive last lecture and then if I wanted to decode x from y then I could just compute the pseudo inverse of c and therefore xk would equal c pseudo inverse times yk alright and so since this equation is essentially the inverse of the optimal linear estimator sometimes or not sometimes this decoder is referred to as the inverse optimal linear estimator or the inverse OLE alright and the inverse OLE is this is the exact decoder that was used by the Pittsburgh clinical trial featured on the 60 minutes video and so I said that they use an optimal linear estimator to be more detailed they use the inverse OLE so they find a c matrix that relates the kinematics to the neural data and then they decode using the pseudo inverse of that c matrix right any questions there alright so that's how we would decode neural data if this was the only equation I have but now what I'm going to say is that we have another equation and that equation relates how my kinematics evolved through time in principle the way so I'm going to have an equation like xk equals a times xk minus 1 alright and this a matrix then captures the natural evolution or some properties of the evolution of my kinematics through time alright and this is something that you all have like we've seen before so if you think back to your physics class where you learned Newton's laws of motion right we can write them as an equation xk plus 1 equals a times xk relating again the kinematics xk to the kinematics of the future time xk plus 1 alright and so let's imagine that we have additional information in this case let's say that we are modeling a ball falling on earth right and so if we model a ball following on earth falling on earth what I'm going to do is I'm going to put the x and y position of the ball and the x and y velocity of the ball into this vector and this xk plus 1 equals a xk will then summarize my laws of motion so the first equation is going to say p xk plus 1 let me write it over here the x position at time k plus 1 is going to equal one time the x position at time k right and then plus delta t time the x velocity at time k that's just my integrated velocity so plus v at time k delta t alright this ball is falling on earth and so if we rack a y velocity right we're going to have acceleration due to gravity and we know from physics that that y position should follow this equation it's your initial y position plus your integrated velocity plus 1 half a t squared right so 1 half time the acceleration due to gravity which is minus 9.81 meter per second squared times a delta t squared alright so that's just one of those equations from physics when you solve kinematic problems and so we have that equation right here the y position at time k plus 1 is the y position at time k plus the integrated y velocity and then minus 1 half a delta t squared okay and then similarly we have then an equation that relates the velocities and so if you're in a vacuum then your x velocity is just going to be the same but your y velocity is going to increase according to the acceleration due to gravity okay and so this is a long-winded way of saying I can write Newton's equations succinctly in this matrix vector multiply that tells me how the positions and velocities of my ball evolve from time k to time k plus 1 right and they're all summarized by this matrix okay any questions there all right so what I want you to notice is that in addition to this equation that gives me a relationship between neural data and kinematics xk remember xk is what I want to decode right I now have one more equation I have an equation that tells me how the kinematics at a prior time step inform the kinematics at the next time step so now I have two equations giving me information about the kinematics and so why might this matter for this I'm going to go into my slides and and show a few videos so we're going to consider this problem of the ball falling except now it's going to be a cannon ball and we're going to be shooting a cannon ball into the sky so believe it like forward so here I'm showing a video of you can imagine a cannon ball being fired off into the sky right and we know that it follows a parabolic trajectory right and we know that if you know the initial position and the initial velocity because we know the dynamics and Newton's laws of equation sorry Newton's laws of motion we can solve for the position of the ball of the cannon ball at any single point in time so if I give you the initial position and the initial velocity and I ask you what is the position of the cannon ball at a particular point in time you can solve that for me with Newton's equations of motion right any questions there all right so then let's say that you were an alien who came to visit earth and you didn't know Newton's laws you didn't know what our gravitational constant was all you could know you don't know anything about the physics of earth all right and so if you're an alien that's come to visit earth and I ask you as an alien to track the um position of the ball as it flies through the sky you can still do it because you actually get to observe the ball flying through the sky and so even though you don't know Newton's laws of motion what you can do is whenever you see the ball you can plop down an orange dot and um if you do this for every single frame in time then you're going to know the trajectory of the ball all right so there are two ways to get the trajectory of the ball one you know physics and so you can calculate it uh two you may not know physics but you can wash the ball flying through the sky and at every single point in time you can just say okay the ball is here the ball is here the ball is there etc okay now let's say you're still in the alien shoes um but now we ask you to track the location of the ball in this sky except now you as the alien can only view the ball through a noisy camera okay and so you can only view the ball through this camera and maybe you're watching a post-tot video the camera has low resolution there and so here that low resolution is from blurring this pixels by analogy and uh further the video is noisy so it turns out that uh when the light is dim and it's at a dusk there are a few photons that are entering the the camera and the photon arrival times are actually follow a Poisson process not unlike the the spike times all right and so in the analogy um the observation of this noisy low resolution video is analogous to how if we were to try to decode a cursor position from neural data that neural data is low resolution because I only get to measure a hundred neurons out of a hundred million and the neural data is also noisy because the spikes um uh have various zochastic processes and we can model that with a Poisson process right so now if I ask the alien to try to trace this ball in the sky um he can watch the video again and you can see there's a ball going there is kind of hard to make out right at the ball is almost fallen okay and so now if the alien goes from this noisy video and says okay this is where the ball was at every single point in time he gets out this blue trajectory right now this blue trajectory generally gets the position of the ball across time but there are some aspects of this trajectory that you know uh just are impossible like when the ball here is reported to fall down and then float back up right and we know that that can't happen the cannonball in space uh in the sky can't just uh fall and then float back up due to gravity right so even though the alien did a pretty good job there are some of these sub-optimalities in the observed trajectory that we can definitely improve upon because we know the laws of physics and so the idea of the common filter is to say my observations might be for example low resolution or noisy meaning that if I try to track this cannonball with just my observations only I'm going to get a trajectory that guessed a lot of the features correct but we get some features wrong okay and that's because my observations are noisy and low resolution just like when we decode from the neural data right we will get a lot of the features correct like in the optimal linear estimator but some things will be at a lower level uh incorrect like the jitteryness of the cursor however uh if we then take the trajectory and incorporate our prior knowledge that the trajectory um that the cannonball has to obey Newtonian mechanics then what we can do is say well when I saw the cannonball fall down and then float back up because my video was noisy I know that can't happen and so maybe I can denoise the trajectory to return something in orange and so this orange trajectory incorporates both my noisy observations as well as my knowledge of Newtonian mechanics right so what I'm doing is I'm taking both sources of information the low rise video camera as well as the laws of physics to get a better estimation of the actual flight of the cannonball shown there in orange okay any questions on that analogy Jonathan I have a question yeah come on yeah so for example you if you take a like the winner filter right and you get some decoding right so if you denoise that decoding results will you get something comparable to Kalman filter um it turns out and this is um something that I won't answer completely here uh it turns out that the Kalman filter as we will implement it is actually a specific case of a winner filter but uh tansher your question some way yes under some uh constraints on xk uh the winner filter uh will approximate a Kalman filter correct thank you come way all right and then let me also just show the video that I skipped before showing the performance of um our state of the art Kalman filter so one thing you can note about this video is that the cursor movements were very smooth there is no jitteriness and that smoothness is conferred by the Kalman filter modeling all right so if no questions then let me get on to the equations so what we are now saying is that instead of having just an observation equation an observation that's noisy and low resolution like the video camera in this case our low resolution noisy observation is neural data instead of just taking our noisy low-res neural data and using that to infer kinematics I'm also now going to use information about how the kinematics evolve through time in principled ways to further improve my estimation of what the decoded kinematics are so instead of having just one equation here the inverse ole I'm now going to have two equations and I'm going to use both of those equations to solve for xk all right and so these two equations together are called a linear dynamical system and there are entire classes taught on the analyses of linear dynamical systems and I highly recommend taking a class like that it both solidifies your linear algebra and then also gives you tools that help you to analyze time series data so this first equation xk plus 1 equals a xk this is typically recalled a state process in a dynamical system xk which are for us the kinematics are typically called the state and yk in a general dynamical system are usually called the observations it's what you get to observe in an experiment in this case is our neural data okay and so and this equation is therefore called the observation equation or observation process okay so our dynamical system is comprised of a state equation or state process and then observation equation or observation process this matrix a is typically called the dynamics matrix and it models how your state evolves through time which in our previous example could be things like the laws of physics or in the BMI example is going to be the inertia of the movement alright okay so just like I said before now we have an additional equation that gives us more information onto what our decoded kinematics should be all right so concretely instead of just decoding xk equals c inverse yk the optimal linear estimator I should now have more information from this equation my state process equation to better update and refine my estimate of the decoded kinematics xk okay any questions on that intuition all right I'm just going to throw out the are you following question just to make sure people are following since I haven't seen any questions yet in TAs if you can just tell me what the percent to gs is okay great looks like most are following all right any questions here all right so here's a thought that you might have if I have an equation like yk equals c times xk well what happens if or let that actually let's take this equation xk plus 1 equals axk and let's say I'm modeling a ball following right if I'm modeling a ball falling and it's falling in a vacuum on earth right this equation is almost perfect right and so if this equation is almost perfect isn't all I need to solve for the position and the velocity of the ball at any single time point and the answer is yes if one of these equations is perfect you don't need the other but the thing is that in most cases these equations are not perfect both of them are providing a noisy estimate of what xk is and if they're both noisy then we're going to have to find a way to incorporate them based off of their noise to best estimate xk and we're going to see in the common filter solution if you assume that one of these equations is perfect the common filter does tell you to just use one of the equations right so how do we get the optimal estimate of the decoded kinematics from these two equations neither of these equations are perfect because if one of them were then we should just use that one equation to solve for the kinematics in real life noise is going to corrupt these equations and so for the BMI case the reason that this equation isn't perfect is because the only thing that we're going to be modeling in this class is the inertia of the velocities so we're going to say in general our movements are smooth but it doesn't tell me what my velocity at time k plus one is given the velocity at time k all right so in real life neither of these equations are perfect they have noise and so the noise on the state update process is going to be called WK and the noise on the observation process is going to be called QK right and we bottled these noise as random additive noise and they're also going to be Gaussian noise that corrupts these equations okay any questions there all right so for the common filter we're going to make several assumptions when we get to the details of the math we're going to assume that the noises are Gaussian noise terms and so if xk were velocities right so if xk was my x and y velocity at time k right then wk would be a two-dimensional noise source with covariance with mean zero and covariance big w and then QK is a noise source on the same dimensionality as yk my neural data and so if I have 96 neurons this QK would be 96-dimensional so if the covariance matrix w was equal to zero for example right then all my wk would be zero tiny that this equation is perfect but as long as w and q are not zero then these equations have noise in them and again then we need to know how to estimate how to use both of these equations to estimate xk given my neural data and my historical kinematic data any questions there all right so then you might ask how do we get the optimal estimate of x hat of k from these two equations so in general this is an extremely complicated problem without nice closed form solutions but under certain assumptions which are linearity and Gaussian noise so under the assumption that the state update process is a linear equation which is which is which is this here is a times xk and the observation process is linear too as well as Gaussian noise at wk and qk are Gaussian then there's a recursive solution to estimate xk and that's called the Coleman filter and again at least for these starting slides we're going to just start with the high level intuition the take-home point and intuition behind the Coleman filter is that my decode equation of x hat k now instead of just being a mapping from the neural data so if I if I didn't have this m1 x hat k minus 1 x hat k would be equal to m2y k which looks like an optimal linear estimator but now that I have this dynamics equation the state update process I'm going to also have another term m1 which is another matrix times my kinematics at the prior time step all right so instead of only using neural data to decode kinematics which is what we did for both the optimal linear estimator and the wiener filter we now also use the previously decoded kinematics xk minus 1 so this term here corresponds to this term here since the previously decoded kinematics also has information about what the next kinematics should be through this dynamics equation all right and so that leads to a decoding equation of this form all right any questions there and then Brandon asked and checked and I repeat what m1 and m2 are so m1 and m2 are just going to be matrices that map the prior kinematics and the neural data into my currently decoding kinematics we haven't arrived with m1 and m2 are yet in fact m1 and m2 are going to take on pretty complicated expressions that need to be recursively calculated but and and that recursion is is called is the common filter algorithm but at least at a high level m1 will map by prior kinematics to my current kinematics influenced by this equation and then m2 will map my neural data to my kinematics so basically m1 and m2 are summarizing how to combine these two equations to get my current kinematics from past kinematics and neural data all right any questions there okay so the next part of this lecture is we're going to tell you essentially the approach of how we solve for the common filter at a high level and then we're going to write out the common filter solution so that you could code it up in your homework if you wanted and we're also going to check that the common filter solution makes intuitive sense and so this is all in terms of gaining intuition of the common filter and then after that we'll dive into the details right so the common filter will give us a recursive method to calculate m1 and m2 and m1 and m2 these two matrices here are going to be some combination of a c big w the covariance of this noise term and big q the covariance of the q k noise term so they're going to be a function of these terms and the common filter is going to tell us exactly those equations right and so the goal of the common filter is going to be to estimate the distribution p of your current kinematics xk given all of my prior neural data y1 to yk so the picture that we should have in mind is the following and I just saw a question in chat it says why are we using the covariance of the noises so the code on any given time point time k w k and q k are going to take on particular sample values from this Gaussian distribution and that's going to vary from time point to time point but knowing that these noise sources we're going to see our independence and identically distributed and so on any given time point they may take on a they'll take on a particular value to get the best estimate of xk given that w k and q k take on random values from these from these particular distributions right the way that we characterize them is by the distribution parameters which are the mean and the covariance is and so this means that we don't observe the noise at every single time points update the common filter we just know the noise is statistics and using the noise is statistics will know what's an optimal way to combine them is and a question from shall ran yeah professor just wondering for these m1 and m2 are they the same across all time points?

All right, everyone. I'm happy to commend soft as ours and take questions in the order that hands are raised. Eric, I wasn't sure about 3C. It's the one where you can involve a signal and the HFT that they drew and you want to find a period where it's a constant. I did the question by like, I considered it conceptually if I have some periodic signal and then if I like to look and drag it, then intuitively if I had the same period, then no matter how you drag it, you'll get the same convolution. And then if you decrease it, decrease the period by like, integer values, it'll also work. But I'm not sure how to actually mathematically work that out. Great. That's the correct intuition, which is you'll notice that in the sink there are zeros at particular locations. And we know that a periodic signal corresponds to a sample signal, the periodic signal and time corresponds to a sample signal in the frequency domain. And so if that sampled signal has the samples at the zero points of the sink function, then they'll all be zeroed out and then any integer multiples of that period will also be that way. So we can do that mathematically. I want to ask if anyone had any questions on 3ARB before we did that. So as to be helpful to anyone, if not, we'll just start with 3C.

Alright, so we'll do 3C. And let me just drive the intuition that Eric was talking about. So as a 3C asks for non-constant periodic signal XFT for what period does Y of T equals X of T convolved with H of T equal a constant C. And then we tell you to think about the Fourier series and the Fourier transform. So we know that in the frequency domain, Y of J omega is going to be H of J omega times X of J omega. Right. And then from the previous part of the question, well, since we know that H of T is, H of T is a racked, right, then H of J omega has to be a sink. Right. So you should have gotten in 3B that we asked you. Yeah. In 3B, we asked you to sketch the amplitude response. So in 3B, we asked you to sketch the amplitude response. So it would be the amplitude of H of J omega. And you should have gotten that this was equal to sink omega big T over 2 pi, I believe. That's what our solution says at least. And therefore, H of J omega looks like the absolute value of a sink. And it keeps going on, but I'm just going to draw a few. Right. So this is a capital H of J omega. And we want that we know therefore, or we know from our frequency response that the magnitude of Y of J omega is equal to the magnitude of H of J omega times the magnitude of X of J omega. Right. And so if we, if we tell you that Y of T is going to equal C, then can someone tell me what we would expect Y of J omega to be equal to. Delta function. Yeah. Great. So Daniel, tell us a delta function. That's right. So Y of J omega should just be equal to C time to delta function. Since it's in inverse Fourier transform.

And so, would be the constant. And so next we, we use the fact that we learned that if a signal is periodic in time, right, it's sampled in the frequency domain. So a periodic signal periodic X of T in time is going to be sampled in the frequency domain. So it's going to be an impulse train. And so we know that the Fourier transform X of J omega is going to look like a bunch of impulses based at some separation given by given by the period of the signal. So from this, we can see that if I want Y of J omega to be equal to a delta, right. And I know that Y of J omega is H of J omega times X of J omega. And that H of J omega is zero at these particular points. If I design my periodic signal X of T to be periodic such that it's samples all occur at the zero points of H of J omega. And I multiply H of J omega and X of J omega all I'm left with is a delta at zero. And it's inverse Fourier transform is going to be a constant. So that's the key trick for this question. Are there any questions on this intuition and then after that, we'll do the math, which answers Eric's question. Why did you focus on the magnitudes instead of just the original function? Was there a reason for that? Yeah, you're right, Daniel. I didn't need to focus on the magnitudes, especially since we want Y of J omega to be C times delta omega. So actually, I sort of just drawn H of J omega being a single mega T upper two pi. And so this would go negative and positive and negative and positive. Actually, let me change that now so that it's not confusing. And I draw I drew this one because I thought it was a picture I saw in the solutions. So since in part, they asked us to plot the magnitude. And the same intuition holds, which is the product of H and X will only equal a delta function, as long as all these replica deltas for as long as all these other deltas occur at the zero points of H of J omega. And Professor, I had a question. So this graph that you have right now, what does this graph represent? Because this is not the magnitude of H of J omega. Yes, so. Yeah, so H of J omega in the prior part is also multiplied by by a phase component. And so at the same time, though, that phase component is not going to change. So this sink will still be zero at these points. So I haven't drawn H of J omega exactly. Yeah, that's steric, rent. So what I've drawn is is the.

The extra on here is sink omega T over two pie. But there is also there is also a phase component to the signal, but where it went, think of omega T over two pie is there a then the overall H of J omega will be Sarah. So for the purposes of this thought, I'm just going to leave it up since the intuition still holds, which is at these points, the sink is around. Thank you. So, um, if everyone's following that intuition, then we'll go ahead and write out the math for this. So, um, we know that if X of T is periodic. So what do we know we can write X of T as people can put in the chat also if you don't want to unmute and raise your hand. I know there's a lower activation energy to write something in the chat. How should I write X of T it's periodic. Four A series. Great. Yeah. So X of T can be written as a four A series. And so, uh, this is C K E to the J K omega not T where omega not is equal to two pie over big T and big T is the period of the signal. And so, um, we derived in class that this has four A transform X of J omega equals two pie times some from K equals minus infinity to infinity of C K and then multiplied by delta. So this is a delta omega and then the impulses occur where real may they're not great. So this would be K omega not. And so, um, then we want to know, um, let me copy and paste this onto the next slide. Um, we want that Y of J omega is equal to a constant times delta T, sorry, delta of omega. And Y of J omega is going to be equal to, um, each of J omega climbs X of J omega. But now X of J omega is going to be, um, this sum of impulses that we wrote. And then it's going to be a two pie, um, and then a sum from K equals minus infinity to infinity. And then we're going to have, um, a C K. And our delta function is going to sample the value of H at K omega not. So it's going to be H of J K omega not times a delta omega minus K omega not. So then, uh, what we need to do then is determine what, uh, what the period or, uh, when does this, when does this equal, uh, C times delta omega.

All right, everyone, we're going to begin now. So first, the TSO uploaded homework number seven last Friday and homework number seven is going to be due this Friday on the last day of classes at 11.59 p.m. uploaded to grade scope. The first three questions on this homework are on sampling and because we haven't fully covered the Laplace transform yet for this quarter, we gave you three questions, four, five, six on homework number seven, which we are going to grade based off of correctness, which is based off of effort. And so as long as you make any attempt to do the question, you're going to get full credit. And so in that sense, we want you to think of these more practice questions for the final exam. The final exam will be commuative and it'll cover material up to and including the inversion of the Laplace transform, which we'll start today and then we'll finish in the first half of Wednesday.

And so be sure to, if you don't do a significant attempt on these questions for homework number seven, be sure that when you look at the solutions, you understand how to do these questions because they are material that could be covered on the final exam. Tonmoi will be holding a final exam review session this Sunday, so in six days. And there's going to be a link, which is here. And I believe Tonmoi will either, Tonmoi will send an announcement on Piazza or CCLE and asking for what time works best for the Sunday review session. All right, so I encourage you all to attend that review session. And then the final exam is going to be held on Wednesday, December 16th, from 3 to 6 PM over Zoom. We're going to be sending on email with further exam details, but in broad strokes, the exam, the final exam logistics are going to be very similar to the midterm exam logistics. The final exam is commuative and so it'll cover topics across the entire class up to and including the inversion of Laplace transform. All right.

So we'll dive back into material. So in the second half of last lecture, we had started to cover our last topic in this class, which is the Laplace transform. And the motivation for the Laplace transform is that a lot of signals that we want to analyze may be growing over time and may be power signals. And we can't take the Fourier transform of a power signal. It's up for a few, which we did a generalized Fourier transform up. But in general, we can't take these Fourier transforms. And so while the Fourier transform reconstructed signals with bases e to the j omega t, for the Laplace transform, we get an additional sigma term e to the sigma plus j omega t that allows these cosines and signs to have a growing amplitude and therefore allow us to model power signals. All right. So the key thing is that there is also this e to the sigma part, which is a real exponential in time and models this growth.

All right. And so we notice then, well, we presented the equation for the Laplace transform, that's this equation here. And we notice that it bears a striking resemblance to the Fourier transform. The only difference is that the Laplace transform instead of an e to the minus j omega t, we have an e to the minus s t, where s here is sigma plus j omega. All right. So now this exponential here in the Laplace transform integral includes not just the j omega term, but also the sigma term. All right. And we said it would be good to conceptualize the sigma and the j omega as in a complex claim where the real part of s is sigma, that's the x axis. And the imaginary part of s is omega and that's the y axis. And what we note is that the Fourier and Laplace transform look very similar except for the Fourier transform.

It's a special case of the Laplace transform when s is just equal to j omega. So when sigma equals the row. All right. And we talked about then when are these two things the same or when can I just take the Laplace transform and replace all the s's with j omega's so that I get out of Fourier transform. So we started this example at the end of last class where we decided to take the Fourier transform of e to the minus a t times u of t. Its Fourier transform is 1 over a plus j omega. And when we took the Laplace transform we got the Laplace transform was 1 over a plus s. All right. And so we see that the Fourier transform and the Laplace transform are indeed of the same form except in the Fourier transform the s is replaced by j omega. All right. But we notice something interesting which is that in the course of doing this integral to get to 1 over a plus s.

When we did this integral we had this expression here evaluated at infinity minus this expression evaluated at zero. And we saw to get to the answer 1 over a plus s we have to have that as time went to infinity this term over here this term would go to zero. All right. And this is critical because this defines the region of convergence of the Laplace transform. So again for this integral here to equal 1 over a plus s this term here has to go to zero as time here t goes to infinity. All right. And so we have to then find a condition under which this term goes to zero as t goes to infinity. And that's how we did that at the end of last lecture where we saw that as long as as long as so this is the term e to the minus a plus s t goes to zero as long as e to the minus a plus sigma t goes to zero or sigma remember a s is equal to sigma plus j omega. So sigma is the real part of s.

All right. And so if sigma is greater than a minus a then the Laplace transform exists because this term again there goes to zero. And therefore we say that the region of convergence for this Laplace transform occurs when sigma is bigger than minus a. And sigma again is the real part of s. And so in our picture of a 2d plane right if the x axis is the real part of s sigma and the y axis is the imaginary part omega. And a is some positive number. Right. Then minus a is going to be a negative number. And as long as the real part of s is bigger than minus a by e to the entire green region then we're going to be able to compute the Laplace transform. And what you're going to notice here is that this region of convergence contains the j omega axis which is where the Fourier transform is evaluated upon.

And so for the Fourier transform the Fourier transform is going to be a special case of the Laplace transform where you can replace all the s's with j omega as long as the Laplace transform region of convergence contains the j omega axis. All right. We drew another example maybe another Laplace transform has a region of convergence shown here in purple. And this would not include the j omega axis. And so the Laplace transform that you find for this purple quantity you cannot just replace s with j omega and arrive at a Fourier transform. Okay. I want to pause here and ask if there are any questions recapping what we talked about last lecture. Chris. Hi. Could you please repeat what you were saying about the green and the purple interval?

Yeah. So we're going to do a purple example next. I'm going to focus on green just for now. So the green interval corresponds to the region of convergence which far the values of s for which the Laplace transform exists. And so for the Laplace transform to exist we have to choose a value of s such that this exponential goes to zero as t went to infinity. And we derived that the condition for this exponent, this exponential to go to zero as t went to infinity was that it's real part sigma had to be bigger than minus a. And so what I'm drawing here is I'm drawing I'm highlighting here in green the entire region for which these particular values of s anywhere in this green lead to you being able to compute a Laplace transform for for e to the minus e to the minus a t times u of t. So as long as you choose an s that is in this green region, then the Laplace transform will exist. I'm going to meet you one more time.

All right. Can everyone hear me now? All right. I'm sorry about that. I'm not sure what happened with Zoom. If you if anyone again has trouble entering, please just write something over chat. All right. So I'm going to just restart. We lost 10 minutes, but given that we maybe had half of the class not here, it'll be, I did, I think, to restart. So very quickly, homework is going to be released this Friday and then do in a week on Friday, upload to Grayscope by 11.59 p.m. Homworks in general will be due 11.59 p.m. the day that we say they're due. And so when you're uploading to Grayscope, make sure that you have some leeway to submit by 11.59 p.m. The portal will close after two late days following 11.59 p.m.

We sent out instructions on how to sign up for Piazza and Grayscope. And so please do so if you haven't already. And then we will send out an announcement shortly on the consolidation of discussion sections. And so, Tonmoi, one of our TAs was able to find three sections that everyone who wanted to attend the live discussions can make. And so we'll be able to do that consolidation. All right. I see the chat lighting up. If there are any problems, TAs, can you just unmute yourselves and let me know. I'm going to just forward to have here. So I just wanted to mention on the syllabus, it appears when we write homework, we release that they're released on Wednesday, but we release them typically one week before the due date. Unless they are a longer assignment. So we, these homework assignments are all due on Fridays and we're going to release them on Fridays. But we will announce all of that in class. And so hopefully none of that will be ambiguous.

All right. So last, we were in last lecture. We ended talking about the syllabus, which we didn't complete, but we'll complete right now. So I had mentioned that homework is worth 50% of the grade. And it is a very high weight for this class. That reflects how important we believe the homeworks are. As we'll discuss later on, we designed the homeworks to be fairly difficult, but the exams to be no more difficult than the homework. We also made the homeworks work a lot to decrease the percentage that the midterm and final exam assessments are worth to try to lower the stakes on those exams. And to discourage any academic dishonesty. And so I wanted to then talk about academic integrity, which is very important to me and to the teaching staff of this class. And so this slide reminds you all of the true brewing UCLA academic integrity principles. I take academic integrity very seriously because it's important that we are all on a fair and level playing field. When you cheat, it's not only the service to yourself, because it's not a reflection of the material you've learned, but it's also unfair to your fellow peers who may have worked very hard to study for the exam. Another thing that we'll do in the class and I'll discuss shortly is that this class is rated on an absolute scale not on a curve. So you aren't compared to your fellow peers. In any case, for this slide, my point is showing this is to tell you that I take academic integrity very seriously.

And if we believe that you have been academically dishonest on an assignment or exam, I will follow up and submit the case to the office to the dean of students office. So again, please be academically honest in this class and ultimately fair to your fellow peers. So this is how exams are going to work. We, the teaching staff, recognize that during online instruction with remote exams, it's very difficult for us to prevent cheating on exams, for example, due to collaboration. So in trying to make things as equitable as possible, the first thing that we're going to do is make all exams open note, open book, and you're free to access CCLE on your computer. Exam will only be closed internet and that you shouldn't, you can't Google the answers to questions. And to try to discourage this, the TAs and I are also going to aim to write exam questions that I cannot be easily searched. To deal with this issue of collaboration, the TAs and I are also going to perform analyses on exam answers after that. And if we suspect any students of collaborating on the exam, we reserve the right to administer an oral exam to the suspected students and the results of the oral exam will supersede the results of the written exam. And if there are any other further policies, we'll announce those closer to the gate. In Jonathan. So I know that you said the exam is closed internet, but since we're also allowed to use the notes on CCLE, can we have several tabs of CCLE open like of like, oh, this is chapter one notes. So I don't have to flip through CCLE. Yes, just click on this tab. Yes, you're welcome to do that. So it is close and try to accept to access CCLE. You can have as many CCLE tabs open as you like.

Okay. Thank you. Any other questions on exam policies? All right, great. So we also recognize that students may be in different time zones that make taking the exam during class time or during the scheduled final exam time difficult. So if you're in that situation where you need to take the exam during a different time, please email me by the end of this week so that we can prepare for this. I just saw in chat someone asked about a map lab. So there's not going to be any map lab syntax questions on the exam. You won't be asked to write card. All right, so grading in this class is done on an absolute scale. And so this is the scale for how we're going to assign grades. And so throughout the quarter, you'll be able to calculate your grade exactly given the breakdown. We had in the prior slide and this scale here. Now, when I was an undergrad, I knew when I went into a class and there's an absolute scale, I was a bit scared because what if the professor writes a very difficult exam and all the grades are really low. And so we work, we try to design the exam so that that isn't the case. But if we do end up giving an exam where the average grade is very low, causing the students causing many students to be in a lower grade range, then we reserve the right to relax the scale so that the scale would be the scale would be an easier scale would be relaxed, but it will not be made more stringent. All right, I'm seeing a bunch of questions on the exam. So I'll really quickly look at the chat Angela asked the exam written or typed out. So we are going to give you questions, which if you want to type them on my logic, you can, but typically most students just write their work and their answers on a sheet of paper and then also that PDF to grade scope. We will not be using respond this lockdown and we will not monitor you during the exams. And so we will distribute the exam for the in class exam, it'll be a one hour or 50 minute exam. And the TAs and I will also be in a zoom room in case you need to come and ask questions, but we won't ask you to turn on your cameras and we won't monitor you.

Okay, any other questions. Right, and then are we submitting it to grade scope? Yes, the exams will be submitted to grade scope. All right. And so what I was just saying about if we, the scale may be relaxed, but it will not be made more stringent. And then I'm writing this because I sometimes receive these requests at the end of class. I will not make a change to your final grade unless I made a calculation error. And so please do not send any requests of this nature and I will not reply to emails making such a request. And then you have the option of taking this class. If you're an undergraduate student pass or no pass or if you're a graduate student satisfactory or unsatisfactory. And I just wanted to copy down the UCLA registrar rules for what is assigned a grade P and what's assigned a grade S. Any questions. All right. So, as we mentioned before, the homework has a significant weight in this class. I personally believe that a lot of learning happens on the homework. And we will strive to give homeworks where there is a lot of learning that takes place. So, the homeworks, we try to make thorough and instructive on the flip side. They end up being sometimes a lot of work and time consuming. And so I want to give you a fair warning about that to please start the homeworks early. The upside to this is that we design the exams so that they are at a difficulty no more than the homework.

All right. So now we're recording. So I was just responding to Jerry's question, which said what before we start the project. I don't recall the exact week off the top of my head, but it's around Thanksgiving and it's listed on the syllabus. So the syllabus date will be the date that we release the project. And it'll be equivalent to about two ECU 102 homeworks. All right. Okay. So are there any questions that people had about this example brain machine interface system from last lecture since I know we ended while we were talking about it? All right. So we'll go then to some other examples building off of this idea. And so if you can control a computer cursor on the screen, you can bet you can also control a robotic arm. So I'm going to show you a video here. What's happening is just like for a computer cursor, what you're doing is you're controlling the 2D position of a cursor on the screen.

You can also generalize that to control the 3D position of a robotic arm. And so what this participant here, Jan, which I'm not Jan, a capy, what capy is thinking about is she's going to be controlling the 3D endpoint of this robotic arm. It turns out that this arm is fairly complex. You can see it has several joints and angles. She's not controlling those joints and angles. She's not thinking I want to have the angle, have the first 20 at 45 degrees in the second joint, the BS 7 degrees. She's not thinking that she's only thinking about the 3D endpoint. And this is actually something reasonable because when you think about how we move, we are in thinking I'm going to move with my elbow at, you know, a 90 degree angle. We think about the endpoint that we're trying to reach to and the biomechanics of our arm takes care of the rest. And so that's kind of the goal for this, that is the goal for these robotic, very machine interfaces as well, which is that we would decode the endpoint. And then a biomechanical model to solve all the inverse kinematics of what these joints should be. There's going to be one more thing she controls in here.

So just like in the keyboard example, there's a click signal to select the target. In this example, there's also going to be a click signal or a binary signal, except it's not going to select the target, it's going to rotate the hand so that she can drink from the bottle. So let me go ahead and play this video. Sorry, I'm not sure. You can see she controls her water gun to bring the cup of coffee to her. And then she's able to drink some. All right, in the video, so at that point, I said, when it bit longer, you would see the completion of the trial. And she has this big smile across her face, because this was one of the first times that she was able to drink from a coffee cup without needing someone to directly hand it to her. Any questions here? All right, so in another example of the robotic arm. So the video that we just showed you is from a clinical trial called the Brain Gate Clinical Trial. It's part of Massachusetts General Hospital in Boston, as well as Brown University. And now they have sites also at Case Western and Stanford University. There's also another clinical trial at the University of Pittsburgh.

And that's going to be the topic of this 60 minutes segment with Scott Pelley. So I'm going to play the news clips that that aired on 60 minutes with respect to a robot controlled brain machine interface here. More than 1300 Americans have lost limbs on the battlefield. And that fact led the Department of Defense to start a crash program to help veterans and civilians by creating an artificial arm and hand that are amazingly human. But that's not the breakthrough. We don't use that word very often because it's overused. But when you see how they have connected this robotic limb to a human brain, you will understand why we made the exception. To take this ultimate step, they had to find a person willing to have brain surgery to explore new frontiers of what it is to be human. That person would have to be an explorer with desperate need, remarkable courage, and maybe most of all, a mind that is gained. All right, so that's the intro. And then we're going to go on to now the clips from their clinical trial. And so this participant, her name is Jan, and actually the person screwing in the circuitry that connects to her, her, her electrode array that records signal from the brain. That's one of the leads of the clinical trial, Jen Collinger. Plugged her brain into the computer, and this is what we saw. So I can move it up and straight down and left and right and diagonally. I can close it and open it and I can go forward and back.

That is just the most astounding thing I've ever seen. Can we shake hands? No, really. Yeah. Like, come right over here. Yes, you can. Okay. Oh, my goodness. Wow. And I can do a fist bump that you like. It's amazing. What are you doing, Jan? What's going on in your mind as you're moving this arm around? What are you thinking? Okay, the best way to lay is raise your arm. Uh-huh.

What did you think about and he did that? Well, not very much. I do it all the time. It's not a matter. Is that hard work? Are you having to concentrate? All right. So with that video, I want to highlight a few things. First, you can see that she could move the robotic arm. Through the space, she was able to open and close the hand and even give. Scott tell you a fist bump. Interestingly, um, Scott tell you asked her, what are you thinking about when you. When you when you move the arm and her response was essentially, uh, I don't really think about anything. I'm or I'm naturalistic.

Like I think about or like when you raise your right arm is something that haven't automatically and that's the goal of boy, you won't bring machine interfaces that your limb would be controlled in a similar way to how you would have controls your native arm. So your robotic limb would go up when you want to move, when you want to move your native arm up. And that's called a biomatic brain machine interface. A biomatic because it's exploring the same types of, uh, motor commands that would have originally moved your arm. And so they can fully these things should be relatively naturalistic. That's the goal at least. Okay. Any questions here? All right. So this project is one of is also was also later funded through funds made available by President Barack Obama's Brain Initiative, which has been really critical for research in this area.

And there's a picture you can Google it up from this clinical trial of another participant doing the same fist bump thing with President Obama. And that's I think really cool. All right, I just to also convey that there are really interesting problems that come up with brain computer interface control or brain machine interface control. Here's another video that you might find interesting. Help with some of the things that Jan has trouble with. Okay, for example, sometimes when she looks right at an object, she can't grab it. Okay, I went to take the cone away to go ahead and close it. Sure. So that's the cone away. There's no problem. But as soon as I put the cone there, she can't do it. Why is still a mystery. All right.

And so yeah, this is a very interesting observation that she has no problem opening and closing the fists of the robotic arm. However, when there is in particular a red cone, she struggles to grasp it. I've seen some talks and talk to the postdocs who are involved with this study. And they did an analysis and it's interesting when she sees the red cone. There is a large change in the firing rates of neurons in her motor cortex. And because something very special about this red cone or not special, but something different about this red cone causes her neurons to fire so differently. That throws off the decoder and that's why she's no longer able to grasp. To try to achieve the support, they also book a virtual and reality environment where they showed her a red cone in the virtual reality environment. And even in that environment, she still had this big change in firing rate that caused her to not be able to grasp. Any questions here? Yes, pressure. Was that was she only unable to grab the red cone because she was able to shake the guy's hand. That's kind of an object to look at.

All right, we're starting our office hours now. Hi, Professor. Hi, hi, Chris. Hey, um, I got a little bit confused in the last graph that we talked about in lecture just now. Okay. The one with the introduction of the plus. All right. Ion channel, I just wanted to clarify a few things. Yep. First of all, is there a reason why we use that negative 65 millivolts as our resting potential, or sorry, not our resting potential, our resting voltage because like in an actual neuron you have both K plus and NA plus?

That's correct, yeah. So let me pull up this slide right now really quickly too, so that I can just make sure that we're all looking at the same thing. So I'm going to bring up this slide here, and then I'm going to share my screen. All right, so I think Grace, you're talking about this slide, right? Yes. Yeah, so, and your question was, is the reason that we have the negative 65 millivolts due to the introduction of sodium into the mix right.

Yeah, so I'm looking at this chart we see that k plus is minus 75 for equilibrium potential so if only k plus was there, the equilibrium potential would be minus 75. If only NA plus was there, then the resting potential would be plus 55. Now, when K plus and NA plus are both there, the resting potential is gonna be in between these two numbers because K plus is gonna pull it down to minus 75, NA plus is gonna try to pull it up to plus 55. Why does it end up at minus 65? Is that essentially what happens is that it's going to be a weighted sum of K plus equilibrium potential and any plus and the weight is going to be based off of how permeable.

The channels are to K plus and plus. And so because at rest, K plus ion channels. There are many more of them at rest and plus. than any plus the weight of K plus is going to be much higher and so that's why it ends up being closer to minus 65, then to like say, minus 35 or plus 25. Okay, thank you. And, and really quickly, we'll talk about this, the equation that tells you this would be Goldman's equation. And, and, as I mentioned briefly in lecture, the PK is going to be much higher than PNA at rest and so that's why the K plus terms will dominate what the overall voltages and it ends up coming out to negative 65. Okay. Thank you. And then one more question was the increase like that, like that little red line that increases between B and C. I'm still not understanding why that increase happens. So is it because we have less positive ions outside because of the introduction of the NA plus ion channel. That's correct. Yes. So I'm going to put a negative charge in at minus 75. When I introduced this, and a plus channel and a plus float into the cell so now there's less positive charge outside the cell and more positive charge inside the cell, meaning that the voltage is going to go up. Because I mentioned the voltage from inside to outside the cell and I put more positive charge inside the cell.

charge internally will also, I guess, we can electric field. That's correct. Yeah, and that's why the. And that's why in the picture they go from like three plus signs here to just two plus signs to indicate there's less positive charge outside and I get therefore since the voltage is at the voltage was at zero, there would be no electric field, the voltage has gotten closer to zero so the electric field is weaker. Okay, and then the, sorry, the blue, is the blue line the electric field? The orange line is the electric field. Okay. The orange line is the current due to the electric field. Okay, got it. Thank you. Great. Thanks for the questions. Professor. Yeah. Hey, Brandon. Oh, I should also mention something I just looked it up, grace as to because you have the question about the Ion channel that I didn't know the answer to. And the reason why it's called hydration is because this gap is too narrow for it to pass with this waters of hydration, not because of the hydrophobic forces. Okay, thank you.

And then sorry Brandon you're saying something. Yeah. Do you think you could just scroll up and I can probably find the thing. Yeah, sure. Right there, 21. 21. Yeah. So, I was just curious. We know that the outside is more positive compared to the inside, right? Correct. And then, so, is there a preference as to why we define VN as the inside of our cell versus VNS outside.

Like when I was first seeing this, I thought it would be more natural to have like the VNB associated with the positive side. No particular reason, just convention in the field. So whoever first measured these, put their positive electrode inside the cell. And so since then we've all been defining it going from inside to, excuse me, inside to outside. If we were to just reverse everything, everything would be consistent, but all the signs would just be negated.

Right. So is this basically for all aspects of bioinformatics and biology where we always take VN as the inside of our cell, like by convention? I don't know about fields beyond neuroscience and how they, what conventions they define. I would guess that they use the same conventions as in neuroscience, which is the field that uses the most electrical engineering in terms of cells.

Okay, yeah, I just wanted to clear up on that convention. Thanks a lot. Yeah, yeah, of course. Professor, can I ask about the previous figure, the figure on the page 34? Yes. For the, so the C is a steady state as compared to the change at the figure B, right? Sorry, I was scrolling. I missed the first part of your question. Can you repeat that? And the C is T equal one second, you mean that is a steady state. After the be right. Um, so, in this, in this drawing it is a steady state.

It's just a view of it. One second later, but it ends up being. It ends up being this is ends up being the picture of the steady state, because of this opposing na plus k plus pump which we'll talk about next lecture, but essentially, this pump is going to let me get my annotator. And there's going to be one more thing here, called the pump. And the pump is going to push out any plus. And it's going to push in K minus, sorry K plus. And so for that reason, the concentration gradients are going to stay the same such that this arrow, always stays the size. So this is a picture of the steady state, but it's only the steady state when you also consider this, this NA plus K plus pump. So the chemical driving force on B and C are for the last channel. Like should be in the same lens right for the figure. The blue, the blue arrow. It seems like the part C is longer. I don't know. Maybe it's just a mistake. Oh, I see what you're saying. Yeah. Like this arrow looks a bit shorter than this arrow, right? Yeah. I think you're right. It is shorter. It should not be shorter. It should be the same length, right?

All right, everyone, we can go ahead and start office hours. Please raise your hand if you have any questions and then I'll call on you to unmute yourself. Alexandra. I was wondering if we could go over number two. Should we go over number two from the top? I think I get part A. I just want to make sure I'm interpreting the diagram correctly and then from there on. I'm not sure how to deal with the complex exponential in terms of graphing. Yeah, okay, sure. For the complex exponential in terms of graphing, I think that we may have given, we gave that hint to approximate as one plus j omega t and then for the j, you can just approximate it. You can draw it on the graph as it dotted line, but we'll get into that. People write in chat if you want me to go, actually, you know what, we'll just do number two from the top. All right, thanks for asking me the question.

Alexandra. Okay, so for number two, it says to write an expression for F of t, which is our sampling function and what we ought to see is that this sampling function is non-uniform, but it's non-uniform in a consistent way, which is that the impulses that should happen at odd samples, like negative three, negative one, one and three, are all shifted by some amount to tau. All right, so I'm going to draw this as two different impulse trains. I'm going to have the even ones I'll draw on purple. One, two, three, negative one, negative two, negative three. So the even ones are on the integers and then the odd ones I'll draw on this blue are shifted by some amount tau. All right, so if someone who did this question either unmute or are all right in chat, you're approached to write out this function as two uniformly space sampling functions. Sure. So the even ones are, well, the way it is, we can represent even number as like two k.

So for the even least space samples of, by even, I mean like on even numbers, we can represent it as a sum of delta of t minus 2k and then k is the whole range of integers. And we can represent an odd number as 2k minus 1 or 2k plus 1 or any 2k plus minus an odd number. I just used 2k minus 1, I think. So in that case, we would, I used, I wrote delta of t minus and then in parentheses, 2k minus 1 plus tau. Great. Great. Yeah, so that's, that's correct. Thank you, Eric, for that. So this is correct and then we have the plus tau because all of these blue impulses are delayed by tau. So we need a minus tau in here. And so that's what we get from this expression that Eric wrote for us.

So this is correct. I'm going to use the short hand notation that we have from class that we could write that this here is a delta with a subscript 2 of t. And then this one, I can write as a delta. And so the subscript is still 2 because all of these blue ones are evenly spaced by 2, right? And that's what Eric had with this 2k minus 1. So we'll have a 2. And then now all of the delta's are shifted by, so if I just had a delta of 2t, the blue impulses would be on 0, 2, 4, etc. But they need to be at 1 plus tau. So it would be a 1 plus tau here, right? So if you write either Eric's representation or, or this simplified notation representation, those are both correct. And so this would give us that f of t is equal to delta 2t plus delta 2t minus 1 plus tau.

All right. Any questions on part 1? All right. So if you have questions for number 2, just feel free to unmute yourself and ask them. All right. Let's go on to, let me write that this is 2a. All right. So then 2b says find the Fourier transform, big f of j omega, right? And so this one is fairly straightforward if we've gotten 2a correct. So can someone unmute and tell us how you did, or what was your thought process in doing 2b? I guess let me do the first part, which is that we know that the Fourier transform of delta 2 of t is going to be, well, we know that in general delta big tt is going to be omega not delta omega not omega. And so for delta 2t, omega not is going to be 2pi over 2, which is just pi.

So it's going to be pi, and then delta pi omega. All right. So that's for the purple one, and then what happens for the blue one? I think Professor, we can just multiply the same pi delta pi omega with e to the negative j omega times 1 plus tau. Yep, that's correct. Thank you, Brompton. So that gives you then your answer. Your big f of j omega is just going to be the sum of these two. Any questions on 2b? Yeah, Professor, I had a question. What is that? I got confused about which omega that actually was on the e to the minus j omega? Yes. This omega is the frequency variable.

So it would be the x axis of our spectrum. Okay. I'm going to get the delta. Because if we wrote the delta train out as a summation, we'd have delta of omega minus k pi, right? So then would we say that omega on that exponential would be k pi if we were writing it in some machine notation? I knew our correct. Yeah. So yeah, so it makes for pointing that out. Daniel is giving you a further simplification, which is this impulse here is impulse train. The delta pi omega is you can view it as sampling this exponential. And so Daniel was saying that you could further simplify this as a pi. These impulses are delta omega minus k pi e to the minus j omega 1 plus tau. And because the omega occurs, the impulse occurs that, sorry, the impulse occurs that omega equals k pi.

This simplifies to a k pi in the numerator. Oh, I see that works because it's of the shifting property then. Correct. I just have to think about it. I wasn't sure if it would work and why, but I think I see it now. Thank you. Great. I wonder if that's actually a further part of the question. Yeah, this is actually, oh, you know what? Sorry. So yeah, for 2b, I shouldn't have stopped here. We researched simplified according to what Daniel, Daniel brought up. So let's go ahead and do that. So we're going to take these 2 impulses and I'm going to carry over this work to the next page.

Thank you. All right, everyone, we're going to get started for today. So, our first announcement is that homework number seven is due this Friday, uploaded to grade scope. And as we've announced for this homework questions four or five and six which are on the plus transform and inverse Laplace transform are graded based on effort. And any attempt that you make on the question will give full credit to you. In that sense, we want you to think of these questions more as practice questions for the final sensible plus transform. And this inversion is fair game on the final. And then earlier today, we sent out an announcement on CCLE and Piazza on the final exam logistics.

So, first, Tonmoy is going to be holding a review session this Sunday, December 13th, from 2 to 6pm, and I highly encourage you to attend that review session by Tonmoy. Tonmoy also increased his office hours during final exam week. And I made a mistake in the initial announcement.

For his Wednesday office hours, I wrote it was from 9 to 11 p.m., but of course, that's after the final exam. That should read 9 to 11 a.m. And so I've corrected the announcement on both CCLE and Piazza, but these announcements were also emailed out. So I just want to make sure that that correction is clear, that on Wednesday, the day of the final exam, Tom Boyd will have office hours from 9 to 11 a.m. All right. The logistics of the final exam are going to be very similar to the midterm exam. First, the final exam is Wednesday, December 16, 2020, from 3 to 6 p.m. We are going to upload the exam to CCLE at 2.55pm, again to give anyone who wants to print out the exam some time to do so, although you're more than welcome to do the exam on a separate sheet of paper. And then the exam must be completely uploaded to Gradescope by 6.10pm, so we're giving you 10 minutes after the conclusion of the exam at 6pm to scan in your exam and assign all of your pages by 6 10 p.m.

All right, even though we're giving you until 6 10 p.m. to submit the exam, you're not to work on the exam after 6 p.m. Pacific Standard Time. And then the portal closes at 6 10 p.m. and so be sure to get in your exam before then. If you're a student in another time zone, in the exam announcements, I sent out the instructions for emailing me about your availability so that we can schedule your alternate exam time. And then just like on the midterm, the TAs and I are going to be online at this Zoom link to take any exam questions. This Zoom link will have the waiting room enabled, meaning that when you first click on the link, you may not be admitted into the room. So we take questions from students one at a time. And then after a student finishes, we'll admit the next student from the waiting room.

All right, so I want to pause here and ask if there are any questions on final exam logistics or homework or any other class logistics. Alright, great. Sounds like there are no questions so we're going to get back into material then. And then we will then address Laplace transform applications. All right. I see a question in chat saying how many questions will there be on the exam? We can't disclose that information, but the length of the exam will be will be comparable to prior years and the prior years as final exams are all on CCLE. I should say the exam length will be comparable to or shorter to prior year since we also recognize that this year. There are extraordinary circumstances and online teaching. Okay, so So in the lecture we had talked about how we had talked about the Laplace transform and several properties.

And then we had also talked about how, if you have any general differential equation, or this expression over here. going to be equal to this quantity here, which is a rational fraction. The numerator and denominator are both polynomials in S, and therefore it stands to reason that we have to know how to deal with these expressions and simplify them so that we can invert these Laplace transforms. So that led us to this thing called partial fraction expansions, where we said we could take a rational fraction where we have ratio polynomials and we could reduce it into a bunch of roots of A of S, the denominator here, and the numerator has a constant which is called a residue. So we said that this rational fraction can be written as this partial fraction expansion, and we were in the middle of doing the partial fraction expansion in the case where n is less than n.

This constraint means that the order of the polynomial in the numerator is strictly less than that in the denominator. And then we had talked last time about nomenclature, namely, A of S and B of S has m roots, which are called zeros of F of S. A of S has n roots, which are called the poles of F of S. And the poles are denoted lambda 1 up to lambda n, since we have an nth order polynomial, we'll have n roots. And then these numbers are one to rn are the residues. Any questions on this recap thus far. because it's easy for me to invert this Laplace transform. All of these are the inverse Laplace transform of 1 over s minus lambda, and we know from our Laplace transform table that the inverse Laplace transform of 1 over s minus a number is just e to that number t. So in the case where F of s has no repeated polls and m less than n. Our solution is going to be some sum of exponentials.

And then to get the partial fraction expansion we said that there are two steps first we need to find the polls, which means finding the zeros of the denominator of the best, which is a of s, and then after that we have to find the residues. So, getting the polls lambda one to lambda and all that is is solving for the roots of this equation. Right. But then the new thing that we have to do is be able to solve for the residues are one to our. And so we gave three methods to do this. And we're going to get back to these methods today so we started off by saying there's a method one which is typically labor intensive and we won't do it. And method two is the most commonly used method which is called the cover up procedure. Right. And so, in the cover up procedure. We went through this a bit more slowly today because this is the technique that you'll most frequently use when inverting the Laplace transform. of s factored into its form such that we have our three poles lambda one, lambda two, and lambda three. And then this is equal to three partial fractions each with the residue r1, r2, and r3. All right, so the idea behind method two is that to solve for one residue, let's say r1 term. But it's going to cancel out with one of the s minus ones in the denominator and so these two terms are going to cancel out. But now for R2 and R3, they're also going to be multiplied by s minus lambda one.

All right. So I have a question about part F and part G of 12 and 2. Okay. I have a question in general about some simplifications that I'm not sure I can do. Okay. So in part F where we're trying to see if B is independent of C, I'm able to isolate out C. So like from the probability of B and C, I say, I do the joint distribution, summing over all of A and D. And then once I go through all of that, I can take C out. And A given C just factors out as probability of, as one because it's law of total probability. But my question is I get like a summation over A, a summation over D of the probability of B given A, D. And so if we're summing over everything that's given, can that become probability of just B? So in general now, although we have extra terms in there, but a P of B given A, a, D summed over A, D in general only equals P of B when it's P of B, A, D. And so, in general P of B given A, D summed over A, D is not equal to P of B. But then, sorry, let me get my idea. I should have done this before, once I get it. So let me just connect my idea so we do have, and then make sure that everything is consistent.

So for 2F, I mean, you said that you had factorized how the key of C is, and you had a key of C, right? And then the summation of A for probability of A given C just goes to 1, right? That's correct. You had something like this. Yeah. Okay, and you had some over D. Yeah. So P of D, probability of A given A, D. Yeah. Great. All right. Oh, yeah. Okay. Then if you had the set, then, then, then, then you are good. Sorry. I think I misheard you earlier. So then that would just go to A given C just goes to 1, and then, probably, of D goes to 1 also, right?

Or do I have to use that for the chain rules? The thing is, you can't, sorry, you are generally correct that some over A of P of A given C will go to 1. Yeah. But in this case, it doesn't simplify nicely because P of A given C is still multiplying this expression here. So we will have a question. Yeah. I didn't know if I could separate that up or not. Yeah. So sorry. I'm writing it out, hopefully. So we should have a P of A given C, a P of D, and then a P of B given A and D. Okay. Yeah.

And it's because there are two, like, there's, it would be an A multiplied by an A probability. Yeah. There's another, yeah, there's another A here. So then from this, I have to do the simplification from there. Exactly. And then, but this, at this stage, you're pretty much done because we know that for this to simplify to P of B, this is, if we have, we need a sum over A, a sum over D of P of B comma A comma D. And in general, this does not simplify to that. So P of D times P of B given A, D, that would give us a P of B comma D given A. Yeah. Oh, sorry. No, that's not even true. Because it's not D given A. So actually, in general, this does not, and so you can say, and you can write some text arguing out why that isn't the case.

And then that's sufficient for this question. Okay. I see. So that answers my next question because I got to that place for part G, where it was this sum over D for P of D times P of B given A of D. And my question was, can that simplify to probability of B given A, and I don't think it can, right? It can, yes. So for that one, that is law of total probability. So if you have, can you repeat the sum that you have? I had sum over D of probability of D multiplied by probability of B given A comma D. B given A comma D. So can that simplify to probability of B given A? This one cannot. You would need of, if this was, if this was D given A, then this would equal sum over D probability of B comma D given A. And then this would simplify to probability of B given A.

Okay. However, we do have, this is for 2G, right? Yeah. And 2G. So I did show you row first and then I went to the joint probability over D. Yeah. And then I took out all the probability of C times probability of A given C from the ratio over D. But so then I'm left with, yeah, with what you wrote down in the first line. You're left with a, you're left with this. A is probability of B, yeah. Yeah. The probability of B given A comma D. Yeah. Yeah. So sorry, let me just look at the solution because that's the first step that you do also, but then they incorporate a probability of B given A. So let me just see how we get that.

Is it because V and A are independent and we've already shown that so it was a model? That could be the case. Yes, it is. Thank you. Okay. Yeah. So in part D, we showed that A and D are independent. And so actually this upper one would also simplify to P of D given A. So you can make an argument there. Yeah. So that's the case, P of D and P of D given A are the same. And so it doesn't define this way. Okay. Okay. So then that's how I can go to P. Okay.

Perfect. Yeah. Yeah. I have another question. All the other people ask and all circle back around. Sure. We have a good number of people here. So maybe we could do the race hand system. So if you have a question, you'll be the razor hand and then we'll just take them in order. So we can do that. Okay. So my question was also on 2F. I was wondering when we're simplifying probabilities. I got like the sum over D of P given A, D times P of B. And I was wondering.

Sorry, Rhett. I'm just going to write the answer that I know exactly. Can you say that sum again? The sum over D of P of D given P of B. Like sorry, P of D times P of B given A D. And I know that when we have two variables, we can say like the sum of P of C times P of A given C is like equal to P of A. Absolutely. If we could analogously do that for three variables and say that like P of D times P of B given A D summed over D is equal to P of B given A. I see. So in general, the expression for P of B given A written as a sum over D would be a sum over D of P of B comma D given A. And so when we do the summation for the law of total probability, we can think of that as taking out a variable that is in front of the conditioning sign.

All right, everyone we're going to get started for today. So the only announcement that we have right now is that homework number six was released last week, again, and last week. And it's going to be due nine days from today on June 2, 2021, uploaded to a grade of scale, all right? Any questions on any course logistics? All right, so we'll go ahead and continue where we left off last time, which was we were now going to derive the comment filter. So last lecture, we spent time discussing the intuition of the comment filter and how essentially it's using information from two sources of information, a state update process, which is this xk equals axk minus one equation, as well as the neural data and its relationship to kinematics, which is this yk equals c times xk equation, right? And so we talked about the intuition of that and we presented the comment filter solution and saw that it made intuitive sense on a few simple scalar examples, all right? So now we're getting into the comment filter derivation and that's where we're going to resume today. So a few things just to remind you of that we had these a few preliminaries. We had that if y and x are random vectors and they're linearly related like this, that the expected value of y is a times the expected value of x plus b. And the covariance of y is a times the covariance of x times a transpose, all right? And then we also had talked about Gaussian random vectors and these were these random vectors where they're jointly Gaussian and they can therefore be written in this form where the random variables, which are elements of this vector, they all have some means and then some covariance matrix. And the facts that we talked about last time were if I were to marginalize over this random vector, so say I have this random vector with x1, x2, x3, but I just wanted the distribution of x2. So I marginalize away x1 and x3 via my law of total probability, right? If I do that, then x2 is a Gaussian random variable with mean u2 and variance 4, all right? And if I want to marginalize, so I'm just left with x1 and x2, they would have mean 1, 2 and then this covariance gets in by this block here for the 1 and 2 indices. So marginalizing over this Gaussian random vector results still in a Gaussian random vector. If we have linear functions of Gaussian random vectors, they remain Gaussian. And then this one, which again, we'll talk about in more detail if we get to it today, which is if we condition with a Gaussian random vector. So if I have a Gaussian random vector where I'll split up this elements into two components, x1 and x2, then x1 given x2 and x2 given x1 are also Gaussian.

So the facts of this Gaussian random vector are marginalization, linear transformations and conditioning. When you do these operations, the output is still Gaussian, right? Were there any questions from any of the preliminaries? So Jonathan, I have a question. Yeah, Tom White. So for fact number three, it will also hold if we like if we have a like say an dimensional joint leg Gaussian and we condition any subset on another subset, it will still be joint leg Gaussian, correct? That's correct. Yeah. And x1 and x2 here are vectors. And so let's say that the overall x was 10 dimensional. x1 could be 6 dimensional and x2 could be 4 dimensional or x2 could be 2 dimensional. Essentially, when you condition some of these random variables on the other ones, the result is always Gaussian. All right. Thank you. Thanks for clarifying that, Tom White. All right. So, okay, let's get to our proof then. So last time we had written out our dynamical system with the following assumptions, the noise terms WK minus 1 and QK, they are Gaussian noise and they're independent, which means that at every single time step, I'm drawing new noise, okay? And then last lecture, we had asked this question, if x0 is Gaussian and we have the state update equation. So x0 being Gaussian means that the first initial state is Gaussian distributed and downstream states, states later on in time will also be Gaussian distributed because they're linear transformations of a Gaussian random vector. So if we have xk equals axk minus 1 plus WK minus 1, I asked what is the distribution of xk given xk minus 1? All right. And we deduced last lecture that it would be axk minus 1 and as the mean and the covariance would be W. All right. We saw that because if xk minus 1 is given to us, right? A is deterministic, xk minus 1 then becomes deterministic because we observed it. And then WK minus 1 is zero mean noise. So the overall mean of this expression is going to be a vector, a times xk minus 1 that we observe plus some noise with zero mean. And so the mean is just going to be the deterministic part axk minus 1. All right. And then axk minus 1 is observed, it's deterministic. So it doesn't have any randomness associated about it. And so the only randomness in xk given xk minus 1 would come from this noise term WK minus 1. And WK minus 1 has covariance matrix big W. All right. So that leads us to conclude that xk given xk minus 1 has this normal distribution. All right. Any questions there? All right.

And so we did do something similar for YK given xk following the exact same logic. We found that YK given xk has a normal distribution with mean dxk because cxk is observed. And then the randomness in this equation, if xk is observed, now only come from QK and QK has covariance q. And so that's the covariance right here. All right. And then we were also going to, for convenience, just say that the initial state x1 was also normally distributed with mean new one and covariance v1. All right. So the first thing that we wanted to do now in our common filter setting is train the common filter or train learn the parameters of my system, a w, c, and q that maximize the likelihood of the data. And so what we did, what we started to do then is we want to write the likelihood of the data, which is a probability of having observed my kinematics and my neural data at every single time step. And then of course, this will be a function of data. And I get to choose data. I get to choose data to make this likelihood as big as possible. All right. Any questions on this part of the recap? All right. So then we start to write out the log likelihood last time. So we drew off the graph of the dynamical system where we have our states, the kinematics evolving through time. And at every single point in time, there is this observation process, the yk equals cxk, which lead xk to be a parent of yk. And we know based off of the graph theory lecture that if I want to know the distribution of say x4 given x3, x2, and x1, we know that x4 is going to be conditionally independent of anything before x3 given that I observe x3. So we were able to write that p of xk given xk minus 1, xk minus 2, all the way to x1 equals p of xk given xk minus 1. And also p of yk given all the prior states as well as the prior observations. So the prior kinematics and the prior neural data, this is just equal to p yk given xk for the same exact reason, which is let's say equals 3. And I want p of y3 given I know x3, x2, y2, x1, and y1. But everything here is conditionally independent of y3 given that I observe x3. Okay. Okay, any questions there?

Sorry, sorry, I have a question. So when we see that yk given xk through xk minus 1 up to y1, right? So is equal to probability of yk given xk. So is xk capturing all the information that has happened in the past? Yes, that's correct. Thanks, Tomway. So xk is capturing all the information in the past about x1, x2, y1, and y2. So said differently, the reason that these are this conditional independence implies that all the information from x1, y1, x2, y2 is stored in x3. And therefore, if I know x3, knowing any of these prior, knowing any of these variables that were passed in time, doesn't give me any more additional information about y3. So all of the information from these variables is summarized systemically in x3. Right? And then a question to chat, what do xk and yk represent? Here xk are going to be our kinematics. So that's going to be the velocity of a computer cursor. And the yk is going to be our neural data. I remember then this equation tells me how, or I can model things like the inertia of the cursor that the velocity at time k minus 1 has some information about the velocity at time k. Because, for example, when I move to the right, if I'm moving to the right at a high speed, my velocity at the 200 millisecond pointing to the right implies that at the 225 millisecond, I should still be moving to the right. So this is the thing that captures inertia. And more generally, to capture any laws of physics or temporal structure in the data. And then this equation here represents how the neural data relates the kinematics and the neural data relate to each other.

All right, happy to start off anywhere. Yeah, Link's here. Okay, so I actually have a question about a lecture. I can really confuse about the Q and W matrix. I assume they are similar. So I'm just confused about first, why is this symmetric? It's one of my problems. And basically, I know that in lecture, we discussed about how we can just ignore the W matrix because somehow we average them out. Essentially, and then we have xk equals to a xk minus 1. I just would like to know, like, what's the reason behind it? Yeah. I think I'm just like wanting to do like idea about it. So that's basically, I'm just really confused about how the noise was set up basically. Okay, great. So first, let me wait for this to catch up.

The question is why is W and Q symmetric? It's because they are covariance matrices. So W and Q are, where's my apple pencil? Here, W and Q are covariance matrices. And so the ijth element of W is going to be the covariance between W i and W. Let me do superscript, so it's clear there. And this is the ith element of W and the jth element of W. And this is covariance itself is a, it's a symmetric quantity. So covariance of W i w j is equal to covariance w j w i. And so for that reason then, this equals w j i. And so, I'm sorry. Can you all hear me? I'm not sure if my internet connection has frozen. Hey, oh, I'm sorry about that. Can you all hear me?

Can you all hear me? Can you all hear me?

again. Okay, so, uh, Wingshee, your question was, uh, why are W and Q symmetric? Uh, and that's because the covariance operator is symmetric. Um, so I'm not sure how much of that answer came through before my internet cut out. Uh, is that all good? Yeah, but uh, it's a little bit laggy, but I know what you're talking about. Okay, and yeah, so can we go on to explaining the, uh, why can we ignore it in the expression we have in here? Yeah. Yeah, great. So, uh, for the, uh, this question is why, why when we derive the least squares solution, we can, uh, ignore the effect of W. And sorry, I think it's frozen again. Okay, okay, I think it's back now. Okay. Um, I, I'm so sorry, everyone. Uh, I think on Wednesday, I'll, I'll try, uh, I'm in a different internet situation right now and, um, and I'll try to make sure this isn't a problem on Wednesday.

So, for, why can we ignore the effect of W? First off, for the, okay, I'm going to turn off my video. I think that, that, we'll hopefully help with the lag. So, um, I'm sorry, I need to do one more thing. Okay, can people hear me? I guess, very clearly, way better than with the video. Okay, great. Okay, let's go with this for now. So, uh, why can we ignore the W? Um, so first off, for the maximum likelihood solution, we didn't ignore the W at all. And so, uh, first off, that solution requires no assumption. So, ignoring the W was really a consequence of, um, the least squares, uh, intuition. And so, uh, for that intuition, the way that I would think about it is as follows. Um, will WK in the end cause a change in my regression?

And so, um, I, um, I would like to draw. I'm going to just try this one more time, because it's going to help with the explanation. If it doesn't work, uh, I will give up on, on trying to share my screen. Uh, I'm glad that, that the problems are happening now and not during lecture. Um, okay, so hopefully, okay, so, okay, I think things are potentially working. Um, why can we ignore W for the intuition sake? So, the, the picture I have in mind here is the following. What I want to do is I want to get a regression between xk minus one and xk, right? And, uh, there are going to be data points around here.

So, let's say that I had, um, data points that look like this, right? Then my least squares regression would give me a line that goes to these cut of points. Now, WK is going to be zero mean noise. And so, if I were to now take these points and perturb them by adding the noise to them, some of them would go up, some of them would go down. But on average, the effect of, of these perturbations WK to all the data points has a zero mean average. And so, um, if there isn't any, um, uh, for example, if WK had a mean of one, all of these points in general would shift up and that would change the regression line. But because WK has a mean of zero, when I average across many data points, that isn't going to change my overall new slope here.

The slope is still going to, in the end, uh, be the effect between xk minus one and xk. Does that help with the intuition? Okay. Oh, yes. Uh, that helped a lot. And, uh, yeah, I think I'm clear on why, why we can't just, you know, in our least square solutions. So, uh, one last quick question. It's a clarification. Okay. So, at the end of the class, we have this curve, which is the probability of xk given xk all the ways to yk, right? Yeah.

So, is that a normal distribution? Like, or is it just a shape like a normal distribution? Great question. So, uh, it turns out that when we compute this distribution, it will be a normal distribution. And that's, that's because of, um, this, um, this, um, that if we can, addition on a Gaussian random vector, it produces a Gaussian random vector. So, we haven't get proven why it's going to be Gaussian, but it ends up that it is going to be Gaussian. And we'll prove that next lecture. Yeah, gotcha.

Yeah, that's, that's all my questions. Thank you very much. Thanks, thanks, too. All right, hopefully internet has stabilized a bit. If it ends up being bad again, can someone, uh, please let me know. And then, um, and then I will try to reset things. I have a question about the homework. Great. I'm just trying to sanity check, uh, my answers. Okay. And I got, for the means for error, for the optimal linear estimator, the weiner filter, I think I have pretty much the same number. Okay.

Um, I don't think that's. Uh, sorry, me. I, I broke up. I don't think that's correct. Yeah, that's all I said. Okay, got it. Uh, okay, let me, uh, let me, sorry, let me just pull up the homework. All right. All right. So, um, is this for, uh, notebook number three? Yeah. So it's, yeah, for three C, it says calculate the means for error. I got a certain number. Yes, it says does it do better worse than the optimal linear estimator?

And it's like almost the same number. Um, I'm going to pause the video for a moment so I can just talk to you about the numbers.

again. Okay, so, uh, Wingshee, your question was, uh, why are W and Q symmetric? Uh, and that's because the covariance operator is symmetric. Um, so I'm not sure how much of that answer came through before my internet cut out. Uh, is that all good? Yeah, but uh, it's a little bit laggy, but I know what you're talking about. Okay, and yeah, so can we go on to explaining the, uh, why can we ignore it in the expression we have in here? Yeah. Yeah, great. So, uh, for the, uh, this question is why, why when we derive the least squares solution, we can, uh, ignore the effect of W. And sorry, I think it's frozen again. Okay, okay, I think it's back now. Okay. Um, I, I'm so sorry, everyone. Uh, I think on Wednesday, I'll, I'll try, uh, I'm in a different internet situation right now and, um, and I'll try to make sure this isn't a problem on Wednesday.

So, for, why can we ignore the effect of W? First off, for the, okay, I'm going to turn off my video. I think that, that, we'll hopefully help with the lag. So, um, I'm sorry, I need to do one more thing. Okay, can people hear me? I guess, very clearly, way better than with the video. Okay, great. Okay, let's go with this for now. So, uh, why can we ignore the W? Um, so first off, for the maximum likelihood solution, we didn't ignore the W at all. And so, uh, first off, that solution requires no assumption. So, ignoring the W was really a consequence of, um, the least squares, uh, intuition. And so, uh, for that intuition, the way that I would think about it is as follows. Um, will WK in the end cause a change in my regression?

And so, um, I, um, I would like to draw. I'm going to just try this one more time, because it's going to help with the explanation. If it doesn't work, uh, I will give up on, on trying to share my screen. Uh, I'm glad that, that the problems are happening now and not during lecture. Um, okay, so hopefully, okay, so, okay, I think things are potentially working. Um, why can we ignore W for the intuition sake? So, the, the picture I have in mind here is the following. What I want to do is I want to get a regression between xk minus one and xk, right? And, uh, there are going to be data points around here.

So, let's say that I had, um, data points that look like this, right? Then my least squares regression would give me a line that goes to these cut of points. Now, WK is going to be zero mean noise. And so, if I were to now take these points and perturb them by adding the noise to them, some of them would go up, some of them would go down. But on average, the effect of, of these perturbations WK to all the data points has a zero mean average. And so, um, if there isn't any, um, uh, for example, if WK had a mean of one, all of these points in general would shift up and that would change the regression line. But because WK has a mean of zero, when I average across many data points, that isn't going to change my overall new slope here.

The slope is still going to, in the end, uh, be the effect between xk minus one and xk. Does that help with the intuition? Okay. Oh, yes. Uh, that helped a lot. And, uh, yeah, I think I'm clear on why, why we can't just, you know, in our least square solutions. So, uh, one last quick question. It's a clarification. Okay. So, at the end of the class, we have this curve, which is the probability of xk given xk all the ways to yk, right? Yeah.

So, is that a normal distribution? Like, or is it just a shape like a normal distribution? Great question. So, uh, it turns out that when we compute this distribution, it will be a normal distribution. And that's, that's because of, um, this, um, this, um, that if we can, addition on a Gaussian random vector, it produces a Gaussian random vector. So, we haven't get proven why it's going to be Gaussian, but it ends up that it is going to be Gaussian. And we'll prove that next lecture. Yeah, gotcha.

Yeah, that's, that's all my questions. Thank you very much. Thanks, thanks, too. All right, hopefully internet has stabilized a bit. If it ends up being bad again, can someone, uh, please let me know. And then, um, and then I will try to reset things. I have a question about the homework. Great. I'm just trying to sanity check, uh, my answers. Okay. And I got, for the means for error, for the optimal linear estimator, the weiner filter, I think I have pretty much the same number. Okay.

Um, I don't think that's. Uh, sorry, me. I, I broke up. I don't think that's correct. Yeah, that's all I said. Okay, got it. Uh, okay, let me, uh, let me, sorry, let me just pull up the homework. All right. All right. So, um, is this for, uh, notebook number three? Yeah. So it's, yeah, for three C, it says calculate the means for error. I got a certain number. Yes, it says does it do better worse than the optimal linear estimator?

And it's like almost the same number. Um, I'm going to pause the video for a moment so I can just talk to you about the numbers.

All right, everyone. We're going to get started for today. So just to announce for today, the first is a reminder that homework number six is due a week from today. And the other is that today we're going to be doing the derivation of the common filter recursion. And this is probably the most challenging derivation in the class. And it's going to piece together a lot of things we've talked about. The derivation is challenging, so you're not going to be tested on reproducing the derivation. But when you look over the derivation, you should be able to understand every step that occurs in the derivation in terms of us knowing the facts for each step. And so again, this derivation will be challenging and you won't be tested on reproducing it. But we really do encourage you to look at the derivation and make sure that you understand every step along the way. So today we should be finishing common filter. And then if we have some time, we're going to start the last topic for the course, which will be dimensionality reduction. And we'll be discussing a technique called PCA, which is principal component analysis, and one of the most commonly used dimensionality reduction techniques. All right. Any questions before we get into the common filter derivation? All right. Question from Andrew.

I'm a question. So this is like the first finding today on common filter and then starting dimensionality reduction. What are we going to be doing during V10? I assume we won't be spending like two and a half lectures on dimensionality reduction. Yeah, that's right. So next week because of the holiday, we only have one vector after this. And so during that lecture, we will finish off PCA. And that'll be likely all will cover for dimensionality reduction in detail. But we may talk about some ideas related to how it's attached to extent PCA to probabilistic PCA as well as vector analysis. I totally forgot about the holiday. Yeah. Yeah. And in a more typical offering of this class, we do get beyond when we get the vector analysis, we also talk about the expectation maximization algorithm, which some of you may be interested in learning. And even though we likely won't get to those this year, we have uploaded those materials into the lectures tab of CCLE. And so if you're interested in looking over that material, that material is available on CCLE. And and TA tonight would be happy to answer any questions about it. All right. Any other questions? Okay. Let's recap where we were. So last week, we did the derivation of the optimal parameters in a maximum likelihood sense for a linear dynamical system. Right. So we had our state update dynamics process and our observation process.

And these two equations, they had parameters A, C, and then the covariance of WK-1, which is big W and the covariance big Q. All right. And we said if we're given neural data, the YKs and the kinematics, the velocities, the XKs, how do I take this training data and use that to learn A, C, W, and Q. And so last lecture, what we did is we did the same thing that we did in for homework number four, which is that we wrote the likelihood of the dynamical system, the linear dynamical system by writing out its graph structure and then writing out the joint probability of observing all the data given the parameters. And we saw that because of the graph structure, where there are these conditional independencies, the likelihood factorizes into essentially three probabilities, the probability of X1, which we said was going to be a normally distributed random variable. And then a bunch of products of P of XK given XK minus 1. And this had a normal distribution as well as the probability of YKs given XKs. And this also had a normal distribution. So because everything has a normal distribution, we were able to write out the likelihood and then we were able to differentiate it with respect to AC, W, and Q to get the optimal parameters. And this slide here is a summary of what that derivation was. And so we talked about if you had your kinematics data X and your neural data Y and you can catenate them for all time, then these are the equations that give you the best AW and AW, C, and Q. And that completes the training process. So I want to pause and ask if there are any questions from last lecture on the training process of the common filter parameters. All right. So now at the end of last lecture, where we left off was that we are now going to get to the testing phase. So from training, we've learned AW, C, and Q. And now I want to predict how to decode the optimal velocity or how to decode a velocity XHK given all the neural data I've seen thus far from Y1 all the way up until YK. So in homework number four, for a given trial, we just had one neural observation, which are the bin spike counts in some window. And we want to calculate the probability of every single class. And there were eight classes for eight different directions I could choose.

And so for this, we could calculate this straightforwardly. We just have to calculate eight of these probabilities and then pick the biggest one. All right. In the case where now we're controlling a continuous cursor moving on the screen, we're not choosing one out of eight classes. We're decoding a continuous velocity XK. All right. And so what we said last time we would need to do then is we would need to calculate the actual distribution of what the velocity should be XK, what the kinematics should be given all my neural data from time Y1 to YK. All right. And so that distribution could look like this, for example. And this tells me that given my neural data from Y1 to YK, I could expect my velocities to be centered around 10 centimeters per second. And there's a low probability that it's higher and the low probability that it's less than 10 centimeters per second. So likely my velocity is around 10 centimeters per second. And so this is a distribution of what my velocities will be given my neural data. When I decode, I need to pick one single value of my velocity. And at the end of last lecture, what we showed is if you have a distribution over your velocities to pick the one single velocity to decode to move the cursor, we would pick the mean because the mean is the minimizer is the one single value that minimizes the squared error of the random vector of the random vector Z or the random vector representing the kinematics. All right. So that's what we set up last lecture, which is that to go to the testing phase where I get new neural data and I want to move this cursor, what I need to do is I need to calculate the probability of the cursor's kinematics of its velocity, given all my neural data, and then my decode value will just be the mean of this distribution. So that's the setup that we talked about at the end of last lecture. Are there any questions here? Question from Grace. And today you should just be able to unmute yourself.

Okay. I just wanted to ask if you could please repeat that last portion of what you just said. Like we're calculating the probability of the cursor and like finding the mean and stuff. Exactly. So what we're doing here is for every time step k, so at a certain point in time, the time that I'm decoding right now, I'm going to calculate a distribution of what I think my velocities will be given all of my neural data. That's what this pink distribution is here. And so if my pink distribution looked like this, then I would guess that my velocity should be somewhere in the vicinity of 10 centimeters per second. And then I need to actually choose just one velocity to actually move the cursor on the string. And so the velocity that I'm going to choose is going to just be the mean of this distribution. So the decoded velocity will be the distribution mean. So I need to first calculate the distribution. And then after that, I need to take its mean and that gives me my decoded velocity. Not an other question. Yeah, coming. Yeah, so over here, they're minimizing the mean squared error and that gives the mean, right? But let's say you minimize the absolute error instead of the mean squared, then you will get the median of the distribution as the optimizer. So is there any intuitive reason behind this government filter using this mean squared error versus some other error? Yeah, it is straightforward for us to calculate the mean of, actually, it's also straightforward for us to calculate the median of the gas distribution. So actually, in this case, it would be fine to also use absolute error. And you would just need to decode using instead of the mean of the distribution, the median of the distribution.

All right, everyone. We're going to get started for today. So just to announce for today, the first is a reminder that homework number six is due a week from today. And the other is that today we're going to be doing the derivation of the common filter recursion. And this is probably the most challenging derivation in the class. And it's going to piece together a lot of things we've talked about. The derivation is challenging, so you're not going to be tested on reproducing the derivation. But when you look over the derivation, you should be able to understand every step that occurs in the derivation in terms of us knowing the facts for each step. And so again, this derivation will be challenging and you won't be tested on reproducing it. But we really do encourage you to look at the derivation and make sure that you understand every step along the way. So today we should be finishing common filter. And then if we have some time, we're going to start the last topic for the course, which will be dimensionality reduction. And we'll be discussing a technique called PCA, which is principal component analysis, and one of the most commonly used dimensionality reduction techniques. All right. Any questions before we get into the common filter derivation? All right. Question from Andrew.

I'm a question. So this is like the first finding today on common filter and then starting dimensionality reduction. What are we going to be doing during V10? I assume we won't be spending like two and a half lectures on dimensionality reduction. Yeah, that's right. So next week because of the holiday, we only have one vector after this. And so during that lecture, we will finish off PCA. And that'll be likely all will cover for dimensionality reduction in detail. But we may talk about some ideas related to how it's attached to extent PCA to probabilistic PCA as well as vector analysis. I totally forgot about the holiday. Yeah. Yeah. And in a more typical offering of this class, we do get beyond when we get the vector analysis, we also talk about the expectation maximization algorithm, which some of you may be interested in learning. And even though we likely won't get to those this year, we have uploaded those materials into the lectures tab of CCLE. And so if you're interested in looking over that material, that material is available on CCLE. And and TA tonight would be happy to answer any questions about it. All right. Any other questions? Okay. Let's recap where we were. So last week, we did the derivation of the optimal parameters in a maximum likelihood sense for a linear dynamical system. Right. So we had our state update dynamics process and our observation process.

And these two equations, they had parameters A, C, and then the covariance of WK-1, which is big W and the covariance big Q. All right. And we said if we're given neural data, the YKs and the kinematics, the velocities, the XKs, how do I take this training data and use that to learn A, C, W, and Q. And so last lecture, what we did is we did the same thing that we did in for homework number four, which is that we wrote the likelihood of the dynamical system, the linear dynamical system by writing out its graph structure and then writing out the joint probability of observing all the data given the parameters. And we saw that because of the graph structure, where there are these conditional independencies, the likelihood factorizes into essentially three probabilities, the probability of X1, which we said was going to be a normally distributed random variable. And then a bunch of products of P of XK given XK minus 1. And this had a normal distribution as well as the probability of YKs given XKs. And this also had a normal distribution. So because everything has a normal distribution, we were able to write out the likelihood and then we were able to differentiate it with respect to AC, W, and Q to get the optimal parameters. And this slide here is a summary of what that derivation was. And so we talked about if you had your kinematics data X and your neural data Y and you can catenate them for all time, then these are the equations that give you the best AW and AW, C, and Q. And that completes the training process. So I want to pause and ask if there are any questions from last lecture on the training process of the common filter parameters. All right. So now at the end of last lecture, where we left off was that we are now going to get to the testing phase. So from training, we've learned AW, C, and Q. And now I want to predict how to decode the optimal velocity or how to decode a velocity XHK given all the neural data I've seen thus far from Y1 all the way up until YK. So in homework number four, for a given trial, we just had one neural observation, which are the bin spike counts in some window. And we want to calculate the probability of every single class. And there were eight classes for eight different directions I could choose.

And so for this, we could calculate this straightforwardly. We just have to calculate eight of these probabilities and then pick the biggest one. All right. In the case where now we're controlling a continuous cursor moving on the screen, we're not choosing one out of eight classes. We're decoding a continuous velocity XK. All right. And so what we said last time we would need to do then is we would need to calculate the actual distribution of what the velocity should be XK, what the kinematics should be given all my neural data from time Y1 to YK. All right. And so that distribution could look like this, for example. And this tells me that given my neural data from Y1 to YK, I could expect my velocities to be centered around 10 centimeters per second. And there's a low probability that it's higher and the low probability that it's less than 10 centimeters per second. So likely my velocity is around 10 centimeters per second. And so this is a distribution of what my velocities will be given my neural data. When I decode, I need to pick one single value of my velocity. And at the end of last lecture, what we showed is if you have a distribution over your velocities to pick the one single velocity to decode to move the cursor, we would pick the mean because the mean is the minimizer is the one single value that minimizes the squared error of the random vector of the random vector Z or the random vector representing the kinematics. All right. So that's what we set up last lecture, which is that to go to the testing phase where I get new neural data and I want to move this cursor, what I need to do is I need to calculate the probability of the cursor's kinematics of its velocity, given all my neural data, and then my decode value will just be the mean of this distribution. So that's the setup that we talked about at the end of last lecture. Are there any questions here? Question from Grace. And today you should just be able to unmute yourself.

Okay. I just wanted to ask if you could please repeat that last portion of what you just said. Like we're calculating the probability of the cursor and like finding the mean and stuff. Exactly. So what we're doing here is for every time step k, so at a certain point in time, the time that I'm decoding right now, I'm going to calculate a distribution of what I think my velocities will be given all of my neural data. That's what this pink distribution is here. And so if my pink distribution looked like this, then I would guess that my velocity should be somewhere in the vicinity of 10 centimeters per second. And then I need to actually choose just one velocity to actually move the cursor on the string. And so the velocity that I'm going to choose is going to just be the mean of this distribution. So the decoded velocity will be the distribution mean. So I need to first calculate the distribution. And then after that, I need to take its mean and that gives me my decoded velocity. Not an other question. Yeah, coming. Yeah, so over here, they're minimizing the mean squared error and that gives the mean, right? But let's say you minimize the absolute error instead of the mean squared, then you will get the median of the distribution as the optimizer. So is there any intuitive reason behind this government filter using this mean squared error versus some other error? Yeah, it is straightforward for us to calculate the mean of, actually, it's also straightforward for us to calculate the median of the gas distribution. So actually, in this case, it would be fine to also use absolute error. And you would just need to decode using instead of the mean of the distribution, the median of the distribution.

I'm happy to take questions in any order. Hi. I like to ask a question about the question 4 of homework 6. Okay. The comment figure 1. So I understand that on the journal lecture, would you write these formulas to calculate AC and I guess 7Q? But I'm confused about what does it mean to compute the values we calculated into a 5 by 5 matrix for example for 8? Yeah. And then let me just get my pad also. All right. So for part a, we have this vector where we have the x and y positions at time k and then the x and y velocities at time k and then 1. And then what we wanted to do was to fit the a matrix like you mentioned using the equation in class which is essentially the least squares bit.

And so in class we had mentioned and I think we asked you in the homework to just do it for velocities. So what you should do is when you compute a, I think we call it a s. This is a 2 by 2 matrix and it's done via doing the least squares between the a equals x of k times x of k minus 1 pseudo inverse. But now these x's of k is going to be just your 2D velocities. So it's going to be 2 by k minus 1. So after you do this least squares regression, a s will be a 2 by 2 matrix. So then you ask the question what does it mean to impute? What we mean by that is that now the whole a matrix is going to be 1 that is 5 by 5. However we're only going to put in the 2 by 2 matrix here because remember that this a matrix is relating px at time k plus 1, py at time k plus 1, the x at time k plus 1, py at time k plus 1 and 1 to the positions and velocities at time k. And so we want the position equations to obey physics. So the position equation will always be 1, 0, delta t, 0, 0, 1, 0, delta t, 0 and the one will always be equal to 1.

And so what we then want you to do is you take this matrix a s and you plop it over here. And so this 2 by 2 entry here will be a s. Does that answer your question, Sean? Yes. I just have, I'll see you. Why don't you just include, also include the position when computing the matrix. That's a really great question. Yeah. So we could do that. We could do a being 5 by 5 and just stack all the positions and velocities together and do the least squares regression. The thing is that's going to give values in this matrix. And I think the values in this matrix like these will be close to 1 and these will be close to delta t, but they won't be exactly those.

And so by doing the regression over positions and velocities, you're getting values here, which are informative. But you know, you because we know that the position is the integrated velocity and that has to obey the loss of physics. We can put what we know from physics here and our physics knowledge is more accurate than what the least squares regression would pull out. So in that case, because the physics is perfect and the least squares is not, then we use the physics. Yeah, that makes sense. Thank you. Yeah. Great. Great question. Yeah. Professor.

Exactly. Is that a question I'm prone to? Is it OK if I share my screen like the plot? Yeah. Let me go ahead and I'm going to tell anyone who's watching this after. I'll just pause the recordings during share screens because it shows the students code and whatnot and so I want to just be sensitive to that. So I'm going to pause recording now.

I'm happy to take questions in any order. Hi. I like to ask a question about the question 4 of homework 6. Okay. The comment figure 1. So I understand that on the journal lecture, would you write these formulas to calculate AC and I guess 7Q? But I'm confused about what does it mean to compute the values we calculated into a 5 by 5 matrix for example for 8? Yeah. And then let me just get my pad also. All right. So for part a, we have this vector where we have the x and y positions at time k and then the x and y velocities at time k and then 1. And then what we wanted to do was to fit the a matrix like you mentioned using the equation in class which is essentially the least squares bit.

And so in class we had mentioned and I think we asked you in the homework to just do it for velocities. So what you should do is when you compute a, I think we call it a s. This is a 2 by 2 matrix and it's done via doing the least squares between the a equals x of k times x of k minus 1 pseudo inverse. But now these x's of k is going to be just your 2D velocities. So it's going to be 2 by k minus 1. So after you do this least squares regression, a s will be a 2 by 2 matrix. So then you ask the question what does it mean to impute? What we mean by that is that now the whole a matrix is going to be 1 that is 5 by 5. However we're only going to put in the 2 by 2 matrix here because remember that this a matrix is relating px at time k plus 1, py at time k plus 1, the x at time k plus 1, py at time k plus 1 and 1 to the positions and velocities at time k. And so we want the position equations to obey physics. So the position equation will always be 1, 0, delta t, 0, 0, 1, 0, delta t, 0 and the one will always be equal to 1.

And so what we then want you to do is you take this matrix a s and you plop it over here. And so this 2 by 2 entry here will be a s. Does that answer your question, Sean? Yes. I just have, I'll see you. Why don't you just include, also include the position when computing the matrix. That's a really great question. Yeah. So we could do that. We could do a being 5 by 5 and just stack all the positions and velocities together and do the least squares regression. The thing is that's going to give values in this matrix. And I think the values in this matrix like these will be close to 1 and these will be close to delta t, but they won't be exactly those.

And so by doing the regression over positions and velocities, you're getting values here, which are informative. But you know, you because we know that the position is the integrated velocity and that has to obey the loss of physics. We can put what we know from physics here and our physics knowledge is more accurate than what the least squares regression would pull out. So in that case, because the physics is perfect and the least squares is not, then we use the physics. Yeah, that makes sense. Thank you. Yeah. Great. Great question. Yeah. Professor.

Exactly. Is that a question I'm prone to? Is it OK if I share my screen like the plot? Yeah. Let me go ahead and I'm going to tell anyone who's watching this after. I'll just pause the recordings during share screens because it shows the students code and whatnot and so I want to just be sensitive to that. So I'm going to pause recording now.

All right, everyone. We're going to get started for today. So a few announcements. First is a reminder that homework number six is due tonight. I uploaded to grade scope by 1159 p.m. Last night we sent out an announcement on CCLE regarding the final exam and particular details about it. So the first detail is that Tomoy will be holding a final exam review session from 3 to 6 p.m. on Sunday June 6, 2021 at this Zoom link. And Tomoy will be uploading a problem set to CCLE and the solution will be also uploaded after the review session. He will also record the review session for people who can't make it at the time until posted on CCLE. And we highly encourage you to attend the review session. Right? Any questions about the review session? All right, beyond that, we have details about the final exam. So the final exam is in six days on June 8th and is from 3 to 6 p.m. We sent out an email regarding the details, which will be similar to the midterm.

So be checking Piazza because we're going to be posting exam updates there. And we also have a Zoom room where you can ask us questions during this exam time. Right? The final exam is going to cover up to and including everything from last lecture. And so today's lecture, we're going to cover PCA, but there will be no PCA tested on this exam. All right, so everything up to and including last lecture, everything up to and including homework six. That's what is fair again for the final exam. We have prior years as final exams, uploaded to CCLE in the past exams folder. But we want to note that last year we gave the exam in class. And so the final exam last year was an hour or 50 minutes. And it was shorter than a normal final exam, which is three hours. So if you're looking at last year's exam, please keep that in mind that it was for a shorter time slot. And then lastly, just like for the midterm, if you need an alternative exam time, we're happy to accommodate. But please read the instructions in the CCLE announcement and then send me the appropriate email to reschedule for the exam. All right, any questions on anything related to the final exam or any other course logistics for that matter.

All right, then we are going to then go ahead and what we'll do today is we'll talk about. I'll say some videos of the callman filter performance since we spent a lot of time last night during the derivation, but what's come up at level now and see how the callman filter actually does. And then after that, I wanted to introduce this idea of dimensionality reduction and present the mathematical derivation of principal components analysis, which is one of the most simple techniques for dimensionality reduction. But incredibly powerful and like the callman filter, powerful beyond applications and neural data, just generally in data science, PCA can be very helpful. All right, so for the callman filter, we call that as in homework number six, right for the comment filter, we have a linear dynamical system where now we have instead of just one equation relating neural data and kinematics. We have a second equation, which tells us how kinematics evolve through time in principle ways. And this a matrix that dynamics matrix reflected that. And so in the homework, we had you all implement this a matrix where the positions obeyed physics because they are integrative velocities. And then we had you use maximum likelihood estimation to infer the parameters of this little two by two matrix here, which tells us how the velocities at time k, update to the velocities at time k plus one. And this model is inertia, so in the homework, you should have gotten values in this matrix that are below one, but are, but that are not close to zero. They're the closer to one. And that means that the velocity at time k plus one is going to be close to the velocity at time k. And so that means that if you're moving to the right at time k, then this equation is saying at time k plus when you're probably still moving to the right. Okay, and so that's how these velocity terms incorporate a smoothness or inertia. Right. Any questions here. So then after that, we went through the common filter recursion, which we derived last week, and you should have been able to implement the wild loop that runs through these equations and then calculated these m1 and m2 matrices.

Eventually, you could decode kinematics using the equation x k hat equals m1 times x k hat of time k minus one plus m2 times y k. And that's how the common filter uses the m1 matrix, so we pass kinematics and the m2 matrix to consider the current observation, the neural data. Any questions on any setup of the common filter, any part of the derivation, any part of intuition on the common filter? All right. So in homework number six, you all implemented the common filter and we're able to calculate the mean square error. And now show you the video of what the common filter looks like when we ask one of the monkeys to control it, and early in real time. So I'm just waiting for the zoom when they catch up with my eye. Okay. So this video is going to be the vanilla common filter that you all implemented in homework number six. I'm going to bring the same task that we've shown before where his goal is to reach to one of the targets, but if he hovers over any target for 500 milliseconds, it's going to select that target. And if he selects a target that is incorrect, then you'll hear the negative sound, but if he selects the correct target, you'll hear a positive sound. Okay. I think maybe the sound isn't working for this video. I apologize. All right. So 20 trials are so in. We see the success rate. He's over 90% success rate. The bit rate is 2.2 bits per second, which is respectable, but actually less than the winner filter. So looking at this video, either in the chatter, if someone could raise their hand, could you tell me what you feel are the pros and cons of this decoder in terms of how it looks? Yeah. William. Kind of feels like the movements are too big every single time, like, yeah, over shooting.

Yeah, great. Yeah. So William and then Grace wrote something similar in chat, which is looks like they're over shooting. That's exactly right. So it turns out that with the common filter, when we incorporate the inertia, we reduce it incorporated. So when you look at the movements, they are relatively smooth towards the target. You don't see the jitteryness that we saw with optimal linear estimator, for example. The problem is that it's in a way too smooth as if there's too much inertia. So usually when there's a target that's far away, you see that he overshoots it. Although actually two minutes in, which is what let me restrict the video. It turns out that the monkey will learn a good control strategy. So two minutes in, he kind of compensation for this over shooting by by not going as fast. But you can see early on when he's trying to get to a target. He often overshoots it. And that's because there's too much smoothness in the velocity. And so while this incorporates inertia, it causes you to be so much inertia sometimes that that the monkey will overshoot. And then when there are small movements that he needs to make, you can see he goes back and forth around that again, because as far as it's kind of stopped the inertia of the common filter. Any questions there? All right, so we saw this problem of the common filter and we decided to make an innovation on it to make it higher performance. And so we had two ideas, two main ideas to do this. And these comprise what is now called the refit common filter control algorithm. And this is something that we published in Nature Neuroscience in 2012. And so the first idea is less related to the common filter. But it has to do with the training data. So let me talk about that innovation first and then it's related to our second innovation, which helps with this over shooting problem. So let's say that the monkey is starting at a center target and wants to reach to a target that is up into the right. When the monkey makes such a reach, the trajectory isn't the straight line to the target, but when we reach up into the right, we make a curve trajectory because of the biomechanics. And so if you think about how in the homework, the way that we calculated the velocities that we read rest to is that we just took adjacent positions. And then we just did a first order or error approximation of the velocity. So we took the two adjacent positions subtracted them and then divided by T. What this means is that at the very start of the trial early on, even though the monkey intends to go up into the right, the velocities are actually pointing upwards. And at the end of the trial as he gets this target, even though he's again trying to go up into the right, the velocities point just to the right. Whereas we believe the neural data reflects that he wants to always go up into the right, but the movement itself has some of these suboptimalities just due to the way of biomechanics how hands naturally move.

All right, everyone. We're going to get started for today. So a few announcements. First is a reminder that homework number six is due tonight. I uploaded to grade scope by 1159 p.m. Last night we sent out an announcement on CCLE regarding the final exam and particular details about it. So the first detail is that Tomoy will be holding a final exam review session from 3 to 6 p.m. on Sunday June 6, 2021 at this Zoom link. And Tomoy will be uploading a problem set to CCLE and the solution will be also uploaded after the review session. He will also record the review session for people who can't make it at the time until posted on CCLE. And we highly encourage you to attend the review session. Right? Any questions about the review session? All right, beyond that, we have details about the final exam. So the final exam is in six days on June 8th and is from 3 to 6 p.m. We sent out an email regarding the details, which will be similar to the midterm.

So be checking Piazza because we're going to be posting exam updates there. And we also have a Zoom room where you can ask us questions during this exam time. Right? The final exam is going to cover up to and including everything from last lecture. And so today's lecture, we're going to cover PCA, but there will be no PCA tested on this exam. All right, so everything up to and including last lecture, everything up to and including homework six. That's what is fair again for the final exam. We have prior years as final exams, uploaded to CCLE in the past exams folder. But we want to note that last year we gave the exam in class. And so the final exam last year was an hour or 50 minutes. And it was shorter than a normal final exam, which is three hours. So if you're looking at last year's exam, please keep that in mind that it was for a shorter time slot. And then lastly, just like for the midterm, if you need an alternative exam time, we're happy to accommodate. But please read the instructions in the CCLE announcement and then send me the appropriate email to reschedule for the exam. All right, any questions on anything related to the final exam or any other course logistics for that matter.

All right, then we are going to then go ahead and what we'll do today is we'll talk about. I'll say some videos of the callman filter performance since we spent a lot of time last night during the derivation, but what's come up at level now and see how the callman filter actually does. And then after that, I wanted to introduce this idea of dimensionality reduction and present the mathematical derivation of principal components analysis, which is one of the most simple techniques for dimensionality reduction. But incredibly powerful and like the callman filter, powerful beyond applications and neural data, just generally in data science, PCA can be very helpful. All right, so for the callman filter, we call that as in homework number six, right for the comment filter, we have a linear dynamical system where now we have instead of just one equation relating neural data and kinematics. We have a second equation, which tells us how kinematics evolve through time in principle ways. And this a matrix that dynamics matrix reflected that. And so in the homework, we had you all implement this a matrix where the positions obeyed physics because they are integrative velocities. And then we had you use maximum likelihood estimation to infer the parameters of this little two by two matrix here, which tells us how the velocities at time k, update to the velocities at time k plus one. And this model is inertia, so in the homework, you should have gotten values in this matrix that are below one, but are, but that are not close to zero. They're the closer to one. And that means that the velocity at time k plus one is going to be close to the velocity at time k. And so that means that if you're moving to the right at time k, then this equation is saying at time k plus when you're probably still moving to the right. Okay, and so that's how these velocity terms incorporate a smoothness or inertia. Right. Any questions here. So then after that, we went through the common filter recursion, which we derived last week, and you should have been able to implement the wild loop that runs through these equations and then calculated these m1 and m2 matrices.

Eventually, you could decode kinematics using the equation x k hat equals m1 times x k hat of time k minus one plus m2 times y k. And that's how the common filter uses the m1 matrix, so we pass kinematics and the m2 matrix to consider the current observation, the neural data. Any questions on any setup of the common filter, any part of the derivation, any part of intuition on the common filter? All right. So in homework number six, you all implemented the common filter and we're able to calculate the mean square error. And now show you the video of what the common filter looks like when we ask one of the monkeys to control it, and early in real time. So I'm just waiting for the zoom when they catch up with my eye. Okay. So this video is going to be the vanilla common filter that you all implemented in homework number six. I'm going to bring the same task that we've shown before where his goal is to reach to one of the targets, but if he hovers over any target for 500 milliseconds, it's going to select that target. And if he selects a target that is incorrect, then you'll hear the negative sound, but if he selects the correct target, you'll hear a positive sound. Okay. I think maybe the sound isn't working for this video. I apologize. All right. So 20 trials are so in. We see the success rate. He's over 90% success rate. The bit rate is 2.2 bits per second, which is respectable, but actually less than the winner filter. So looking at this video, either in the chatter, if someone could raise their hand, could you tell me what you feel are the pros and cons of this decoder in terms of how it looks? Yeah. William. Kind of feels like the movements are too big every single time, like, yeah, over shooting.

Yeah, great. Yeah. So William and then Grace wrote something similar in chat, which is looks like they're over shooting. That's exactly right. So it turns out that with the common filter, when we incorporate the inertia, we reduce it incorporated. So when you look at the movements, they are relatively smooth towards the target. You don't see the jitteryness that we saw with optimal linear estimator, for example. The problem is that it's in a way too smooth as if there's too much inertia. So usually when there's a target that's far away, you see that he overshoots it. Although actually two minutes in, which is what let me restrict the video. It turns out that the monkey will learn a good control strategy. So two minutes in, he kind of compensation for this over shooting by by not going as fast. But you can see early on when he's trying to get to a target. He often overshoots it. And that's because there's too much smoothness in the velocity. And so while this incorporates inertia, it causes you to be so much inertia sometimes that that the monkey will overshoot. And then when there are small movements that he needs to make, you can see he goes back and forth around that again, because as far as it's kind of stopped the inertia of the common filter. Any questions there? All right, so we saw this problem of the common filter and we decided to make an innovation on it to make it higher performance. And so we had two ideas, two main ideas to do this. And these comprise what is now called the refit common filter control algorithm. And this is something that we published in Nature Neuroscience in 2012. And so the first idea is less related to the common filter. But it has to do with the training data. So let me talk about that innovation first and then it's related to our second innovation, which helps with this over shooting problem. So let's say that the monkey is starting at a center target and wants to reach to a target that is up into the right. When the monkey makes such a reach, the trajectory isn't the straight line to the target, but when we reach up into the right, we make a curve trajectory because of the biomechanics. And so if you think about how in the homework, the way that we calculated the velocities that we read rest to is that we just took adjacent positions. And then we just did a first order or error approximation of the velocity. So we took the two adjacent positions subtracted them and then divided by T. What this means is that at the very start of the trial early on, even though the monkey intends to go up into the right, the velocities are actually pointing upwards. And at the end of the trial as he gets this target, even though he's again trying to go up into the right, the velocities point just to the right. Whereas we believe the neural data reflects that he wants to always go up into the right, but the movement itself has some of these suboptimalities just due to the way of biomechanics how hands naturally move.

All right, everyone. We're going to get started for today. So a few announcements before we begin. I sent out a CCLE announcements with instructions on how to sign up for Piazza and grade scope. And so those are just recapituated here as well as a link to a Python tutorial. If you have only used MATLAB in the past, this tutorial will probably be helpful for you. And then we have also uploaded to CCLE material reading for this class, the lecture notes, and then we also uploaded all the midterms and final exams dating back to 2017. If you're someone who isn't sure if the pre, if you satisfied the prerequisite material, I encourage you to take a look at the midterms and final exams as well as the common filter dot pdf which are probably the most technically demanding for this class. Any questions on any announcement related matters? Any course logistics?

All right, so excuse me. We're going to go ahead and finish off the syllabus. So last time we talked about the grade breakdown for this class and that it was graded on an absolute scale. And I know I went through it a bit quickly at the end, so I just want to put up this slide to ask if there are any questions here on the grade breakdown for the grading scale. All right. So then we had also put up this information about pass no pass or satisfactory on satisfactory grading if you choose to take the class in that manner. We also talked about how for exams during remote learning, the exam are going to be open note open book and you can access your notes and CCLE on your computer, but it's going to be closed internet. And that the TAs and I we are going to perform some analysis on the answers given. And if we suspect anyone collaborating, which are not allowed to do for the exams, we request reserve the right to give a superseding oral exam. Right. And then also if you are in a different time zone, we can make accommodations for you to take the exam at a different time.

And so please send me an email this week if you plan on taking us off on this. We will we use that just to get a sense of how many accommodations we'll need to make and then we're going to send out more details closer to the exam date to handle those accommodations. Any any questions here? All right. I have to slide on academic integrity, which I give in all my classes. And it's my way of saying up front that I care a lot about academic integrity and that we all follow the principles of academic integrity and fairness and respect to our fellow classmates. And so I put up the slide to say that I take this very seriously. And if we catch any students of cheating or violating the principles of academic integrity, that I take that seriously. And I will follow up and report those cases to the dean of the students office. And we will follow their recommendation as they investigate the case and do what they do what they determine us to do. All right. So I just want to put that up front that again. If we if we catch you violating academic integrity, we will follow up on it and report their case to the dean of students. All right. Any questions here?

All right. So with that other course information throughout this class, we're going to cover a wide range of topics, including some intrusion neuroscience, which we'll do for these first two and a half weeks. And then after that, we're going to cover topics in modeling spikes. That's going to be drawn from this theoretical neuroscience textbook, as well as topics from machine learning and statistical signal processing, which we take from this Chris Bishop textbook. And because we didn't want to have students need to to purchase all three books, what we did was we took the excerpts of the chapters that we used for these books and we put those on CCLE. Right. So that material should all be on CCLE. Other notes for this class. So the last two notes that we use in class, I just asked that they not be publicly posted due to matters related to copyright. And so we'll be happy to distribute them in the annotated notes on CCLE. But we just asked that they remain within the class population. Like we said, last time a piazza should be used for almost all major class discussions. And of course, these office hours to get any other questions answered.

But we hope that the piazza form will be lively. And like I mentioned last time, we give bonuses based off of participation on piazza. And so even though you can consider anonymous to classmates, your posting is not anonymous to us. And we make that setting so that we can assign bonuses based off of how much you participate on piazza. And the TAs will be checking piazza also regularly to make sure that any questions that couldn't be answered by students can be answered by teaching staff. If you have a question that isn't appropriate for piazza and it's related to class material, I just asked that you email, Tonmoys, Shashank and I and me together. We do this so that no single TA gets overloaded because in prior classes, sometimes once TA is very responsive and then all the students learn to email that TA all the time and that TA becomes overburdened. And so we just asked that you send us any class related questions to all three of the teaching staff if you don't post it on piazza. And then of course, if you have any personal matters, please email me directly there. Any questions here? All right. Some last notes. I think we mentioned this last time.

It's time consuming to make the transition from that lab to Python. So please factor that into your work. I think it's worth it because Python is the de facto standard for machine learning and signal processing today. I also mentioned this last time this class according to student feedback is a lot of work. And so I want to state this upfront so that you can plan accordingly. We do try to have this class be very fun, but we cover a bunch of topics and test it understanding. So we have seven homework assignments and we're told that those assignments are somewhat time consuming. All right. Like the mentioned piazza should be the primary names of asking and getting questions answered. And we just talked about this on the last slide also so I won't review that again. Any questions here? All right. So with that. Sorry. It looks like I have that was not last notes. I have a few more slides. For those coming with a background in machine learning. I want to say that up front since we start off doing neuroscience and we do derive maximum likelihood solutions for classifiers and regression in this class, which is something that may have seen before.

You may feel the pace of the class is somewhat slow. And then we're going to keep the pace because a lot of people in the class don't have background in machine learning. And I want to make sure that everyone can learn and master the material. And so if you're coming in with the prior background in machine learning, just be aware that the class may feel a bit slower for you. And you can factor that into your decision as you decide what class is to take. And then I like we talked last time. I think we're going to take halfway through lecture because the lectures are fairly long here at UCLA. And in our discussion sections, which we're going to be run live by Tonloi and Shashank. First off, you can attend whatever discussion section you would like. And the TAs will also be recording at least one of the discussion sections to put it on CCLA. And then we have already sent out the P.O.T.S.ing page to send up information so this full of points should not be here. All right, a lot of logistics I just went through any questions on anything that I went through. Okay, so I just want to do brief introductions. So about me and sorry. This is not updated well. I did update the other parts of the slide, but I didn't update this introduction.

This is not my first year at UCLA. This is now my fourth year. And it's my seventh time teaching this class. And so I, I've taught it at Stanford twice with my PhD advisor there and previously taught at UCLA four times. This is a class related very closely to the research that my research group does. And so we care a lot about improving the performance of brain machine interfaces. And then other classes that teach at UCLA include ECU 102 in the fall quarter, so that signals and systems. And then in the winter quarter, I teach a class on neural networks and deep learning. And so with that, I'm going to pass it off to one of our TAs, Tom, why so Tom, why if you can go ahead and unmute yourself. Yeah. Hi, so hello everyone. So my name is Tom, I'm on tour. I will be one of your keys for this quarter. So this is my third time, Tying for this course on neural signal processing and my fifth time, Tying for Jonathan. All of my Tying experience at UCLA has been for Jonathan. I really like Tying for Jonathan and especially for this course. So this course is one of my favorite courses here at UCLA. I took this course first when it was offered for the first time and back in 2017. And I really enjoyed the course material. I also hope that you also enjoyed this course too. So I am a PhD student at UCLA working with Professor Rajudhary and my research in interest, primary lies in reinforcement learning and pattern, exception and dynamic networks. So that's all about me and I hope to have a very enjoyable quarter with all of you. Thank you very much. Great. Thank you, Tom Boyd. And then we have our second TA, Shishank, who won't be able to give an introduction today. We'll ask him to give an introduction on Monday. But this will be Shishank's first time Tying this class here at UCLA and he's of course taken this class before and done, absolutely. And so we'll leave Shishank's introduction for the start of next class on Monday.

All right, everyone. Welcome to each one of two for the fall quarter of the academic year. I'm your professor Jonathan cow. And we also are we also have three TAs here are we not. Go Sh Conway Montsler and Guangwen Zhao right and so I wanted to just start off with a few words about online lecture before we begin. So the first is that as you all know, the lectures, discussions and office hours for this class are going to be carried out on zoom. So lectures and discussions are also going to be recorded and uploaded to CCLA. We encourage you to attend lectures live, which will be Monday Wednesdays, 2 to 350 PM Pacific time, as will aim to have these lectures be interactive and also have an opportunity to ask questions. If you don't want to appear in lecture video, it's fine to opt out, but because we're recording these for the benefit of all students, we asked that you opt out by not attending. This will be the link for lecture for the entire quarter and also be the link that I use for my office hours. Alright, so in addition to that, so by default, everyone in this lecture is muted during by lectures. However, I do want these lectures to be interactive. And so we allow there to be questions and we'll do it through the following mechanism. The first is through the raise hand button. And so if you raise your hand, which I'm sure all of you know how to do, but if not there are instructions here, then when there's a natural breaking point during lecture, then I'll take questions. The other way is through chat functionality. And so our TAs are not on boy and Guangwen who introduced at the end of class. They'll be monitoring the chat. And so if you type in a question into the chat, that will be seen by the TAs and the TAs will be able to answer that question. And we just asked that you also record your question.

We asked you record your question, sorry, that you post your question publicly to everyone so that it would be for the benefit of everyone. And then I just also wanted to mention something about asking questions. This is a large class. I think we have right now on zoom. We have 158 students in here. And I remember being an undergrad and having questions during lecture, but oftentimes being afraid to ask them because I fear that I might ask a stupid question or others might judge me. So I want to say that in this class, the likelihood of stupid questions is very rare. So for the purposes of practice for this class, I'm going to say there are no stupid questions. And I think that if you have a question like that, it's really important to ask it for two reasons. The first is that it's very, there are going to be concepts in this class that builds on top of prior knowledge. And if you aren't following something that's more fundamental, then it's really important to get that addressed so that we can build on that. And you can understand later material. The second is that if you have a question about something that you don't understand is very likely that others in the class also may not have picked that up. And that might be because I didn't explain it well. And so if I ask you a question, you give me an additional opportunity to try to explain a concept. All right. And then the TAs are also going to be operating on this quarter. I put all of their links here for their office hours, as well as for their discussion sections. And we'll talk about logistics there later on in class. All right. Any questions before we start off? All right. Great. So in this first lecture of signals and systems, I wanted to give an overview of what this class is all about. And so we'll also after talking about the high level details of this class, go into the course logistics and the syllabus.

Though I know that this is a required class for many of you. I hope that you'll still have a lot of fun learning the material in this class. I'm obviously biased, but I think that the material here is really interesting and will dovetail well into a lot of your future classes in the right. And again, as I was just saying, please ask questions, even questions that you may think are elementary. Those are the most important ones. All right. All right. So a first question that you might have is this class is called signals and systems. So what are signals and what are systems? To unpack this question, I wanted to take a high level view of what signals and systems are. And so what do we mean when we say signals and systems? Well, at a very high level. We know that our world and our society relies on being able to represent information, to represent the knowledge that we have. And then to to communicate that information knowledge, others to process that information and to operate on that information. And one way in which we can see the importance of this in our society is through technology. And technology is correlated to our ability to do these things to represent information and then also to communicate information. And so we can go back as far as prehistoric times by dining times before we had recorded history to ages of country gatherers where predominantly there, the means of communicating information was through personal interaction. And so you might be able to get some information by meeting up with someone, reading their facial expressions or if they had some form of language by word of math. All right. As civilization developed, we had a major advantage, which was the knowledge or the ability to write things down. And here I'm showing a picture of Egyptian hieroglyphics. But as we were able to write things down, now knowledge could be written down for future generations. And so this allowed information within, gained within a specific generation to propagated future generations.

And we see how information from past generations, for example, the ancient Greek, like the writings of Aristotle, Plato, Socrates, even ancient texts, texts like the Bible, for example. Are all things where we have information about what happened in prior times and, and are able to build off of that information subsequently. See a question quickly. Sorry. I'm looking to see how to get on mute. Melissa, can you unmute? Yeah, I can unmute now. What is the structure of your recorded? I guess the lectures will be recorded and they'll all be uploaded to CCLE. Any other questions? All right. So again, as we're able to write down information, we can now propagate that to future generations, but not everyone had access to these writings. And so another major part of advancement in technology was what I'm showing here, which is a good and bird press. This printing press allowed essentially written down information to be democratized and to be accessible to many people, allowing ages such as the enlightenment where we really saw an explosion and ideas and information. One very important, one very important advance towards modern technology that we know of today was the ability to begin to write down and store information electronically.

So does anyone know what this device I'm showing there is? You can write in the chat if you know. Ryan says battery as several people say battery that exactly what this is. This is the Voltaic pile. It was developed by Alice Andrew Volta and essentially it's a stacking of several electric electrochemical cells and it allowed there to be a voltage now across two terminals, which could be used to power circuits. Later on, Gayord Oom came along and he derived this very important equation V equals I R, which I'll be it simple. If you've taken a circuits class, you know that from V equals I R, we get in for we derived equations and concepts like that in a North team equivalent circuits or care cost voltage and current laws. And then here I'm showing one of the very first transistors, which we know then allowed us to make active circuits and underlie many of the integrated circuits that are in our computer today as well as in analog circuits and devices that we use. And so with this and with electricity, we were able to start to communicate information in new ways. And example of this is. Alexander Graham Bell, one of his first ways in which he transuse voice into electrical signals was he would take. And then he would put a needle in water and that needle would vibrate according to the voice, the sounds that you made with your voice and when the needle vibrate in the water that would change the conductance of water and we know from V equals I R, if you change the conductance or the resistance of water that's going to change the current. And so that current could be transmitted as electrical signal and you can convey the signals from your voice to many people. Right. And so this was a voice to electrical. And so why am I talking about information and how we convey it will it turns out that signals and systems correspond to these two things. And so signals are things that represent information. And so in this class and the signal systems perspective will conceptualize these information or signals as our information and then how information then is changed in process is what we refer to as systems. And as electrical engineers or engineers in general, we tend to think of signals and systems in electromagnetic or computer terms.

All right, everybody. If you're coming from EC-102, my computer is, I hope that this is okay. My computer is still converting the EC-102 video, but I started recording for EC-1809, so hopefully there isn't a negative interaction there. All right. All right, cool. So this is EC-1809, which is the Advanced Honors Seminar. I think most, if not all of you, are probably taking this concurrently with EC-102. Today, we'll just talk about the structure of this class and then introduce the problem that we'll talk about for the rest of this class, which is going to be related to my research. And so a quick few things to get out of the way. The first is for grading. So there are many more students in the class this year than in prior years. What I like to do is I like to have this class be largely interactive. And so this is not the case.

I'll modify things, but you should be able to unmute yourself and talk. What we'll do is we'll still use the raise hand system here, but I won't need to unmute you. And if you want to interject at some point in time, as long as it doesn't become disruptive later on, then that's all good with me. And so I'm happy to have this very open class where you can ask your questions, where we can think of that things together. And I think that that's fitting for such an honor seminar. This class is one unit. And the way I gauge what the work in this class should be for something that's one unit, which is the following. ECE-102 has four units and about, or has seven problem sets. So what I've done for this class is I've designed this class of just have one project, which is given at the end of class. And it's roughly equivalent to doing two problem sets in ECE-102.

And so that'll be the entire work load for this class. Here will be the Zoom link for this class, which we'll use for lecture. I will be recording lectures and also uploading them to CCLE in case people can't make it. And the office hours for this class are just going to be the same as my ECE-102 office hours. And so if you're not taking this class in currently with ECE-102, those office hours are at this link here. And they're going to be on Thursday from 12 to 2 p.m. Okay. Any questions to start off? All right. Yeah, so this class is something where I want to focus a large part of it on how to think independently and how to use tools at your disposal to answer questions of interest. And so towards this end, the structure of this ECE-189 is that we're going to focus on a particular problem.

Hopefully of interest to you, definitely of interest to me, but hopefully I can convince you that it's interesting. It'll be in the area of so-called brain-machine interfaces. And we're going to try to use the tools that we have at our disposal, particularly from EC-102, as well as calculus, to try to solve a problem in this particular area of study in this particular area of brain-machine interfaces. And so the project will be that I'm going to give you some neural signals that were recorded from neurons in the brain. And we're going to ask you to translate those neural signals into movements. That sounds like a daunting problem if you've never been introduced to the area of brain-machine interfaces. But that's what the structure of this seminar is going to be. We're going to discuss what brain-machine interfaces are. We're going to discuss how neurons work. From there, we're going to discuss what kind of tools we need to solve this problem.

That's going to be the major component of this class. The first three weeks of EC-102 are not application-heavy. I mentioned in EC-102 today, that class, the first three weeks are just a lot of math and foundation-building. We're going to spend the first three weeks of this seminar just introducing the background of what brain-machine interfaces are. And then after that, we'll talk about the kind of tools that we'll want to try to solve the problem of decoding neural signals into movements. And then we'll finish off this seminar by just thinking of other problems and how we might solve them with tools we have. And so again, an overall goal of this seminar is to take a question that is probably new to all of you, think about how to approach that question, and then how to break it down so that we can tackle it with the tools that we have. Any questions in terms of course goals or how the ECE189 is structured? I had a question about what you mean by movements.

Is that essentially reading a neural signal? And then corresponding that to what a person, what movement a person would be trying to make and I would give off that signal? Is that okay? Okay, cool. Yes, I'm sorry, what is your name? I try to remember names for this class, and this is smaller than I can manage it. I'm Blake. Okay, thanks Blake. Actually, I realize that it might be more difficult because I don't think I see videos. So I didn't think about this beforehand. You don't have to turn on your videos if you don't want to, but if people want to turn the videos, that could help me to remember your faces. All right, yeah. So Blake, your question is for this particular problem of brain-mishing interfaces, yeah, the movement corresponds to the intended movement that someone might want to make.

And so if you think about someone with paralysis and we'll talk about this a bit later on in this lecture, their brains are totally functioning, but they can't move because for example, their spinal cord is severed, so they can generate the intention to move, but no movement will come out. And so, again, we'll talk about this in a bit more detail, but then for people with paralysis, what we can do is we can try to bypass this broken spinal cord by directly reading out from the brain. And if we can interpret those signals into movements, the movements that they want to make, like, moving my arm to the right or to the left, then we can restore some communication and prevent them to them. Okay, any other questions? Well, yeah. Is this course going to, like, roughly follow what we learned in 102, or is it kind of slightly by itself? It's more by itself.

So the process is going to be motivated based off of material in 102. So we're going to do some filtering of signals, of neural signals, and then decode them, but it's going to be largely sent away. So if you're not taking this class concurrently with 102, that's also fine. Although, if you're not taking it concurrently with 102, ideally, you would have had 102 before. Otherwise, the project, the project would be very difficult. And sorry, just give me a second to see if I can change my view to be the video so that I can see everyone. Let me see if I can just make this to gallery view so that, looks like I'm inside gallery. Okay, and sorry, could we just ask the question there? Oh, Jerry. Okay. Thanks, Jerry. And then I see Bradley has a question, Bradley, could you please unmute yourself and ask?

Yes, so you mentioned that the brain machine interfaces have applications for complete paralysis, but deep does that also have an application for things like essential tremor, where it's like they can move, but it's impaired? That's a great question. It could have application there. If the tremor is due to, for example, Parkinson's disease, then there are better treatments, a treatment alternative, like deep brain stimulation. And I'll show some examples of that in lecture today. In theory, it could be, but we'll also learn, at least for what we'll talk about in lecture, to record from neurons, you have to do neurosurgery. And so there are a lot of costs and risks associated with the system. And to justify those costs and risks, the ability to decode has to be very high, or the tremor has to be completely removed. And I'm not aware of anything aside from deep brain stimulation to address Parkinsonian tremor right now.

Okay, thank you. Thank you very much, Brianna. Can the brain machine interface also affect like pain transmitted, like with fibromyalgia, there is pain that occurs, could that prevent? In theory, yes. So I'm not aware of any studies related to looking at pain pathways. And for example, preserving them, or writing an information to those pathways to prevent the perception of pain. But in theory, that's something that I would call a futuristic brain machine interface. In general, it's very hard to write information into the brain, or to exactly perturb the brain in the way you desire, not just because of the lack of tools, but also because I would say that we understand less than 0.001% of how the brain works. And I'm a computational neuroscientist, that's my field. We understand very little, but we still have made great strides. But then as to how to particularly perturb brain pathways to solve the perception of pain, I'm not aware, I don't know that area too well.

0:00:00
All right, everyone. We're going to get started for today's lecture. So one of the nastiest before we begin, which is that R&OB, R&OB and RTAs are uploaded homework number one to CCLE in the week one materials on Friday of last week. This homework is due this Friday, I'll upload to grade scope by 11.59 p.m. Right? Any questions on any course logistics? All right, so we're going to get right into material then. So last lecture, we had for us to talk about signal operations and properties. We talked about time scaling, reversal and shifting, as well as even in odd signals. And we had finished class talking about periodicity. So today, I want to finish periodicity and then also want to get through material on energy and power signals as well as complex numbers. And hopefully we'll also be able to start some signal models today. So last lecture, we had talked about what a periodic signal is, which is a continuous time signal is periodic. It's an only if there exists a big T0 and this big T0 is called the period of the signal. And if the signal repeats every big T0, then the signal is called periodic. Right? And then we also showed last lecture that if it repeats every big T0 and is periodic, then it'll also repeat every two T0, three T0, four T0, five T0, etc. All right? So I wanted to start off this lecture by just talking about the simplest periodic signal, or one of the simplest periodic signals. And then we'll have a brief poll question after that. So one of the most basic signals in this class that we'll cover is the sign or the cosine wave.

0:02:07
And we're going to be using them extensively in this class. So it's worth reviewing some of their properties. So a cosine is typically written in this way. You'll see it written two ways, actually. It'll be A times cosine omega. That's this discrete letter T minus theta. Or you'll also see it written as A cosine 2 pi f T minus theta. And so omega is, in this case, equal to 2 pi f. And the difference between these is whether we're talking about frequency or angular frequency. So omega is called an angular frequency. And it has units of radians per second. Whereas f is called frequency. And it has units of hertz, which is equal to 1 over seconds. Now, how we relate the frequency to this concept of the period, big T0 is the following. The frequency is equal to 1 over big T0. Where big T0 here is a fundamental period of the signal. So if the signal repeats every one second, big T0 equals one second, then the frequency is 1 hertz. If it's faster and it repeats every 0.5 seconds, then the frequency is 2 hertz. So it happens, it repeats more frequently. And then when we convert it, angular frequency, we just multiply everything by 2 pi. All right. So let me draw up the sine wave. Or we'll do the cosine wave here looks like. This is, hopefully, review for most of you. So if I have a cosine wave, and I'll just draw this following. There are a few terms in this expression that we need to point out.

0:04:22
So you want to know what are a, f, and theta. And to answer these questions and to explore what these terms are, I have to write down a few time points. So let's say that this here, this is equal to 2. So this cosine is going to repeat every two seconds. Right. And then let's say that the amplitude here, let's say that this value is equal to 4. Right. So we know that if we don't have an a here of a is equal to 1 cosine omega T minus theta is a signal that has a peak of 1. And so when we see here, this cosine goes up to 4, that tells us that a is equal to 4. And so a is just an amplitude scaling factor is equal to 4 here. Right. That's here is again the frequency of the signal, which is 1 over big T zero. And so here, if big T zero is equal to 2, as it is, because the signal repeats every two seconds. No matter where I am, I go two seconds down in time and I'm going to be at this same point. Right. So F is going to be equal to 1 over the period, which is 1 half. So the frequency of this signal is is 0.5 hertz. Right. Okay. And then theta here, you'll see is a quantity that's going to look like it's a time scaling factor. Sorry, not a time scaling, a time shifting factor. And so if I subtract theta, right. Last lecture, we talked about how we subtract a number. What we're going to do is we're going to delay the signal. And so cosine, we know, starts at 1 goes to zero, goes to negative 1 et cetera. So this cosine is not shifted in any way. So in this case, theta is equal to zero.

0:06:30
All right. Well, let's say that I did want to shift the cosine. So let's say I wanted to shift this cosine. So it looked like the following. All right. And so this cosine now has its peak occurring at time t equals 1. All right. So I'm going to ask if I want to take this blue signal here and shift it by an amount of theta. So now it's time shifted delayed so that it's peak instead of happening at zero, happens at time one. For what value of theta ought that be? And so take 10 to 15 seconds to think about that. And I'll ask someone to write the answer over chat. All right. So I see several answers in the chat. I see pi. I see pi over 2. I see 1. All right. So the correct answer is pi. And let me explain why that's the case. So the first answer that you might have thought of is that the answer is 1. Because what I'm going to do is I'm going to shift this blue signal over by time one. So if I call this blue signal here x of t and I call this green signal y of t, then your first attempt may be to say, okay, y of t is going to be equal to x of t, which is 4 times cosine of 2 times pi times the frequency, which is 1 half t. And then because it's shifted over by 1, then I should write a minus 1 here. All right. We put a question mark there. Let me simplify this a bit. This is equal to 4 cosine of pi times t minus 1. All right. So you can do a few sanity checks to see if this is correct or not.

0:09:07
If this is correct, then the peak of the cosine should occur at time t equals 1. However, if I plug in time t equals 1 here, so at time t equals 1, when the cosine should have its peak, this function is equal to 4 cosine of pi minus 1. All right. But we know that cosine has its peak when the argument to cosine is 0. Cosine has its peak at time t at when the input is 0, but pi minus 1 is not equal to 0. So this here must be incorrect. All right. So I want so much to write or someone can someone raise their hand and tell me why this is incorrect? Because I did what we taught last vector, right? I shifted the time over by 1. So why didn't this work? Salvador. Hi. So it wouldn't work because pi minus 1 is not 0 or pi, which is at its peak, which is 1. Cosine of 0 is 1. Cosine of pi is 1. It would be somewhere between 0 and 1. Cosine of pi minus 1. Yeah, that's correct. Yeah. So then that's a good explanation for why this is not the correct answer. I'm going to now go to Rampton and I'm going to rephrase my question. So in terms that we know that this is an incorrect answer, why was it incorrect to just subtract one here? And so I'm going to go having ask Rampton. Okay. So because you have to subtract one from the parameter T, meaning that you have to replace T with T minus 1. So that 2 pi times 1 half will be multiplied by T minus 1 and you have to distribute that pi into negative 1 as well.

All right, everyone. We're going to get started now for ECE189. I want to ask if there are any questions on the seminar or even one or two related things before we begin. All right. So we're going to continue where we left off last time, which is last time we start to talk about neurons and how they signal. And so last lecture, we had talked about how neurons essentially have three parts that are important. They have the inputs, which are the dendrites, and they're these arborous life structures. And then there's an integration center, which is called the axon collect. And what this integration center does is it receives voltages from all of these input dendrites and it adds them together. And if that sumed voltage is above a threshold, it's going to send an output called an action potential. And that goes along is axon, which then connects to the dendrites of other neurons. And so the axon, we can think of as the output of the neuron. All right. And we talked about how even though neurons have many different morphologies and shapes and sizes, they are essentially the same in function that they are always going to have inputs coming in on their dendrites. They're going to sum them together. And if it's a big enough value, they're going to fire off an action potential to them downstream neurons. And the complexity then in signaling arises from how we connect neurons, much like how in computer circuits or analog circuits, all transistors operate based off of fundamental principle. And all transistors have per CMOS, the source of draining the gates, but then connect them in special ways. And you have a common source amplifier, or you have a common gate follower, or you have an x-or gate, or an or gate, or an AND gate, etc.

Okay. Any questions here? All right. So we talked about how these action potentials are, all are nothing events, and they look like these spikes over here. And in this sense, we can think of signaling in the nervous system as being digital, right? Either you have no spikes, or else you do have a spike, which you can think of as a one. And this digital signaling allows for high fidelity communication across long distances in your body. And so your brain has to tell your toes how to move, or else you have to be able to feel things like if you step on something pointy, you feel that pain. And that pain should be conveyed to your brain at high fidelity. And that happens because the neurons are going to fire these spike signals, and the spike signals are the digital signal. And as they get transmitted across the nerves in your body, they're going to decay like all signals do. They're going to decay as they travel. However, there's going to be these repeaters in your nervous system called nodes of Rambiae that when the signal is decaying too much, it's going to amplify the signal so that it regenerates the spike. All right. So if you've heard of a motor disorder called multiple gulerosis MS, actually the degeneracy of this disease is that the spikes can't get to the next repeater. And if the spikes die before the next repeater, before the next node of Rambiae, then the signal is lost. And so people who have multiple gulerosis have the degeneracy and not being able to regenerate or re-amplify these spike signals. Any questions? All right. So, I'm on this. Yeah. Okay. Thomas, I think you were having a sorry. Yeah, I think you're on me to be there. Yeah.

Okay. For the second bullet point. Yeah. Thomas, I heard for the second bullet point. I heard for the second bullet point and then I got cut off. Okay. Is it working out? It's working out, I guess. Okay. Yeah. For the second bullet point, what is like highly stare type to mean in this context? Oh, it means the shape is highly stare type. And so the shape is going to look consistent across different neurons. Now, it's not 100% true that all neurons have the exact same shapes in terms of waveform shapes, but they look fairly similar. They will differ. And for example, they're with or actually with is a major one. But that's usually reserved for more advanced neuroscience classes. And so this stereotype refers to this shape. All right. Any other questions? All right. So, we mentioned then that since neurons communicate via these spikes and these spikes are spikes in voltage. What we can do is we can drop an electrode and we can try to record the voltages that these neurons emit. And we mentioned that when you drop an electrode into the brain, there's no guarantee that you get very close to the neuron. So usually what happens is that I have an electrode over here. This is going to be my positive terminal. And I'm going to have some ground electrode that's far away. And so really what I'm measuring, if we assume that the ground electrode stays at some static level, what I'll be measuring is the changes in voltage in this vocal proximity. And even though I'm not right on top of the neuron, if I'm close enough, I'm going to get some detection of a change of voltage whenever there's a spike. But instead of it being 100 millivolts, it's going to be, let's say, 100 microvolts. So, one thousand times less than amplitude.

All right. So then last lecture, I was showing you what raw data looks like coming off from one of these electrodes. We see this slow, unjuating voltage. But then on top of these voltages, we see these big spikes. And these spikes correspond to the neurons that correspond to spikes coming from neurons that we're recording from. And we were unpacking this a little bit. We saw that in this way they form. There are actually spikes of two amplitude. So there are these red spikes here, which have a really high amplitude. And there are these medium amplitude spikes here, showing green. If we just plot them on top of each other, we'll see that they do have different amplitudes. And last lecture, I asked, what does this mean? And you all correctly responded that it means that we're listening to two neurons. So we might have a green neuron and then a red neuron. And what happened is our electrode landed over here. And so even though it's close enough to fear from both neurons, because it's closer to the red neuron, it's action potential, it's spiking pit to pit voltage is going to be higher than for the green neuron. And in this way, one electrode can actually give you measurements from multiple neurons at the same time. I so want to pause here and ask if there are any questions about this. So the way that we usually, what we call this is spikes sorting. And so what we'll usually do is we'll record from these electrodes. And if we see spikes at different amplitudes, we'll talk them about each other and then isolate them into clusters. So here, we'll have a red and a green cluster.

And then those correspond to neurons for whom we can know exactly when they spiked. All right, so from there, we'll get into neural encoding and decoding. This class will primarily be about neural decoding. And we're going to give some more details on how an algorithm works, but then either at the end of the sector or off the next lecture, we'll actually derive the decoder that we're going to be using in this class for the project. All right. And so these neurons, their fundamental currency of information is their spikes. And neurons represent and transmit information by their sequences of spiking. All right. And so this is how information is both encoded from the brain. So encoding means how does how do neurons represent stimuli from the outside world. And so this encoding could be, for example, light, right? You could be looking at a picture and a light of different frequencies is coming into your, as I coming onto your retina, and then being sent into your brain, and then how the neurons represent that light or sound intensity, those would be neural encoding problems. All right. And so in neural encoding, what we try to do is we have a model where the neurons firing rate, so they'll call the neurons, or try the neurons spiking response, why? This is going to be some function of stimuli s. And so that stimuli could be like modatory sound, or again, light or video. All right. And so neural encoding is trying to learn how the neurons represent your stimuli by learning this function path.

All right, everyone. We're going to get started for today. So a few announcements before we begin. I sent out a CCLE announcements with instructions on how to sign up for Piazza and grade scope. And so those are just recapituated here as well as a link to a Python tutorial. If you have only used MATLAB in the past, this tutorial will probably be helpful for you. And then we have also uploaded to CCLE material reading for this class, the lecture notes, and then we also uploaded all the midterms and final exams dating back to 2017. If you're someone who isn't sure if the pre, if you satisfied the prerequisite material, I encourage you to take a look at the midterms and final exams as well as the common filter dot pdf which are probably the most technically demanding for this class. Any questions on any announcement related matters? Any course logistics?

All right, so excuse me. We're going to go ahead and finish off the syllabus. So last time we talked about the grade breakdown for this class and that it was graded on an absolute scale. And I know I went through it a bit quickly at the end, so I just want to put up this slide to ask if there are any questions here on the grade breakdown for the grading scale. All right. So then we had also put up this information about pass no pass or satisfactory on satisfactory grading if you choose to take the class in that manner. We also talked about how for exams during remote learning, the exam are going to be open note open book and you can access your notes and CCLE on your computer, but it's going to be closed internet. And that the TAs and I we are going to perform some analysis on the answers given. And if we suspect anyone collaborating, which are not allowed to do for the exams, we request reserve the right to give a superseding oral exam. Right. And then also if you are in a different time zone, we can make accommodations for you to take the exam at a different time.

And so please send me an email this week if you plan on taking us off on this. We will we use that just to get a sense of how many accommodations we'll need to make and then we're going to send out more details closer to the exam date to handle those accommodations. Any any questions here? All right. I have to slide on academic integrity, which I give in all my classes. And it's my way of saying up front that I care a lot about academic integrity and that we all follow the principles of academic integrity and fairness and respect to our fellow classmates. And so I put up the slide to say that I take this very seriously. And if we catch any students of cheating or violating the principles of academic integrity, that I take that seriously. And I will follow up and report those cases to the dean of the students office. And we will follow their recommendation as they investigate the case and do what they do what they determine us to do. All right. So I just want to put that up front that again. If we if we catch you violating academic integrity, we will follow up on it and report their case to the dean of students. All right. Any questions here?

All right. So with that other course information throughout this class, we're going to cover a wide range of topics, including some intrusion neuroscience, which we'll do for these first two and a half weeks. And then after that, we're going to cover topics in modeling spikes. That's going to be drawn from this theoretical neuroscience textbook, as well as topics from machine learning and statistical signal processing, which we take from this Chris Bishop textbook. And because we didn't want to have students need to to purchase all three books, what we did was we took the excerpts of the chapters that we used for these books and we put those on CCLE. Right. So that material should all be on CCLE. Other notes for this class. So the last two notes that we use in class, I just asked that they not be publicly posted due to matters related to copyright. And so we'll be happy to distribute them in the annotated notes on CCLE. But we just asked that they remain within the class population. Like we said, last time a piazza should be used for almost all major class discussions. And of course, these office hours to get any other questions answered.

But we hope that the piazza form will be lively. And like I mentioned last time, we give bonuses based off of participation on piazza. And so even though you can consider anonymous to classmates, your posting is not anonymous to us. And we make that setting so that we can assign bonuses based off of how much you participate on piazza. And the TAs will be checking piazza also regularly to make sure that any questions that couldn't be answered by students can be answered by teaching staff. If you have a question that isn't appropriate for piazza and it's related to class material, I just asked that you email, Tonmoys, Shashank and I and me together. We do this so that no single TA gets overloaded because in prior classes, sometimes once TA is very responsive and then all the students learn to email that TA all the time and that TA becomes overburdened. And so we just asked that you send us any class related questions to all three of the teaching staff if you don't post it on piazza. And then of course, if you have any personal matters, please email me directly there. Any questions here? All right. Some last notes. I think we mentioned this last time.

It's time consuming to make the transition from that lab to Python. So please factor that into your work. I think it's worth it because Python is the de facto standard for machine learning and signal processing today. I also mentioned this last time this class according to student feedback is a lot of work. And so I want to state this upfront so that you can plan accordingly. We do try to have this class be very fun, but we cover a bunch of topics and test it understanding. So we have seven homework assignments and we're told that those assignments are somewhat time consuming. All right. Like the mentioned piazza should be the primary names of asking and getting questions answered. And we just talked about this on the last slide also so I won't review that again. Any questions here? All right. So with that. Sorry. It looks like I have that was not last notes. I have a few more slides. For those coming with a background in machine learning. I want to say that up front since we start off doing neuroscience and we do derive maximum likelihood solutions for classifiers and regression in this class, which is something that may have seen before.

You may feel the pace of the class is somewhat slow. And then we're going to keep the pace because a lot of people in the class don't have background in machine learning. And I want to make sure that everyone can learn and master the material. And so if you're coming in with the prior background in machine learning, just be aware that the class may feel a bit slower for you. And you can factor that into your decision as you decide what class is to take. And then I like we talked last time. I think we're going to take halfway through lecture because the lectures are fairly long here at UCLA. And in our discussion sections, which we're going to be run live by Tonloi and Shashank. First off, you can attend whatever discussion section you would like. And the TAs will also be recording at least one of the discussion sections to put it on CCLA. And then we have already sent out the P.O.T.S.ing page to send up information so this full of points should not be here. All right, a lot of logistics I just went through any questions on anything that I went through. Okay, so I just want to do brief introductions. So about me and sorry. This is not updated well. I did update the other parts of the slide, but I didn't update this introduction.

This is not my first year at UCLA. This is now my fourth year. And it's my seventh time teaching this class. And so I, I've taught it at Stanford twice with my PhD advisor there and previously taught at UCLA four times. This is a class related very closely to the research that my research group does. And so we care a lot about improving the performance of brain machine interfaces. And then other classes that teach at UCLA include ECU 102 in the fall quarter, so that signals and systems. And then in the winter quarter, I teach a class on neural networks and deep learning. And so with that, I'm going to pass it off to one of our TAs, Tom, why so Tom, why if you can go ahead and unmute yourself. Yeah. Hi, so hello everyone. So my name is Tom, I'm on tour. I will be one of your keys for this quarter. So this is my third time, Tying for this course on neural signal processing and my fifth time, Tying for Jonathan. All of my Tying experience at UCLA has been for Jonathan. I really like Tying for Jonathan and especially for this course. So this course is one of my favorite courses here at UCLA. I took this course first when it was offered for the first time and back in 2017. And I really enjoyed the course material. I also hope that you also enjoyed this course too. So I am a PhD student at UCLA working with Professor Rajudhary and my research in interest, primary lies in reinforcement learning and pattern, exception and dynamic networks. So that's all about me and I hope to have a very enjoyable quarter with all of you. Thank you very much. Great. Thank you, Tom Boyd. And then we have our second TA, Shishank, who won't be able to give an introduction today. We'll ask him to give an introduction on Monday. But this will be Shishank's first time Tying this class here at UCLA and he's of course taken this class before and done, absolutely. And so we'll leave Shishank's introduction for the start of next class on Monday.

All right, everyone, we're going to get started for today. A few in essence before we begin. The first is that homework number one, a reminder is do this Friday, uploaded to grade scope by 11.59 PM. We're also going to release homework number two this Friday. And if you have already submitted homework number one, there are several of you. We ask that you please log in again and assign your pages to questions because when we initially made the assignment, we didn't have the outline. So the outline is there now. And so again, if you've submitted homework number one, please log in again and assign your pages to the question outlines. And then for everyone else who's submitting, please be sure when you log in to assign your pages to questions so that the graders know where to look for your work for each question. I also wanted to answer a question that I saw on Piazza, which is in the homework, there's going to be a signal that you'll see with infinite energy and zero power.

This is the signal one over square root of T, that is in the homework. When you square the signal, it becomes one over T. And we know from high school or college calculus that the harmonic series where you're summing one over T as T goes from one to infinity, even though that decays towards zero, that has an infinite sum or if you were to integrate an infinite area under the curve. And so that's an example of a signal that has infinite energy but zero power. And so last lecture, I said that there are some signals that are neither energy nor power signals. And this is another example of one of those. Okay, any questions on any course logistics before we dive back into material? Excuse me for like assigning the question by outline, just the question number or like part A or part B also? It also includes part A and B. So we created the outline, we'll say it will say one A, one B, etc. And so please assign all of this.

Thanks, Chumlin. Let me give you a. So what you mean is like you you want us to put numbers on each question, right? And you're muted by the way. Sorry. So I ended up using because I forgot to set the mute all to and so thank you Ryan for raising your hand for the question as to what you need to assign. So on grade scope, we have said there's a question one A, one B, one C, etc. And so what you need to do is you need to indicate where on your submitted PDF or your submission to grade scope, which question which work refers to one A, one B, etc. Okay, thank you. All right. Any other questions? All right. So we're going to continue where we left off last time, which is we started to talk about signal models, which are the signals that we will essentially are basic signals that we will use to construct other signals in this class.

And so we talked about our basic cosine wave. And then last lecture, we ended talking about this complex sinusoid, which is this function AE to the J omega T plus theta, which from oilers formula, we can compose at we can decompose as a real part, which is a cosine omega T plus theta as well as an imaginary component, a sine omega T plus theta. And again, with complex numbers, we think of them as a collection of two sets of numbers. In this case, two signals. So the cosine omega T plus theta is this solid line right here. And the second set of signal is the a sine omega T, which is this dotted line. All right. And that's how we should conceptualize such signals in this class. Any questions here? Right. So for, there's also an exponential signal, which is E to the sigma T. You all know what E to the sigma T looks like when sigma is greater than zero, this corresponds to a growing exponential with time.

And when sigma is less than zero, then it's a decay exponential through time. So now what we can do is we can start to combine these signals together. So one thing I could do is I could take a cosine omega T plus theta that's here in this solid line. And then I can multiply that by E to the sigma T, which if sigma is greater than zero, corresponds to a growing exponential. Right. So now E to the sigma T multiplies this cosine. And so each of the sigma T can be thought of as changing the amplitude of this cosine over time. And so sigma is greater than zero, meaning that our exponential signal increases. Then it's slowly going to cause this sinusoid, this cosine, to also increase an amplitude. If sigma was less than zero, then the cosine would be large at negative time and then decrease our damping in the amplitude. And so one common terminology is that this exponential, which essentially sets the amplitude of the cosine, is sometimes called the envelope of this signal because it envelops the signal.

Any questions here? All right. So then we could also have a complex exponential. And so a complex exponential is of the form E. And then it has a purely real part, which is the sigma T component. That's the growing or decaying exponential. So this is equal to a growing or decaying exponential times our complex sinusoid, e to the j omega t. And so what this would look like is a complex sinusoid, again with the real component being this cosine, the dotted line being the imaginary component. And so the solid and dotted line are what you get from e to the j omega t. And then this amplitude is going to be scaled by e to the sigma T. Any questions here? All right. So I'm going to now show you a picture which will return to later on in this class, which is if we have a signal x of t equals e to the sigma plus j omega t, the exact same signal that we had here.

We can see that for different values of sigma and omega and j omega, it's going to have very different behavior. If sigma is bigger than zero, it's going to be a growing complex exponential. If sigma is less than zero, it's going to be a decaying exponential. If omega is small, it's going to be a slow, it's going to be a sinusoid that changes very slowly, where it's omega is a very large number that's going to be a high frequency sinusoid. It's going to have faster oscillations. Right. And so what we can do is we can conceptualize this in a plane where on the x axis, we have the value of sigma. And on the y axis, we have the value of omega. And so like we just said, if you're in the right hand side of the plane, which means that sigma is greater than zero. So here, sigma is greater than zero. This means that the complex exponential grows with time.

And so it's going to be an example like this where we see the, it's going more and more, it's having higher and higher amplitude. Whereas if you're in the left hand side of the plane, so that sigma is less than zero, this is going to correspond to a complex exponential where it decays over time. And then similarly, again, we know that omega here, omega sets the frequency of the sinusoid. And so if omega grows, if you go up on this y axis, that corresponds to faster oscillations because now the frequency of your sinusoid are going to be higher. And so for a complex exponential with some value of sigma and some value of omega, if you plot that sigma and omega somewhere in this plane, you can conceptualize the signal. If the sigma value, if you're at this point of plane, then sigma is negative, so it's going to be a decaying exponential. And the frequency omega is relatively small, so it's going to also weight slowly. Whereas if you're up here, then you're going to have a growing complex sinusoid that has a pretty fast oscillation because you're high on the y axis, you're high on omega. Okay.

All right, everyone. So today we're going to finish going through the sides of this discrete brain machine interface that we started talking about last Monday. And then after that, we'll start doing the tool derivation, which will be used for the project. All right. Before diving in, does anyone have any questions from last week? All right. And then I received a few emails asking about the participation grade for this class, especially some are some are not in the same time zone and therefore can't attend these lectures. And so what this means is for this quarter during the time of COVID, I'm just going to give full participation points with understanding honor system that you'll watch all the seminars, which is typically how I would give the participation points in the past. All right. And so really for this class, you'll be graded on the project, but you'll have received the 80% participation points automatically since this class is now being taught during an extraordinary time. All right. So I want to remind you where we were last Monday, which is we were talking about this delayed reach task where we have a monkey. The monkey touches and holds a center target. And then after that, a target appears somewhere on the screen. In this example, it appears below the center target. And the monkey during this time period develops a plan to reach towards this target. And so from the delay period to the go queue, the monkey is planning to reach to downwards target. And then after if he gets the go queue, then the monkey is free to actually reach to the downward target. And so during this phase, the monkey actually reaches to the downward target. And the reason that we're distinguishing these two is because we want to build a brain-mishing interface where the monkey just plans to reach somewhere and we can decode that immediately and then show that and say, okay, he's thinking about going down.

And the reason that this could be a really cool brain-mishing interface is because you could imagine a system where then someone who is paralyzed is looking at a screen full of targets. And the target has a letter. So this could be a communication prosthesis where they're trying to type. And instead of needing to imagine a reach towards each letter, which would take more time, the person controlling the brain-mishing interface could just plan to reach to each target, plan to reach to the target with a T, then the target with the H, then the target with the E. And we could decode those automatically if we know how to decode planned reaches. Any questions there? All right. So this is a video of the task that I showed last time. I'm just going to show it again to refresh memory. All right. So again, when the target is small, he's planning to that target and then when the target becomes big, he reaches. But that plan of tippy is what we're going to decode. Any questions here? All right. So what we do then is we implant the Utah electrode array. We talked about in a prior seminar lecture into the motor regions of the monkeys brain. And so this area here is the motor cortex. And we're going to aim for a particular area called the pre-motor cortex. The pre-motor cortex is a part of motor cortex. And what pre-motor cortex represents are actions made, are things related to movement that are made prior to actually making the movement. And so plan activity, for example, before you actually make your movement is represented strongly in the pre-motor cortex. All right. So I'm going to then show you what the spikes tend to look like for a typical pre-motor cortex neuron. So this PMD here is an abbreviation for pre-motor cortex. And what we would do is we would have the monkey make many reaches to one target. In this case, let's call it the upright target. And when he makes these reaches, what we're going to do is we're going to record from the neurons on that Utah array. And so what we're going to show here is just one neuron where we do many trials. So remember also last lecture, we talked about how when we want to get the firing rate of a neuron, a measurement, because the neural data is noisy, we need to do repeated trials and keep measuring what the spikes look like to get a less noisy view. And so what happens is we measure from one neuron in every single row of this spike raster that we see here corresponds to a trial. And what you'll see is that in this phase where the monkey is planning, that's when the small target is shown and the monkey plans to it, there is some increase of activity for this neuron. And then when the monkey actually makes the movement, which corresponds to when the go to is given the monkey actually performs a reach, you see all of these neuron, sorry, this one neuron is firing many more spikes and it does so consistently trial after trial. And so if we just calculate from these what the average spikes per second are, what we'll see is that when the monkey plans this upright target, there's a brief increase in activity when he plans and when he moves, there's a big increase in activity that then comes down. Okay, any questions on this plot that I just explained here? Alright, so what we could do is this is for just upright reaches, so this is for upright, we could do this for reaches in all directions. And so I could have the monkey reach up many times and record the activity of this neuron. And what you're going to notice is that when he goes to different directions, whether it be like for example upright versus let's call it left, there is very different to plan activity. So when he goes upright or maybe even right, during the plan activity shown here in green, you see that it increases to some intermediate level.

But when he reaches the left, the plan activity stays low. For up left, it also stays low. And it seems to increase more when you go from a leftward reach towards a rightward reach. Okay, any questions there? Alright, so this is going to be the key feature that we use to then do a decode. Because now if I record from this neuron and I ask and I don't know what target the monkey is planning to reach to, but I record from this neuron and the neuron is quiet since it isn't firing very much, then I can guess that the monkey is probably going to go left or up left or up. But if the monkey if the neuron is firing many spikes per second, then I'm going to guess that the monkey is planning a rightward reach or an upright or a downright reach. Okay, any questions there? Alright, looks for the graph on D. Yeah, looks down like on sort of line on the top. Oh yes, this is showing the monkey's hand position. I think it's showing his X hand position. So this is the X position of his hand. And so it's showing that he starts off in the center. And then when he actually makes this reach up into the right, that his X position is going to increase rapidly and then go to a steady state value. So it's going to increase until it stops here at this X position corresponding to where this target is. Any other questions? Alright, so now we are recording from many neurons with this Utah ray, we get 96 neurons.

And as the monkey is in the baseline state, then in this window, this is going to be the plan period for the lay period. We're going to measure spikes from all these 97 neurons and then the go cue comes and then the monkey is going to make a reach. So this is a reach. And really, the trick is going to be in this plan period, there's some interesting information here that I need to figure out how to decode to predict where the monkey is planning to reach. So let's revisit that neuron that we were just looking at on the prior slide, where remember for the right upright, the right and then maybe the downright target to the neuron fired at a higher level, but for the three targets, like the left, the up left, and the up target, the neuron fired at a lower level shown here in green. So the color reflects how hard the neuron is firing. Right. And so, sorry, I forgot to explain. So this is just looking at the targets in 2D space. So the x axis is x position, the y axis is y position. These are my seven targets that he's reaching to over here, these same seven targets. And we're saying that one way that I can represent the planet of the firing rate is that these targets have a lower firing rate than these targets that have a higher firing rate. Right. And so exactly like we said, if we have this one neuron and I measure that you're that is firing at 100 spikes per second, right. Then I can guess that he's probably reaching to this target here, which corresponds to letter E, because it's going to be E or A. Right. It's not going to be A because A is in this region where it's 40 spikes per second. Okay. Any questions there? All right. So then if you look at this, you can see that there's some ambiguity here. Right. If you measure that the neuron fires at 40 spikes per second, it's going to be very hard to know the monkey was going to see B, A or G, because all of these correspond to when the neuron fires at 40 spikes per second. And so then the way that we get around this is that we don't record just from one neuron. We record from 96 neurons or 100 neurons, 100 electrodes. Right. So each of these will be the tuning curve of one electrode. All right. And if each electrode, if each neuron had the same tuning curve as we show here, we would be totally doomed.

Hi, good work. Hi, I don't know if you can. Good, how are you? I'm good. Happy to take any questions any time. I don't have any questions for regarding the course material. I didn't have a question or two that I wanted to ask. It wasn't related to the course. I'm going to go ahead and wait until the end maybe or something. I'm happy to take those questions. My question was about number two. In part B, it says that the peak to peak amplitude is 110 millivolts for this new alien neuron. I just had a general question. I'm wondering if I'm interpreting the material we had in class correctly. For a regular action potential, it goes from negative 65 millivolts up to is it positive 55?

It goes from the top of the curve to the lowest point. That's correct. We haven't covered this up to the hyperpolarization of the action potential. We have a particular example of the resting potential 50 plus to EK plus. For two B, when they say that the action potential has a 110 millivolts peak to peak, that should be related to part A. You were able to calculate EK plus and ENA plus and you know that the voltages can't go beyond those bounds. If you've calculated in part A, that the range from EK plus to ENA plus is less than 110, then you know that there's a contradiction there. I couldn't understand the relationship between the two parts. I had another question about myelination. Maybe this is not as related to the homework, but what is the difference in terms of specifically demyelination for Parkinson's disease and multiple sclerosis? For Parkinson's disease, so for multiple sclerosis, I believe that, sorry, let me just look at the Parkinson's demyelination. I don't know the answer off the top of my head, but I wonder if for MS, it's for primarily motor neurons. For Parkinson's, Parkinson's is implicated in some of those deep brain areas. I wonder if it has to do with neurons. I think it's the locality of the neurons. Yeah, it's actually specifically, I think this is a stancho Niagara.

That's okay. Or yeah, and the dopamine energy can neurons specifically in that region. Great. Perfect. Thank you for that. Great. All right. I'm not quite sure about like the page 14 of lectures, which is the. I think that I own see. Yeah, I could hold the slide up really quickly. I know exactly what site you're talking about, but just let me. Slide 14 lecture three. He's loading from you right now. Why don't you ask your question while I bring it up? Yeah, so I think you mentioned like the main driver of like how it works is the size, right? It's like the size in a pile.

Energetics is a mix of energetics and size. So for the potassium channel is energetics because the lining side is low energy enough such that sodium will not shed its waters of hydration. But then for the. For the sodium ion channels, which have a high energy, binding site is also size related. Since potassium is bigger and can't pass through. I see. Could you explain again how the energy thing works? I'm not very familiar with all the dining stuff. Yeah, of course. So here's that slide. So what happens is that. For a low energy binding site. So that could be like a binding site to a carbonyl like this oxygen that doesn't have an explicit negative charge like this one is a high energy binding site. But binding to this carbonyl group would be a low energy bonding site because it has a slight negative dipole. But not an explicit negative charge like this. Oh, mine is here. Sorry, let me just see if I can silence this one.

Call from wireless. Sorry. Okay, get someone else picked up. So. If the amino acid has low energy binding site. Yeah, the E in this slide is energy or electric field energy. Sorry. Yeah. Okay. Energy. So if there's a low energy binding site, remember that to pass through it has to shed its waters of hydration and NA plus holds this waters of hydration more closely because it has the same amount of charges potassium but in a smaller. And so NA plus holds many more waters and so if the energy site is low energy, then it's not favorable for it to release all of its waters around it. However, potassium has relatively less water around it. And so it can shed its waters to bind to a low energy binding site. And so that's how K plus can pass through a low energy binding site but NA plus cannot. Does that make sense? Yeah, I'm just thinking about how it works like. Does like the high energy binding site kind of takes the iron out of like the water shell or.

Yeah, that's a picture that you should have here like a when when this NA plus gets sufficiently close. Oh, minus is so attractive that it's going to come down to bind to it, but to bind to it, it has to shed these waters around it so it can make it close bond to it. And so. Yeah. And so. That's that's how the binding site works. And. And the reason. K plus doesn't pass through a high energy binding site is site. Yeah, I believe size is the biggest factor. There may be also things related to confirmation, but. But some textbooks say primarily size. There's one other thing interesting you'll note, which is. I can find it. Yeah, we wrote that K plus channels are 100x more permeable, both the K plus and NA plus. Whereas NA plus channels are only 10 to 20 times more permeable to NA plus and K plus. And so it seems also based off of this that. The selection via the energy site is a stronger filter.

The NA plus channels which are high energy and would cause both NA plus and K plus to shed the waters of hydration. But then K plus is filtered mostly to the size, but then that filtering is not as robust. I see. So you're saying like this property is like way merged from the. Sorry, what were to use there? This is this property is what? This is an emerging property from like the. Energy like by. And the size of the. Iron channel. And the property being that the energy binding is more is better filter. The permeability is like like an observed property of like the energy and size at work. Yes, yes. That energy and the size filtering lead to these select permeabilities. I see. Okay, that makes sense.

All right, everyone. We're going to get started for today. A few instances before we begin. The first is that homework number two was uploaded to CCLE on Friday and it's going to be due this Friday, October 23rd, uploaded to Gravescope by 1159 PM. In general, homework solutions are going to be posted after the late deadline has passed and so the TAs will, they have already uploaded homework number one solutions to CCLE but if not, they will certainly be by the end of today. And then lastly, if you're going to be using late days for an assignment, you don't have to notify us. We keep track of that via the Gravescope portal and we count those late days at the end. We don't have to notify us if you're using a late day. All right, any questions before we begin? All right, so at the end of last lecture, we had to start to talk about systems and so we had defined a system as something that transforms some input signal x of t into an output signal y of t. This should say signal map system. And last lecture, we just went over a few example systems and so I'll just recap one of them. One of them was AM radio where we may have some message xt that conveys information that we want to convey and the AM radio system what it does is it takes your message x of t and will multiply it by a signal cosine 2 pi fct and then y of t then would be the output of our system where again the message the input x of t has been transformed through multiplication by this cosine. All right, so that's just an example system and we'll talk about we'll talk about today several properties of systems. And so on homework number two and throughout the rest of this class will sometimes be asked to show some proper that some systems exhibit various properties and so today we're going to start talking about some of those properties and so a first property is called stability. Okay, and so a system is called bounded input bounded output stable or bibo stable if every bounded input leads to a bounded output. Right, so that's the definition of stability. Let's unpack this and see what it means. So the first thing we want to define is what is a bounded input or a bounded output. All right, so a bounded input is a following. Some input x of t is a bounded input. If there exists a constant n subscript x such that the following is true. The absolute value of x of t is less than or equal to this constant m subscript x which itself is less than infinity and this is true for all time. So let me draw a picture then of what a bounded input, sorry, what a bounded input looks like. So let's say that we had some signal, draw more straight line. Let's say that we have some signal x of t here. x of t is going to be called a bounded input if I can define some constant. And so this constant would take on the value mx on the y axis. This value here is mx. If x of t its magnitude is always less than mx. So if x of t stays below this line then it's going to be a bounded input. And that makes intuitive sense. If bounded means that I can find a constant such that x of t goes no higher than that. And similarly a bounded output is the following y of t is a bounded output. If there exists a constant m y such that similarly y of t is absolute value is less than or equal to m subscript y which is a finite constant for all time. Any questions on the definition of bounded input or output? Alright so then the system is called bivostable when a bounded input meaning that the absolute value of x of t is less than a constant which is less than infinity implies that the absolute value of y t is less than a constant which is less than infinity. So that's what bivostable means. Give me an input that I can bound and it's going to be bivostable if I can also bound the output. Alright so let's go ahead and do some examples. So the first one that we'll do is a and radio where again a and radio means we take some input x of t and the system is I'm going to take x of t and multiply it by cosine of omega ct and together that's going to give me my output y of t.

Right? Let me just go ahead and make my t is copus one second. So tonwards of cosine. Alright so we want to show whether a and radio is bivostable or not. Right? And so if a and radio is bivostable what that means is that if my x of t is bounded then y of t is going to be bounded. Alright? So to show it this is stable I'm going to start with the assumption that x of t is bounded. So I'm going to assume that x of t is bounded. So it's less than some constant and x. Alright? Now if I take this assumption and through math I can show that a y of t is also bounded by some constant. Then I've shown that a and radio is a bivostable system. Right? So let's go ahead and see if we can show this to be true. Right? So if x of t is less than mx then we'll follow mathematical argument. If I were to take the absolute value of both sides I would get that the absolute value of y of t is equal to the absolute value of x of t times cosine omega c times t. Alright? And this we know is equal to the absolute value of x of t times cosine of omega c t. It's absolute values. I can still have the absolute value. Right? Now remember for bivostability I want to show that the absolute value of y t is less than some constant. Alright? So can someone tell me what a next step would be if I want to show that y of t is less than some constant?

Feel free to either raise your hand or write it in the chat. Great. Okay. So Andrew says explain that cosine is bounded by negative one and one. Okay? How on with the same thing? So yes, we know that cosine omega c t this is a function whose maximum value is one and whose minimum value is negative one. And so for and therefore the absolute value of cosine omega c t has to be less than or equal to one. Right? So that means that I can write that this is less than or equal to the absolute value of x t times the maximum value of this term and the maximum value of this term is just equal to one. Okay? And then I'm going to use my last relationship here. Here I have assumed that x of t is bounded so x of t can be written as less than some m x. And so therefore this term absolute value of x of t is also less than or equal to m x. And so this is less than or equal to m x. Right? So we show that as long as x of t is bounded, then when we plug in the algebra for this system, we also get that y of t is less than some constant m x. And therefore y of t is bounded. And since bounded x of t leads to a bounded y of t, we can say therefore a m radio. The a m radio system is stable. Right? Okay, any questions here? All right, no questions. So then I'm going to go on to the square. So the square is y of t equals x square. And so therefore I can write that the absolute value of y of t is equal to the absolute value of x squared of t. Right? And this is equal to the absolute value of x of t squared. All right? And I want to show that the system is by those tables. So I'm going to again assume that x of t is less than or equal to m x. If that's the case, then this expression, the absolute value of y of t is less than or equal to m of x squared, which itself is some finite constant. Right? It's a number of times itself, m x times itself. And so since I can bound y of t by a constant, then y of t is also non infinite. And therefore I can say that this system, the square is also by those tables.

All right, everyone. So it's been two weeks since our last 189 class. I was going to pick off where we left off, but before that, I want to stop to ask if there are any questions about overall class logistics, anything about this class. All right. So what the rest of this class will look like is today, we're going to finish deriving the tools and equations that you need to build a brain machine interface for cursor control. And then next week, we're going to release the project. So next week, I'm going to release the project assignment on CCLE. And what we're going to do is in class, we're going to go through the data together, be sharing my screen. So the project has three tasks. And what I found in the past is that it really helps the students a lot if we do want to pass together. So the project has three tasks and we'll do tasks one together for the project.

And then hopefully that'll give you a pretty good jumping grounds to do the rest of the project. Which again, should not be extremely time consuming. I would say it's about equivalent to two ECLE 102 problem sets. And you all, you have several weeks to do the project. All right. So lastly, left off, we were talking about the problem that we want to solve, which is equivalent to that video that I showed you at the very start of class of the 52 year old women who was controlling a brain computer interface, the cursor on the screen to type on a keyboard. Right. So we said that this is called a regression problem. And what we want to do is we're going to have data where we simultaneously record the position of a cursor on the screen. And the information that we're going to get out is velocity.

So we had this vector x k, we're k denotes time. And x k is going to be a 2d vector that contains the x velocity at time k and the y velocity at time k. And we're going to have x k, x k plus 1. We're going to have this for all time. And simultaneously to this, we're going to have neural data, which are spikes from 192 neurons. And we talked about how this data will become formatted as a spike raster. So again, next week, when we go through the project data together in class, we'll actually see this spike raster matrix where every single road corresponds to a neuron. So this is the spikes of neuron 1, the spikes of neuron 2, the spikes of neuron 3. And every column corresponds to a millisecond in the trial. So this is the first millisecond, the second millisecond, the third millisecond, etc. And what we do to count the neural data is recall we want to get a firing of rate. And so what we do is we count the number of spikes that happen in some window. So last lecture, we said, let's call it 50 milliseconds.

So in 50 milliseconds, I'm going to count how many spikes happened for neuron 1, 3 spikes happened for neuron 2, 3 spikes happened for neuron 3, 0 spikes happened all the way down to 192. All right. And so this is going to be YK, my vector of neuron firing rates at time K. All right. And the goal then of this problem is when we want to build a brain machine interface, we want to calculate some function of Y of K that predicts my velocity X of K. All right. So this is saying, given that I observed some neural data in the future, I want to pass that through some function F that tells me what my velocities should be. And that then lets me do the following. Let's me record a paralyzed person's neural activity. Pass it through a function F to get the velocity of the cursor and then update the cursor's movements on screen. And so that was very yet. We were going to try to learn this function F. And so we talked about just how to do a simple 1D example.

We're going to derive the answer in 1D. And then I'm going to tell you the answer when you have multiple dimensions like 2 and 192. The reason we're going to derive the answer in 1D and then I'm going to tell you the answer for 2 and 192 is because in the 1D case, we're just working with scalars, which we can differentiate. It turns out that you can differentiate with respect to vectors and matrices also, but this requires a bit more formalism. And so in this class, we're going to develop the principles to derive it in 1D. And I'm going to tell you the answer in multiple B, but you'll see that the answer in multiple B is like a multiple dimensional generalization of the 1D example. So in the 1D example, let's say that we just had neural data from neuron 1. And so that's why super-ship 1 of k. This is the neural data from neuron 1 at time k. And on the y-axis, we have the x-flossy at time k.

And then we said we had the monkey or the human do experiments. And at every single point in time, we recorded the number of spikes that neuron 1 fired and the velocity of the cursor. And so this could be for k equals 1. Here for k equals 2, we have this neural activity and this velocity for k equals 3. We have this neural activity and this velocity. And we want to learn a relationship that tells me how to get the x at time k from y1 at time k. So we want to learn some function f that tells me the x at time k from y1 at time k. Any questions here? All right. So last lecture, we decided we would just go with a simple veneer model. And that's what we're going to do in this class. We'll talk about later on in this seminar lecture how to make this non-linear, which is actually very easy after we do this simple veneer example.

So we'll start off with a linear example. And so last lecture, we said that if we model the velocity as a linear function of the neural data, right? So in calculus, we had y equals mx plus b. Here it would be the velocity, kyw is equal to a times the y1k, which is the x axis value, plus some y intercept b. So b here is the y intercept and then a is the slope of this line. And what we want to do is we want to say in our model, if I model it to be this way, I get to choose a and b to make this fit as good as possible. So that when I later on observed neural data, I predict the y velocity well. And so we said, if I choose a and b to be, so if I choose b to be this value and a to be this slope, this red line is a pretty good. And so this is a very bad choice of a and b, whereas this green line here is a very bad choice of a and b. I shouldn't choose b thick and b big and a negative.

And I asked at the end of last lecture, we can obviously see that the red line is better than the blue, sorry, the red line is better than the green line because it passes through more of these data points. These blue data points seem to show an awkward trend. But then if we want to be able to state this mathematically, we have to be able to define how good these lines are via rigorous math. All right. And so the ideas we talked about here are the basic ideas of machine learning. And so I asked class last time, how is it that we know that a is good and b is, sorry, that red is good and green is bad in a mathematical way. And so I'm going to at the end of last lecture said, well, what we could do is we could look at the errors between the blue points and the predictions from the red and the green lines. And so let's say that this point here is my 10th data point. If I had the neural data at this value, right, my red line would predict that the velocity should be this value, whereas my green line would predict that the velocity should be this value, right?

And the red line has a smaller error to the blue point than does the green line. All right. So let's say this blue point was for k equals 10 to 10th data point that we have. This error at salon 10 from my blue line to my, sorry, from my blue dot to my red line is smaller than from my blue dot to my green line. All right. And now it might not always be the case that every single data point is closer to the red and the green. We also, let's say we have this purple data point over here. Let's say that this was the 11 data point. So we have Y1, 11 and then the X time 11, right? This actually has a smaller error to the green line than it does to the red line. Right. So this epsilon 11 is smaller has a smaller error under our green bad model than under our red good model, right?

All right, everyone, we're going to get started for today. So a few in essence, just one announcement actually before we begin, which is a reminder that homework number one is do this one day up the fatigue rate scope. Any questions on any course logistics? Question from Jonathan. Yeah, I don't have a question on administrative stuff, but I have a curiosity on some of the biology. This is an okay time to ask that. Sure. Okay, so one of the things that we've been talking about is myelination of axons and nodes around VA. And I'm curious to know what is the scale of the difference in size between a myelinated section of axon and a node of around VA because a node of around VA can't be like a single point in space. It hasn't have a size.

So do you have any idea of like what's the scale of maybe how many nodes of around VA are on an axon and how much larger the myelination sections are? Yeah, so the myelons used to appear to be one to two millimeters. And then the nodes of around VA, the bare patches are about two microns on length. Oh, excellent. It's right on this slide. Thank you. Great. Yeah, thanks for the question, Jonathan. All right. And then any other questions? All right. So today, we are going to finish in the first half of lecture, the basic neuroscience, and then after that, we're going to move on to talk about experimental setups and get into firing rate statistics.

All right. So just a recap last lecture, we talked about passive properties of neurons and how they have membrane resistance. Those are the ion channels that go from that take ions inside to outside the cell or vice versa. They have a membrane capacitance because the cell wall has positive charges and negative charges separated across two plates, which reminds us of a capacitor. And we talked about how these sets the dynamics over which the membrane voltage can change. And then we also had talked about how propagation happens or the types of parameters were interested in terms of neuron propagation, how there's this internal axial resistance and that along with CM, first the axial resistance and RM dictate how far a charge can propagate and then RAMCM detect RAMCM may affect action potential speed. And so we had also discussed the slide here on action potential velocity and how the action potential velocity is a function of parameters of the cell, including the cells by ammeter as well as the thickness of the cell wall or the cell wall plus myelination.

And I want to pause here and ask if there are any questions recapping anything from last lecture. So Professor, when you were talking about how the action potential speed is correlated to one over RA times CM, do we not take into account RM in this scenario? Yeah, so in this scenario, this has to do, so when we say velocity, we are conceptualizing how along an axon, the action potential spike travels along the length. And so RM, the resistance here is certainly going to affect the voltage, affect how far it propagates. But RM here in terms of time only affects how quickly the voltage will change from inside to outside the cell. But then for action potential speed, we were curious as to how this voltage changes propagating inside the cell. And so that's why the parameters of interest will be the axial resistance and the membrane capacitance, but not the membrane resistance. So the membrane resistance does affect the overall action potential amplitude.

Any other recap questions here? That's a great question, Brandon. All right, and so the last slide that we ended on last time was talking about the nodes of RON-VA, which is how action potentials can propagate over very long distances, even though the voltage is attenuating and can convey that information with high fidelity. And so because information is not certain, the amplitude of the action potential, only on the fact that action potential happened, we learned that the nervous system uses these nodes of RON-VA to essentially regenerate the signal every one to two millimeters. And so at first off, we talked about how the mile in sheep's, what they do is they effectively increase the width of the cell wall. And therefore, they decrease the capacitance because now our positive and minus chargers are further away from each other. This distance here has increased. I think I called this, oh yeah, deep. And so because of this lower capacitance, there is faster propagation.

And also there is less charge that needs to go into filling the capacitor because the capacitors are smaller. And so what this means is that across these mile in intersections, the action potential both travels faster. And even though it attenuates, it attenuates less slowly. And so it's able to make it to the next node of RON-VA, where as long as when we make it to this next node of RON-VA, the voltage is above the threshold needed to start a positive feedback loop, these nodes of RON-VA are filled with a ton of voltage-gated sodium ion channels. And so as long as the voltage is above threshold is going to re-initiate the feedback loop, that then will regenerate the spike and the spike can travel down the mile-innated axon until the next node of RON-VA, etc., etc. Right? And so that's the mechanism by which essentially you have a repeater, this node of RON-VA, you can think of that as a repeater that regenerates their signal every so often and allows you to then convey this action potential over very long distances, including axons that go from your brain all the way down to your toes.

Right? Any questions here? All right. And I saw a question that Chad, a great stature, because of decreased resistance, and Tom William said, reduce capacitance, also reduce resistance, or if you were able to reduce the axon resistance, you would also increase the action potential speed since it's inversely proportional to both of them. But for the particular mile-innation, it's because of the reduced capacitance, like Tom always said. All right. Any questions from any of this recap? Question from Shishank. Prof. Shishank, the technique that you have marked there, isn't it only supposed to be the technique of the mile-in sheet, and are in the charges supposed to be positive on both ends and negative on the interior of the cell wall?

Thank you, Shishank. That's a really great point. I've drawn this wrong. So Shishank is right. D is this distance here. This is not inside and outside. The inside is just inside the cylinder. Thank you for that correction, Shishank. All right. Any other questions here? All right. So now we're going to head on to just the last topic, which is throughout these early lectures, like Kat saying, that we're going to see the action potential at different 10,000 foot view, or 1,000 foot view, et cetera. This is going to be our most in-depth view at the action potential now, and we're going to talk about the key dynamics that be to the action potential shape.

And so we've talked a lot about how action potentials are generated, and we have a hint of several things, and we've talked about how the surprising edge is due to the influx of sodium. And we've talked about how the falling edge is due to the efflux of potassium. It's interesting to think about how they originally came to these conclusions and how they originally mapped out the dynamics of the action potential, which will, again, give us a even clearer picture of what's going on during an action potential being generated. And so there were experiments, which we're going to talk about, but researchers back in the range of the 50s and 60s were interested in learning exactly what types of flow of ions led to action potentials. And they had some clues, for example, if they had low extracellular NA plus potential, and that would be to a low action potential amplitude, which made them think that the rising edge of the action potential is caused by the influx of NA plus. And similarly, they had evidence about K plus being responsible for the falling edge. But then the reason that this is hard to study is because of these voltage gated ion channels.

All right, everyone. We're going to get started for today. A few announcements before we begin. The first is at homework number three with uploaded to CCLE last Friday. It's going to be due this Friday, uploaded to grade scope by 11.59pm. On grade scope, you also have the option of submitting homework regrages. And typically, we're going to open them for one week after we release the homework grades. And so when you submit regrages, that question will go back to the greater, the reader who created that question and the reader will be able to assess for comments and determine whether to give you back points. And so that's how regrages should be handled for this class. All right, we also saw that on Piazza, there was a post about how long the homeworks are taking you. And there's a poll that showed over 30% of students are taking 15 to 20 hours or over 15 hours to do the homework. And so I want to start off by saying the homeworks that we are giving are of similar length and difficulty to the ones that we gave in prior years. However, we know that this year is not normal in particular.

Of course, we're also dealing with the pandemic and with online teaching. And we understand how that could lead to homeworks taking a longer amount of time. Given the poll that was on Piazza, the TAs and I discussed how we responded this. And we propose to do the following, although I'll be happy to take any other feedback. First, although we've released homework number three already, what we're going to do is effectively shorten it to be 80% of the original length by giving you full points to these three questions on the homework. So these three questions comprise 20% of the homework points. And you don't have to do them to receive full credit. And so if you left these questions blank, you'll get the full 20% points on those questions. That said, we still encourage you to understand how these questions are solved and to be able to replicate their arguments because they're very game for the exams. The TAs and I will also look to shorten future homeworks. And so we'll try to write for example, less sub parts for some of the questions. And then the PAs also communicated that feedback received in discussion is that MATLAB is very time consuming. And so we aim to, although we're still starting out the details, we aim to distribute a MATLAB dedicated discussion video each week as well to help with the MATLAB part of the homework. All right. So I wanted to take a pause here and ask if there are any questions about any class related logistics.

Viana. Hi. So you mentioned that those three problems for homework three are still fair game for the exam, even though that they're not graded. And I was wondering, is it possible that even though the homeworks are shorter, we have access to maybe the original problems that would have been on them so we can have additional practice for the exams? That's a great question. I'll discuss that with the TAs. I guess one potential thing that we could do is we could release what the original homework was going to be but then dedicate a subset of the questions to the optional. And so not sure exactly what we'll do there. I'll discuss that with the TAs to figure out what's the best thing going forward. But thanks for that suggestion. Viana. Any other suggestions? All right. And so with that, we're going to get back to material. So in last lecture, we had talked about this concept of the impulse response. And we had this fact that if I have any linear time invariant system, then as long as I know the impulse response, which is how that system responds when I input a delta into the system. And as long as I know this impulse response, I can calculate the output of the system, Y of T. For any input X of T. Again, as long as I know the impulse response. So this impulse response is a whole characterization of the system. And last lecture, we did ride that the way this happens is via this convolution integral. And the solution integral is the manifestation of this fact on the prior slide, where, as long as I know the impulse response, H of T. I can calculate my Y of T.

Given any input and the way that I do that is I complete this convolution integral. And so this convolution integral again, and is usually, we talked about notation last week, or usually, you might see this denoted X star H of T, which is the rigorous notation. But usually in this class, we'll write this as X of T. Convolved with H of T. Since that's how most of the role that had a convenience. Right. So any questions on these concepts? All right. So we talked about the intuitions of what convolution is doing. Admittedly, this integral can be complicated. And so we talked about how to intuit it with this remonion sum slide, which is something that again, we encourage you to review those parts of the lecture videos and materials to make sure you understand those concepts. And we also talked about how to compute it using the flippin drag technique. And so last I left off lecture, we had given these example questions, and we had written the solutions of these convolutions. I'm going to do just one of them to recap how we do the flippin the drag method. But then I want to take any questions on what people, if people had any questions about how to solve these convolutions. Let's say I wanted to convolve this input X of T and this impulse response H of T. And so what I would do is I would take my impulse response. Here in green, and I would flip and drag it.

And so when I flip it, this impulse response ends up being over here. I haven't drag it yet. So it's leading edge corresponds to the impulse response. Flip and drag that time T equals or out. And if I advance this by one second, so that, sorry, if I delayed this by one second, so that now the impulse response I'll draw this in orange was at this location. That would be my flippin drag impulse response at time T equals one. Move it over one second if I moved it over. So it was over here. That would be for T equals to. And then we said for the flippin drag, what we would do is we would multiply the input, which is the black rectangle. And my impulse response. Multiply them together and then find the area under the curve. And that would be my convolution. And so when I just flip my impulse response and I'm at T equals or when I multiply my orange. Sorry, let me actually let me erase this orange one. So it's less confused. When I multiply my green. And I'm at the black rectangle. And my black rectangle, they're not going to overlap. And so they're going to be just a zero signal. And I've integrated that. It's going to be zero. And so at time T equals zero. My convolution is going to be zero. And then if I were to drag this rectangle to the left, meaning that T would be negative. And it would never overlap. And so it would be a zero convolution at the result of the integral of a zero signal would be zero for all this time less than or equal to zero. Now the interesting thing is when I start to drag this rectangle. So it starts to overlap with the black one.

So let's say I dragged it by a time T equals 0.5. Oh, sorry. I wasn't keeping track of my x-axis. This would be 0.25. And earlier when I said earlier when I said that the orange wrecked was over at time one I met time T equals 0.5 because I wasn't paying attention to these axes. Apologies for that. So if I drag it halfway over, which is at T equals 0.25, then when I multiply these two rectangles together, I'm going to get a wreck that has with 0.25 and height of one. Alright, and so at time 0.25, the overlap of these two rectangles when I integrate it will also be 0.25. And so at time T equals 0.25. The area under the curve is 0.25. So as I continue to drag this rectangle to the right as it overlaps this black rectangle, they're going to have more and more overlap leading to a higher and higher integral until at time T equals 0.5. That's this time over here. T equals 0.5. They're fully overlapping. And so when I multiply them and integrate the area under the curve is 0.5. And so this pink point is T equals 0.5. And we talked about last lecture as I drag this rectangle in the trend at which increases is going to be allowing. Alright, and then as it drags out, we're going to have a line decreasing. And then after time T equals one, which is when the rectangle. When the impulse response rectangle is at time. There T equals one. Then they no longer overlap and the integrals is there. So with this same procedure, you should be able to calculate or you may be already calculated the convolution of these three as well. I want to pause here and ask if there are any questions on any of the answers.

All right everyone. So we're going to start off for today for ECU 189. For today, if you go to CCLE, you'll see that in the week. Eight materials, I believe I've uploaded the final project for this class. And what we're going to do today is we're going to go over the project and the data so that you can be prepared to tackle it. The project, you'll see consists of three tasks at the end. And for the purposes of today, we're going to actually already do one of those three tasks and hopefully you'll see that it's not a something. And so to go through and introduce you to the data sets and the code, I'm going to be sharing my net lab screen on my computer. But my computer is right now also compiling the ECU 102 lecture that we just had and also showing my video. And so if it's running to slow, I may turn off my video for just a lecture. Let me go ahead and share my net lab window. Right. Can everyone see a net lab window here? Or can someone give me a thumbs up or I meet yourself and say yes or no? Yes. Okay. Great. Thanks. All right.

So last lecture, we had derived the equation to go from mapping neural data to kinematics to velocities via least squares. There are a few more details in that lecture about training and testing data that we didn't get a chance to talk about yet. However, I thought it would be better to just go over the project data first and if you had time, we'll talk about those things later on. But it's not, we'll cover them next week, but you don't need to know them to do the product. So the goal of today is to really just get you all the, I guess you set up so that you can do the project. So let me just move around windows. Okay. So on CCLE, we put up the 189 project PDF and there's a lot of description in there. We're going to walk through code today that essentially goes through that description. And so please read the description again, even though hopefully I will be clear after we go through some of the code. We gave you a data set, which is this J are 2015 12 of our truncated to dot max file. And so this is going to contain the data recorded from a monkey while the monkey was making reaches and we were recording neural data so that we could get training data to map neural data to reaches. We also uploaded a helper function, then, which is a supporting function that that will use to make our lives simpler. And then the critical thing that should be on CCLE is assignment code plus part one dot M, which walks you through building a decoder. And walk through that today, as well as it does part one of the project. All right. And so, let me just look at part one of the project. Sorry, I should have the PDF open.

Let me just. Part one of the project is to build a decoder with with a low pass filter, which will do in the slides. Part two is to use an intermediate or high pass filter. So it should not be too difficult to generalize the code that we're going to look at today to do part two. And then part three is more open ended. So we give. We give the idea to say what about other features of the neural data like the derivative. And so, and so we'll. We leave that a bit open ended to you all. Okay, so if you were to load, if you were to open the assignment code plus part one, what we have here is a bunch of map code, which walks you through the data set. So we're going to go through that all together now. I can't see the chat and I can't see raised hands. And so if you have a question, just unmute yourself and please, please feel free to just ask it. So what I'm going to do is I'm going to go ahead and load this data. So I'm running this first cell. And what you should see is that it drops. A data variable called big R into our workspace. Right. And so what big R is, is it is an array of structures. And so if I just type R. What you're going to see here is that R is a one by five hundred and six structure ray with these fields. Right. So you may not be familiar with trucks or raising or you supply via familiar with the raising that what this means is that R is something an array that has five hundred six components. So I can reference R of one. This is the first component are I can reference R of two. This would be the second element of R. Now every single element of our itself is a structure. And what a structure is in MATLAB is it's. You can essentially think of it as a data structure that hold some custom data that we wanted to write down. Right. And so first there are five hundred and six of these structures because again R has five hundred and six elements.

So each of these elements, each of these are corresponds to one trial that the monkey did. And so we're giving you a data set where there are five hundred and six trial. And then for every single trial, we write down information. So if I want to access the 10th trial, I would do R of 10. And so what are 10 would show me is information about the trial. And so it would include information like where what was the target that the monkey reached to on this trial. And so if I look at if I asked you what target is the monkey reached to on the 20th trial, you would do our 20 and you would look at the target field here. So again, our 20 is a struct and then our 20 dot target is going to be a 3D vector where the 3D vector tells you the X, the Y and the Z position of the target. And so the position is always going to be minus 70. So we really just care about X and Y. We just care about 2D. And so in this first cell right here, what I ask you to do is I ask you or what we do in the code. I don't ask you to do this. We say what are the targets of the monkey reaches to. And so what I've done here is I'm going to iterate over every single trial. Now R is just going to return 506 a number of trials. So for a for loop over trial, 1 to 506. I'm going to plot the X position of the target and the Y position of the target with a marker. So if I run this cell. I'm running right now and it's normally not the slow, but again, my computer is doing too many things right now. Do people see a figure has appeared? Or do you only see the mat lab? You only see the mat lab. Okay, let me show my entire desktop. I'm running a share screen only shares the current screen that you're on. Yep. Right. So I'm sharing my desktop now.

Let me run this code again. I can see the traffic. All right. So do you all see a plot now that has appeared? Yeah. Yes. So this again is iterating through all 506 trials and it's plotting the target location of that the monkey was reaching to. I can see that there are only nine unique targets. The target is going to be either zero zero and X position. 84.85 comma 84.85. This would be the upper target, which is zero and X position and 120 millimeters and Y position. And so there are only eight unique targets at the monkey reaches to at the periphery and then center target. All right. So those are the targets the monkey reaches to. We also have fields like if I were to do our. We must look at the 20 trial again.

There's a trial like is successful. And this is a Boolean is either one of the monkey successfully acquired a target or zero if he didn't. And so the reason that we have this is because. You can imagine if the monkey was failing a child, we may not want to use that data to learn our model. And so. What I could do is I could define the successor ray, which starts off empty. And then I'm going to loop over all the trials and then every single time. I. In the for loop, I'm going to just copy down the value of. I got it successful, which is going to be one if he was successful and zero if he wasn't. So if I run this code. It's going to loop on through. And if I look at now what success array is, what you're going to see is it's a 506 dimensional. Array or vector vector where every single element is a one. And so if I do the sum of success array to the equal 506 and that tells you that every single trial that I've given you was successful.

All right. We have quite a few people here so I'm happy to do a race and system where if you raise your hand then I can call on each of you and Brandon you can go first, and you should be able to meet yourselves. Yeah, so, Professor cow I was just wondering, um, what's the monkeys name in the video. That monkey was named George. Yeah. And then we'll show you later on when I show you my experimental setup, which was the rig that I had built actually in that same area, but it's a totally modernized for 2D cursor control. You'll see a video later of a monkey called Jenkins. And so usually in the papers, we just write monkey G or monkey J. But yeah, each of the monkeys have had their own unique personalities and whatnot.

Cool. Arthur. Hi, Professor Tao, I had a question on number five on the homework really quickly. Great. Let me just pull it up. One second. Sorry, so the homework is just downloading for me right now. It's taking some time. I'm not sure why it's taking so long. Let me just try to save the pdf. Cannot be opened. Okay, sorry. Let me just open up an old homework number five from homework one from prior year. All right. I wanted to ask about like for the myelin sheets for like part B or does the myelin sheets around the axons and neurons strictly improve the conductance of the action potentials or is there like drawbacks to it? Yeah, so for, Sorry, you're saying this is for 5b, right?

Yeah, because I was wondering, like, or that question was just in general, but for like 5b, I was wondering, like, what does it mean when it says, like, is it worth it? It's like, are we comparing, like, do we want species A versus species B? Or I was just confused about the wording of that. Yeah, so for 5B, a potential con of myelinating is that it takes up more space. And so, looking at the answer in 5A, we wanted you to see that, because we know that action speed is going to increase if we increase the diameter as well. And so both of these things take up more space but which one is better. And from the calculation five a you should have seen that it was better to have myelination, then just the compared to the wider diameter neuron. Yeah. And so therefore, myelin helps.

Can you explain again why, since they're both thickness of like 1000 micrometers, right, when you do the calculation, can you explain why it's better to have the mile and sheath than versus the bigger diameter? So I'm just wondering how to, how to show this without showing the actual answer by sharing my screen. What you should see is that. The other one in in we accept both solutions so in one case you might find that you have in a little are for the radius in the diameter, sorry in the denominator, or else, or else, a radius squared term in both cases, it's the what the neuron that has more myelination is going to get a much bigger change in speed, since you're changing one of the denominators by a factor of 25, compared to about a factor of two, or a factor of four if you had the R squared in the denominator. or four if we had the r squared in the denominator. Okay, thank you. That helps a lot. Okay, great, yeah. All right, while we're on this, can anyone unmute if you have any further questions on question number five and then after that we'll continue to go down the list. I actually had a question on this one as well. Okay. This one is like the last part of five.

Okay. Well, I mean, five A and five C. Five A, do we have to draw an RC model? Sorry, this is for question five A you said? Yes. For question five A, when you say draw an RC model, you mean do you have to draw a resistor and a capacitor circuit? I quite literally do have to draw one because I use like proportionality to reason through it. Yeah, you don't have to draw it. All you have to do is calculate rA times cm. And so I, since I will set the time constant, so you don't have to do any drawing of a model here. Okay, great. And then for 5C, is the y at the end, just a repeat of the first question asking what is the purpose of these nodes?

It is. We should remove that in future homework. Yeah, we're just asking what the purpose of these nodes are and how do they function. Great, thank you. Great, thanks, David. I also had a question on 5A. So for the area for, so both the resistance and the capacitance use an area, and I want to clarify that in this scenario that they're different, right? That is correct. Yeah.

So for the capacitance, the area should be 2 pi r times the length. For the resistance, the area should be a pi r squared. You think you'd draw that out? Yeah, sure. Let me grab my iPad really quickly so that I can share the screen. So, I'm just going to join the Zoom room. Let me share my screen. Let's do one more thing, which is stop the audio of my iPad. All right. Alright, so for 5A, we want to compute an R axial times C membrane.

And in this particular question, we should look at And so our axial is going to refer to this equation, where it's going to be our a which is some row, which is going to be a constant that's the same for for both neurons, and divided by the pie a squared so this will be the surface, the A here would be this radius right here. And the axial resistance is going to be a function of the area of the cylinder. And the intuition for that again was that if we have, if you have the same amount of ions there are less things in the way for your na plus to move through and so if there is more space than the resistance should be lower. And so that was the rationale for how we got this row pi squared. So this quantity is going to equal row times. which was our, where's the slide, yeah, little rA, which was our resistance in a particular, for a particular length in Cm, had the equation rho, rA equals rho over the area, which is pi times R squared.

All right, everyone. We're going to get started for today. So just one announcement. One second. All right, great. Just one announcement before we begin, which is that homework number two is do this Friday uploaded to grade scope by 11.59 PM. Any questions on class logistics? All right, so we're going to continue talking about the impulse response today. And so last lecture, we had started off with the motivation of why an impulse response. And we said, well, oftentimes in life when we are interacting with the system, we may not have the luxury of knowing exactly what the system is. What is the function that it implements? Or maybe we only know it imperfectly. Further, even if we did know what as well as it could take on a complicated form. And so if we don't know what S is, how are we going to calculate what a system does to an input to transform it to an output. And so the impulse response gives us an answer to this question. It says, as long as I know the impulse response, I don't have to know exactly what S is. But if I know it's impulse response, then I'm going to be able to calculate the output, which is my Y of X. For any input given which type Y of T, Y of X. I can calculate the output Y of T for any input X of T. And so though I don't know S, I can get from X of T to Y of T as long as I know what the impulse response is. Right. So this is a very powerful tool for in particular linear time and varying systems. So the last lecture we said, well, what is the impulse response? What it is is if I take some system here, capital H, and I apply an impulse at the input.

I put the input to the system being the direct delta. Then the output of the system is my impulse response. Alright. And so this function here, this is the impulse response. So very straightforward, intuitively again, what's the app on my system when I put an impulse, that's the impulse response. But we'll see that with it, we can do very powerful things. So last lecture I had mentioned this subtlety about the difference in the T's on the left and the right hand side of the equation. I mentioned this because I made this mistake when I was an undergrating this class. And so I just wanted to explain this one more time since just to clarify it. And so if I have my impulse response, H of T, which is the output of my system H when the input is an impulse delta of T, the T on the left hand side for H of T and on the right hand side, this T here are not the same. Right. So what's the difference? So let's take a system where I have an impulse delta of T, that's my input. And it's going through a system H. And now this system H in particular is an integrator. And so the system H is going to integrate this input. We derived either last lecture to the lecture before that the integral of the impulse is the step function. So the impulse response of the integrator system is a step function. So the step function is the output when my input is a delta. Right. So the T on the left hand side, which I'll do here in green, the T on the left hand side refers to the value of the impulse response at a specific value of time.

So let's say that little T is equal to one. Right. So what the T on the left hand side is. This T over here is telling me that I want to go to time T equals one and measure the value of the impulse response. Okay. So this would be the impulse response H at time one. Okay. On the other hand, the T on the right hand side. So the T here is something that varies across all time. Meaning if I want to integrate my impulse, I'll have to know the value of the impulse from negative infinity up to time T. And so in general, the T on the right hand side is going to be a T that isn't just the finite time at one point one time point, but I'll need to know multiple values of what this function X of T or delta T was at multiple times. I need to know that to integrate it. I need to know if value from negative infinity to T. And so the output, the impulse response at a specific time T on the left hand side in here in green is going to depend on the input at several times T on the right. And so in particular, it's going to depend on the value of the input from negative infinity all the way up to time T for this particular integrator system. And so that's why you can't just plug in the same value for T here. Okay, any questions there. All right. So with that, this side again is just explaining what we said on the prior set. I'm putting it here for completion. We have a question from one. This is just more of a clarification. Like it makes sense now the different T's and stuff, but the left hand T is that the same T as the upper bound of the integral that we have inside of the H function right now.

Yes, in this particular example, that is correct. So this one. This T here that says the T in the argument of H is going to be the same T as the upper bound of this integral. I'm sorry, I think my internet went out for like a split second. I think it just did it again. Okay, it's going the T here is the same as the T here. So that's correct. I also just realized that I have a little type of it. I want to correct actually make this actually tell detail. So what you said is correct one. Thank you. And Marga. So why is the in the system is X of TOW detail? How come we're not integrating with respect to T? In this system, we are saying we want to calculate the area under the curve until a time T. So when I do an integral, I could call this whatever variable I want. TOW are just a dummy variable that tells me I'm going to be integrating along the X axis up until sometime two. Okay. Alright. Alright, so then at the end of last lecture, we were also describing a time invariant impulse response for a system that is time invariant. This impulse response will of course also be time invariant, meaning that if I put adults into the system and I get out an impulse response that looks like this. If I have a delay by time TOW sorry for these typos.

If I have a delay the impulse now by a time TOW, then my impulse response is going to be the exact same shape, but now delayed by a time TOW. Okay. And so this is a nice property that happens if we have a time invariant system. Of course, we know delay in the input means corresponding delay in the output and give me one second just to make the T is host. One second. Alright, so that's for a time invariant system. Now we're going to start to see how we can use the impulse response to show this fact that we had on this slide, which is as long, even if I don't know what the system is, as long as I know the impulse response, I can calculate my output for any input. And so before we do that, I just want to give one more terminology thing, which is something that is called extended linearity. This is a very simple idea. It earlier we had our concept of linearity, which is that if I apply a system, H to inputs X one and X to with X one X to being added, then if a system has linearity, sorry, A X one plus B X to if the system has linearity, this is equal to a times H of X one plus B times H of X to. And so I call this linearity, some people call this extended linearity, so this is for terminology. Extended linearity extends this definition of linearity to end input instead of just two. And so for extended linearity, if you have H applied to the sum of, sorry, this is correct. If you have, each applied to the sum of an, an XN, that's going to be equal to the sum of an, right, so this is this is saying that, H applied to A one X one plus A two X to all the way up to A and XN, this is equal to A one H of X one plus A two H of X to all the way up to A and H of X and. You can see definition of linearity applied to and inputs. And then the reason that we talk about this is that there's also continuous version analog, which is instead of having a sum here, we could have an integral, right. And so right here, we're writing linearity in the script form here. It isn't continuous form, which is to say if I take a system H and I apply it to a sum, now represented here as an integral of constants A, tau times. Some X of T minus tau, that's the same thing as taking the sum of my constants A times the outputs of the system applied to X, which is Y.

All right, everyone. I had mentioned, I think, I don't see style here right now. He had a question at the end of 102. When comes an off against question. Yeah, so, I hope everyone had a good Thanksgiving break. And hope most of you were able to take a look at, at least read over the project. So, I know we went over a lot last lecture in terms of just going through the MATLAB data. And so, I want to start off this lecture by just taking any questions you may have on the project setup, the project code, etc. All right, I'm, I, I plan for most of the lecture to the project questions. If there are no questions right now, what I could do is I could at least finish off.

So, let's start with lecture six, where we were talking about these squares from several weeks ago, and then give you some time to also think about any project questions to talk about. So, I'm just curious as to how many people started the project. So, I'm going to put up an anonymous poll. Although, I'm not going to be able to see the responses. So, one of the students left it, one of you will have to write me in the chat. I'm just curious if you started the project and answer yes for yes and no for now. All right, and then can someone tell me what the percentages are? Hey, how can we tell that? Oh, you all can't see it either. I think like the ton, a ton, why and all the other TAs can know we can't because we're not host. Okay, that's, let me see if I can. Let me just answer yes and see for my iPad and see if. She, let me try to launch the poll for my iPad. So, I think that I can see the results on my iPad, but I cannot see it.

I can see it on my computer, poll and progress, posts and panelists cannot. Okay, never mind. So, I don't think I can do this poll. Honestly, who has? I have it. If people are happy to share the, they start the project, you can write yes and chat. I'm not just wanting to get it. I just, I understand if not, it was Thanksgiving. It was Thanksgiving weekend. Okay, so based off of this. The project is doing a week. We'll, we'll give an extension on the project until. A Friday of next week. And I will then take the time to answer any questions you have on the project as well on Monday at next lecture. I just want to make sure you all have an opportunity to ask those questions so that the project is not.

Is not overwhelming. All right, so. I'm going to just. Overview where we were, which is something that you use for the project, which is in the project. I'm going to give you data from a monkey and. The monkey has simultaneously recorded kinematics and neural activity. And in the 189 project, we actually get out this kinematics and that neural activity. So the kinematics were the R dot cursor pause. All right. And then. The neural data is the R dot spike rasters. And so. We also talked about how we've been the data so that we get the. The velocity kinematics every 25 milliseconds. And we similarly get the neural data every 25 milliseconds by counting the number of spikes that occur in a thing.

Right. And so that's our XK and our YK. And then we talked about then. We talked about the, what we want to do is to derive a decoder, the decoder, what we want to do is we want to take new neural data YK and multiply that by a matrix L to get my kinematics, my velocities XK. Right. So I need to learn this matrix L. And we saw that if we set up this question. With this decoder, we got a linear equation X equals L Y. We're big X and big Y. Big X are my neural data's contaminated across all time. And big Y is my neural data can cadmated across all time. Which would be your neural data can cadmated across all time. And then X pin would be your neural data can cadmated across all time. So your X pin again, remember has a 2D velocities. And so we would have a VX and a VY. And you have this for the first time bin.

The second time bin. All the way up to the big K time bin. All right. And in this case, big K in the data set that we gave you is going to be equal to 16 465. And so we have 16,465 X and Y velocities. And then we also have the same thing for neural data. So for neural data, we had 192 neurons. So we have Y1, Y2 all the way to Y192. And then we have this just constant row of 1s that we talked about at the bottom. And we have this at time 1, at time 2. All the way up to time big K. So there's a Y big K. And so this matrix would be 193 by big K, which is 16 465. So then we have this big X equals big L times big Y. And then we told you that the answer, which is found in the analogous way to the least squares example.

We didn't 1D is going to be XY transpose times YY transpose inverse. All right. And we showed you how to do this in that lab. L is equal to X times Y. This apostrophe is a transpose. And then this inverse is this big minus 1. And we do Y times Y transpose. And so in the 189 project code, that is this right here. L equals, so another way to write it is X times pseudo inverse of Y. This PN of Y is equal to this expression Y transpose inverse YY transpose. And so here in the code, we do that exactly L equals X bin times the pseudo inverse of Y bin. And so this gives us our matrix L. And then that matrix L tells us how do we decode new neural data coming in. So now if we were to run an experiment forward, we're trying to decode the intention of someone who's paralyzed. We read out their neural data. And from that neural data, we get a 193 dimensional vector that has the number of spikes on the 192 neuros and our constant one.

And so we'll call this new neural data Y tilde. Okay. And all we do to get the kinematics is we take Y tilde K. And we multiply it. We pre-multiply it by the matrix L. And that will give me my decoded kinematics. And so in the 189 project code. That corresponded to this for loop right here, which starts off by iterating over new test trials, our test. For each new trial, we get out the neural data that we've never seen before. That's why test and we've been it. And then to get out my decoded positions, I do L times Y test. And so that gives me X test, which are my decoded positions. And then, and then I compare those decoded positions eventually, or I guess in the code first, we plot those decoded positions. That was this plot that we saw last time where the center out trials are pretty good, but the center back trials are not good. And then, and then we can compare those to the actual true positions that we recorded in the experiment to see how off we are in that led to our mean square error metric. Okay, so I just want to ask any questions on any of that since that's the key component of this project. All right. So now I want to talk about extensions of this least squares algorithm. So you'll recall all two meetings ago.

All right, everyone. For today, we have two announcements. The first is that homework number one is due today uploaded to Grayscope by 11.59 pm. And this morning, Tonwoye already uploaded homework number two onto CCLE. And homework number two will include a Jupyter notebook, and so it includes Python coding. Again, this is your first time doing Python coding. And you're, for example, transitioning from that laptop to Python, please just budget in some time for that. We have just a few things for the homework. Question one C was about patch clamp, and I had removed some slides on that in the interest of time. So patch clamp is a technique to measure the current through a single antenna. So this is the answer for that question one C. And then in question two G, you'll need to use a technique called least squares, which hopefully you have from a prerequisite class.

And here we're, we're reminding you of that solution. And then in lecture today, we're going to be talking about expectations. In the last question, the homework, you will also have conditional expectations. And so we've given that definition here, but we'll talk about distributions and give a probability refresher today in class. Any questions? All right, so we're going to get back to material. And so last lecture, we had finished the basic neuroscience by all of you part of the class. And now we're getting into actually modeling the action potentials that neurons fire. And we talked about how we might have a task where, for example, and we'll see this in this class, the monkey is controlling a tracer and moving it on a screen. And the spikes are going to be indicative of the movement that the monkey wants to make. And so we will be interested later on and how to interpret these patterns of spikes and use that to decode the movements that the monkey intends to make. And we had given a virtual lab tour last time showing some of the recording setup and whatnot.

All right. Any questions from last lecture in the setup? All right. So broadly, when we look at what spikes mean, they fall into two categories, which are neural encoding and decoding. So neurons, we know that they transmit information by the firing of a sequence of spikes. And it's the pattern at which they fire that encodes information. Remember, it's not the spike shape, but the actual pattern. And these patterns are used to represent all types of information. And so neurons, the visual cortex, they will be involved in coding or encoding. Stimuline the natural world like a light coming into your retina. All right. It can also be audio waves coming into your being processed by your cook. All right. So neural encoding is how we map these external stimuli from the world into a neural response.

We're going to talk about a few encoding models in this class, especially some tuning models. We'll see in later slides. But that'll be the extent to which we talk about neural encoding. And this class will be focused more on neural decoding, which is the map from the neural responses, like spikes, to the stimuli. And the stimuli in this case could be the motor action. So I want to decode how a monkey intends to move his or her arm so that the monkey can move a cursor on the screen or play pong like in that most recent neural link video. All right. So in this case, we attempt to reconstruct this stimulus or the motor action from the spike sequence. And this is what will spend more time extensively discussing during this class. So before that, we'll just talk a bit at a high level about neural encoding. And that's what these next slides will go through. Before I move on, any questions here?

All right. So neural encoding. So in neural encoding, what we want to do is you want to go from an external stimulus to a neural response. So you see, it can be a neural response. And the first thing that we're going to talk about is some of the difficulties in modeling this, which leads to some techniques we see later on. So again, when we do neural encoding, what we want to do is we want to record from a neuron and understand how it represents some external stimulus. And so let's say that the stimulus is audio. And so let's say that you're listening to an orchestra. And so the stimulus would be, for example, let's say you're at the very beginning of the concert and you hear the obo playing obo or violin playing the concert A, which is the 140 Hertz audio wave. All right.

And what you want to know is this audio wave goes into the air, last cochlea. And then eventually the air, the hair cells in the cochlea are going to send information about the sound wave being heard to neurons in the temporal lobe. Right. And so let's say that it sends it to a neuron here, which I just denote by a circle in the temporal lobe. All right. And so ideally we can just record from this neuron and then we would be able to treat a relationship between the frequency of sound heard, the stimuli and the neuron. All right. But it actually isn't that simple. And so one of the first challenges that we'll find is that spike sequences reflect both in intrinsic neural dynamics and temporal characteristics of the stimulus. So temporal characteristics of the stimulus, this just refers to how this stimulus is changing over time.

So let's say instead of just 144, 140 Hertz wave, you're listening to now orchestra music. That's a changing stimulus over time and that's going to be represented in this neuron. Right. So the second thing temporal characteristics of the stimulus, that's just again, relating how the stimulus affects the neurons firing rate. Right. Now what do we mean by intrinsic neural dynamics? What this means is that this neuron isn't in isolation. Rather it's participating in a neural circuit connected to other neurons. And so all these other neurons I'm going to draw also a circles and these all have connections to each other. All right. And then maybe also it turns out that the year cochlea has projections on to some other neurons as well. All right.

So now when I record from this one neuron here and I see that it's neural responses, spike chain changes, I need to understand if the change in this neuron or if this neuron spike train is reflecting just the external stimulus or if it's also reflecting internal dynamics meaning that all these other neurons are also inputs to this neuron and could also cause it to fire. All right. And in practice, of course, it's going to be some mixture of the two. All right. And so that complicates relating how we can map the external stimulus to this single neurons neural response. Right. Okay. Any questions there? All right. So that's the first challenge.

A second challenge is that features of the response could change on the time scale that's faster than the inter spike interval. Right. So first let's define the inter spike interval. The inter spike interval is merely the time in between two spikes and we're going to be talking about this a lot today when we get to post on processes. We know that because neurons have a refractory period that the time in between spikes has to be at least four milliseconds and realistically is often more than 10 to 15 milliseconds more than the absolute more than the relative refractory period. All right. So let's just say the absolute refractory period period of four milliseconds. If a spike fires every four milliseconds, then the most time that it can fire in one second is 250 times. And so we call that 250 hertz. Hertz means a rate.

And so it would be 250 spikes per second. This is spikes per second. So that's the fastest that a neuron can fire. But if I just consider, for example, we were talking about listening to an orchestra and an oboe playing a concert A, right? A concert A is already a 440 hertz signal, meaning that the time between consecutive peaks is around two milliseconds. All right. And so this signal is changing faster than a neuron can spike. Right. And so this poses additional challenges for how such fast changing signals would be represented in the neural system. Okay. Any questions on this challenge? All right.

Hi David. Hi Dr. Cao. How are you doing? How are you? I'm doing well. How are you? I'm doing well as well. I just woke up actually. Okay. I had a couple of questions regarding last homework.

Yes. Also, a question about the last assignment. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that.

I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. I'm not sure if you can answer that. questions regarding last homework? Yes. Also a question from lecture. All right, let me pull up the last homework. I ended up submitting last night but I had some conceptual things. Yes. All right, I have the homework up. Great. So this is in reference to the multi-ion species question.

So last time we asked about, I asked about equilibrium versus steady state, and so like you mentioned that equilibrium is a situation where it's like a single ion species that can come to a situation where no ion is really moving. There's no net movement, there's no movement, but like a steady state it's like no net movement. Am I interpreting that correctly? The key difference to the steady state is that in steady state and equilibrium, there is both no net current, but in steady state, you need to put energy into the system. gradients so that you can maintain these unequal drift and diffusion currents for each ion species so that the ion species concentration, the diffusion currents don't reach some steady state value. So still the steady state and equilibrium eventually do come to like a no net movement. Yeah, both of them result in no net current. No net current. Not necessarily. I see now. So you're saying that, okay, that makes sense because with your steady state, you can still have a chemical gradient as we see in the Constant voltage.

Yep, that's right. But to do that you needed to pump and the pump isn't there, and leak channels and sodium channels would still be present, I'm guessing. Yeah. Wouldn't the neuron just equalize or equiliberalize slightly higher? It wouldn't. There would be a...or... It depends. Let me pull up the slides, just so that I can have the diagram to point you to. All right. So Okay. So the reason that we that we still need the pump is that for the K plus channels these three on the left.

Overall K plus is still leaving the cell. And overall, and a plus is entering the cell. So, if the pump wasn't there, then it would reach. Then the diffusion currents in the blue. This would be getting smaller and smaller. Okay. So, So if you were to pump like for any plus ions out and three k plus ions in, then it would reach in a slightly different steady state, and the VM would be different because essentially this thing is saying what the steady state concentration of K plus my sense. Only the, the, the sodium potassium pump is pushing na plus that. And so if you didn't have that you would have a ton of influx of na plus in, and this overall system would reach some different equilibrium would be probably through leak channels, right? Because the sodium potassium voltage channels seem to be a bit more finicky.

It's like they require more force to get through. Yeah. So the channels are primarily K plus, and a few and a plus channels but primarily K plus. The thing is the the new equilibrium if you didn't have this sodium potassium pump would be likely a higher voltage. I'm not sure what the new equilibrium would be without the na plus k plus pump, but it would be a different concentrations and if it were at a higher voltage then, you know, if the nervous system didn't have this na plus k plus pump. I imagine there is an answer. Since, again, yeah, these blue arrows will be changing to reach the actual equilibrium, this blue arrow, either this blue or this orange arrow has to oppose the other one. Yeah. Or like the some of them eventually right. like there would be a constantly changing concentration.

And so that's why I think it's- For each channel. Yeah, for each channel. That's why without the NA plus K plus pump, I think it would just require a bit more thought to think through that hypothetical situation if it would be possible. Yeah. Yeah. But I guess that actually makes a lot more sense as to why it's important that the pump is there.

It's like, without it, it would take a while like without it, it's just natural processes. So it would be it's more efficient for the neuron to like, get there. So they can fire for the get ready to fire for the next round. Yeah, and then it'll interact with other things like if the if the at the equilibrium, if there is an equilibrium without the NA plus k plus pump, and it causes the equilibrium voltage to be for example higher than, then I guess like the the voltage gated ion channels that have evolved in a different way. Because, yeah, so so at least this is a system that we have now. how this, if this would reach a different equilibrium without this pump. Right, okay. And then to clarify about the pump too, the pump is constantly sort of working throughout the whole action potential, its effect is not really observable.

All right, everyone. We're going to get started. So for today, announcements are a reminder that homework number three is to this Friday. And last lecture and on CCLE, we announced that 20% of the homework three questions are going to, you're going to automatically receive full credit for them so you don't have to do them. So be sure to check out the CCLE announcement to make sure you know what those questions are. A heads up, the midterm is not this Monday, but the Monday after November 9, 2020, and it's going to be held during class time. If you're in another time zone and cannot take the midterm during this time, please be sure to have emailed me already. As we said in the beginning of class, all the past exams from prior quarters that I've taught this class are on CCLE. I want you to note that we usually hold the midterm in the week of the Veterans Day holiday and in former years that holiday occurred later on in the quarter.

And so the practice midterms, the last year's midterms, the past year's midterms on CCLE also included material on Fourier transforms, which you will not be tested on in this midterm. So this midterm will only cover material up to and including Fourier series. All right. And so we'll probably finish Fourier series on next lecture or maybe even today's lecture. And then with respect to the timing, we're going to send out an announcement on CCLE closer to the date about the timing of the exam. And when we expect you to finish an upload to grade scope by, right? Any questions on those core statistics? Okay. And then I want to take a poll on the pace of the class. So again, like I mentioned for the homework, this is a unique version of a class where we're teaching this solely online and over zoom. And so I wanted to launch this poll to get your feedback on how the pace of the class is going thus far.

And so I'm going to launch this poll and it's anonymous. Please answer as you feel truly. And then I'm going to ask the TA to send me the results of the poll since I cannot see them right now. All right. And then TA, can you just send me the poll over on the chat when I enough have filtered out? Let me know if I need to end the poll for you to see the values. Or can you see them right now? Yeah, we can see the right now. And 96 out of four, while either two students have answered. And 97, yeah, and keep coming the percentages, please. Okay. One percent of students say it's too slow. Three percent of students say that it's slow.

And 40 percent of students say that it's just a right. And 44 percent of students say it's too fast. And 11 percent of students say it's too fast. Okay. Yeah, I want to say that it's too slow. And 34 percent of students say it's slow. 40 percent of students say it's just a right. Okay. 44 percent say it's fast. And 11 percent say it's too fast. Okay. Thank you. Okay. And the poll now. All right.

So, thanks for this feedback. So it sounds like we're airing on the side of too fast for this class. I will take this into account when trying to explain this material. And again, feel free to stop me for questions. And if you have any suggestions for any particular feedback where we could help to get the pace right, feel free to let me know via email. All right. And thanks for the feedback again. Any questions on material or not material admin and announcements? All right. So we're going to continue where we left off last time. So at the end of last lecture, we were going over the intuition of the Fourier series and why we want it. And we talked about how the Fourier series, they have implications for LTI systems. But they also helped to extract different structure in the signal in particular.

The frequency structure, that is, when I see a signal that looks like this in the time domain on the left. And what kind of sinusoid at what frequencies are they composed of? And if I can see the structure, it can help to give me more intuition or more insight sometimes over what this signal is. And so we've given this example a few times that this is a C major chord in music. And while it looks very complex in the time domain and the frequency domain, it looks very simple and elegant. And so there could be additional information, additional insight, I mean, not information, additional insight from looking at a signal in the frequency domain. All right. So we also give the bottom line last lecture, which I'm just repeating here. We're going to derive all of these results today. And the bottom line is the following. If I have a signal FFT and it's well behaved, we talked about how last time the lecture by this we mean that it's. It doesn't have discontinuities.

There are a few other conditions, which are called the derelict conditions that are beyond the scope of this past. But if you're interested in what well behaved means, please look into that further. If we have a well behaved periodic signal FFT, then we can write its Fourier series as the following. FFT can be written as a sum of complex exponentials each the Jk omega not to. Recall that this is a cosine plus a sign an imaginary sign a J times a sign times a Ck. And really the magic is in what are these Ck's the Ck's tell me how much of each complex exponential I need to how much I need to weigh each complex exponential by so that when they sum together, they give me my original signal FFT. Right. And today we're going to derive what those Ck are. So again, that's a high level of you of Fourier series what the rival of these results. But again, the statement is I can write my function FFT as a sum of complex exponentials. Any questions here? All right. So we last left off lecture with this concept of the eigenfunction. And eigenfunctions we said are these functions where if I have a system a function or a signal is called an eigenfunction. If when I put this signal into the system, I get back out the same signal except that it's scaled by some value a. Right. And so in this case, a is a real number and here it amplified my complex exponential in general. And time why asked this is a clarifying plus and last time a can be complex.

And so it doesn't just have to be a real value, it can be a real plus and imaginary part. So a could equal some x plus j times y or equivalently some r times e to the j data. Right. So it can be a complex number. If you look at the complex number representation as a phaser, what this tells us is that it can get scaled up or down. That's r and then this e to the j data means that it can get phase shifted left or right. The signal can be shifted left or right. Right. And so again, high level. And eigenfunction is when I put that function into the system, I get that function out scaled by a. And now I made this claim at the end of last lecture and I told you we would prove it in this lecture. And this is the first result for LTI systems. The eigenfunction is for LTI systems complex exponentials are eigenfunctions of LTI systems. So if I take any complex exponential. And I put it into an LTI system that complex exponentials going to come on the outside scaled by scaled by some value. All right. Any questions on this concept? Okay. So we're going to go ahead and prove it now.

All right everyone, sorry for a bit of a wait start. Reminder that the project is due this Friday by 11.59pm emailed to my email address here at cowatces.ucla.edu. All right, I want to start off this class going over any questions about the project. I had a question of part three. Part three, yep. So for the suggestion of a decoding from differences, when I sort of had a wide-bend snooved set equal to the formula used left there, I had a matrix that was missing that was shorter by one column, so then the rest of the decoder wasn't working. So I wasn't sure it was sort of alterations to X spin or X test would be required. Yeah, that's a great question, Michael. If you do the, so if we just had data Y1, Y2, let's say we just had five data points and then corresponding kinematics. And Michael is saying that when he does the differences, right, you need two values to calculate a difference.

So his new values would be pairs Y2 minus Y1, Y3 minus Y2, Y4 minus Y3 and then Y5 minus Y4. And now we have five X values, but just four Y values. And so the question is then what alterations are what we make to the X's? I'm curious, has anyone run into this and have an idea or want to share what you did? And this is for the task three of me doing the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, one where the Zh That's a reasonable solution, which is Y2 minus Y1 uses data over a window. Let's say that the bin size was 10 milliseconds, so Y1 and Y2 use data over 0 to 10 milliseconds. And so maybe we use the x value at 5 milliseconds, which could be approximated by taking the average of x1, x2. Since the value at 5 milliseconds would be the average between x1 and x2, the average would be the linear midpoint between a line connecting x1 and x2. So that's one solution. The solution that I would do is I would just take x2, x3, x4, x5. And so I would chop off the first value. That is one other solution. You could use x1, x2, x3, x4.

In essence, this is something that I might think of as like a design choice. The reason that I would use x2, x3, x4, x5 is if I was under the rationale that I should use the most recent data. So if I need to calculate Y2 minus Y1, I've had kinematic data up to time bin 2. And I could say that this is the derivative that corresponds to the data time bin 2 therefore. So like this is the derivative at time 2. So there I'm defining that x prime of t, x prime of t would equal x of t minus x of t minus 1 over delta t. Where you don't need the delta t because the delta t is just a constant. So that's what I would do. But in truth, you could do any of these as long as you give a rationale for what you choose. I should say for this project, you don't have to give a rationale. But in other settings where you make a choice like this is good to make sure that you have a reasonable or a reasonable rationale for your design choice. Great. Any other project questions? Yeah, I think I think I did number three like this. Do we also have to do like the Y2 minus Y1 values for the Y test values? Yes, you do. Yeah, so this is related to a question. I think Brianna asked last week, which is when you make a change to your Y data in training, you want to make sure that that change is replicated everywhere and test as well. But because you want your test data to have the same statistics and distribution as your training data sense, you're expecting that your test data will look like what you change your decoder to do. So when you do these differences or when you do any smoothing, you want to make sure that you also do it to the Y data. Okay, so I think I think I did it right. Question mark. And the like I grafted and the resulting graph was like just a bunch of dots. Yeah, so I think that we said. Yeah, so I think I remember this from last year also. So I put on this question because I was at a conference one year and a researcher at another university told me that they had good results when they when they use the derivative to decode. But I believe that if you just use the derivative to decode, it performs poorly, although someone else can write in chat if you've done this to see if that's actually the case.

And I think that students in the past may have been able to get. Okay, or under the rationale that if I add more features, I can do no worse right at least in training data that we talked about seminar lecture. Some students will do like why then and they can animate D Y bin. And then see at that helps. And so the rationalist follows maybe D Y bin has very little information about the kinematics. And so when you decode just D Y bin on its own, it doesn't do well. But it might still contain information that when combined with your original neural data leads to a better decode. So there's a field of electrical engineering called information theory. And this is sometimes called synergistic information. So very interesting term that says some data may not be linearly related to the kinematics. So somehow if I incorporate this data with another observation, then it could be that the data is more formative and that's called synergistic information. Okay. Did that answer your question Jonathan? I think so. So should I or like am I doing something wrong or like so I'm not doing something wrong if it's just like a bunch of dots. Yeah, so if you got that D Y bin decoded very poorly. First off. And then you went into class right now if you just decode off of D Y bin and you see that it is poor or you've already done it and you've written your report saying I decoded D Y bin and it was really bad. I would give you full credit for for part three. That's sufficient.

And then my description here was to say if you were interested in trying to push it further. You try can tell me in D Y bin as an additional 192 features on top of your original Y bin and maybe that does better than Y bin alone. But answered Jonathan's question if you decoded off of D Y bin and you saw that it was awful and you. Then your conclusion could be if I just decode off of D Y bin, I don't have good performance. Okay, cool. Thank you. I also ran into the problem. So all I changed was the Y bin. I changed Y bin into D Y bin like the with the formula and also Y test into Y D Y test with the formula. And I ran the code and it was like it was seg faulting where I was supposed to calculate mean square error. Oh, not loud was seg faulting or or it was like it says like you access like too many you access out of the array of. I'm not sure if this is just like an answer if I could it the other part wrong the D Y's wrong, but yeah, yeah. Sorry, I'm just curious. Did not lab the matlab crash and you had to reopen it or did it just throw an error? No, it's just through an error is it okay? Okay. Yeah, I have had matlab seg fault in the past, but I found that it usually takes quite a bit to get it to site fault. And so I think that tells me the magnitude of the error. Yeah, so I think that. To debug this. So what I would do is I would go into your for loop and and put a break point so with the visual editor for matlab, you should be able to insert a break point. If not, you can also just insert a statement here that looks at a single trials, you could say like if I equals one meaning the first trial.

All right, everyone, we're going to get started for today. So just one announcement before we begin, which is that as per Tanwei's announcement on CCLE, we extended the homework number two due date to now be due a week from today. So it'll be due Monday, April 26, uploaded to Gradescope by 1159 PM. All right. Excuse me, any questions on any course logistics. Alright, so I want to remind you of where we left off last week which is that last week we started to look at plus on processes as a way to mathematically model. The spikes that neurons fire. Right. And so we're going to define the process more formally today. But what we considered was that there's a timeline on the timeline. They're going to characterize the so called inter spike interval, the time in between spikes and so little t one is the time from the process start to the first spike little t two is the time in between the first and the second spike little t three the time between the second spike in the third spider, etc.

Right. And we said that we would define for this Poisson process, we would define these inter spike intervals to be exponentially distributed with a rate lambda, and that all of these TIs were independently and identically distributed. Right. to define the exponential distribution. And so we set a random variable big T with rate lambda bigger than zero. And this rate lambda has units of spikes per second. This big T is exponentially distributed. If it's density function looks like the following. When T is less than zero, it's zero. And when T is greater than zero, it's this lambda E to the minus lambda T, right?

And it has this shape like in red, right. And then we also mentioned that it could be described by a cumulative distribution function, which is the integral of the probability density function. exponentially distributed random variable, then the probability that big T is greater than some time little t is going to be e to the minus lambda little t. All right, any questions on just any of the setup thus far? All right, hopefully relatively straightforward and you probably encountered the exponential distribution in your first probability class. All right, so last we left off then, we were on this slide where we were going to quantify some statistics of interest for the exponential distribution, in particular the mean and the variance. All right, so the expected value of the random variable t that is exponentially distributed we know follows this formula. And we didn't do this derivation since it's something that you've done in a prior class but we've put it in the online notes.

And so you can see the notes for the integration by parts from which you can derive that the expected value of the exponential distribution, expected value of T is equal to one over lambda. All right. And so we mentioned that, or right at the end we were saying, what does this expected value of T mean in words? And so this expected value of big T, right, big T is a random variable that for the exponential, for the Poisson process is going to be describing the distribution of these inter-spike intervals. And so the expected value of big T is gonna be the expected value of these inter-spike intervals, which means it's going to be the expected amount of time I need to wait before I see my next spike, all right?

And so in words, the expected value of my exponentially distributed random variable is the average time in between spikes, in between events, right? And if this is what our exponential distribution looks like, the expected value is going to be the mean value that my random variable big T is going to take on. It's going to take on low values with high probability and high values with low probability. And so the expected value is going to be somewhere in the middle.

All right. So whenever we see a result like this, it's also good to make sure things make sense. So the first thing that we could check are the units, right? So remember lambda was a rate with units of spikes per second, right? And so we would expect the expected time in between spikes have units of seconds, right? And so one over lambda will have units of seconds per spike, but it'll be the massive time in between spikes. All right, so the units make sense.

And another thing to make sure that makes sense is, what happens if I change lambda, all right? So this distribution in red is this exponential, is this function right here. We saw that at time t equals zero, right? F big T of t is just equal to lambda because I have lambda times e to the zero. So this value right here is lambda. Let's consider that lambda is big. Right so if lambda is big.

This means that my neuron fires many spikes per second, then I rightly should see that the expected the average time between spikes will decrease. If lambda is big my PDF right is going to go up to a larger value of lambda. This will be lambda big. Right. And then remember for the probability density function, the area under the curve has to equal one. So if for lambda being big is starts off higher, it better decay to zero more quickly. And so, if you look at this distribution here in blue, and you ask what's the average value of time, right, then let's say lambda was equal to this value. The distribution starts at a lower value, and the area under the curve is one and so when it exponentially decays it's going to have a more drawn out tail. And here is expected value would be larger.

Right. And that makes sense because if your neuron is firing less spikes per second, we would expect that the time in between spikes should extend. Any questions here or on any intuition? Alright, so the other quantity that would be of interest, that was the mean, would be the variance. And you should recall from your probability class that variance of a random variable is defined as follows. It's going to be the expected value of big T squared, which is called the second moment, minus the expected value of t minus expected value of t squared. And then this quantity here, if you wanted to compute it, it would be the integral from minus infinity to infinity of t squared f t of t dt.

of T squared, F T of T, DT. Whatever is in this expected value bracket is the term that multiplies the PDF in this integral. And we won't do the calculation, but if you do the integration by parts, which again is in the posted lecture notes, you'll get that this is equal to one over lambda squared. You can see that this answer ought to make intuitive sense to what is what this is telling you is that as lambda gets bigger, the variance of my spike times is going to get smaller and smaller. Right. You can see that in the blue, in the blue lambda is small we have a slow decaying PDF like shown here in green, and the spike times can take on values across a much larger range and therefore have greater variance.

Okay. Right. Everyone happy to start office hours, as always, if you want to speak about something that you don't want recorded just let me know and I'm happy to turn off the recording. Okay, Professor. Oh, so I actually had confusion about the, the one that you make the discrete in the class but I think my confusion mainly come from when you said like, it's not because of the law, total probability that we expand the party into the sun. Actually I wrote something. Can I share the screen. Yes, let me, let me make sure it's enabled. You should be able to share now. Okay yeah. Oh sorry, wait a second I think there is a problem on my side. Oh did you guys, can you guys see my screen? I cannot see it. Okay. You may have to hit the share button after you select the window. I made that mistake. Okay. Like, this is what you wrote on the lecture note right.

So, the concrete case is something like this right so TS more than SS minus one s minus two. So, you said is not because of law of probability, but in the class I interpreted it is because of the law of probability that we can write in term of the summation, like this. But, so yeah, any subtlety that I confused myself. Yeah, I'm not sure. I actually have in the past said that is a lot total probability but it's really just a subtlety and definition. The law of total probability takes a, a, and B, and sums over all cases of be to give just the event A happening. In this case, we have two events A and B. We aren't trying to just get the probability of one of the events, which would be the law of total probability, but we are doing something law of total probability ask, which is that we are essentially writing out all of the cases of Tn less than or equal to S out, which gives us a sum.

But, strictly speaking, we're not doing marginalization out of a random variable. We're still calculating the probability of two events here. We're not taking the probability of two events and then marginalizing out one of them. So that's why I said it's not the law of total probability. Okay. Okay. But it seems the calculation only makes sense in this kind of way. Yeah. Yeah. Yeah. So this is a reminiscent of the law of total probability. So we can transform this less than or equal to sign here into the integral over a density.

So the key thing is that tn can be less than or equal to s so we have to consider all possible values of big tn and and changing this tm less than or equal to s into the integral over tm equals t is the is the key insight there. Oh, I see. Yeah, but also I searched on the, I don't know, the Wikipedia, it said like the, there is a continued cases for the, I don't know, the law of probability. It's kind of similar to what in the end you take to the limit and that's something like that. So, yeah. Yeah. I guess I searched in the 10 minute break between the lecture and the office hours. Yeah, so in this case, the the multiplication Can you see my mouse on your screen. Sorry, see what can you see my mouse on your screen. I saw like you draw some like lion dash lion circle before. Okay, when I scroll the screen is gone. Got it. So, um, so I'm going to just annotate it. So this is still the law of total probability, because we are going from two variables, down to one. So, this is a lot of total probability, that's all good, because at this point we're kind of splitting hairs rigorously, it's not. Yeah. Okay, I see. But the intuition over of summing over all possible events that that is applying here.

Okay, thank you. Yeah, let me stop share. And then I saw David, yeah. I was hoping, and I don't need to bother other people, I was out of town, so I didn't see the tour of the lab, which I don't need to bother others. I can come back at another time when everybody's not asking questions. Yeah. Does anyone have any quick questions that we could go over right now?

Or are people fine going with the lab partner. I also realized, actually the monkeys only shown in the beginning of the lab tour so I'm going to record the rest of the lab, I'm going to pause during the monkey part, and I'm going to record the rest of the lab tour so that anyone else who who is curious can see it then. So let me do that again. And then we'll go on to other questions. So, let me just pull up the slides. I almost have it, I think, this keynote presentation. Great, okay, I have it. All right, so I'm just going to pause the recording for the first part of the slides. And then I will resume it after the monkey part is over. Great. So this is the lab tour.

Oh, I need to share the audio. Let me just share my screen one more time. But with the audio shared. audio share. So this is a lab tour from from my PhD advisors lab at Stanford, and it starts off by showing the inside of the lab space. And so, let me sorry let me make sure I did pause the recording. Okay. Yeah, so, I'm here, what you see is a monkey named George, and George is currently sitting in an experimental rig where we're recording from neural data. And he's doing a reaching task. And so, you'll see that he has essentially an implant here. This is dental acrylic that sits on top of his skull.

We do that so that we could fix his head during the experiments, because we want, because essentially the experiment has to be very precisely calibrated the monkey centers to be in a particular position and we also put a juice tube into his mouth so that they receive a juice reward. And the monkey doesn't feel discomfort during this. His head is just fixed into a single location. Neuralink released a video last week where they actually did everything freely and wirelessly. And they had already trained the monkey to hold onto a juice tube, which essentially is the monkey holding his head still in a single area. And so they removed this whole implant part, which is remarkable.

There's actually a couple of CNN and another source that did a commentary on the video over the video that I watched as well as the video. Yeah. Did you watch a Professor Paul Nijikian's by any chance. No, this is getting some traction so I can send that to. He was my colleague during our we did our PhDs together and he goes into a lot of details. If you could send us the link. Thank you. Yeah, let me get that link right now. So that's a Paul new chickens at deconstruction of this. We zoom in on the monkey and this is all the hardware that record from the electrode array and then that sends it out to computers later on to be processed.

0:00:00
All right, everyone. We're going to get started for today. A few announcements. First, homework number four is, do this Friday uploaded to GreatScope. And you'll notice that on the homework, we've designated several questions to be optional. And so those are our questions that can be additional practice for you if you so desire. We are going to be setting up an announcement tonight with several midterm logistics, including the timing of the midterm and Zoom link for where the TA's and I will be in case you have questions about the midterm. And so all of that information is going to go out tonight and an announcement. There are a few things I want to state here in class, which is, I guess, people in different times zone will see this in the video. But if you're taking the midterm at a different time, I know that last week I wrote to send me an email. It turns out I should have provided more detail because it's become a bit haphazard with the emails.

0:01:08
And so here, let me add a bit more structure. Even if you've already emailed me, what I'm saying is, if you cannot take the exam during class time because you're in a different time zone, or if I have approved you to take the exam during a different time already, please send me a new email with the following. The email subject is going to be EC 102 underscore MT underscore reschedge. And that makes sure that I can search everyone and have everyone included. And then in the email, I need you to tell me what time zone you are in. And then the time that you cannot take the exam in specific standard time. If you don't provide this information, then I'm going to assume that any other time other than class time is going to be fine for you. And please send me this information by Friday so that we can reschedule all of these other exams. All right. And then this will be in the announcement that we sent out tonight, which is that Tonmoi is going to hold the midterm review session on Sunday.

0:02:07
And there's going to be a poll that we send out tonight that let's you indicate what times are, what times are available. And then from there, Tonmoi will select the time most students can make. All right. And then I just wanted to chat. Someone asked if this is a standard or daylight or standard time now. So we just had our change of time this past weekend. Okay. Any class announcement, logistic questions? All right. Great. So we're going to get back into material. So last lecture, we spent quite some time talking about the intuition of Fourier series and then also deriving the results of the Fourier series, which is that F of T. If it's a periodic signal with some period big T zero.

0:03:05
Then I can write F of T as a sum of complex exponentials. And last lecture, we converted as a sum of complex exponentials each weighted by some coefficient C k. Right. And so last lecture, we derived what these values C k are and how to find them. And we use this very cool proof where we use this trick where if you integrate and a complex exponential over a period, it equals zero. Except in the case when K equals when the complex exponential has a value of K equals where we're out here. So please review that proof if any of the steps were unclear. All right. So last lecture, we derived how to find these coefficients C k such that when you multiply each of the J k omega and R T, our complex exponentials, they add together to reproduce your periodic signal F of T. All right. So then we ended class last week by going over an example of a square wave where we took the square wave signal right here and we calculated its Fourier series coefficients.

0:04:19
And we derived that C k was equal to one half sink of K over two. All right. And so these C k are going to be these coefficients C k for the square wave. And what this means is that for this square wave, if I wanted to write the square wave as the sum of complex exponentials, then I could write the square wave, F of T, as being equal to the sum from K equals minus infinity to infinity of C k e to the negative J, that's right, e to the J k omega not T. And here again, C k is one half sink K over two. And so this equation, sum from K equals minus infinity to infinity, where C k is one half sink of K over two times this complex exponential e to the J k omega not T. All right. This expression here is going to be the sum of complex exponentials such that when they sum together with these coefficients, one half sink K over two, they add up to the square wave. And so then I showed you some simulations. I'm going to paste this.

0:05:44
So we said, okay, what happens when we only allow one complex exponential frequency? What that means is that we're doing this sum here, except we're doing K equals minus one to K equals one. Right. And so what I would have here is a K equals zero term. Right. The zero term would just be a sink of zero is one and an e to the zero is one. So the C zero term would equal one half and that's this this blue line over here. So C zero equals one half is this blue line. And then I'm going to have a complex exponential with C one times an e to the J omega not T. And so these are complex exponentials with frequency omega not. And here those complex exponentials actually give me this red sinusoid. Right. And so we see if we add our blue line and our red sinusoid together, we approximate the square wave, but not perfectly.

0:07:00
We have overshoot here at the extremities and then we have under shoot along the edges of this square wave. Right. But now if I go ahead and I let you have three complex exponentials of frequencies. So now I have a sum from K equals minus three to plus three of our C ke to the J K omega not T. Now with these additional frequencies, I can start to approximate even better. And as to add more and more complex exponential frequencies all the way up until here I'm showing you 100. You can see the sum of all of these signs and cosines are going to give me a fairly good approximation of the square wave. All right. Okay. So I want to pause and say any questions here. I know we went over this rather quickly at the end of last lecture.

0:07:56
In practice, is there a certain amount of complex exponentials that would be, I guess, quote, unquote, good enough to approximate a certain signal? Because we could never get an infinite amount which would perfectly approximate or perfectly simulate what a signal is. But would there be like a kind of like a certain amount that would approximate it close enough? Yeah, that's a great question. I am not aware, although it may exist and I just may not know it. So if the TAs know, please chime in. I am not aware of a general formula that tells you if you use K complex exponentials, how little your error is going to be. Like me is going to depend on several properties of the signals. For example, discontinuities make it harder to approximate. And so in general, I believe that the answer is a signal dependent. And so I am not aware of any general equation that will tell you how many times you need.

0:09:05
Okay. That makes sense. Thank you. All right. The TAs chime in if you know, if you know more than I do here. All right. Okay. So that's the square wave. Well, what I want to now ask is a question, which is you'll notice that even when we had 100 complex exponential frequencies and my sign waves added together approximate the square wave pretty well, you'll notice that at these discontinuities, they're not perfect. And if you zoom in a bit, you can see that there's a bit of ring. It might be clearer when we have 10 frequencies, which is that at these discontinuities, it seems to overshoot and then ring a bit. And that ringing, if you were to zoom in on this plot, is still visible.

0:09:58
Right. And so I'm going to tell you an answer, which is that even if I were to make this an infinite number of complex exponentials, we would still have this overshoot. It would not perfectly go away. And so this is odd because we did approve where we said that as long as CFK is equal to this expression, that F of t is equal to the sum of the ck times this complex exponentials. And in math equals has a very particular meaning. It means that this is going to the left and the right hand side are going to equal to the same value at every single point in time t. And yet we see that at particular points in time t, in particular these discontinuities, F of t and my square wave and the right hand side, my Fourier series expansion, they are not equal to each other. So this is a hard question, but I want you to think for maybe 20 to 30 seconds about why this might be the case. Why is it that the Fourier series expansion doesn't equal F of t at every time point t, even if we prove that to be the case?

All right, we're going to get started for today. So a few announcements. The first is a reminder that homework number two is do on Monday in five days on April 26th, uploaded to Gradescope by 1159 PM. And then last lecture, someone had asked about the phantom factor of the Pricot G. It's a thousand cerebellum. And from this paper by Brits and Sour Bray and colleagues in their on 2015, they reported that during locomotion, the Pricot G cells actually have a very variable firing rate. And so their phantom factor on the y-axis is variance and on the x-axis is mean. And so the phantom factor was one, then the data points would all lie along the line, meaning that the mean and the variance are always equal. You can see the dots are generally above the line. Each of these dots is one neuron. And so neurons generally have, for Pricot G, neurons, higher variance and higher spike count mean.

And so their phantom factor is indeed greater than one. All right. Okay, any questions on any course with just 64 we get back into material? All right. So our last lecture we defined the Poisson process and we defined it as a process where we had events given by the big T's and the events have exponential into our rival times or ISIs. All right. And so those are these little T's. These are all independent and identically distributed and the distribution is an exponential distribution with parameter lambda. All right. And then we said the Poisson process is that or this expression here, NS, what it does is it accepts the time S and then returns a number of spikes that happened up into, up into including that time.

So if S was here, then the number of spikes would be one. If S is here, the number of spikes would be three. All right. And then last lecture, we spent the majority of lecture proving the main property of the Poisson process, which is that the number of spikes at time S. And so this means the number of spikes going from zero to all the way up until time S in that window. Big N of S is going to be a Poisson distributed random variable with mean lambda times S. Again, S is the enough of time that we're looking at for spikes. All right. And what that means is that I know the exact probability to distribution over the number of spikes I'm going to observe in my window from zero to time S. And so I know the exact probability of observing zero spikes, one spike, two spike, et cetera. And that's computed from this expression right here. Right. Any questions here?

Okay. So lastly left off, we were on property two of the Poisson process. And this was the so-called restart property. And so the property formally said that N of T plus S minus N of S, right, which is the number of spikes that occur in a window between time S and time T plus S over here. The number of spikes in this window is a Poisson process or this end of T plus S minus N of S follows a Poisson process. The number of spikes in this window is going to be a Poisson random variable with lambda times the length of this window, which is little T, right. And it's going to be independent of N of R where R is before S. So the number of spikes that happen in this window and the Poisson process in this window is going to be entirely independent of what has happened all along in the past. We say independence, right. We should be thinking something is independent. For two things are independent.

If knowing one of them does not give you any additional information about the other one, but you didn't already know, right. So when we're saying that the Poisson process from zero to N R is independent of the Poisson process between S and T plus S, what we're saying is that for example, knowing the number of spikes that happen in this window tells me nothing about the number of spikes that are going to happen in my window between S and T plus S. And then we mentioned that we would give the intuition over this property and you're not going to be responsible for the rigorous proof, although we have it on the next page and I'll go through it at a high level. But to show that starting from NS and looking forward to show that we have a Poisson process, right, all we have to do is show that our definition of the Poisson process holds and our definition of the Poisson process is that it's going to be a process where events happen at IID into arrival times or IID, ISIs, each of the ISIs being drawn from an exponential lambda distribution. A question from Andrew.

So, thinking about property to from a biological standpoint, I'm confused how this makes sense because I can't mirror on spikes be affected by some biological processes, how much, how many new trends you have to continue firing after IIDs or listening to a song. If you listen to a chord, if you feel like the first note of a chord, you're expected to hear the second or stuff like memories. If you've experienced something to pass, can't you mirror on fire differently, things like that. So, I guess, come up biological point of view, how the restart property makes sense. That's a great question, Andrew. Yes, so, the restart property in general won't be true for a biological system. The restart property, which is think of as just true for this attraction, which is the Poisson process. And even though it isn't something that will be absolutely true for the biological system, still the approximate, just like, for example, that are inter spike intervals are not exponential, it's one of the modeling things that is a consequence of defining these exponential inter spike intervals that allow us to do math looking at any inter bobble Poisson process.

There are several things about it that are, there are several things about the Poisson process that are non biological, and we'll start to address each of these later on, or we'll talk about some of them. For example, for this Poisson process, we're even assuming a constant rate lambda, right, that the firing rates have a constant rate lambda, but we know the firing rates change through time. And so, that's another place where we will have to make some updates to the model. And so, we'll talk about, for example, in homogeneous Poisson processes later today. But at face level, you're correct that in general, for a biological system, knowing about what's happened in the past could give you additional information about what's going to happen in the future. Okay. Thanks, Andrew. All right. Any other questions here?

All right. So, to show that we have a Poisson process starting at time S, and we have a Poisson process starting at time S, and looking forward, right, all we have to show is that all of the events that happen after time S are IID exponentially distributed, interspinently, happen with IID exponentially distributed interspinally intervals. And we're mostly good because between T4 and T5, we know that this is exponential lambda, between T5 and T6, we know that it's going to have an exponential inter-arrival time. Actually what we don't know is if this first interval over here is this exponential, is this ISI between time S when I restarted my Poisson process, and the next spike event is this ISI exponential with parameter lambda. Because if it is, then every single ISI looking forward is exponential parameter lambda, and that defines a Poisson process. Right? So at the end of last lecture, we gave the intuition for why this is true, which has to do with the memoryless property of the exponential distribution.

That is, the last boss or the last spike came at T3, and I started waiting for my spike, my next spike at time S, or my next plus at time S. The amount of time I have to wait for in the future, the distribution of that is going to be the same as if I had not waited at all. And therefore, this inter-spike interval ends up being exponential, and if this is exponential and everything else is exponential then the line, then looking forward we have the Poisson process. All right. Any questions on the intuition there? All right. So let me just go through this slide at a high level. So at least you can read this slide, and if you're interested in understanding this proof rigorously, then this slide will give you that information. So at the top here, we just have this fact that we've been using last lecture that for an exponentially distributed random variable, the probability that big T is less than little T is each the minus lambda T.

Okay. Hi, Amina. Hello, how are you? Sorry? I said, hello, how are you? Oh, I'm good. How are you doing? Thank you. I just had a couple questions on the homework. Great.

Let me pull it up. All right, I have it up. So I was going through problem 4C, and I had set up the integral in the same way that you did in office hours, but I was looking through Tanmoy's notes after, and I don't think I get the same response. So like, I understand how he went through it, because he did the, he used the equation of the expected, expected value with the summation, and then turns it into an integral but when I try to solve for that with the bounds of being one over lambda to infinity. I get to over lambda is the correct answer. Oh, okay, then I guess I'm misinterpreting. Yeah, I can. I haven't seen Tom boy slides. With, did you say that that was for 40.

For, for, for C I mean sorry. Yeah, it says it on his notes. Okay, let me pull that up really quickly so that I can see what he said. It's the notes from... 4-20 or 4-18? It's the notes from April 20th. Okay. Oh, that's an E, not a C. Oh, sorry. I confused... Oh, yeah. Oh, okay, okay. Okay, that makes sense to me. Great. Yeah. Okay. And so actually for for E.

For you we have to switch to the person process right for you sorry, what is the expected number of spikes that will be fired before one season, ISI, greater than the mean ISI. So, so this one actually requires some thinking outside the box beyond the process. I had it set up as like the expectation of an of s, given, t is less than one over lambda big T is less than one over lambda. Is that what I'm trying to solve for Okay. One piece of greater. So, I believe if you go that route. You.

possible. The expected, you're calculating the expected number of spikes, given that the ISI is less than 1 over lambda. Yes. I believe that is, I believe that's an interesting way to go and I think that, are you able to calculate that expectation? So I didn't go through with it because I didn't even know if I was set up correctly, but that was like my English sense of it. Yeah, I think that at least getting that conditional distribution, that conditional distribution challenging, because you would have a P of NFS, given, T is less than something. And however you split it up, you would, you would have to compute either a probability of an NFS given T or probability of a T given NFS with very conditional probability.

Let me, let me tell you a simpler way to set it up. So, essentially, what you can do is you can consider this as essentially a coin flip. So, you flip a coin and either the spike that comes will have an ISI less than the mean ISI. And so, what you want to know, so let's say that tails is tails is when you see an ISI less than the mean ISI. So you want to know how many tails are going to flip before you get ahead. Does that make sense. Okay, yes. And we know the probability of tails and heads because we calculated it in earlier part, I believe. Probability of T greater than one over lambda, we calculated to be one over E. Yes.

So basically we have coin flips that occur with probability, the tails occurs with probability 1 minus 1 over e and the heads occurs with probability 1 over e. And so if you keep flipping this coin you want to know the expected number of coin flips before you get to the heads. And so this is something that is modeled by what is called the geometric distribution. And so for the geometric distribution, let me just pull up the page here. So in a metric distribution. We are computing the number of coin flips before we get one success. And so, in this case, one minus p. We would have k minus one of them for the success to be on the kid coin flip so the probability that takes cave coin flip to get the success would be this probability. Okay, so resolving for K, which is one over the probability of a success. So it's one over one over e, which is d. Okay, I didn't know where that where his distribution was coming from, but now I realize this geometric.

So, yeah, that's right. Yeah. Okay. And then For 3D, the one where we're modeling the neurons by 50 spikes per second. I just sort of did a calculated the area under the curve for the refractory period, using exponential I just wanted to make sure that that was sort of the correct intuition. that the random variable is less than 0.001. And then I'm just a little confused. I think I'm still a little confused about the concept of combining Poisson processes for part five. So when we're saying the probability that no neurons are detected on either electrode, Would I set that up as like the probability that that n, n one, and the probability that n two equals zero, like it would be a multiplication between the two Poisson processes.

That's what we have in the solution. Let me just read the question really quickly to make sure that we stated that they're independent so that you can do that. Yep. So, in question five it says each neuron spikes independently according to a homogeneous Poisson process. So when you. And a they say what's the probability that no neurons are detected on either electrode. And so that would be a probability that parentheses. And one of t equals zero, and then because they're independent then the and can reduce to just multiplying those two together. Okay, okay, and I was a little confused, so when we set up the Poisson distribution, right? Yeah. So the S represents the number of spikes, that's the unit? No, N represents the number of spikes, and then S represents time, but lambda times S gives you a mean number of spikes.

All right, everyone. We're going to get started. So this is ECE C143A, C243A, Neurostic No Processing. I am your instructor, Jonathan Cowell, and on behalf of RTA's Tonmoie and Shashank and myself, we're excited to be your teaching staff for this quarter as we delve into topics related to neural signal processing. And today we're going to unpack a bit about what that means. We're just going to start off lecture by going over a few quick Zoom guidelines. So if you're taking a class with me before, you know that I'm very happy to answer questions and try to clarify things and that I encourage you to ask questions. If you have a question, very likely another student has a question and it gives me an opportunity to re-explain something or clarify that. And so please ask questions by raising your hand. You won't be able to unmute yourself on your own, but if you raise your hand, I will see that and then be able to unmute you so that you can ask your question. In addition to this, you can ask questions in the chat. So I will not be monitoring the chat, but RTA's Tonmoie and Shashank, they are going to be monitoring the chat continuously. And so if you write questions there, then they'll be able to answer questions in the chat. And then lastly, everything that we do for lecture in terms of everything we present here, as well as the annotated notes, we are going to be uploading those to CSELE. And so if you're not able to view a lecture live, which we do encourage, the lectures you can watch post talk on CSELE as well.

And so like I just mentioned, all lectures, as well as at least one discussion section, and oh, sorry, all lectures, discussions, and office hours will be carried out on Zoom. Lectures and at least one discussion is going to be recorded and uploaded to CSELE. We encourage you to attend live lectures. But like I mentioned, we're going to be recording the live lecture for the people who aren't able to make it to the lecture on time. And so because of that, if you are uncomfortable being recorded in a lecture, these lectures will be recorded for the sake of the entire class. And we ask that you off that of not being recorded by not attending a live lecture. But if that's not a concern for you, again, we encourage you to attend the live lectures. All right. And if there's someone who feels they learn better through live Zoom lectures by having your camera on, I know that I welcomed that. It's a lot better for me than just looking at black boxes with names. All right. And so feel free to do so if you prefer that. All right. And then like I mentioned, during live lectures, we're happy to answer questions. So to do that, you can raise your virtual hand in which case I will ask you to unmute. And then we'll have you ask the question, then I'll answer that. And then the other way is through the chat functionality. And so if you write your question to chat, then Tuan Moai and Tishank will be able to answer questions over the chat. All right. Before we just started, any questions on just any Zoom setup? Reveited things? All right, then. So I wanted to start off this class by giving you a sense of what this class is all about. Because when we hear a title like neural signal processing, maybe we may know what some of those words mean in isolation, but what's they mean when you put them all together? And what is going to be Tuan in this class? All right. So we'll go over through the motivation of this class and essentially the high-level areas that we'll cover. Some high-level thoughts is that first, this class is essentially the brain-meat engineering. So how can we, as people come from electrical engineering, computer science, develop techniques to try to understand how the brain does computation as well as interface with the brain to support human health? The other thing is that I know this class is in elected, and so although there's going to be a bunch of work in this class and we're going to go over this syllabus by the end of class, it's my sincere hope that this class is also fine. And so we'll talk again about what we're going to do in this class, but in this class we'll be playing with neural data actually recorded from the brain and building algorithms to decode that neural data. All right.

And lastly, as I said before, please feel free to ask questions and we'll do our best to answer those. All right. So I thought about what the motivation for this class would be, and there's actually been some recent news that you may know of that provides a pretty good motivation for this class. And so last year a company called Neuralink, which is founded by Elon Musk, held essentially a conference of sorts where they demonstrated some new technology that they are developing to try to interface with the brain. And we'll talk about some of that technology throughout the class. It speaks to the fact that what was previously something that was more in the research domain and the domain of academics is starting to be something embraced by industry, where perhaps next generation of industry is looking towards how do we interface with the brain through new technologies? And how do we use that to both understand the brain and engineer with the brain, build devices that can be either driven by your brain or right into your brain. All right. And so a lot of this builds off of some subtle work beginning in the mid-2000s. And we're going to try to address this through, unpack this through answering this question, what is Neuralink's processing? So again, I mentioned to you earlier, you may know what some of these words mean in isolation. Neural refers to neurons, which are the basic computing elements in the brain, and which we'll talk about. And the first two and a half weeks of this class discuss the basic workings of neurons. Signal processing, you all likely know from one of the first EEClaces E102, where you did a Fourier, and you were able to learn techniques that essentially take signals and extract out features and information from the signals that are relevant to solving a particular application problem. All right. So I have shown here three images. The first is an image of actual neurons in the brain. And what we showed here on this long device here is an electrode. The same type of electrode that you would use to measure the voltage between two points on a circuit. And so here this electrode gets very close to a particular neuron in the brain. And we're going to learn in this class that neurons in the brain actually communicate via electrical principles. And so we'll talk about how neurons actually shuttle ions from outside to inside their cell, allowing the voltage across the neuron to change and how that encodes information. Because the signals transmitted by neurons are fundamentally electrical, we can transduce them into signals that then we can operate on via circuits. And so here we're showing a circuit board that can perform operations on neural signal voltages that we measure from the brain. And then taking those transduced voltages, we can apply them to solve engineering problems.

And so here I'm showing the cover of a seminal paper from 2006 in nature from the Hawkberg group at Massachusetts General Hospital and Brown University, where they had a paralyzed participant named Matt Nagel here. And what they did is they inserted these electrodes into his brain. And although Matt Nagel was paralyzed from the neck down after he suffered a paralysis in a knife injury, they were able to read signals from his brain emitted by these neurons and use that to drive and move artificial curses on the computer screen, essentially giving him control of a computer cursor. And so we'll talk about in this class some of the algorithms that are developed in the service of that. All right. So getting back to this question, what is neural signal processing? So we're going to take a bit of a tour of history right now and then we'll get back to this question. But for millennia, right, and people have sought to understand what gives rise to our ability to perceive the role that around us, to reason about it, and then to act. And these are qualities that make us uniquely human. And so for perception, this refers to the sensory inputs that we regularly receive throughout the day. One example, which I give now because it's something that we may experience later on in this lecture, is when we watch a video, like a video on YouTube, right? So when we watch a video on YouTube, we have visual information, which are the actual images within the video being streamed into our eyes. We also take in auditory information.

All right, everyone. We're going to get started for today. So a few announcements before we begin. I sent out a CCLE announcements with instructions on how to sign up for Piazza and grade scope. And so those are just recapituated here as well as a link to a Python tutorial. If you have only used MATLAB in the past, this tutorial will probably be helpful for you. And then we have also uploaded to CCLE material reading for this class, the lecture notes, and then we also uploaded all the midterms and final exams dating back to 2017. If you're someone who isn't sure if the pre, if you satisfied the prerequisite material, I encourage you to take a look at the midterms and final exams as well as the common filter dot pdf which are probably the most technically demanding for this class. Any questions on any announcement related matters? Any course logistics?

All right, so excuse me. We're going to go ahead and finish off the syllabus. So last time we talked about the grade breakdown for this class and that it was graded on an absolute scale. And I know I went through it a bit quickly at the end, so I just want to put up this slide to ask if there are any questions here on the grade breakdown for the grading scale. All right. So then we had also put up this information about pass no pass or satisfactory on satisfactory grading if you choose to take the class in that manner. We also talked about how for exams during remote learning, the exam are going to be open note open book and you can access your notes and CCLE on your computer, but it's going to be closed internet. And that the TAs and I we are going to perform some analysis on the answers given. And if we suspect anyone collaborating, which are not allowed to do for the exams, we request reserve the right to give a superseding oral exam. Right. And then also if you are in a different time zone, we can make accommodations for you to take the exam at a different time.

And so please send me an email this week if you plan on taking us off on this. We will we use that just to get a sense of how many accommodations we'll need to make and then we're going to send out more details closer to the exam date to handle those accommodations. Any any questions here? All right. I have to slide on academic integrity, which I give in all my classes. And it's my way of saying up front that I care a lot about academic integrity and that we all follow the principles of academic integrity and fairness and respect to our fellow classmates. And so I put up the slide to say that I take this very seriously. And if we catch any students of cheating or violating the principles of academic integrity, that I take that seriously. And I will follow up and report those cases to the dean of the students office. And we will follow their recommendation as they investigate the case and do what they do what they determine us to do. All right. So I just want to put that up front that again. If we if we catch you violating academic integrity, we will follow up on it and report their case to the dean of students. All right. Any questions here?

All right. So with that other course information throughout this class, we're going to cover a wide range of topics, including some intrusion neuroscience, which we'll do for these first two and a half weeks. And then after that, we're going to cover topics in modeling spikes. That's going to be drawn from this theoretical neuroscience textbook, as well as topics from machine learning and statistical signal processing, which we take from this Chris Bishop textbook. And because we didn't want to have students need to to purchase all three books, what we did was we took the excerpts of the chapters that we used for these books and we put those on CCLE. Right. So that material should all be on CCLE. Other notes for this class. So the last two notes that we use in class, I just asked that they not be publicly posted due to matters related to copyright. And so we'll be happy to distribute them in the annotated notes on CCLE. But we just asked that they remain within the class population. Like we said, last time a piazza should be used for almost all major class discussions. And of course, these office hours to get any other questions answered.

But we hope that the piazza form will be lively. And like I mentioned last time, we give bonuses based off of participation on piazza. And so even though you can consider anonymous to classmates, your posting is not anonymous to us. And we make that setting so that we can assign bonuses based off of how much you participate on piazza. And the TAs will be checking piazza also regularly to make sure that any questions that couldn't be answered by students can be answered by teaching staff. If you have a question that isn't appropriate for piazza and it's related to class material, I just asked that you email, Tonmoys, Shashank and I and me together. We do this so that no single TA gets overloaded because in prior classes, sometimes once TA is very responsive and then all the students learn to email that TA all the time and that TA becomes overburdened. And so we just asked that you send us any class related questions to all three of the teaching staff if you don't post it on piazza. And then of course, if you have any personal matters, please email me directly there. Any questions here? All right. Some last notes. I think we mentioned this last time.

It's time consuming to make the transition from that lab to Python. So please factor that into your work. I think it's worth it because Python is the de facto standard for machine learning and signal processing today. I also mentioned this last time this class according to student feedback is a lot of work. And so I want to state this upfront so that you can plan accordingly. We do try to have this class be very fun, but we cover a bunch of topics and test it understanding. So we have seven homework assignments and we're told that those assignments are somewhat time consuming. All right. Like the mentioned piazza should be the primary names of asking and getting questions answered. And we just talked about this on the last slide also so I won't review that again. Any questions here? All right. So with that. Sorry. It looks like I have that was not last notes. I have a few more slides. For those coming with a background in machine learning. I want to say that up front since we start off doing neuroscience and we do derive maximum likelihood solutions for classifiers and regression in this class, which is something that may have seen before.

You may feel the pace of the class is somewhat slow. And then we're going to keep the pace because a lot of people in the class don't have background in machine learning. And I want to make sure that everyone can learn and master the material. And so if you're coming in with the prior background in machine learning, just be aware that the class may feel a bit slower for you. And you can factor that into your decision as you decide what class is to take. And then I like we talked last time. I think we're going to take halfway through lecture because the lectures are fairly long here at UCLA. And in our discussion sections, which we're going to be run live by Tonloi and Shashank. First off, you can attend whatever discussion section you would like. And the TAs will also be recording at least one of the discussion sections to put it on CCLA. And then we have already sent out the P.O.T.S.ing page to send up information so this full of points should not be here. All right, a lot of logistics I just went through any questions on anything that I went through. Okay, so I just want to do brief introductions. So about me and sorry. This is not updated well. I did update the other parts of the slide, but I didn't update this introduction.

This is not my first year at UCLA. This is now my fourth year. And it's my seventh time teaching this class. And so I, I've taught it at Stanford twice with my PhD advisor there and previously taught at UCLA four times. This is a class related very closely to the research that my research group does. And so we care a lot about improving the performance of brain machine interfaces. And then other classes that teach at UCLA include ECU 102 in the fall quarter, so that signals and systems. And then in the winter quarter, I teach a class on neural networks and deep learning. And so with that, I'm going to pass it off to one of our TAs, Tom, why so Tom, why if you can go ahead and unmute yourself. Yeah. Hi, so hello everyone. So my name is Tom, I'm on tour. I will be one of your keys for this quarter. So this is my third time, Tying for this course on neural signal processing and my fifth time, Tying for Jonathan. All of my Tying experience at UCLA has been for Jonathan. I really like Tying for Jonathan and especially for this course. So this course is one of my favorite courses here at UCLA. I took this course first when it was offered for the first time and back in 2017. And I really enjoyed the course material. I also hope that you also enjoyed this course too. So I am a PhD student at UCLA working with Professor Rajudhary and my research in interest, primary lies in reinforcement learning and pattern, exception and dynamic networks. So that's all about me and I hope to have a very enjoyable quarter with all of you. Thank you very much. Great. Thank you, Tom Boyd. And then we have our second TA, Shishank, who won't be able to give an introduction today. We'll ask him to give an introduction on Monday. But this will be Shishank's first time Tying this class here at UCLA and he's of course taken this class before and done, absolutely. And so we'll leave Shishank's introduction for the start of next class on Monday.

